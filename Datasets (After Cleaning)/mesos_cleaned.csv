assignee.name,created,creator.name,description,issuetype.name,priority.name,reporter.name,resolutiondate,status.id,status.name,status.statusCategory.name,summary,updated,key,storypoints,project,sprint
js84,2016-05-06T20:44:49.000+0000,js84,"The LocalAuthorizer is supposed to use the OS `user` under which tasks are running for authorization.
As the master keeps track of running and completed processes we need access to this information in Task in order to authorize such tasks.",Improvement,Major,js84,,10006,Reviewable,New,Add `user` to `Task` protobuf message.,2016-05-06T20:45:44.000+0000,MESOS-5338,1.0,mesos,
js84,2016-05-06T20:39:26.000+0000,js84,"As the fine-grained filtering of endpoints can the rather expensive, we should create a master flag to enable/disable this feature.",Improvement,Major,js84,,10006,Reviewable,New,Add Master Flag to enable fine-grained filtering of HTTP endpoints.,2016-05-07T07:59:26.000+0000,MESOS-5337,1.0,mesos,
,2016-05-06T10:01:05.000+0000,adam-mesos,"We already authorize which http users can set/remove quota for particular roles, but even knowing of the existence of these roles (let alone their quotas) may be sensitive information. We should add authz around GET operations on /quota.",Improvement,Major,adam-mesos,,1,Open,New,Add authorization to GET /quota,2016-05-06T10:04:53.000+0000,MESOS-5336,3.0,mesos,
gradywang,2016-05-06T09:52:56.000+0000,adam-mesos,"We already authorize which http users can update weights for particular roles, but even knowing of the existence of these roles (let alone their weights) may be sensitive information. We should add authz around GET operations on /weights.

Easy option: GET_ENDPOINT_WITH_PATH /weights
- Pro: No new verb
- Con: All or nothing

Complex option: GET_WEIGHTS_WITH_ROLE
- Pro: Filters contents based on roles the user is authorized to see
- Con: More authorize calls (one per role in each /weights request)",Improvement,Major,adam-mesos,,1,Open,New,Add authorization to GET /weights,2016-05-06T10:45:55.000+0000,MESOS-5335,3.0,mesos,
,2016-05-02T19:32:16.000+0000,greggomann,"After the agent's {{/containers}} endpoint is authenticated, we should enabled authorization as well.",Improvement,Major,greggomann,,1,Open,New,Authorize the agent's '/containers' endpoint,2016-05-06T10:14:08.000+0000,MESOS-5317,2.0,mesos,
a10gupta,2016-05-02T19:30:32.000+0000,greggomann,The {{/containers}} endpoint was recently added to the agent. Authentication should be enabled on this endpoint.,Improvement,Major,greggomann,,3,In Progress,In Progress,Authenticate the agent's '/containers' endpoint,2016-05-06T09:57:46.000+0000,MESOS-5316,2.0,mesos,
gyliu,2016-05-01T05:56:05.000+0000,gyliu,"{code}
root@mesos002:~/test# curl -d jsonMessageBody -X POST http://192.168.56.12:5050/quota
Failed to parse set quota request JSON 'jsonMessageBody': syntax error at line 1 near: jsonMessageBodyroot@mesos002:~/test# cat jsonMessageBody
{
	""role"": ""role1"",
	""guarantee"": [{
		""name"": ""cpus"",
		""type"": ""SCALAR"",
		""scalar"": {
			""value"": 1
		}
	}, {
		""name"": ""mem"",
		""type"": ""SCALAR"",
		""scalar"": {
			""value"": 128
		}
	}]
}
root@mesos002:~/test# curl -d weight.json -X PUT http://192.168.56.12:5050/weights
Failed to parse update weights request JSON ('weight.json'): syntax error at line 1 near: weight.js
root@mesos002:~/test# cat weight.json
    [
      {
        ""role"": ""role1"",
        ""weight"": 2.0
      },
      {
        ""role"": ""role2"",
        ""weight"": 3.5
      }
    ]
{code}

The right command should be adding {{@}} before the quota json file {{jsonMessageBody}}.",Documentation,Minor,gyliu,,10006,Reviewable,New,Failed to set quota and update weight according to document,2016-05-02T22:36:57.000+0000,MESOS-5313,1.0,mesos,
lins05,2016-04-29T23:01:11.000+0000,jieyu,"This is in the context of Mesos containerizer (a.k.a., unified containerizer).

I did a simple test:
{noformat}
sudo sbin/mesos-master --work_dir=/tmp/mesos/master
sudo GLOG_v=1 sbin/mesos-slave --master=10.0.2.15:5050 --isolation=docker/runtime,filesystem/linux --work_dir=/tmp/mesos/slave/ --image_providers=docker --executor_environment_variables=""{}""
sudo bin/mesos-execute --master=10.0.2.15:5050 --name=test --docker_image=alpine --command=""env"" 

MESOS_EXECUTOR_ID=test
SHLVL=1
MESOS_CHECKPOINT=0
MESOS_EXECUTOR_SHUTDOWN_GRACE_PERIOD=5secs
LIBPROCESS_PORT=0
MESOS_AGENT_ENDPOINT=10.0.2.15:5051
MESOS_SANDBOX=/tmp/mesos/slave/slaves/2d7e44bb-3282-4193-bdc4-eeab9e0943c2-S0/frameworks/1a1cad18-2d87-43dd-97b6-1dbf2d229061-0000/executors/test/runs/bb8dd72c-fb4c-426a-be18-51b0621339f6
MESOS_NATIVE_JAVA_LIBRARY=/home/vagrant/dist/mesos/lib/libmesos-0.29.0.so
MESOS_FRAMEWORK_ID=1a1cad18-2d87-43dd-97b6-1dbf2d229061-0000
MESOS_SLAVE_ID=2d7e44bb-3282-4193-bdc4-eeab9e0943c2-S0
MESOS_NATIVE_LIBRARY=/home/vagrant/dist/mesos/lib/libmesos-0.29.0.so
MESOS_DIRECTORY=/tmp/mesos/slave/slaves/2d7e44bb-3282-4193-bdc4-eeab9e0943c2-S0/frameworks/1a1cad18-2d87-43dd-97b6-1dbf2d229061-0000/executors/test/runs/bb8dd72c-fb4c-426a-be18-51b0621339f6
PWD=/mnt/mesos/sandbox
MESOS_SLAVE_PID=slave(1)@10.0.2.15:5051
{noformat}

`MESOS_SANDBOX` above should be `/mnt/mesos/sandbox`.",Bug,Major,jieyu,2016-05-03T05:05:19.000+0000,5,Resolved,Complete,Env `MESOS_SANDBOX` is not set properly for command tasks that changes rootfs.,2016-05-03T05:05:19.000+0000,MESOS-5312,2.0,mesos,Mesosphere Sprint 34
qianzhang,2016-04-29T19:28:59.000+0000,avinash@mesosphere.io,"Currently the `network/cni` isolator can only load the CNI configs at startup. This makes the CNI networks immutable. From an operational standpoint this can make deployments painful for operators. 

To make CNI more flexible the `network/cni` isolator should be able to load configs at run time. 

The proposal is to add an endpoint to the `network/cni` isolator, to which when the operator sends a PUT request the `network/cni` isolator will reload  CNI configs. ",Task,Major,avinash@mesosphere.io,,10006,Reviewable,New,Enable `network/cni` isolator to load CNI config at runtime. ,2016-05-04T15:12:40.000+0000,MESOS-5310,1.0,mesos,
jieyu,2016-04-29T01:18:29.000+0000,jieyu,"Currently, if a container uses container image, we'll do a bind mount of its sandbox (<sandbox> -> <rootfs>/mnt/mesos/sandbox) in the host mount namespace.

However, doing the mounts in the host mount table is not ideal. That complicates both the cleanup path and the recovery path.

Instead, we can do the sandbox bind mount in the container's mount namespace so that cleanup and recovery will be greatly simplified. We can setup mount propagation properly so that persistent volumes mounted at <sandbox>/xxx can be propagated into the container.

Here is a simple proof of concept:

Console 1:
{noformat}
vagrant@vagrant-ubuntu-trusty-64:~/tmp/mesos$ ll .
total 12
drwxrwxr-x 3 vagrant vagrant 4096 Apr 25 16:05 ./
drwxrwxr-x 6 vagrant vagrant 4096 Apr 25 23:17 ../
drwxrwxr-x 5 vagrant vagrant 4096 Apr 25 23:17 slave/
vagrant@vagrant-ubuntu-trusty-64:~/tmp/mesos$ ll slave/
total 20
drwxrwxr-x  5 vagrant vagrant 4096 Apr 25 23:17 ./
drwxrwxr-x  3 vagrant vagrant 4096 Apr 25 16:05 ../
drwxrwxr-x  6 vagrant vagrant 4096 Apr 26 21:06 directory/
drwxr-xr-x 12 vagrant vagrant 4096 Apr 25 23:20 rootfs/
drwxrwxr-x  2 vagrant vagrant 4096 Apr 25 16:09 volume/
vagrant@vagrant-ubuntu-trusty-64:~/tmp/mesos$ sudo mount --bind slave/ slave/                                                                                                                                                                                                                            
vagrant@vagrant-ubuntu-trusty-64:~/tmp/mesos$ sudo mount --make-shared slave/
vagrant@vagrant-ubuntu-trusty-64:~/tmp/mesos$ cat /proc/self/mountinfo 
50 22 8:1 /home/vagrant/tmp/mesos/slave /home/vagrant/tmp/mesos/slave rw,relatime shared:1 - ext4 /dev/disk/by-uuid/baf292e5-0bb6-4e58-8a71-5b912e0f09b6 rw,data=ordered
{noformat}

Console 2:
{noformat}
vagrant@vagrant-ubuntu-trusty-64:~/tmp/mesos$ cd slave/
vagrant@vagrant-ubuntu-trusty-64:~/tmp/mesos/slave$ sudo unshare -m /bin/bash
root@vagrant-ubuntu-trusty-64:~/tmp/mesos/slave# sudo mount --make-rslave .
root@vagrant-ubuntu-trusty-64:~/tmp/mesos/slave# cat /proc/self/mountinfo
124 63 8:1 /home/vagrant/tmp/mesos/slave /home/vagrant/tmp/mesos/slave rw,relatime master:1 - ext4 /dev/disk/by-uuid/baf292e5-0bb6-4e58-8a71-5b912e0f09b6 rw,data=ordered
root@vagrant-ubuntu-trusty-64:~/tmp/mesos/slave# mount --rbind directory/ rootfs/mnt/mesos/sandbox/                                                                                                                                                                                        
root@vagrant-ubuntu-trusty-64:~/tmp/mesos/slave# mount --rbind rootfs/ rootfs/
root@vagrant-ubuntu-trusty-64:~/tmp/mesos/slave# mount -t proc proc rootfs/proc                                                                                                                                                                                                            
root@vagrant-ubuntu-trusty-64:~/tmp/mesos/slave# pivot_root rootfs rootfs/tmp/.rootfs                                                                                                                                                                                                      
root@vagrant-ubuntu-trusty-64:~/tmp/mesos/slave# cd /
root@vagrant-ubuntu-trusty-64:/# cat /proc/self/mountinfo
126 61 8:1 /home/vagrant/tmp/mesos/slave/rootfs / rw,relatime master:1 - ext4 /dev/disk/by-uuid/baf292e5-0bb6-4e58-8a71-5b912e0f09b6 rw,data=ordered
127 126 8:1 /home/vagrant/tmp/mesos/slave/directory /mnt/mesos/sandbox rw,relatime master:1 - ext4 /dev/disk/by-uuid/baf292e5-0bb6-4e58-8a71-5b912e0f09b6 rw,data=ordered
128 126 0:3 / /proc rw,relatime - proc proc rw
{noformat}

Console 1:
{noformat}
agrant@vagrant-ubuntu-trusty-64:~/tmp/mesos$ cd slave/
vagrant@vagrant-ubuntu-trusty-64:~/tmp/mesos/slave$ sudo mount --bind volume/ directory/v1
vagrant@vagrant-ubuntu-trusty-64:~/tmp/mesos/slave$ cat /proc/self/mountinfo
50 22 8:1 /home/vagrant/tmp/mesos/slave /home/vagrant/tmp/mesos/slave rw,relatime shared:1 - ext4 /dev/disk/by-uuid/baf292e5-0bb6-4e58-8a71-5b912e0f09b6 rw,data=ordered
129 50 8:1 /home/vagrant/tmp/mesos/slave/volume /home/vagrant/tmp/mesos/slave/directory/v1 rw,relatime shared:1 - ext4 /dev/disk/by-uuid/baf292e5-0bb6-4e58-8a71-5b912e0f09b6 rw,data=ordered
{noformat}

Console 2:
{noformat}
root@vagrant-ubuntu-trusty-64:/# cat /proc/self/mountinfo
126 61 8:1 /home/vagrant/tmp/mesos/slave/rootfs / rw,relatime master:1 - ext4 /dev/disk/by-uuid/baf292e5-0bb6-4e58-8a71-5b912e0f09b6 rw,data=ordered
127 126 8:1 /home/vagrant/tmp/mesos/slave/directory /mnt/mesos/sandbox rw,relatime master:1 - ext4 /dev/disk/by-uuid/baf292e5-0bb6-4e58-8a71-5b912e0f09b6 rw,data=ordered
128 126 0:3 / /proc rw,relatime - proc proc rw
132 127 8:1 /home/vagrant/tmp/mesos/slave/volume /mnt/mesos/sandbox/v1 rw,relatime shared:4 master:1 - ext4 /dev/disk/by-uuid/baf292e5-0bb6-4e58-8a71-5b912e0f09b6 rw,data=ordered
{noformat}",Improvement,Major,jieyu,2016-05-02T16:53:01.000+0000,5,Resolved,Complete,Sandbox mounts should not be in the host mount namespace.,2016-05-02T16:53:39.000+0000,MESOS-5307,5.0,mesos,Mesosphere Sprint 34
kaysoky,2016-04-28T22:07:10.000+0000,js84,"After https://github.com/apache/mesos/commit/066fc4bd0df6690a5e1a929d3836e307c1e22586
the help for the /metrics/snapshot endpoint on the agent doesn't appear anymore (Master endpoint help is unchanged).",Bug,Major,js84,,10006,Reviewable,New,/metrics/snapshot endpoint help disappeared on agent.,2016-04-29T14:57:07.000+0000,MESOS-5304,1.0,mesos,Mesosphere Sprint 34
jojy,2016-04-28T21:53:20.000+0000,jojy,Add support for `user` and `capabilities` to execute cli. This will help in testing the `capabilities` feature for unified containerizer.,Bug,Major,jojy,,10006,Reviewable,New,Add capabilities support for mesos execute cli.,2016-04-28T21:54:48.000+0000,MESOS-5303,3.0,mesos,Mesosphere Sprint 34
anandmazumdar,2016-04-28T19:04:44.000+0000,anandmazumdar,"Currently, all the business logic for HTTP based command executor/driver based command executor lives in 2 different files. As more features are added/bugs are discovered in the executor itself, they need to be fixed in two places. It would be nice to have some kind of a shim/adapter that abstracts away the underlying library details from the executor. Hence, the executor can toggle between whether it wants to use the driver or the new API via an environment variable.",Improvement,Major,anandmazumdar,,1,Open,New,Consider adding an Executor Shim/Adapter for the new/old API,2016-04-28T19:04:44.000+0000,MESOS-5302,5.0,mesos,
,2016-04-28T16:29:17.000+0000,anandmazumdar,"Currently, we do a best effort validation for all calls sent to the master from the scheduler by invoking {{validation::scheduler::call::validate(call, principal)}}. This is a generic validation helper for all calls. However, for more coarse grained validation for a particular call, we invoke the validation as part of the call handle itself.

{code}
Option<Error> validationError = roles::validate(frameworkInfo.role());
{code}

This in turn makes all validations asynchronous i.e. the framework gets them as {{Event::ERROR}} events later. It would be good if such validations can be handled while processing the {{Call}} message itself synchronously.",Improvement,Major,anandmazumdar,,10020,Accepted,In Progress,Add synchronous validation for all types of Calls.,2016-05-03T22:27:10.000+0000,MESOS-5301,5.0,mesos,
nfnt,2016-04-28T07:51:45.000+0000,nfnt,"Coarse HTTP endpoint authorization using the {{GET_ENDPOINT_WITH_PATH}} ACL rule needs to be added to the ""/flags"" endpoint of the master.",Task,Major,nfnt,,10006,Reviewable,New,"Add authorization to the master's ""/flags"" endpoint",2016-04-28T17:27:47.000+0000,MESOS-5297,3.0,mesos,Mesosphere Sprint 34
jvanremoortere,2016-04-28T00:50:57.000+0000,jvanremoortere,"The protobufs for the V1 api regarding inverse offers initially re-used the existing offer / rescind / accept / decline messages for regular offers.
We should split these out the be more explicit, and provide the ability to augment the messages with particulars to either resource or inverse offers.",Improvement,Major,jvanremoortere,,10020,Accepted,In Progress,Split Resource and Inverse offer protobufs for V1 API,2016-04-28T00:51:05.000+0000,MESOS-5296,5.0,mesos,Mesosphere Sprint 34
greggomann,2016-04-26T14:31:33.000+0000,greggomann,"Now that the libprocess-level HTTP endpoints have had authentication added to them in MESOS-4902, we can add authorization to them as well. As a first step, we can implement a ""coarse-grained"" approach, in which a principal is granted or denied access to a given endpoint. We will likely need to register an authorizer with libprocess.",Improvement,Major,greggomann,,10006,Reviewable,New,Add authorization to libprocess HTTP endpoints,2016-05-06T21:21:45.000+0000,MESOS-5286,5.0,mesos,Mesosphere Sprint 34
jojy,2016-04-25T18:05:31.000+0000,jojy,"Add capabilities support for unified containerizer. 

Requirements:
1. Use the mesos capabilities API.
2. Frameworks be able to add capability requests for containers.
3. Agents be able to add maximum allowed capabilities for all containers launched.

Design document: https://docs.google.com/document/d/1YiTift8TQla2vq3upQr7K-riQ_pQ-FKOCOsysQJROGc/edit#heading=h.rgfwelqrskmd
",Task,Major,jojy,,10006,Reviewable,New,Add capabilities support for unified containerizer.,2016-04-28T21:54:29.000+0000,MESOS-5275,5.0,mesos,Mesosphere Sprint 34
js84,2016-04-25T08:49:38.000+0000,nfnt,We should add information about authentication to the help message and thereby endpoint documentation (similarly as MESOS-4934 has done for authentication).,Improvement,Major,nfnt,2016-05-02T18:04:21.000+0000,5,Resolved,Complete,Need support for Authorization information via HELP.,2016-05-02T18:04:21.000+0000,MESOS-5273,3.0,mesos,Mesosphere Sprint 34
gilbert,2016-04-25T06:25:59.000+0000,gilbert,"Docker image labels should be supported in unified containerizer, which can be used for applying custom metadata. Image labels are necessary for mesos features to support docker in unified containerizer (e.g., for mesos GPU device isolator).",Task,Major,gilbert,,1,Open,New,Support docker image labels.,2016-04-30T16:55:58.000+0000,MESOS-5272,3.0,mesos,Mesosphere Sprint 34
vinodkone,2016-04-25T05:41:28.000+0000,vinodkone,"Currently there is no support for a flag to have an alias. Such support would be useful to rename/deprecate a flag.

For example, for MESOS-4386, we could let the flag have `--authenticate` name and a `--authenticate_frameworks` alias. The alias can be marked as deprecated (need to add support for this as well).

This support will also be useful for slave/agent flag rename. See MESOS-3781 for details.
",Improvement,Major,vinodkone,,10006,Reviewable,New,Add alias support for Flags,2016-04-28T18:21:52.000+0000,MESOS-5271,5.0,mesos,Mesosphere Sprint 33
janisz,2016-04-24T20:21:27.000+0000,janisz,"When compile on ARM, it will through error.
The current code logic in src/linux/fs.cpp is:

{code}
#ifdef __NR_pivot_root
  int ret = ::syscall(__NR_pivot_root, newRoot.c_str(), putOld.c_str());
#elif __x86_64__
  // A workaround for systems that have an old glib but have a new
  // kernel. The magic number '155' is the syscall number for
  // 'pivot_root' on the x86_64 architecture, see
  // arch/x86/syscalls/syscall_64.tbl
  int ret = ::syscall(155, newRoot.c_str(), putOld.c_str());
#elif __powerpc__ || __ppc__ || __powerpc64__ || __ppc64__
  // A workaround for powerpc. The magic number '203' is the syscall
  // number for 'pivot_root' on the powerpc architecture, see
  // https://w3challs.com/syscalls/?arch=powerpc_64
  int ret = ::syscall(203, newRoot.c_str(), putOld.c_str());
#else
#error ""pivot_root is not available""
#endif
{code}

Possible sollution is to add `unistd.h` header",Bug,Major,janisz,2016-04-26T19:23:29.000+0000,5,Resolved,Complete,pivot_root is not available on ARM,2016-05-06T20:45:19.000+0000,MESOS-5263,1.0,mesos,Mesosphere Sprint 32
kaysoky,2016-04-23T00:16:46.000+0000,kaysoky,"After [MESOS-5259], the {{mesos-fetcher}} will no longer need to be a separate binary and can be safely folded back into the agent process.  (It was a separate binary because libcurl has synchronous/blocking calls.)  

This will likely mean:
* A change to the {{fetch}} continuation chain:
  https://github.com/apache/mesos/blob/653eca74f1080f5f55cd5092423506163e65d402/src/slave/containerizer/fetcher.cpp#L315
* This protobuf can be deprecated (or just removed):
  https://github.com/apache/mesos/blob/653eca74f1080f5f55cd5092423506163e65d402/include/mesos/fetcher/fetcher.proto",Task,Major,kaysoky,,10020,Accepted,In Progress,Combine the internal::slave::Fetcher class and mesos-fetcher binary,2016-05-02T17:52:53.000+0000,MESOS-5261,3.0,mesos,
kaysoky,2016-04-22T23:42:08.000+0000,kaysoky,"In order to replace the {{mesos-fetcher}} binary with the {{uri::Fetcher}}, each plugin must be able to determine/estimate the size of a download.  This is used by the Fetcher cache when it creates cache entries and such.

The logic for each of the four {{Fetcher::Plugin}}s can be taken and refactored from the existing fetcher.
https://github.com/apache/mesos/blob/653eca74f1080f5f55cd5092423506163e65d402/src/slave/containerizer/fetcher.cpp#L267",Task,Major,kaysoky,,10020,Accepted,In Progress,"Extend the uri::Fetcher::Plugin interface to include a ""fetchSize""",2016-05-02T17:53:13.000+0000,MESOS-5260,2.0,mesos,
kaysoky,2016-04-22T23:27:48.000+0000,kaysoky,"This is an intermediate step for combining the {{mesos-fetcher}} binary and {{uri::Fetcher}}.  

The {{download}} method should be replaced with {{uri::Fetcher::fetch}}.
https://github.com/apache/mesos/blob/653eca74f1080f5f55cd5092423506163e65d402/src/launcher/fetcher.cpp#L179

Combining the two will:
* Attach the {{uri::Fetcher}} to the existing Fetcher caching logic.
* Remove some code duplication for downloading URIs.",Task,Major,kaysoky,,10020,Accepted,In Progress,Refactor the mesos-fetcher binary to use the uri::Fetcher as a backend,2016-05-02T17:52:45.000+0000,MESOS-5259,3.0,mesos,
,2016-04-22T22:37:17.000+0000,klueska,"The Nvidia GPU isolator has an external dependence on `libnvidia-ml.so`. As it currently stands, this forces *all* binaries that link with `libmesos.so` to also link with `libnvidia-ml.so` (including master, agents on machines without GPUs, scheduler, exectors, etc.).

By turning the Nvidia GPU isolator into a module, it will be loaded at runtime only when an agent has explicitly including the the Nvidia GPU isolator in its `--isolation` flag.",Task,Major,klueska,,1,Open,New,Turn the Nvidia GPU isolator into a module,2016-04-22T22:37:17.000+0000,MESOS-5258,3.0,mesos,
,2016-04-22T21:58:09.000+0000,klueska,"Right now, the only way to enumerate the available GPUs on an agent is to use the `--nvidia_gpu_devices` flag and explicitly list them out.  Instead, we should leverage NVML to autodiscover the GPUs that are available and only use this flag as a way to explicitly list out the GPUs you want to make available in order to restrict access to some of them.",Task,Major,klueska,,1,Open,New,Add autodiscovery for GPU resources,2016-04-22T22:00:43.000+0000,MESOS-5257,3.0,mesos,
klueska,2016-04-22T21:53:42.000+0000,klueska,"Currently the top level containerizer includes a static function for enumerating the resources available on a given agent. Ideally, this functionality should be the responsibility of individual containerizers (and specifically the responsibility of each isolator used to control access to those resources).

Adding support for this will involve making the `Containerizer::resources()` function virtual instead of static and then implementing it on a per-containerizer basis.  We should consider providing a default to make this easier in cases where there is only really one good way of enumerating a given set of resources.",Task,Major,klueska,,1,Open,New,Add support for per-containerizer resource enumeration,2016-04-22T21:59:30.000+0000,MESOS-5256,3.0,mesos,
,2016-04-22T21:38:07.000+0000,klueska,"Currently the usage callback in the Nvidia GPU isolator is unimplemented:

{noformat}
src/slave/containerizer/mesos/isolators/cgroups/devices/gpus/nvidia.cpp
{noformat}

It should use functionality from NVML to gather the current GPU usage and add it to a ResourceStatistics object. It is still an open question as to exactly what information we want to expose here (power, memory consumption, current load, etc.). Whatever we decide on should be standard across different GPU types, different GPU vendors, etc.",Task,Major,klueska,,1,Open,New,Add GPUs to container resource consumption metrics.,2016-04-22T21:42:48.000+0000,MESOS-5255,3.0,mesos,
kaysoky,2016-04-22T19:17:45.000+0000,kaysoky,"The {{uri::Fetcher}} theoretically supports all URIs, per [RFC3986|http://tools.ietf.org/html/rfc3986].  To do this, we need a spec-compliant parser from string to URI.

[uriparser|http://uriparser.sourceforge.net/] appears to fit the bill.",Task,Major,kaysoky,,10006,Reviewable,New,Add URI parsing function/library,2016-04-23T00:49:49.000+0000,MESOS-5254,2.0,mesos,
gilbert,2016-04-22T17:58:00.000+0000,gilbert,"If the mesos containerizer destroys a container in PROVISIONING state, isolator cleanup is still called, which is incorrect because there is no isolator prepared yet. 

In this case, there no need to clean up any isolator, call provisioner destroy directly.",Bug,Major,gilbert,2016-04-22T22:26:32.000+0000,5,Resolved,Complete,Isolator cleanup should not be invoked if they are not prepared yet.,2016-04-22T22:26:40.000+0000,MESOS-5253,2.0,mesos,Mesosphere Sprint 33
,2016-04-21T20:46:18.000+0000,greggomann,The {{/system/stats.json}} endpoint was deprecated by MESOS-2058. This endpoint can now be removed.,Task,Major,greggomann,,1,Open,New,Remove '/system/stats.json' endpoint,2016-04-21T20:46:33.000+0000,MESOS-5243,1.0,mesos,
alexr,2016-04-21T13:57:50.000+0000,alexr,"In command executor, {{escalated()}} may be scheduled before the task has been killed, i.e. {{reaped()}}, but called after. In this case {{escalated()}} should be a no-op.",Bug,Minor,alexr,,10006,Reviewable,New,Command executor may escalate after the task is reaped.,2016-04-21T14:07:17.000+0000,MESOS-5240,1.0,mesos,
gilbert,2016-04-21T00:26:09.000+0000,neilc,"Observed on the Mesosphere internal CI:

{noformat}
[22:56:28]W:     [Step 10/10] F0420 22:56:28.056788   629 containerizer.cpp:1634] Check failed: containers_.contains(containerId)
{noformat}

Complete test log will be attached as a file.",Bug,Major,neilc,2016-04-26T01:03:57.000+0000,5,Resolved,Complete,CHECK failure in AppcProvisionerIntegrationTest.ROOT_SimpleLinuxImageTest,2016-04-26T01:03:57.000+0000,MESOS-5238,2.0,mesos,Mesosphere Sprint 33
mcypark,2016-04-19T23:31:48.000+0000,mcypark,"The POSIX version of {{os::access}} looks like this:

{code}
inline Try<bool> access(const std::string& path, int how)
{
  if (::access(path.c_str(), how) < 0) {
    if (errno == EACCES) {
      return false;
    } else {
      return ErrnoError();
    }
  }
  return true;
}
{code}

Compare this to the Windows version of {{os::access}} which looks like this following:

{code}
inline Try<bool> access(const std::string& fileName, int how)
{
  if (::_access(fileName.c_str(), how) != 0) {
    return ErrnoError(""access: Could not access path '"" + fileName + ""'"");
  }

  return true;
}
{code}

As we can see, the case where {{errno}} is set to {{EACCES}} is handled differently between the 2 functions.

We can actually consolidate the 2 functions by simply using the POSIX version. The challenge is that on POSIX, we should use {{::access}} and {{_::access}} on Windows. Note however, that this problem is already solved, as we have an implementation of {{::access}} for Windows in {{3rdparty/libprocess/3rdparty/stout/include/stout/windows.hpp}} which simply defers to {{::_access}}.

Thus, I propose to simply consolidate the 2 implementations.",Bug,Major,mcypark,,10006,Reviewable,New,The windows version of `os::access` has differing behavior than the POSIX version.,2016-04-19T23:35:32.000+0000,MESOS-5237,2.0,mesos,Mesosphere Sprint 33
jojy,2016-04-19T06:25:08.000+0000,jojy,"To enable support for capability as first class framework entity, we need to add capabilities related information to the ContainerInfo protobuf.",Task,Major,jojy,,10006,Reviewable,New,Add capability information to ContainerInfo protobuf message.,2016-04-27T16:13:21.000+0000,MESOS-5232,1.0,mesos,Mesosphere Sprint 33
jojy,2016-04-18T22:05:03.000+0000,jojy,Add basic tests for the capability API.,Task,Major,jojy,,10006,Reviewable,New,Add tests for Capability API.,2016-04-27T16:13:20.000+0000,MESOS-5228,3.0,mesos,Mesosphere Sprint 33
yongtang,2016-04-18T19:03:19.000+0000,vinodkone,Similar to what we did with the HTTP command executor in MESOS-3558 we should have a HTTP docker executor that can speak the v1 Executor API.,Bug,Major,vinodkone,,10006,Reviewable,New,Implement HTTP Docker Executor that uses the Executor Library,2016-04-25T14:34:09.000+0000,MESOS-5227,5.0,mesos,
,2016-04-14T22:46:04.000+0000,anandmazumdar,It would be good to add a benchmark for scale testing the HTTP frameworks wrt driver based frameworks. The benchmark can be as simple as trying to launch N tasks (parameterized) with the old/new API. We can then focus on fixing performance issues that we find as a result of this exercise.,Task,Major,anandmazumdar,,10020,Accepted,In Progress,Create a benchmark for scale testing HTTP frameworks,2016-05-07T18:40:24.000+0000,MESOS-5222,3.0,mesos,
klueska,2016-04-14T20:32:14.000+0000,klueska,https://reviews.apache.org/r/46220/,Documentation,Minor,klueska,,10006,Reviewable,New,Add Documentation for Nvidia GPU support,2016-04-27T16:13:20.000+0000,MESOS-5221,5.0,mesos,Mesosphere Sprint 33
,2016-04-13T22:05:01.000+0000,greggomann,There are a couple issues related to the {{principal}} field in {{DiskInfo}} and {{ReservationInfo}} (see linked JIRAs) that should be better documented. We need to help users understand the purpose of these fields and how they interact with the principal provided in the HTTP authentication header. See linked tickets for background.,Documentation,Major,greggomann,,1,Open,New,Update the documentation for '/reserve' and '/create-volumes',2016-04-26T03:42:19.000+0000,MESOS-5215,1.0,mesos,
anandmazumdar,2016-04-13T21:48:01.000+0000,greggomann,"If a framework authenticates and then does not provide a {{principal}} in its {{FrameworkInfo}}, we currently allow this and leave {{FrameworkInfo.principal}} unset. Instead, we should populate {{FrameworkInfo.principal}} for them automatically in that case to ensure that the two principals are equal.",Improvement,Major,greggomann,2016-04-15T21:02:38.000+0000,5,Resolved,Complete,Populate FrameworkInfo.principal for authenticated frameworks,2016-04-15T21:02:38.000+0000,MESOS-5214,2.0,mesos,Mesosphere Sprint 33
,2016-04-13T21:39:16.000+0000,greggomann,"Mesos currently provides no way for operators to include their principal with HTTP endpoint requests when HTTP authentication is disabled. To remedy this, we should add optional {{principal}} fields to the relevant protobuf messages. When HTTP authentication is enabled, we can allow the user to leave this field empty and populate it with the principal from their HTTP Auth header.",Improvement,Major,greggomann,,1,Open,New,Operator endpoints should accept a principal without HTTP authentication,2016-04-13T21:39:16.000+0000,MESOS-5213,3.0,mesos,
greggomann,2016-04-13T21:30:37.000+0000,greggomann,"Mesos currently provides no way for operators to pass their principal to HTTP endpoints when HTTP authentication is off. Since we enforce that {{ReservationInfo.principal}} be equal to the operator principal in requests to {{/reserve}}, this means that when HTTP authentication is disabled, the {{ReservationInfo.principal}} field cannot be set.

To address this in the short-term, we should allow {{ReservationInfo.principal}} to hold any value when HTTP authentication is disabled.",Improvement,Major,greggomann,,10006,Reviewable,New,Allow any principal in ReservationInfo when HTTP authentication is off,2016-04-26T03:18:01.000+0000,MESOS-5212,1.0,mesos,
gyliu,2016-04-12T23:29:12.000+0000,gyliu,"{code}
root@mesos002:~/src/mesos/m2/mesos/build# src/mesos-execute --master=192.168.56.12:5050 --name=test --docker_image=ubuntu:14.04 --command=""ls /root""
I0413 07:28:03.833521  2295 scheduler.cpp:175] Version: 0.29.0
Subscribed with ID '3a1af11e-cf66-4ce2-826d-48b332977999-0001'
Submitted task 'test' to agent '3a1af11e-cf66-4ce2-826d-48b332977999-S0'
Received status update TASK_RUNNING for task 'test'
  source: SOURCE_EXECUTOR
  reason: REASON_COMMAND_EXECUTOR_FAILED <<< 
Received status update TASK_FINISHED for task 'test'
  message: 'Command exited with status 0'
  source: SOURCE_EXECUTOR
  reason: REASON_COMMAND_EXECUTOR_FAILED <<<
root@mesos002:~/src/mesos/m2/mesos/build#
{code}",Bug,Minor,gyliu,2016-04-13T10:37:39.000+0000,5,Resolved,Complete,The mesos-execute prints confusing message when launching tasks.,2016-04-13T10:37:39.000+0000,MESOS-5199,1.0,mesos,Mesosphere Sprint 33
anandmazumdar,2016-04-12T00:16:58.000+0000,kaysoky,"When a scheduler registers, the master will create a link from master to scheduler.  If this link breaks, the master will consider the scheduler {{inactive}} and mark it as {{disconnected}}.

This causes a couple problems:
1) Master does not send offers to {{inactive}} schedulers.  But these schedulers might consider themselves ""registered"" in a one-way network partition scenario.
2) Any calls from the {{inactive}} scheduler is still accepted, which leaves the scheduler in a starved, but semi-functional state.

See the related issue for more context: MESOS-5180

There should be an additional guard for registered, but {{inactive}} schedulers here:
https://github.com/apache/mesos/blob/94f4f4ebb7d491ec6da1473b619600332981dd8e/src/master/master.cpp#L1977

The HTTP API already does this:
https://github.com/apache/mesos/blob/94f4f4ebb7d491ec6da1473b619600332981dd8e/src/master/http.cpp#L459

Since the scheduler driver cannot return a 403, it may be necessary to return a {{Event::ERROR}} and force the scheduler to abort.",Bug,Major,kaysoky,2016-04-28T18:40:12.000+0000,5,Resolved,Complete,Master should reject calls from the scheduler driver if the scheduler is not connected.,2016-04-28T18:40:12.000+0000,MESOS-5181,1.0,mesos,Mesosphere Sprint 34
,2016-04-12T00:09:55.000+0000,kaysoky,"The existing implementation of the scheduler driver does not re-register with the master under some network partition cases.

When a scheduler registers with the master:
1) master links to the framework
2) framework links to the master

It is possible for either of these links to break *without* the master changing.  (Currently, the scheduler driver will only re-register if the master changes).

If both links break or if just link (1) breaks, the master views the framework as {{inactive}} and {{disconnected}}.  This means the framework will not receive any more events (such as offers) from the master until it re-registers.  There is currently no way for the scheduler to detect a one-way link breakage.

if link (2) breaks, it makes (almost) no difference to the scheduler.  The scheduler usually uses the link to send messages to the master, but libprocess will create another socket if the persistent one is not available.

To fix link breakages for (1+2) and (2), the scheduler driver should implement a `::exited` event handler for the master's {{pid}} and trigger a master (re-)detection upon a disconnection. This in turn should make the driver (re)-register with the master. The scheduler library already does this: https://github.com/apache/mesos/blob/master/src/scheduler/scheduler.cpp#L395

See the related issue MESOS-5181 for link (1) breakage.",Bug,Major,kaysoky,,10020,Accepted,In Progress,Scheduler driver does not detect disconnection with master and reregister.,2016-04-28T15:57:26.000+0000,MESOS-5180,3.0,mesos,
gyliu,2016-04-11T23:25:06.000+0000,gyliu,Enhance the error message for  https://github.com/apache/mesos/blob/4dfa91fc21f80204f5125b2e2f35c489f8fb41d8/3rdparty/libprocess/3rdparty/stout/include/stout/duration.hpp#L70 to list all of the supported duration unit.,Improvement,Minor,gyliu,,10006,Reviewable,New,Enhance the error message for Duration flag.,2016-04-12T14:48:11.000+0000,MESOS-5179,3.0,mesos,
klueska,2016-04-11T22:06:49.000+0000,klueska,"We should not put this logic directly into the  'Resources::validate()' function.
The primary reason is that the existing 'Resources::validate()' function doesn't consider the semantics of any particular resource when performing its validation (it only makes sure that the fields in the 'Resource' protobuf message are correctly formed). Since a fractional 'gpus' resources is actually well-formed (and only semantically incorrect), we should push this validation logic up into the master.
    
Moreover, the existing logic to construct a 'Resources' object from a 'RepeatedPtrField<Resource>' silently drops any resources that don't pass 'Resources::validate()'. This means that if we were to push the non-fractional 'gpus' validation into 'Resources::validate()', the 'gpus' resources would just be silently dropped rather than causing a TASK_ERROR in the master. This is obviously *not* the desired behaviour.",Task,Major,klueska,2016-04-12T00:48:38.000+0000,5,Resolved,Complete,Add logic to validate for non-fractional GPU requests in the master,2016-04-12T00:48:38.000+0000,MESOS-5178,2.0,mesos,Mesosphere Sprint 33
kaysoky,2016-04-11T18:22:03.000+0000,kaysoky,"There are a couple of problems with the balloon framework that prevent it from being deployed (easily) on an actual cluster:

* The framework accepts 100% of memory in an offer.  This means the expected behavior (finish or OOM) is dependent on the offer size.
* The framework assumes the {{balloon-executor}} binary is available on each agent.  This is generally only true in the build environment or in single-agent test environments.
* The framework does not specify CPUs with the executor.  This is required by many isolators.
* The executor's {{TASK_FINISHED}} logic path was untested and is flaky.
* The framework has no metrics.
* The framework only launches a single task and then exits.  With this behavior, we can't have useful metrics.
",Improvement,Major,kaysoky,,10006,Reviewable,New,Update the balloon-framework to run on test clusters,2016-04-27T17:58:41.000+0000,MESOS-5174,3.0,mesos,Mesosphere Sprint 33
karya,2016-04-11T16:27:56.000+0000,karya,"When loading multiple modules into master/agent, one has to merge all module metadata (library name, module name, parameters, etc.) into a single json file which is then passed on to the --modules flag. This quickly becomes cumbersome especially if the modules are coming from different vendors/developers.

An alternate would be to allow multiple invocations of --modules flag that can then be passed on to the module manager. That way, each flag corresponds to just one module library and modules from that library.

Another approach is to create a new flag (e.g., --modules-dir) that contains a path to a directory that would contain multiple json files. One can think of it as an analogous to systemd units. The operator that drops a new file into this directory and the file would automatically be picked up by the master/agent module manager. Further, the naming scheme can also be inherited to prefix the filename with an ""NN_"" to signify oad order.",Task,Major,karya,,1,Open,New,Allow master/agent to take multiple --modules flags,2016-04-27T16:13:17.000+0000,MESOS-5173,3.0,mesos,Mesosphere Sprint 33
gilbert,2016-04-11T16:23:58.000+0000,gilbert,"When the registry puller is pulling a private repository from some private registry (e.g., quay.io), errors may occur when fetching blobs, at which point fetching the manifest of the repo is finished correctly. The error message is `Unexpected HTTP response '400 Bad Request' when trying to download the blob`. This may arise from the logic of fetching blobs, or incorrect format of uri when requesting blobs.",Bug,Major,gilbert,,1,Open,New,Registry puller cannot fetch blobs correctly from some private repos.,2016-04-30T16:44:53.000+0000,MESOS-5172,3.0,mesos,Mesosphere Sprint 33
karya,2016-04-11T16:16:57.000+0000,karya,"We want the Modules to be able to use replicated log along with the APIs to communicate with Zookeeper. This change would require us to expose at least the following headers state/storage.hpp, and any additional files that state.hpp depends on (e.g., zookeeper/authentication.hpp).",Task,Major,karya,2016-04-18T12:31:13.000+0000,5,Resolved,Complete,Expose state/state.hpp to public headers,2016-04-18T12:31:13.000+0000,MESOS-5171,3.0,mesos,Mesosphere Sprint 33
js84,2016-04-11T16:08:10.000+0000,js84,For authorization based endpoint filtering we need to adapt the json endpoint creation as discussed in MESOS-4931.,Improvement,Major,js84,,10006,Reviewable,New,Adapt json creation for authorization based endpoint filtering.,2016-05-06T20:41:16.000+0000,MESOS-5170,5.0,mesos,
js84,2016-04-11T16:05:29.000+0000,js84,For authorization based endpoint filtering we need to introduce the authorizer actions outlined via MESOS-4932.,Improvement,Major,js84,,10006,Reviewable,New,Introduce new Authorizer Actions for Authorized based filtering of endpoints.,2016-05-06T20:37:21.000+0000,MESOS-5169,3.0,mesos,Mesosphere Sprint 33
js84,2016-04-11T16:02:37.000+0000,js84,"When adding authorization based filtering as outlined in MESOS-4931 we need to be careful especially for performance critical endpoints such as /state.

We should ensure via a benchmark that performance does not degreade below an acceptable state.",Improvement,Major,js84,,10006,Reviewable,New,Benchmark overhead of authorization based filtering.,2016-04-27T16:13:19.000+0000,MESOS-5168,3.0,mesos,Mesosphere Sprint 33
qianzhang,2016-04-11T13:01:46.000+0000,qianzhang,We need to add tests to verify the functionality of `network/cni` isolator.,Task,Major,qianzhang,,10006,Reviewable,New,Add tests for `network/cni` isolator,2016-05-04T23:30:12.000+0000,MESOS-5167,5.0,mesos,Mesosphere Sprint 33
bbannier,2016-04-11T07:46:32.000+0000,adam-mesos,"Operators may want to enforce that only specific authorized users be able to view per-executor resource usage statistics. For 0.29 MVP, we can make this coarse-grained, and assume that only the operator or a operator-privileged monitoring service will be accessing the endpoint.
For a future release, we can consider fine-grained authz that filters statistics like we plan to do for /tasks.",Task,Major,adam-mesos,2016-04-27T16:26:55.000+0000,5,Resolved,Complete,Add authorization to agent's /monitor/statistics endpoint.,2016-04-27T16:26:55.000+0000,MESOS-5164,5.0,mesos,Mesosphere Sprint 33
mcypark,2016-04-10T06:38:08.000+0000,mcypark,"If there is a ""\*"" in a commit message (there often is when we have bulleted lists), due to the current use of {{echo $LINE}}, the {{$LINE}} gets expanded with a ""*"" in it, which becomes a matcher in bash and therefore subsequently gets expanded into the list of files/directories in the current directory.

In order to avoid this mess, we need to wrap such variables in quotes, like so: {{echo ""$LINE""}}.",Bug,Major,mcypark,2016-04-11T18:09:19.000+0000,5,Resolved,Complete,"Commit message hook behaves incorrectly when a message includes a ""*"".",2016-04-11T18:09:19.000+0000,MESOS-5162,2.0,mesos,Mesosphere Sprint 33
avinash@mesosphere.io,2016-04-10T03:09:28.000+0000,avinash@mesosphere.io,"Currently there are no default `network` isolators for `MesosContainerizer`. With the development of the `network/cni` isolator we have an interface to run Mesos on multitude of IP networks. Given that its based on an open standard (the CNI spec) which is gathering a lot of traction from vendors (calico, weave, coreOS) and already works on some default networks (bridge, ipvlan, macvlan) it makes sense to make it as the default network isolator. ",Task,Major,avinash@mesosphere.io,2016-04-11T15:58:49.000+0000,5,Resolved,Complete,Make `network/cni` enabled as the default network isolator for `MesosContainerizer`.,2016-04-11T15:58:49.000+0000,MESOS-5160,1.0,mesos,Mesosphere Sprint 32
klueska,2016-04-08T23:32:07.000+0000,klueska,Fractional GPU requests should immediately cause a TASK_FAILED without ever launching the task.,Task,Major,klueska,2016-04-12T00:48:10.000+0000,5,Resolved,Complete,Add test to verify error when requesting fractional GPUs,2016-04-12T00:48:10.000+0000,MESOS-5159,1.0,mesos,Mesosphere Sprint 32
klueska,2016-04-08T20:30:46.000+0000,klueska,"After adding the GPU metrics and updating the resources JSON to include GPU information, the webui should be updated accordingly.",Task,Major,klueska,2016-04-08T21:24:39.000+0000,5,Resolved,Complete,Update webui for GPU metrics,2016-04-08T21:24:39.000+0000,MESOS-5157,1.0,mesos,Mesosphere Sprint 32
,2016-04-08T18:44:40.000+0000,vinodkone,"This is the last step to declare official support for PowerPC.

This is currently blocked on ASF INFRA adding PowerPC based Jenkins machines to the ASF CI.
",Bug,Major,vinodkone,,1,Open,New,Run mesos builds on PowerPC platform in ASF CI,2016-04-08T18:45:21.000+0000,MESOS-5156,1.0,mesos,
zhitao,2016-04-08T18:14:00.000+0000,alexr,"We should have just a single authz action: {{UPDATE_QUOTA_WITH_ROLE}}. It was a mistake in retrospect to introduce multiple actions.

Actions that are not symmetrical are register/teardown and dynamic reservations. The way they are implemented in this way is because entities that do one action differ from entities that do the other. For example, register framework is issued by a framework, teardown by an operator. What is a good way to identify a framework? A role it runs in, which may be different each launch and makes no sense in multi-role frameworks setup or better a sort of a group id, which is its principal. For dynamic reservations and persistent volumes, they can be both issued by frameworks and operators, hence similar reasoning applies. 

Now, quota is associated with a role and set only by operators. Do we need to care about principals that set it? Not that much. ",Improvement,Major,alexr,,10020,Accepted,In Progress,Consolidate authorization actions for quota.,2016-05-06T10:04:35.000+0000,MESOS-5155,5.0,mesos,Mesosphere Sprint 33
arojas,2016-04-08T10:35:00.000+0000,arojas,"MESOS-4956 introduced authentication support for the sandboxes. However, authentication can only go as far as to tell whether an user is known to mesos or not. An extra additional step is necessary to verify whether the known user is allowed to executed the requested operation on the sandbox (browse, read, download, debug).",Bug,Major,arojas,,3,In Progress,In Progress,Sandboxes contents should be protected from unauthorized users,2016-04-27T16:13:19.000+0000,MESOS-5153,8.0,mesos,Mesosphere Sprint 33
bbannier,2016-04-08T09:48:27.000+0000,adam-mesos,"Operators may want to enforce that only authenticated users (and subsequently only specific authorized users) be able to view per-executor resource usage statistics.
Since this endpoint is handled by the ResourceMonitorProcess, I would expect the work necessary to be similar to what was done for /files or /registry endpoint authn.",Task,Major,adam-mesos,2016-04-14T11:37:10.000+0000,5,Resolved,Complete,Add authentication to agent's /monitor/statistics endpoint,2016-04-14T11:37:10.000+0000,MESOS-5152,2.0,mesos,Mesosphere Sprint 33
gradywang,2016-04-07T23:54:42.000+0000,greggomann,"Observed on the ASF CI:

{code}
[ RUN      ] MasterAllocatorTest/1.RebalancedForUpdatedWeights
I0407 22:34:10.330394 29278 cluster.cpp:149] Creating default 'local' authorizer
I0407 22:34:10.466182 29278 leveldb.cpp:174] Opened db in 135.608207ms
I0407 22:34:10.516398 29278 leveldb.cpp:181] Compacted db in 50.159558ms
I0407 22:34:10.516464 29278 leveldb.cpp:196] Created db iterator in 34959ns
I0407 22:34:10.516484 29278 leveldb.cpp:202] Seeked to beginning of db in 10195ns
I0407 22:34:10.516496 29278 leveldb.cpp:271] Iterated through 0 keys in the db in 7324ns
I0407 22:34:10.516547 29278 replica.cpp:779] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned
I0407 22:34:10.517277 29298 recover.cpp:447] Starting replica recovery
I0407 22:34:10.517693 29300 recover.cpp:473] Replica is in EMPTY status
I0407 22:34:10.520251 29310 replica.cpp:673] Replica in EMPTY status received a broadcasted recover request from (4775)@172.17.0.3:35855
I0407 22:34:10.520611 29311 recover.cpp:193] Received a recover response from a replica in EMPTY status
I0407 22:34:10.521164 29299 recover.cpp:564] Updating replica status to STARTING
I0407 22:34:10.523435 29298 master.cpp:382] Master f59f9057-a5c7-43e1-b129-96862e640a12 (129e11060069) started on 172.17.0.3:35855
I0407 22:34:10.523473 29298 master.cpp:384] Flags at startup: --acls="""" --allocation_interval=""1secs"" --allocator=""HierarchicalDRF"" --authenticate=""true"" --authenticate_http=""true"" --authenticate_slaves=""true"" --authenticators=""crammd5"" --authorizers=""local"" --credentials=""/tmp/3rZY8C/credentials"" --framework_sorter=""drf"" --help=""false"" --hostname_lookup=""true"" --http_authenticators=""basic"" --initialize_driver_logging=""true"" --log_auto_initialize=""true"" --logbufsecs=""0"" --logging_level=""INFO"" --max_completed_frameworks=""50"" --max_completed_tasks_per_framework=""1000"" --max_slave_ping_timeouts=""5"" --quiet=""false"" --recovery_slave_removal_limit=""100%"" --registry=""replicated_log"" --registry_fetch_timeout=""1mins"" --registry_store_timeout=""100secs"" --registry_strict=""true"" --root_submissions=""true"" --slave_ping_timeout=""15secs"" --slave_reregister_timeout=""10mins"" --user_sorter=""drf"" --version=""false"" --webui_dir=""/mesos/mesos-0.29.0/_inst/share/mesos/webui"" --work_dir=""/tmp/3rZY8C/master"" --zk_session_timeout=""10secs""
I0407 22:34:10.523885 29298 master.cpp:433] Master only allowing authenticated frameworks to register
I0407 22:34:10.523901 29298 master.cpp:438] Master only allowing authenticated agents to register
I0407 22:34:10.523913 29298 credentials.hpp:37] Loading credentials for authentication from '/tmp/3rZY8C/credentials'
I0407 22:34:10.524298 29298 master.cpp:480] Using default 'crammd5' authenticator
I0407 22:34:10.524441 29298 master.cpp:551] Using default 'basic' HTTP authenticator
I0407 22:34:10.524564 29298 master.cpp:589] Authorization enabled
I0407 22:34:10.525269 29305 hierarchical.cpp:145] Initialized hierarchical allocator process
I0407 22:34:10.525333 29305 whitelist_watcher.cpp:77] No whitelist given
I0407 22:34:10.527331 29298 master.cpp:1832] The newly elected leader is master@172.17.0.3:35855 with id f59f9057-a5c7-43e1-b129-96862e640a12
I0407 22:34:10.527441 29298 master.cpp:1845] Elected as the leading master!
I0407 22:34:10.527545 29298 master.cpp:1532] Recovering from registrar
I0407 22:34:10.527889 29298 registrar.cpp:331] Recovering registrar
I0407 22:34:10.549734 29299 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 28.25177ms
I0407 22:34:10.549782 29299 replica.cpp:320] Persisted replica status to STARTING
I0407 22:34:10.550010 29299 recover.cpp:473] Replica is in STARTING status
I0407 22:34:10.551352 29299 replica.cpp:673] Replica in STARTING status received a broadcasted recover request from (4777)@172.17.0.3:35855
I0407 22:34:10.551676 29299 recover.cpp:193] Received a recover response from a replica in STARTING status
I0407 22:34:10.552315 29308 recover.cpp:564] Updating replica status to VOTING
I0407 22:34:10.574865 29308 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 22.413614ms
I0407 22:34:10.574928 29308 replica.cpp:320] Persisted replica status to VOTING
I0407 22:34:10.575103 29308 recover.cpp:578] Successfully joined the Paxos group
I0407 22:34:10.575346 29308 recover.cpp:462] Recover process terminated
I0407 22:34:10.575913 29308 log.cpp:659] Attempting to start the writer
I0407 22:34:10.577512 29308 replica.cpp:493] Replica received implicit promise request from (4778)@172.17.0.3:35855 with proposal 1
I0407 22:34:10.599984 29308 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 22.453613ms
I0407 22:34:10.600026 29308 replica.cpp:342] Persisted promised to 1
I0407 22:34:10.601773 29304 coordinator.cpp:238] Coordinator attempting to fill missing positions
I0407 22:34:10.603757 29307 replica.cpp:388] Replica received explicit promise request from (4779)@172.17.0.3:35855 for position 0 with proposal 2
I0407 22:34:10.634392 29307 leveldb.cpp:341] Persisting action (8 bytes) to leveldb took 30.269987ms
I0407 22:34:10.634829 29307 replica.cpp:712] Persisted action at 0
I0407 22:34:10.637017 29297 replica.cpp:537] Replica received write request for position 0 from (4780)@172.17.0.3:35855
I0407 22:34:10.637099 29297 leveldb.cpp:436] Reading position from leveldb took 52948ns
I0407 22:34:10.676170 29297 leveldb.cpp:341] Persisting action (14 bytes) to leveldb took 38.917487ms
I0407 22:34:10.676352 29297 replica.cpp:712] Persisted action at 0
I0407 22:34:10.677564 29306 replica.cpp:691] Replica received learned notice for position 0 from @0.0.0.0:0
I0407 22:34:10.717959 29306 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 40.306229ms
I0407 22:34:10.718202 29306 replica.cpp:712] Persisted action at 0
I0407 22:34:10.718399 29306 replica.cpp:697] Replica learned NOP action at position 0
I0407 22:34:10.719883 29306 log.cpp:675] Writer started with ending position 0
I0407 22:34:10.721688 29305 leveldb.cpp:436] Reading position from leveldb took 75934ns
I0407 22:34:10.723640 29306 registrar.cpp:364] Successfully fetched the registry (0B) in 195648us
I0407 22:34:10.723999 29306 registrar.cpp:463] Applied 1 operations in 108099ns; attempting to update the 'registry'
I0407 22:34:10.725077 29311 log.cpp:683] Attempting to append 170 bytes to the log
I0407 22:34:10.725328 29308 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 1
I0407 22:34:10.726552 29299 replica.cpp:537] Replica received write request for position 1 from (4781)@172.17.0.3:35855
I0407 22:34:10.759747 29299 leveldb.cpp:341] Persisting action (189 bytes) to leveldb took 33.089719ms
I0407 22:34:10.759976 29299 replica.cpp:712] Persisted action at 1
I0407 22:34:10.761739 29299 replica.cpp:691] Replica received learned notice for position 1 from @0.0.0.0:0
I0407 22:34:10.801522 29299 leveldb.cpp:341] Persisting action (191 bytes) to leveldb took 39.694064ms
I0407 22:34:10.801602 29299 replica.cpp:712] Persisted action at 1
I0407 22:34:10.801638 29299 replica.cpp:697] Replica learned APPEND action at position 1
I0407 22:34:10.803371 29311 registrar.cpp:508] Successfully updated the 'registry' in 79.163904ms
I0407 22:34:10.803829 29311 registrar.cpp:394] Successfully recovered registrar
I0407 22:34:10.804585 29311 master.cpp:1640] Recovered 0 agents from the Registry (131B) ; allowing 10mins for agents to re-register
I0407 22:34:10.805269 29308 log.cpp:702] Attempting to truncate the log to 1
I0407 22:34:10.805721 29310 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 2
I0407 22:34:10.805276 29296 hierarchical.cpp:172] Skipping recovery of hierarchical allocator: nothing to recover
I0407 22:34:10.806529 29307 replica.cpp:537] Replica received write request for position 2 from (4782)@172.17.0.3:35855
I0407 22:34:10.843320 29307 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 36.77593ms
I0407 22:34:10.843531 29307 replica.cpp:712] Persisted action at 2
I0407 22:34:10.845369 29311 replica.cpp:691] Replica received learned notice for position 2 from @0.0.0.0:0
I0407 22:34:10.885098 29311 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 39.641102ms
I0407 22:34:10.885401 29311 leveldb.cpp:399] Deleting ~1 keys from leveldb took 88701ns
I0407 22:34:10.885745 29311 replica.cpp:712] Persisted action at 2
I0407 22:34:10.885862 29311 replica.cpp:697] Replica learned TRUNCATE action at position 2
I0407 22:34:10.900660 29278 containerizer.cpp:155] Using isolation: posix/cpu,posix/mem,filesystem/posix
W0407 22:34:10.901793 29278 backend.cpp:66] Failed to create 'bind' backend: BindBackend requires root privileges
I0407 22:34:10.905488 29302 slave.cpp:201] Agent started on 111)@172.17.0.3:35855
I0407 22:34:10.905553 29302 slave.cpp:202] Flags at startup: --appc_simple_discovery_uri_prefix=""http://"" --appc_store_dir=""/tmp/mesos/store/appc"" --authenticate_http=""true"" --authenticatee=""crammd5"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --credential=""/tmp/MasterAllocatorTest_1_RebalancedForUpdatedWeights_9aCAYa/credential"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_kill_orphans=""true"" --docker_registry=""https://registry-1.docker.io"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/tmp/mesos/store/docker"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/tmp/MasterAllocatorTest_1_RebalancedForUpdatedWeights_9aCAYa/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --hostname_lookup=""true"" --http_authenticators=""basic"" --http_credentials=""/tmp/MasterAllocatorTest_1_RebalancedForUpdatedWeights_9aCAYa/http_credentials"" --image_provisioner_backend=""copy"" --initialize_driver_logging=""true"" --isolation=""posix/cpu,posix/mem"" --launcher_dir=""/mesos/mesos-0.29.0/_build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""10ms"" --resources=""cpus:2;mem:1024;disk:4096;ports:[31000-32000]"" --revocable_cpu_low_priority=""true"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --systemd_enable_support=""true"" --systemd_runtime_directory=""/run/systemd/system"" --version=""false"" --work_dir=""/tmp/MasterAllocatorTest_1_RebalancedForUpdatedWeights_9aCAYa""
I0407 22:34:10.906365 29302 credentials.hpp:86] Loading credential for authentication from '/tmp/MasterAllocatorTest_1_RebalancedForUpdatedWeights_9aCAYa/credential'
I0407 22:34:10.906787 29302 slave.cpp:339] Agent using credential for: test-principal
I0407 22:34:10.907202 29302 credentials.hpp:37] Loading credentials for authentication from '/tmp/MasterAllocatorTest_1_RebalancedForUpdatedWeights_9aCAYa/http_credentials'
I0407 22:34:10.907713 29302 slave.cpp:391] Using default 'basic' HTTP authenticator
I0407 22:34:10.908499 29302 resources.cpp:572] Parsing resources as JSON failed: cpus:2;mem:1024;disk:4096;ports:[31000-32000]
Trying semicolon-delimited string format instead
I0407 22:34:10.910189 29302 slave.cpp:590] Agent resources: cpus(*):2; mem(*):1024; disk(*):4096; ports(*):[31000-32000]
I0407 22:34:10.910362 29302 slave.cpp:598] Agent attributes: [  ]
I0407 22:34:10.910465 29302 slave.cpp:603] Agent hostname: 129e11060069
I0407 22:34:10.913280 29303 state.cpp:57] Recovering state from '/tmp/MasterAllocatorTest_1_RebalancedForUpdatedWeights_9aCAYa/meta'
I0407 22:34:10.914621 29303 status_update_manager.cpp:200] Recovering status update manager
I0407 22:34:10.915226 29303 containerizer.cpp:416] Recovering containerizer
I0407 22:34:10.917246 29301 provisioner.cpp:245] Provisioner recovery complete
I0407 22:34:10.917733 29301 slave.cpp:4784] Finished recovery
I0407 22:34:10.918226 29301 slave.cpp:4956] Querying resource estimator for oversubscribable resources
I0407 22:34:10.918529 29301 slave.cpp:4970] Received oversubscribable resources  from the resource estimator
I0407 22:34:10.918908 29304 slave.cpp:939] New master detected at master@172.17.0.3:35855
I0407 22:34:10.918988 29304 slave.cpp:1002] Authenticating with master master@172.17.0.3:35855
I0407 22:34:10.919098 29301 status_update_manager.cpp:174] Pausing sending status updates
I0407 22:34:10.919309 29304 slave.cpp:1007] Using default CRAM-MD5 authenticatee
I0407 22:34:10.919535 29304 slave.cpp:975] Detecting new master
I0407 22:34:10.919747 29308 authenticatee.cpp:121] Creating new client SASL connection
I0407 22:34:10.920413 29308 master.cpp:5695] Authenticating slave(111)@172.17.0.3:35855
I0407 22:34:10.920650 29308 authenticator.cpp:413] Starting authentication session for crammd5_authenticatee(278)@172.17.0.3:35855
I0407 22:34:10.921020 29308 authenticator.cpp:98] Creating new server SASL connection
I0407 22:34:10.921308 29308 authenticatee.cpp:212] Received SASL authentication mechanisms: CRAM-MD5
I0407 22:34:10.921424 29308 authenticatee.cpp:238] Attempting to authenticate with mechanism 'CRAM-MD5'
I0407 22:34:10.921596 29308 authenticator.cpp:203] Received SASL authentication start
I0407 22:34:10.921752 29308 authenticator.cpp:325] Authentication requires more steps
I0407 22:34:10.921957 29307 authenticatee.cpp:258] Received SASL authentication step
I0407 22:34:10.922178 29308 authenticator.cpp:231] Received SASL authentication step
I0407 22:34:10.922214 29308 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: '129e11060069' server FQDN: '129e11060069' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0407 22:34:10.922229 29308 auxprop.cpp:179] Looking up auxiliary property '*userPassword'
I0407 22:34:10.922281 29308 auxprop.cpp:179] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0407 22:34:10.922309 29308 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: '129e11060069' server FQDN: '129e11060069' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0407 22:34:10.922322 29308 auxprop.cpp:129] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0407 22:34:10.922332 29308 auxprop.cpp:129] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0407 22:34:10.922353 29308 authenticator.cpp:317] Authentication success
I0407 22:34:10.922436 29307 authenticatee.cpp:298] Authentication success
I0407 22:34:10.922587 29308 master.cpp:5725] Successfully authenticated principal 'test-principal' at slave(111)@172.17.0.3:35855
I0407 22:34:10.922668 29299 authenticator.cpp:431] Authentication session cleanup for crammd5_authenticatee(278)@172.17.0.3:35855
I0407 22:34:10.923256 29307 slave.cpp:1072] Successfully authenticated with master master@172.17.0.3:35855
I0407 22:34:10.923429 29307 slave.cpp:1468] Will retry registration in 3.220345ms if necessary
I0407 22:34:10.923707 29302 master.cpp:4406] Registering agent at slave(111)@172.17.0.3:35855 (129e11060069) with id f59f9057-a5c7-43e1-b129-96862e640a12-S0
I0407 22:34:10.924239 29309 registrar.cpp:463] Applied 1 operations in 105794ns; attempting to update the 'registry'
I0407 22:34:10.925787 29309 log.cpp:683] Attempting to append 339 bytes to the log
I0407 22:34:10.926028 29309 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 3
I0407 22:34:10.927139 29309 replica.cpp:537] Replica received write request for position 3 from (4797)@172.17.0.3:35855
I0407 22:34:10.929083 29305 slave.cpp:1468] Will retry registration in 39.293556ms if necessary
I0407 22:34:10.929363 29305 master.cpp:4394] Ignoring register agent message from slave(111)@172.17.0.3:35855 (129e11060069) as admission is already in progress
I0407 22:34:10.968843 29309 leveldb.cpp:341] Persisting action (358 bytes) to leveldb took 41.68025ms
I0407 22:34:10.969005 29309 replica.cpp:712] Persisted action at 3
I0407 22:34:10.969741 29309 slave.cpp:1468] Will retry registration in 54.852242ms if necessary
I0407 22:34:10.970118 29309 master.cpp:4394] Ignoring register agent message from slave(111)@172.17.0.3:35855 (129e11060069) as admission is already in progress
I0407 22:34:10.970852 29306 replica.cpp:691] Replica received learned notice for position 3 from @0.0.0.0:0
I0407 22:34:11.010634 29306 leveldb.cpp:341] Persisting action (360 bytes) to leveldb took 39.680272ms
I0407 22:34:11.010840 29306 replica.cpp:712] Persisted action at 3
I0407 22:34:11.011014 29306 replica.cpp:697] Replica learned APPEND action at position 3
I0407 22:34:11.014020 29306 registrar.cpp:508] Successfully updated the 'registry' in 89.684224ms
I0407 22:34:11.014181 29296 log.cpp:702] Attempting to truncate the log to 3
I0407 22:34:11.014606 29296 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 4
I0407 22:34:11.015836 29298 replica.cpp:537] Replica received write request for position 4 from (4798)@172.17.0.3:35855
I0407 22:34:11.016973 29296 master.cpp:4474] Registered agent f59f9057-a5c7-43e1-b129-96862e640a12-S0 at slave(111)@172.17.0.3:35855 (129e11060069) with cpus(*):2; mem(*):1024; disk(*):4096; ports(*):[31000-32000]
I0407 22:34:11.017518 29304 hierarchical.cpp:476] Added agent f59f9057-a5c7-43e1-b129-96862e640a12-S0 (129e11060069) with cpus(*):2; mem(*):1024; disk(*):4096; ports(*):[31000-32000] (allocated: )
I0407 22:34:11.017763 29311 slave.cpp:1116] Registered with master master@172.17.0.3:35855; given agent ID f59f9057-a5c7-43e1-b129-96862e640a12-S0
I0407 22:34:11.018362 29311 fetcher.cpp:81] Clearing fetcher cache
I0407 22:34:11.018870 29311 slave.cpp:1139] Checkpointing SlaveInfo to '/tmp/MasterAllocatorTest_1_RebalancedForUpdatedWeights_9aCAYa/meta/slaves/f59f9057-a5c7-43e1-b129-96862e640a12-S0/slave.info'
I0407 22:34:11.018890 29307 status_update_manager.cpp:181] Resuming sending status updates
I0407 22:34:11.019182 29304 hierarchical.cpp:1491] No resources available to allocate!
I0407 22:34:11.019304 29304 hierarchical.cpp:1165] Performed allocation for agent f59f9057-a5c7-43e1-b129-96862e640a12-S0 in 1.077349ms
I0407 22:34:11.019493 29311 slave.cpp:1176] Forwarding total oversubscribed resources 
I0407 22:34:11.019726 29311 slave.cpp:3675] Received ping from slave-observer(112)@172.17.0.3:35855
I0407 22:34:11.019878 29299 master.cpp:4818] Received update of agent f59f9057-a5c7-43e1-b129-96862e640a12-S0 at slave(111)@172.17.0.3:35855 (129e11060069) with total oversubscribed resources 
I0407 22:34:11.020845 29305 hierarchical.cpp:534] Agent f59f9057-a5c7-43e1-b129-96862e640a12-S0 (129e11060069) updated with oversubscribed resources  (total: cpus(*):2; mem(*):1024; disk(*):4096; ports(*):[31000-32000], allocated: )
I0407 22:34:11.021005 29305 hierarchical.cpp:1491] No resources available to allocate!
I0407 22:34:11.021065 29305 hierarchical.cpp:1165] Performed allocation for agent f59f9057-a5c7-43e1-b129-96862e640a12-S0 in 173907ns
I0407 22:34:11.022289 29278 containerizer.cpp:155] Using isolation: posix/cpu,posix/mem,filesystem/posix
W0407 22:34:11.023422 29278 backend.cpp:66] Failed to create 'bind' backend: BindBackend requires root privileges
I0407 22:34:11.026309 29309 slave.cpp:201] Agent started on 112)@172.17.0.3:35855
I0407 22:34:11.026410 29309 slave.cpp:202] Flags at startup: --appc_simple_discovery_uri_prefix=""http://"" --appc_store_dir=""/tmp/mesos/store/appc"" --authenticate_http=""true"" --authenticatee=""crammd5"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --credential=""/tmp/MasterAllocatorTest_1_RebalancedForUpdatedWeights_h8KW9O/credential"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_kill_orphans=""true"" --docker_registry=""https://registry-1.docker.io"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/tmp/mesos/store/docker"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/tmp/MasterAllocatorTest_1_RebalancedForUpdatedWeights_h8KW9O/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --hostname_lookup=""true"" --http_authenticators=""basic"" --http_credentials=""/tmp/MasterAllocatorTest_1_RebalancedForUpdatedWeights_h8KW9O/http_credentials"" --image_provisioner_backend=""copy"" --initialize_driver_logging=""true"" --isolation=""posix/cpu,posix/mem"" --launcher_dir=""/mesos/mesos-0.29.0/_build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""10ms"" --resources=""cpus:2;mem:1024;disk:4096;ports:[31000-32000]"" --revocable_cpu_low_priority=""true"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --systemd_enable_support=""true"" --systemd_runtime_directory=""/run/systemd/system"" --version=""false"" --work_dir=""/tmp/MasterAllocatorTest_1_RebalancedForUpdatedWeights_h8KW9O""
I0407 22:34:11.027070 29309 credentials.hpp:86] Loading credential for authentication from '/tmp/MasterAllocatorTest_1_RebalancedForUpdatedWeights_h8KW9O/credential'
I0407 22:34:11.027308 29309 slave.cpp:339] Agent using credential for: test-principal
I0407 22:34:11.027354 29309 credentials.hpp:37] Loading credentials for authentication from '/tmp/MasterAllocatorTest_1_RebalancedForUpdatedWeights_h8KW9O/http_credentials'
I0407 22:34:11.027698 29309 slave.cpp:391] Using default 'basic' HTTP authenticator
I0407 22:34:11.028147 29309 resources.cpp:572] Parsing resources as JSON failed: cpus:2;mem:1024;disk:4096;ports:[31000-32000]
Trying semicolon-delimited string format instead
I0407 22:34:11.028854 29309 slave.cpp:590] Agent resources: cpus(*):2; mem(*):1024; disk(*):4096; ports(*):[31000-32000]
I0407 22:34:11.028998 29309 slave.cpp:598] Agent attributes: [  ]
I0407 22:34:11.029064 29309 slave.cpp:603] Agent hostname: 129e11060069
I0407 22:34:11.031188 29309 state.cpp:57] Recovering state from '/tmp/MasterAllocatorTest_1_RebalancedForUpdatedWeights_h8KW9O/meta'
I0407 22:34:11.031844 29300 status_update_manager.cpp:200] Recovering status update manager
I0407 22:34:11.032091 29300 containerizer.cpp:416] Recovering containerizer
I0407 22:34:11.033805 29300 provisioner.cpp:245] Provisioner recovery complete
I0407 22:34:11.034364 29300 slave.cpp:4784] Finished recovery
I0407 22:34:11.061807 29300 slave.cpp:4956] Querying resource estimator for oversubscribable resources
I0407 22:34:11.062371 29300 slave.cpp:939] New master detected at master@172.17.0.3:35855
I0407 22:34:11.062450 29300 slave.cpp:1002] Authenticating with master master@172.17.0.3:35855
I0407 22:34:11.062469 29300 slave.cpp:1007] Using default CRAM-MD5 authenticatee
I0407 22:34:11.062630 29300 slave.cpp:975] Detecting new master
I0407 22:34:11.062737 29300 slave.cpp:4970] Received oversubscribable resources  from the resource estimator
I0407 22:34:11.062820 29300 status_update_manager.cpp:174] Pausing sending status updates
I0407 22:34:11.062952 29300 authenticatee.cpp:121] Creating new client SASL connection
I0407 22:34:11.063413 29300 master.cpp:5695] Authenticating slave(112)@172.17.0.3:35855
I0407 22:34:11.063591 29300 authenticator.cpp:413] Starting authentication session for crammd5_authenticatee(279)@172.17.0.3:35855
I0407 22:34:11.063907 29300 authenticator.cpp:98] Creating new server SASL connection
I0407 22:34:11.064159 29300 authenticatee.cpp:212] Received SASL authentication mechanisms: CRAM-MD5
I0407 22:34:11.064201 29300 authenticatee.cpp:238] Attempting to authenticate with mechanism 'CRAM-MD5'
I0407 22:34:11.064296 29300 authenticator.cpp:203] Received SASL authentication start
I0407 22:34:11.064363 29300 authenticator.cpp:325] Authentication requires more steps
I0407 22:34:11.064443 29300 authenticatee.cpp:258] Received SASL authentication step
I0407 22:34:11.064537 29300 authenticator.cpp:231] Received SASL authentication step
I0407 22:34:11.064569 29300 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: '129e11060069' server FQDN: '129e11060069' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0407 22:34:11.064584 29300 auxprop.cpp:179] Looking up auxiliary property '*userPassword'
I0407 22:34:11.064640 29300 auxprop.cpp:179] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0407 22:34:11.064668 29300 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: '129e11060069' server FQDN: '129e11060069' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0407 22:34:11.064680 29300 auxprop.cpp:129] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0407 22:34:11.064689 29300 auxprop.cpp:129] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0407 22:34:11.064708 29300 authenticator.cpp:317] Authentication success
I0407 22:34:11.064856 29300 authenticatee.cpp:298] Authentication success
I0407 22:34:11.064941 29300 master.cpp:5725] Successfully authenticated principal 'test-principal' at slave(112)@172.17.0.3:35855
I0407 22:34:11.065019 29300 authenticator.cpp:431] Authentication session cleanup for crammd5_authenticatee(279)@172.17.0.3:35855
I0407 22:34:11.065431 29305 slave.cpp:1072] Successfully authenticated with master master@172.17.0.3:35855
I0407 22:34:11.065580 29305 slave.cpp:1468] Will retry registration in 14.268351ms if necessary
I0407 22:34:11.065948 29305 master.cpp:4406] Registering agent at slave(112)@172.17.0.3:35855 (129e11060069) with id f59f9057-a5c7-43e1-b129-96862e640a12-S1
I0407 22:34:11.066653 29296 registrar.cpp:463] Applied 1 operations in 190813ns; attempting to update the 'registry'
I0407 22:34:11.075197 29298 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 59.338116ms
I0407 22:34:11.075359 29298 replica.cpp:712] Persisted action at 4
I0407 22:34:11.076177 29301 replica.cpp:691] Replica received learned notice for position 4 from @0.0.0.0:0
I0407 22:34:11.080481 29309 slave.cpp:1468] Will retry registration in 23.018984ms if necessary
I0407 22:34:11.080770 29309 master.cpp:4394] Ignoring register agent message from slave(112)@172.17.0.3:35855 (129e11060069) as admission is already in progress
I0407 22:34:11.100519 29301 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 24.288152ms
I0407 22:34:11.100792 29301 leveldb.cpp:399] Deleting ~2 keys from leveldb took 98264ns
I0407 22:34:11.100883 29301 replica.cpp:712] Persisted action at 4
I0407 22:34:11.101002 29301 replica.cpp:697] Replica learned TRUNCATE action at position 4
I0407 22:34:11.102180 29309 log.cpp:683] Attempting to append 505 bytes to the log
I0407 22:34:11.102334 29301 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 5
I0407 22:34:11.103551 29309 replica.cpp:537] Replica received write request for position 5 from (4813)@172.17.0.3:35855
I0407 22:34:11.105705 29305 slave.cpp:1468] Will retry registration in 49.972787ms if necessary
I0407 22:34:11.106020 29305 master.cpp:4394] Ignoring register agent message from slave(112)@172.17.0.3:35855 (129e11060069) as admission is already in progress
I0407 22:34:11.126212 29309 leveldb.cpp:341] Persisting action (524 bytes) to leveldb took 22.638848ms
I0407 22:34:11.126296 29309 replica.cpp:712] Persisted action at 5
I0407 22:34:11.127374 29305 replica.cpp:691] Replica received learned notice for position 5 from @0.0.0.0:0
I0407 22:34:11.150754 29305 leveldb.cpp:341] Persisting action (526 bytes) to leveldb took 23.376079ms
I0407 22:34:11.150952 29305 replica.cpp:712] Persisted action at 5
I0407 22:34:11.150992 29305 replica.cpp:697] Replica learned APPEND action at position 5
I0407 22:34:11.154031 29305 registrar.cpp:508] Successfully updated the 'registry' in 87.26784ms
I0407 22:34:11.154491 29305 log.cpp:702] Attempting to truncate the log to 5
I0407 22:34:11.154824 29305 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 6
I0407 22:34:11.155413 29308 slave.cpp:3675] Received ping from slave-observer(113)@172.17.0.3:35855
I0407 22:34:11.155467 29303 master.cpp:4474] Registered agent f59f9057-a5c7-43e1-b129-96862e640a12-S1 at slave(112)@172.17.0.3:35855 (129e11060069) with cpus(*):2; mem(*):1024; disk(*):4096; ports(*):[31000-32000]
I0407 22:34:11.155580 29308 slave.cpp:1116] Registered with master master@172.17.0.3:35855; given agent ID f59f9057-a5c7-43e1-b129-96862e640a12-S1
I0407 22:34:11.155606 29308 fetcher.cpp:81] Clearing fetcher cache
I0407 22:34:11.155856 29304 status_update_manager.cpp:181] Resuming sending status updates
I0407 22:34:11.156281 29308 slave.cpp:1139] Checkpointing SlaveInfo to '/tmp/MasterAllocatorTest_1_RebalancedForUpdatedWeights_h8KW9O/meta/slaves/f59f9057-a5c7-43e1-b129-96862e640a12-S1/slave.info'
I0407 22:34:11.156661 29304 replica.cpp:537] Replica received write request for position 6 from (4814)@172.17.0.3:35855
I0407 22:34:11.156949 29305 hierarchical.cpp:476] Added agent f59f9057-a5c7-43e1-b129-96862e640a12-S1 (129e11060069) with cpus(*):2; mem(*):1024; disk(*):4096; ports(*):[31000-32000] (allocated: )
I0407 22:34:11.157217 29305 hierarchical.cpp:1491] No resources available to allocate!
I0407 22:34:11.157346 29305 hierarchical.cpp:1165] Performed allocation for agent f59f9057-a5c7-43e1-b129-96862e640a12-S1 in 304432ns
I0407 22:34:11.157224 29308 slave.cpp:1176] Forwarding total oversubscribed resources 
I0407 22:34:11.157788 29303 master.cpp:4818] Received update of agent f59f9057-a5c7-43e1-b129-96862e640a12-S1 at slave(112)@172.17.0.3:35855 (129e11060069) with total oversubscribed resources 
I0407 22:34:11.158424 29303 hierarchical.cpp:534] Agent f59f9057-a5c7-43e1-b129-96862e640a12-S1 (129e11060069) updated with oversubscribed resources  (total: cpus(*):2; mem(*):1024; disk(*):4096; ports(*):[31000-32000], allocated: )
I0407 22:34:11.158633 29303 hierarchical.cpp:1491] No resources available to allocate!
I0407 22:34:11.158699 29303 hierarchical.cpp:1165] Performed allocation for agent f59f9057-a5c7-43e1-b129-96862e640a12-S1 in 178482ns
I0407 22:34:11.162139 29278 containerizer.cpp:155] Using isolation: posix/cpu,posix/mem,filesystem/posix
W0407 22:34:11.192978 29278 backend.cpp:66] Failed to create 'bind' backend: BindBackend requires root privileges
I0407 22:34:11.197527 29307 slave.cpp:201] Agent started on 113)@172.17.0.3:35855
I0407 22:34:11.197581 29307 slave.cpp:202] Flags at startup: --appc_simple_discovery_uri_prefix=""http://"" --appc_store_dir=""/tmp/mesos/store/appc"" --authenticate_http=""true"" --authenticatee=""crammd5"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --credential=""/tmp/MasterAllocatorTest_1_RebalancedForUpdatedWeights_EG5sru/credential"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_kill_orphans=""true"" --docker_registry=""https://registry-1.docker.io"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/tmp/mesos/store/docker"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/tmp/MasterAllocatorTest_1_RebalancedForUpdatedWeights_EG5sru/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --hostname_lookup=""true"" --http_authenticators=""basic"" --http_credentials=""/tmp/MasterAllocatorTest_1_RebalancedForUpdatedWeights_EG5sru/http_credentials"" --image_provisioner_backend=""copy"" --initialize_driver_logging=""true"" --isolation=""posix/cpu,posix/mem"" --launcher_dir=""/mesos/mesos-0.29.0/_build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""10ms"" --resources=""cpus:2;mem:1024;disk:4096;ports:[31000-32000]"" --revocable_cpu_low_priority=""true"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --systemd_enable_support=""true"" --systemd_runtime_directory=""/run/systemd/system"" --version=""false"" --work_dir=""/tmp/MasterAllocatorTest_1_RebalancedForUpdatedWeights_EG5sru""
I0407 22:34:11.198328 29307 credentials.hpp:86] Loading credential for authentication from '/tmp/MasterAllocatorTest_1_RebalancedForUpdatedWeights_EG5sru/credential'
I0407 22:34:11.198562 29307 slave.cpp:339] Agent using credential for: test-principal
I0407 22:34:11.198598 29307 credentials.hpp:37] Loading credentials for authentication from '/tmp/MasterAllocatorTest_1_RebalancedForUpdatedWeights_EG5sru/http_credentials'
I0407 22:34:11.198884 29307 slave.cpp:391] Using default 'basic' HTTP authenticator
I0407 22:34:11.199286 29307 resources.cpp:572] Parsing resources as JSON failed: cpus:2;mem:1024;disk:4096;ports:[31000-32000]
Trying semicolon-delimited string format instead
I0407 22:34:11.199820 29307 slave.cpp:590] Agent resources: cpus(*):2; mem(*):1024; disk(*):4096; ports(*):[31000-32000]
I0407 22:34:11.199905 29307 slave.cpp:598] Agent attributes: [  ]
I0407 22:34:11.199920 29307 slave.cpp:603] Agent hostname: 129e11060069
I0407 22:34:11.201535 29297 state.cpp:57] Recovering state from '/tmp/MasterAllocatorTest_1_RebalancedForUpdatedWeights_EG5sru/meta'
I0407 22:34:11.201773 29309 status_update_manager.cpp:200] Recovering status update manager
I0407 22:34:11.202081 29307 containerizer.cpp:416] Recovering containerizer
I0407 22:34:11.202180 29304 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 45.487899ms
I0407 22:34:11.202221 29304 replica.cpp:712] Persisted action at 6
I0407 22:34:11.203219 29302 replica.cpp:691] Replica received learned notice for position 6 from @0.0.0.0:0
I0407 22:34:11.205412 29301 provisioner.cpp:245] Provisioner recovery complete
I0407 22:34:11.205984 29301 slave.cpp:4784] Finished recovery
I0407 22:34:11.206735 29301 slave.cpp:4956] Querying resource estimator for oversubscribable resources
I0407 22:34:11.207351 29301 slave.cpp:4970] Received oversubscribable resources  from the resource estimator
I0407 22:34:11.207679 29301 slave.cpp:939] New master detected at master@172.17.0.3:35855
I0407 22:34:11.207804 29309 status_update_manager.cpp:174] Pausing sending status updates
I0407 22:34:11.208039 29301 slave.cpp:1002] Authenticating with master master@172.17.0.3:35855
I0407 22:34:11.208072 29301 slave.cpp:1007] Using default CRAM-MD5 authenticatee
I0407 22:34:11.208431 29301 slave.cpp:975] Detecting new master
I0407 22:34:11.208650 29309 authenticatee.cpp:121] Creating new client SASL connection
I0407 22:34:11.208976 29309 master.cpp:5695] Authenticating slave(113)@172.17.0.3:35855
I0407 22:34:11.209081 29307 authenticator.cpp:413] Starting authentication session for crammd5_authenticatee(280)@172.17.0.3:35855
I0407 22:34:11.209432 29304 authenticator.cpp:98] Creating new server SASL connection
I0407 22:34:11.209971 29304 authenticatee.cpp:212] Received SASL authentication mechanisms: CRAM-MD5
I0407 22:34:11.210103 29304 authenticatee.cpp:238] Attempting to authenticate with mechanism 'CRAM-MD5'
I0407 22:34:11.210382 29304 authenticator.cpp:203] Received SASL authentication start
I0407 22:34:11.210515 29304 authenticator.cpp:325] Authentication requires more steps
I0407 22:34:11.210726 29304 authenticatee.cpp:258] Received SASL authentication step
I0407 22:34:11.210940 29305 authenticator.cpp:231] Received SASL authentication step
I0407 22:34:11.210980 29305 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: '129e11060069' server FQDN: '129e11060069' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0407 22:34:11.210997 29305 auxprop.cpp:179] Looking up auxiliary property '*userPassword'
I0407 22:34:11.211060 29305 auxprop.cpp:179] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0407 22:34:11.211100 29305 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: '129e11060069' server FQDN: '129e11060069' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0407 22:34:11.211175 29305 auxprop.cpp:129] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0407 22:34:11.211244 29305 auxprop.cpp:129] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0407 22:34:11.211272 29305 authenticator.cpp:317] Authentication success
I0407 22:34:11.211462 29305 authenticatee.cpp:298] Authentication success
I0407 22:34:11.211575 29305 master.cpp:5725] Successfully authenticated principal 'test-principal' at slave(113)@172.17.0.3:35855
I0407 22:34:11.211673 29305 authenticator.cpp:431] Authentication session cleanup for crammd5_authenticatee(280)@172.17.0.3:35855
I0407 22:34:11.212026 29305 slave.cpp:1072] Successfully authenticated with master master@172.17.0.3:35855
I0407 22:34:11.212280 29305 slave.cpp:1468] Will retry registration in 6.415977ms if necessary
I0407 22:34:11.212704 29304 master.cpp:4406] Registering agent at slave(113)@172.17.0.3:35855 (129e11060069) with id f59f9057-a5c7-43e1-b129-96862e640a12-S2
I0407 22:34:11.213373 29311 registrar.cpp:463] Applied 1 operations in 154555ns; attempting to update the 'registry'
I0407 22:34:11.223568 29303 master.cpp:4394] Ignoring register agent message from slave(113)@172.17.0.3:35855 (129e11060069) as admission is already in progress
I0407 22:34:11.224171 29300 slave.cpp:1468] Will retry registration in 22.418267ms if necessary
I0407 22:34:11.243433 29302 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 40.20863ms
I0407 22:34:11.243851 29302 leveldb.cpp:399] Deleting ~2 keys from leveldb took 204965ns
I0407 22:34:11.243980 29302 replica.cpp:712] Persisted action at 6
I0407 22:34:11.244148 29302 replica.cpp:697] Replica learned TRUNCATE action at position 6
I0407 22:34:11.245827 29302 log.cpp:683] Attempting to append 671 bytes to the log
I0407 22:34:11.246206 29310 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 7
I0407 22:34:11.247114 29296 replica.cpp:537] Replica received write request for position 7 from (4829)@172.17.0.3:35855
I0407 22:34:11.248457 29304 slave.cpp:1468] Will retry registration in 14.981599ms if necessary
I0407 22:34:11.248837 29302 master.cpp:4394] Ignoring register agent message from slave(113)@172.17.0.3:35855 (129e11060069) as admission is already in progress
I0407 22:34:11.265728 29301 slave.cpp:1468] Will retry registration in 117.285894ms if necessary
I0407 22:34:11.266026 29301 master.cpp:4394] Ignoring register agent message from slave(113)@172.17.0.3:35855 (129e11060069) as admission is already in progress
I0407 22:34:11.278012 29296 leveldb.cpp:341] Persisting action (690 bytes) to leveldb took 30.789344ms
I0407 22:34:11.278064 29296 replica.cpp:712] Persisted action at 7
I0407 22:34:11.278990 29303 replica.cpp:691] Replica received learned notice for position 7 from @0.0.0.0:0
I0407 22:34:11.337220 29303 leveldb.cpp:341] Persisting action (692 bytes) to leveldb took 58.231676ms
I0407 22:34:11.337312 29303 replica.cpp:712] Persisted action at 7
I0407 22:34:11.337347 29303 replica.cpp:697] Replica learned APPEND action at position 7
I0407 22:34:11.340283 29305 registrar.cpp:508] Successfully updated the 'registry' in 126.71616ms
I0407 22:34:11.340703 29309 log.cpp:702] Attempting to truncate the log to 7
I0407 22:34:11.341044 29309 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 8
I0407 22:34:11.341847 29309 slave.cpp:3675] Received ping from slave-observer(114)@172.17.0.3:35855
I0407 22:34:11.342489 29309 slave.cpp:1116] Registered with master master@172.17.0.3:35855; given agent ID f59f9057-a5c7-43e1-b129-96862e640a12-S2
I0407 22:34:11.342532 29309 fetcher.cpp:81] Clearing fetcher cache
I0407 22:34:11.341804 29303 master.cpp:4474] Registered agent f59f9057-a5c7-43e1-b129-96862e640a12-S2 at slave(113)@172.17.0.3:35855 (129e11060069) with cpus(*):2; mem(*):1024; disk(*):4096; ports(*):[31000-32000]
I0407 22:34:11.342871 29297 status_update_manager.cpp:181] Resuming sending status updates
I0407 22:34:11.342267 29300 hierarchical.cpp:476] Added agent f59f9057-a5c7-43e1-b129-96862e640a12-S2 (129e11060069) with cpus(*):2; mem(*):1024; disk(*):4096; ports(*):[31000-32000] (allocated: )
I0407 22:34:11.342963 29299 replica.cpp:537] Replica received write request for position 8 from (4830)@172.17.0.3:35855
I0407 22:34:11.343101 29300 hierarchical.cpp:1491] No resources available to allocate!
I0407 22:34:11.343178 29300 hierarchical.cpp:1165] Performed allocation for agent f59f9057-a5c7-43e1-b129-96862e640a12-S2 in 242921ns
I0407 22:34:11.342921 29309 slave.cpp:1139] Checkpointing SlaveInfo to '/tmp/MasterAllocatorTest_1_RebalancedForUpdatedWeights_EG5sru/meta/slaves/f59f9057-a5c7-43e1-b129-96862e640a12-S2/slave.info'
I0407 22:34:11.343636 29309 slave.cpp:1176] Forwarding total oversubscribed resources 
I0407 22:34:11.343863 29309 master.cpp:4818] Received update of agent f59f9057-a5c7-43e1-b129-96862e640a12-S2 at slave(113)@172.17.0.3:35855 (129e11060069) with total oversubscribed resources 
I0407 22:34:11.344173 29278 sched.cpp:224] Version: 0.29.0
I0407 22:34:11.344425 29309 hierarchical.cpp:534] Agent f59f9057-a5c7-43e1-b129-96862e640a12-S2 (129e11060069) updated with oversubscribed resources  (total: cpus(*):2; mem(*):1024; disk(*):4096; ports(*):[31000-32000], allocated: )
I0407 22:34:11.344568 29309 hierarchical.cpp:1491] No resources available to allocate!
I0407 22:34:11.344621 29309 hierarchical.cpp:1165] Performed allocation for agent f59f9057-a5c7-43e1-b129-96862e640a12-S2 in 155620ns
I0407 22:34:11.345155 29303 sched.cpp:328] New master detected at master@172.17.0.3:35855
I0407 22:34:11.345387 29303 sched.cpp:384] Authenticating with master master@172.17.0.3:35855
I0407 22:34:11.345479 29303 sched.cpp:391] Using default CRAM-MD5 authenticatee
I0407 22:34:11.346035 29303 authenticatee.cpp:121] Creating new client SASL connection
I0407 22:34:11.346884 29303 master.cpp:5695] Authenticating scheduler-72944dc9-e3cc-4ebc-bca6-d72c77ad6721@172.17.0.3:35855
I0407 22:34:11.347530 29303 authenticator.cpp:413] Starting authentication session for crammd5_authenticatee(281)@172.17.0.3:35855
I0407 22:34:11.349140 29303 authenticator.cpp:98] Creating new server SASL connection
I0407 22:34:11.349580 29303 authenticatee.cpp:212] Received SASL authentication mechanisms: CRAM-MD5
I0407 22:34:11.349707 29303 authenticatee.cpp:238] Attempting to authenticate with mechanism 'CRAM-MD5'
I0407 22:34:11.349957 29309 authenticator.cpp:203] Received SASL authentication start
I0407 22:34:11.350040 29309 authenticator.cpp:325] Authentication requires more steps
I0407 22:34:11.350168 29309 authenticatee.cpp:258] Received SASL authentication step
I0407 22:34:11.350275 29309 authenticator.cpp:231] Received SASL authentication step
I0407 22:34:11.350309 29309 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: '129e11060069' server FQDN: '129e11060069' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0407 22:34:11.350323 29309 auxprop.cpp:179] Looking up auxiliary property '*userPassword'
I0407 22:34:11.350375 29309 auxprop.cpp:179] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0407 22:34:11.350407 29309 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: '129e11060069' server FQDN: '129e11060069' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0407 22:34:11.350420 29309 auxprop.cpp:129] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0407 22:34:11.350430 29309 auxprop.cpp:129] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0407 22:34:11.350450 29309 authenticator.cpp:317] Authentication success
I0407 22:34:11.350550 29303 authenticatee.cpp:298] Authentication success
I0407 22:34:11.350647 29309 master.cpp:5725] Successfully authenticated principal 'test-principal' at scheduler-72944dc9-e3cc-4ebc-bca6-d72c77ad6721@172.17.0.3:35855
I0407 22:34:11.350803 29303 authenticator.cpp:431] Authentication session cleanup for crammd5_authenticatee(281)@172.17.0.3:35855
I0407 22:34:11.350986 29309 sched.cpp:474] Successfully authenticated with master master@172.17.0.3:35855
I0407 22:34:11.351011 29309 sched.cpp:779] Sending SUBSCRIBE call to master@172.17.0.3:35855
I0407 22:34:11.351109 29309 sched.cpp:812] Will retry registration in 82.651114ms if necessary
I0407 22:34:11.351313 29296 master.cpp:2362] Received SUBSCRIBE call for framework 'default' at scheduler-72944dc9-e3cc-4ebc-bca6-d72c77ad6721@172.17.0.3:35855
I0407 22:34:11.351343 29296 master.cpp:1871] Authorizing framework principal 'test-principal' to receive offers for role 'role1'
I0407 22:34:11.351662 29310 master.cpp:2433] Subscribing framework default with checkpointing disabled and capabilities [  ]
I0407 22:34:11.352442 29311 hierarchical.cpp:267] Added framework f59f9057-a5c7-43e1-b129-96862e640a12-0000
I0407 22:34:11.353435 29309 sched.cpp:706] Framework registered with f59f9057-a5c7-43e1-b129-96862e640a12-0000
I0407 22:34:11.353519 29309 sched.cpp:720] Scheduler::registered took 66350ns
I0407 22:34:11.355201 29311 hierarchical.cpp:1586] No inverse offers to send out!
I0407 22:34:11.355293 29311 hierarchical.cpp:1142] Performed allocation for 3 agents in 2.836617ms
I0407 22:34:11.356238 29301 master.cpp:5524] Sending 3 offers to framework f59f9057-a5c7-43e1-b129-96862e640a12-0000 (default) at scheduler-72944dc9-e3cc-4ebc-bca6-d72c77ad6721@172.17.0.3:35855
I0407 22:34:11.357260 29311 sched.cpp:876] Scheduler::resourceOffers took 327028ns
I0407 22:34:11.357628 29278 resources.cpp:572] Parsing resources as JSON failed: cpus:2;mem:1024;disk:4096;ports:[31000-32000]
Trying semicolon-delimited string format instead
I0407 22:34:11.358330 29278 resources.cpp:572] Parsing resources as JSON failed: cpus:2;mem:1024;disk:4096;ports:[31000-32000]
Trying semicolon-delimited string format instead
I0407 22:34:11.358959 29278 resources.cpp:572] Parsing resources as JSON failed: cpus:2;mem:1024;disk:4096;ports:[31000-32000]
Trying semicolon-delimited string format instead
I0407 22:34:11.360607 29278 sched.cpp:224] Version: 0.29.0
I0407 22:34:11.361264 29307 sched.cpp:328] New master detected at master@172.17.0.3:35855
I0407 22:34:11.361342 29307 sched.cpp:384] Authenticating with master master@172.17.0.3:35855
I0407 22:34:11.361366 29307 sched.cpp:391] Using default CRAM-MD5 authenticatee
I0407 22:34:11.361670 29307 authenticatee.cpp:121] Creating new client SASL connection
I0407 22:34:11.361959 29307 master.cpp:5695] Authenticating scheduler-37080386-2aa8-4592-bf09-8288bd04727a@172.17.0.3:35855
I0407 22:34:11.362195 29307 authenticator.cpp:413] Starting authentication session for crammd5_authenticatee(282)@172.17.0.3:35855
I0407 22:34:11.362535 29311 authenticator.cpp:98] Creating new server SASL connection
I0407 22:34:11.362890 29307 authenticatee.cpp:212] Received SASL authentication mechanisms: CRAM-MD5
I0407 22:34:11.362926 29307 authenticatee.cpp:238] Attempting to authenticate with mechanism 'CRAM-MD5'
I0407 22:34:11.363021 29307 authenticator.cpp:203] Received SASL authentication start
I0407 22:34:11.363082 29307 authenticator.cpp:325] Authentication requires more steps
I0407 22:34:11.363199 29311 authenticatee.cpp:258] Received SASL authentication step
I0407 22:34:11.363313 29311 authenticator.cpp:231] Received SASL authentication step
I0407 22:34:11.363406 29311 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: '129e11060069' server FQDN: '129e11060069' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0407 22:34:11.363512 29311 auxprop.cpp:179] Looking up auxiliary property '*userPassword'
I0407 22:34:11.363605 29311 auxprop.cpp:179] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0407 22:34:11.363651 29311 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: '129e11060069' server FQDN: '129e11060069' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0407 22:34:11.363673 29311 auxprop.cpp:129] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0407 22:34:11.363685 29311 auxprop.cpp:129] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0407 22:34:11.363706 29311 authenticator.cpp:317] Authentication success
I0407 22:34:11.363785 29307 authenticatee.cpp:298] Authentication success
I0407 22:34:11.363858 29297 master.cpp:5725] Successfully authenticated principal 'test-principal' at scheduler-37080386-2aa8-4592-bf09-8288bd04727a@172.17.0.3:35855
I0407 22:34:11.363903 29311 authenticator.cpp:431] Authentication session cleanup for crammd5_authenticatee(282)@172.17.0.3:35855
I0407 22:34:11.365274 29297 sched.cpp:474] Successfully authenticated with master master@172.17.0.3:35855
I0407 22:34:11.365301 29297 sched.cpp:779] Sending SUBSCRIBE call to master@172.17.0.3:35855
I0407 22:34:11.365396 29297 sched.cpp:812] Will retry registration in 1.739883809secs if necessary
I0407 22:34:11.365500 29311 master.cpp:2362] Received SUBSCRIBE call for framework 'default' at scheduler-37080386-2aa8-4592-bf09-8288bd04727a@172.17.0.3:35855
I0407 22:34:11.365528 29311 master.cpp:1871] Authorizing framework principal 'test-principal' to receive offers for role 'role2'
I0407 22:34:11.365952 29297 master.cpp:2433] Subscribing framework default with checkpointing disabled and capabilities [  ]
I0407 22:34:11.366518 29297 sched.cpp:706] Framework registered with f59f9057-a5c7-43e1-b129-96862e640a12-0001
I0407 22:34:11.366564 29311 hierarchical.cpp:267] Added framework f59f9057-a5c7-43e1-b129-96862e640a12-0001
I0407 22:34:11.366590 29297 sched.cpp:720] Scheduler::registered took 57363ns
I0407 22:34:11.366768 29311 hierarchical.cpp:1491] No resources available to allocate!
I0407 22:34:11.366837 29311 hierarchical.cpp:1586] No inverse offers to send out!
I0407 22:34:11.366914 29311 hierarchical.cpp:1142] Performed allocation for 3 agents in 340908ns
I0407 22:34:11.369886 29309 process.cpp:3165] Handling HTTP event for process 'master' with path: '/master/weights'
I0407 22:34:11.370643 29309 http.cpp:313] HTTP PUT for /master/weights from 172.17.0.3:59397
I0407 22:34:11.370762 29309 weights_handler.cpp:58] Updating weights from request: '[{""role"":""role2"",""weight"":2.0}]'
I0407 22:34:11.370908 29309 weights_handler.cpp:198] Authorizing principal 'test-principal' to update weights for roles '[ role2 ]'
I0407 22:34:11.372067 29306 registrar.cpp:463] Applied 1 operations in 136060ns; attempting to update the 'registry'
I0407 22:34:11.388222 29299 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 45.245469ms
I0407 22:34:11.388381 29299 replica.cpp:712] Persisted action at 8
I0407 22:34:11.389389 29305 replica.cpp:691] Replica received learned notice for position 8 from @0.0.0.0:0
I0407 22:34:11.435415 29305 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 45.918275ms
I0407 22:34:11.435688 29305 leveldb.cpp:399] Deleting ~2 keys from leveldb took 98518ns
I0407 22:34:11.435835 29305 replica.cpp:712] Persisted action at 8
I0407 22:34:11.435956 29305 replica.cpp:697] Replica learned TRUNCATE action at position 8
I0407 22:34:11.437063 29310 log.cpp:683] Attempting to append 691 bytes to the log
I0407 22:34:11.437297 29300 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 9
I0407 22:34:11.437979 29300 replica.cpp:537] Replica received write request for position 9 from (4834)@172.17.0.3:35855
I0407 22:34:11.479363 29300 leveldb.cpp:341] Persisting action (710 bytes) to leveldb took 41.36295ms
I0407 22:34:11.479432 29300 replica.cpp:712] Persisted action at 9
I0407 22:34:11.480434 29296 replica.cpp:691] Replica received learned notice for position 9 from @0.0.0.0:0
I0407 22:34:11.521299 29296 leveldb.cpp:341] Persisting action (712 bytes) to leveldb took 40.855981ms
I0407 22:34:11.521378 29296 replica.cpp:712] Persisted action at 9
I0407 22:34:11.521412 29296 replica.cpp:697] Replica learned APPEND action at position 9
I0407 22:34:11.524554 29304 registrar.cpp:508] Successfully updated the 'registry' in 152.402176ms
I0407 22:34:11.524790 29298 log.cpp:702] Attempting to truncate the log to 9
I0407 22:34:11.524960 29304 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 10
I0407 22:34:11.525243 29298 hierarchical.cpp:1491] No resources available to allocate!
I0407 22:34:11.525387 29298 hierarchical.cpp:1586] No inverse offers to send out!
I0407 22:34:11.525538 29298 hierarchical.cpp:1142] Performed allocation for 3 agents in 540681ns
I0407 22:34:11.525856 29296 replica.cpp:537] Replica received write request for position 10 from (4835)@172.17.0.3:35855
I0407 22:34:11.526267 29308 sched.cpp:902] Rescinded offer f59f9057-a5c7-43e1-b129-96862e640a12-O1
I0407 22:34:11.526398 29308 sched.cpp:913] Scheduler::offerRescinded took 54437ns
I0407 22:34:11.526425 29298 hierarchical.cpp:894] Recovered cpus(*):2; mem(*):1024; disk(*):4096; ports(*):[31000-32000] (total: cpus(*):2; mem(*):1024; disk(*):4096; ports(*):[31000-32000], allocated: ) on agent f59f9057-a5c7-43e1-b129-96862e640a12-S2 from framework f59f9057-a5c7-43e1-b129-96862e640a12-0000
I0407 22:34:11.527235 29299 sched.cpp:902] Rescinded offer f59f9057-a5c7-43e1-b129-96862e640a12-O2
I0407 22:34:11.527299 29299 sched.cpp:913] Scheduler::offerRescinded took 29764ns
I0407 22:34:11.527825 29300 sched.cpp:902] Rescinded offer f59f9057-a5c7-43e1-b129-96862e640a12-O0
I0407 22:34:11.527920 29298 hierarchical.cpp:1586] No inverse offers to send out!
I0407 22:34:11.527990 29298 hierarchical.cpp:1142] Performed allocation for 3 agents in 1.481251ms
I0407 22:34:11.528009 29300 sched.cpp:913] Scheduler::offerRescinded took 333035ns
I0407 22:34:11.528591 29298 hierarchical.cpp:894] Recovered cpus(*):2; mem(*):1024; disk(*):4096; ports(*):[31000-32000] (total: cpus(*):2; mem(*):1024; disk(*):4096; ports(*):[31000-32000], allocated: ) on agent f59f9057-a5c7-43e1-b129-96862e640a12-S1 from framework f59f9057-a5c7-43e1-b129-96862e640a12-0000
I0407 22:34:11.529536 29311 master.cpp:5524] Sending 1 offers to framework f59f9057-a5c7-43e1-b129-96862e640a12-0001 (default) at scheduler-37080386-2aa8-4592-bf09-8288bd04727a@172.17.0.3:35855
I0407 22:34:11.529846 29298 hierarchical.cpp:894] Recovered cpus(*):2; mem(*):1024; disk(*):4096; ports(*):[31000-32000] (total: cpus(*):2; mem(*):1024; disk(*):4096; ports(*):[31000-32000], allocated: ) on agent f59f9057-a5c7-43e1-b129-96862e640a12-S0 from framework f59f9057-a5c7-43e1-b129-96862e640a12-0000
I0407 22:34:11.530747 29304 sched.cpp:876] Scheduler::resourceOffers took 128400ns
I0407 22:34:11.560456 29296 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 34.585376ms
I0407 22:34:11.560539 29296 replica.cpp:712] Persisted action at 10
I0407 22:34:11.564628 29303 replica.cpp:691] Replica received learned notice for position 10 from @0.0.0.0:0
I0407 22:34:11.601330 29303 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 36.57815ms
I0407 22:34:11.601774 29303 leveldb.cpp:399] Deleting ~2 keys from leveldb took 221499ns
I0407 22:34:11.601899 29303 replica.cpp:712] Persisted action at 10
I0407 22:34:11.602052 29303 replica.cpp:697] Replica learned TRUNCATE action at position 10
I0407 22:34:12.531602 29308 hierarchical.cpp:1586] No inverse offers to send out!
I0407 22:34:12.532578 29308 hierarchical.cpp:1142] Performed allocation for 3 agents in 3.892929ms
I0407 22:34:12.532403 29306 master.cpp:5524] Sending 1 offers to framework f59f9057-a5c7-43e1-b129-96862e640a12-0001 (default) at scheduler-37080386-2aa8-4592-bf09-8288bd04727a@172.17.0.3:35855
../../src/tests/master_allocator_tests.cpp:1587: Failure
Mock function called more times than expected - returning directly.
    Function call: resourceOffers(0x7fffe87e3370, @0x2adef432e6f0 { 144-byte object <E0-9C 76-EA DE-2A 00-00 00-00 00-00 00-00 00-00 1F-00 00-00 00-00 00-00 90-4B 00-2C DF-2A 00-00 30-6A 00-2C DF-2A 00-00 80-6A 00-2C DF-2A 00-00 20-62 00-2C DF-2A 00-00 60-38 00-2C DF-2A 00-00 ... 04-00 00-00 01-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00> })
         Expected: to be called once
           Actual: called twice - over-saturated and active
I0407 22:34:12.533665 29301 sched.cpp:876] Scheduler::resourceOffers took 250853ns
I0407 22:34:12.533915 29306 master.cpp:5524] Sending 1 offers to framework f59f9057-a5c7-43e1-b129-96862e640a12-0000 (default) at scheduler-72944dc9-e3cc-4ebc-bca6-d72c77ad6721@172.17.0.3:35855
I0407 22:34:12.534454 29306 sched.cpp:876] Scheduler::resourceOffers took 157733ns
../../src/tests/master_allocator_tests.cpp:1629: Failure
Value of: framework2offers.get().size()
  Actual: 1
Expected: 2u
Which is: 2
I0407 22:34:12.534997 29278 resources.cpp:572] Parsing resources as JSON failed: cpus:2;mem:1024;disk:4096;ports:[31000-32000]
Trying semicolon-delimited string format instead
I0407 22:34:12.537264 29301 master.cpp:1275] Framework f59f9057-a5c7-43e1-b129-96862e640a12-0001 (default) at scheduler-37080386-2aa8-4592-bf09-8288bd04727a@172.17.0.3:35855 disconnected
I0407 22:34:12.537297 29301 master.cpp:2658] Disconnecting framework f59f9057-a5c7-43e1-b129-96862e640a12-0001 (default) at scheduler-37080386-2aa8-4592-bf09-8288bd04727a@172.17.0.3:35855
I0407 22:34:12.537330 29301 master.cpp:2682] Deactivating framework f59f9057-a5c7-43e1-b129-96862e640a12-0001 (default) at scheduler-37080386-2aa8-4592-bf09-8288bd04727a@172.17.0.3:35855
W0407 22:34:12.537849 29301 master.hpp:1822] Master attempted to send message to disconnected framework f59f9057-a5c7-43e1-b129-96862e640a12-0001 (default) at scheduler-37080386-2aa8-4592-bf09-8288bd04727a@172.17.0.3:35855
W0407 22:34:12.538306 29301 master.hpp:1822] Master attempted to send message to disconnected framework f59f9057-a5c7-43e1-b129-96862e640a12-0001 (default) at scheduler-37080386-2aa8-4592-bf09-8288bd04727a@172.17.0.3:35855
I0407 22:34:12.538394 29301 master.cpp:1299] Giving framework f59f9057-a5c7-43e1-b129-96862e640a12-0001 (default) at scheduler-37080386-2aa8-4592-bf09-8288bd04727a@172.17.0.3:35855 0ns to failover
I0407 22:34:12.539371 29302 hierarchical.cpp:378] Deactivated framework f59f9057-a5c7-43e1-b129-96862e640a12-0001
I0407 22:34:12.540053 29302 hierarchical.cpp:894] Recovered cpus(*):2; mem(*):1024; disk(*):4096; ports(*):[31000-32000] (total: cpus(*):2; mem(*):1024; disk(*):4096; ports(*):[31000-32000], allocated: ) on agent f59f9057-a5c7-43e1-b129-96862e640a12-S0 from framework f59f9057-a5c7-43e1-b129-96862e640a12-0001
I0407 22:34:12.540732 29302 hierarchical.cpp:894] Recovered cpus(*):2; mem(*):1024; disk(*):4096; ports(*):[31000-32000] (total: cpus(*):2; mem(*):1024; disk(*):4096; ports(*):[31000-32000], allocated: ) on agent f59f9057-a5c7-43e1-b129-96862e640a12-S2 from framework f59f9057-a5c7-43e1-b129-96862e640a12-0001
I0407 22:34:12.540974 29301 master.cpp:1275] Framework f59f9057-a5c7-43e1-b129-96862e640a12-0000 (default) at scheduler-72944dc9-e3cc-4ebc-bca6-d72c77ad6721@172.17.0.3:35855 disconnected
I0407 22:34:12.541178 29301 master.cpp:2658] Disconnecting framework f59f9057-a5c7-43e1-b129-96862e640a12-0000 (default) at scheduler-72944dc9-e3cc-4ebc-bca6-d72c77ad6721@172.17.0.3:35855
I0407 22:34:12.541292 29301 master.cpp:2682] Deactivating framework f59f9057-a5c7-43e1-b129-96862e640a12-0000 (default) at scheduler-72944dc9-e3cc-4ebc-bca6-d72c77ad6721@172.17.0.3:35855
I0407 22:34:12.541553 29300 hierarchical.cpp:378] Deactivated framework f59f9057-a5c7-43e1-b129-96862e640a12-0000
I0407 22:34:12.542654 29300 hierarchical.cpp:894] Recovered cpus(*):2; mem(*):1024; disk(*):4096; ports(*):[31000-32000] (total: cpus(*):2; mem(*):1024; disk(*):4096; ports(*):[31000-32000], allocated: ) on agent f59f9057-a5c7-43e1-b129-96862e640a12-S1 from framework f59f9057-a5c7-43e1-b129-96862e640a12-0000
W0407 22:34:12.543051 29301 master.hpp:1822] Master attempted to send message to disconnected framework f59f9057-a5c7-43e1-b129-96862e640a12-0000 (default) at scheduler-72944dc9-e3cc-4ebc-bca6-d72c77ad6721@172.17.0.3:35855
I0407 22:34:12.543525 29301 master.cpp:1299] Giving framework f59f9057-a5c7-43e1-b129-96862e640a12-0000 (default) at scheduler-72944dc9-e3cc-4ebc-bca6-d72c77ad6721@172.17.0.3:35855 0ns to failover
I0407 22:34:12.543861 29301 master.cpp:5376] Framework failover timeout, removing framework f59f9057-a5c7-43e1-b129-96862e640a12-0001 (default) at scheduler-37080386-2aa8-4592-bf09-8288bd04727a@172.17.0.3:35855
I0407 22:34:12.543959 29301 master.cpp:6109] Removing framework f59f9057-a5c7-43e1-b129-96862e640a12-0001 (default) at scheduler-37080386-2aa8-4592-bf09-8288bd04727a@172.17.0.3:35855
I0407 22:34:12.544445 29301 slave.cpp:2226] Asked to shut down framework f59f9057-a5c7-43e1-b129-96862e640a12-0001 by master@172.17.0.3:35855
W0407 22:34:12.545446 29301 slave.cpp:2241] Cannot shut down unknown framework f59f9057-a5c7-43e1-b129-96862e640a12-0001
I0407 22:34:12.544556 29300 slave.cpp:2226] Asked to shut down framework f59f9057-a5c7-43e1-b129-96862e640a12-0001 by master@172.17.0.3:35855
W0407 22:34:12.545661 29300 slave.cpp:2241] Cannot shut down unknown framework f59f9057-a5c7-43e1-b129-96862e640a12-0001
I0407 22:34:12.545774 29300 slave.cpp:811] Agent terminating
I0407 22:34:12.544791 29305 hierarchical.cpp:329] Removed framework f59f9057-a5c7-43e1-b129-96862e640a12-0001
I0407 22:34:12.545241 29296 master.cpp:5376] Framework failover timeout, removing framework f59f9057-a5c7-43e1-b129-96862e640a12-0000 (default) at scheduler-72944dc9-e3cc-4ebc-bca6-d72c77ad6721@172.17.0.3:35855
I0407 22:34:12.544518 29302 slave.cpp:2226] Asked to shut down framework f59f9057-a5c7-43e1-b129-96862e640a12-0001 by master@172.17.0.3:35855
I0407 22:34:12.546140 29296 master.cpp:6109] Removing framework f59f9057-a5c7-43e1-b129-96862e640a12-0000 (default) at scheduler-72944dc9-e3cc-4ebc-bca6-d72c77ad6721@172.17.0.3:35855
W0407 22:34:12.546159 29302 slave.cpp:2241] Cannot shut down unknown framework f59f9057-a5c7-43e1-b129-96862e640a12-0001
I0407 22:34:12.546496 29296 master.cpp:1236] Agent f59f9057-a5c7-43e1-b129-96862e640a12-S0 at slave(111)@172.17.0.3:35855 (129e11060069) disconnected
I0407 22:34:12.546527 29296 master.cpp:2717] Disconnecting agent f59f9057-a5c7-43e1-b129-96862e640a12-S0 at slave(111)@172.17.0.3:35855 (129e11060069)
I0407 22:34:12.546581 29296 master.cpp:2736] Deactivating agent f59f9057-a5c7-43e1-b129-96862e640a12-S0 at slave(111)@172.17.0.3:35855 (129e11060069)
I0407 22:34:12.546752 29296 slave.cpp:2226] Asked to shut down framework f59f9057-a5c7-43e1-b129-96862e640a12-0000 by master@172.17.0.3:35855
W0407 22:34:12.546782 29296 slave.cpp:2241] Cannot shut down unknown framework f59f9057-a5c7-43e1-b129-96862e640a12-0000
I0407 22:34:12.546844 29296 slave.cpp:2226] Asked to shut down framework f59f9057-a5c7-43e1-b129-96862e640a12-0000 by master@172.17.0.3:35855
W0407 22:34:12.546869 29296 slave.cpp:2241] Cannot shut down unknown framework f59f9057-a5c7-43e1-b129-96862e640a12-0000
I0407 22:34:12.547111 29296 hierarchical.cpp:329] Removed framework f59f9057-a5c7-43e1-b129-96862e640a12-0000
I0407 22:34:12.547302 29296 hierarchical.cpp:563] Agent f59f9057-a5c7-43e1-b129-96862e640a12-S0 deactivated
I0407 22:34:12.553478 29278 slave.cpp:811] Agent terminating
I0407 22:34:12.553766 29306 master.cpp:1236] Agent f59f9057-a5c7-43e1-b129-96862e640a12-S1 at slave(112)@172.17.0.3:35855 (129e11060069) disconnected
I0407 22:34:12.555483 29306 master.cpp:2717] Disconnecting agent f59f9057-a5c7-43e1-b129-96862e640a12-S1 at slave(112)@172.17.0.3:35855 (129e11060069)
I0407 22:34:12.555858 29306 master.cpp:2736] Deactivating agent f59f9057-a5c7-43e1-b129-96862e640a12-S1 at slave(112)@172.17.0.3:35855 (129e11060069)
I0407 22:34:12.556190 29307 hierarchical.cpp:563] Agent f59f9057-a5c7-43e1-b129-96862e640a12-S1 deactivated
I0407 22:34:12.559095 29299 slave.cpp:811] Agent terminating
I0407 22:34:12.559301 29300 master.cpp:1236] Agent f59f9057-a5c7-43e1-b129-96862e640a12-S2 at slave(113)@172.17.0.3:35855 (129e11060069) disconnected
I0407 22:34:12.559327 29300 master.cpp:2717] Disconnecting agent f59f9057-a5c7-43e1-b129-96862e640a12-S2 at slave(113)@172.17.0.3:35855 (129e11060069)
I0407 22:34:12.559370 29300 master.cpp:2736] Deactivating agent f59f9057-a5c7-43e1-b129-96862e640a12-S2 at slave(113)@172.17.0.3:35855 (129e11060069)
I0407 22:34:12.559516 29309 hierarchical.cpp:563] Agent f59f9057-a5c7-43e1-b129-96862e640a12-S2 deactivated
I0407 22:34:12.561872 29278 master.cpp:1089] Master terminating
I0407 22:34:12.562566 29304 hierarchical.cpp:508] Removed agent f59f9057-a5c7-43e1-b129-96862e640a12-S2
I0407 22:34:12.562890 29304 hierarchical.cpp:508] Removed agent f59f9057-a5c7-43e1-b129-96862e640a12-S1
I0407 22:34:12.565459 29304 hierarchical.cpp:508] Removed agent f59f9057-a5c7-43e1-b129-96862e640a12-S0
[  FAILED  ] MasterAllocatorTest/1.RebalancedForUpdatedWeights, where TypeParam = mesos::internal::tests::Module<mesos::master::allocator::Allocator, (mesos::internal::tests::ModuleID)6> (2240 ms)
{code}",Bug,Major,greggomann,2016-04-21T12:29:48.000+0000,5,Resolved,Complete,MasterAllocatorTest/1.RebalancedForUpdatedWeights is flaky.,2016-04-21T12:30:13.000+0000,MESOS-5146,1.0,mesos,Mesosphere Sprint 33
nfnt,2016-04-07T08:39:17.000+0000,nfnt,"Flags should be added to the agent to:
1. Enable authorization ({{--authorizers}})
2. Provide ACLs ({{--acls}})",Bug,Major,nfnt,2016-04-27T16:25:26.000+0000,5,Resolved,Complete,Add agent flags for HTTP authorization.,2016-04-27T16:25:33.000+0000,MESOS-5142,2.0,mesos,Mesosphere Sprint 32
gilbert,2016-04-07T03:45:42.000+0000,vinodkone,"Found this on ASF CI while testing 0.28.1-rc2

{code}
[ RUN      ] ProvisionerDockerLocalStoreTest.LocalStoreTestWithTar
E0406 18:29:30.870481   520 shell.hpp:93] Command 'hadoop version 2>&1' failed; this is the output:
sh: 1: hadoop: not found
E0406 18:29:30.870576   520 fetcher.cpp:59] Failed to create URI fetcher plugin 'hadoop': Failed to create HDFS client: Failed to execute 'hadoop version 2>&1'; the command was either not found or exited with a non-zero exit status: 127
I0406 18:29:30.871052   520 local_puller.cpp:90] Creating local puller with docker registry '/tmp/3l8ZBv/images'
I0406 18:29:30.873325   539 metadata_manager.cpp:159] Looking for image 'abc'
I0406 18:29:30.874438   539 local_puller.cpp:142] Untarring image 'abc' from '/tmp/3l8ZBv/images/abc.tar' to '/tmp/3l8ZBv/store/staging/5tw8bD'
I0406 18:29:30.901916   547 local_puller.cpp:162] The repositories JSON file for image 'abc' is '{""abc"":{""latest"":""456""}}'
I0406 18:29:30.902304   547 local_puller.cpp:290] Extracting layer tar ball '/tmp/3l8ZBv/store/staging/5tw8bD/123/layer.tar to rootfs '/tmp/3l8ZBv/store/staging/5tw8bD/123/rootfs'
I0406 18:29:30.909144   547 local_puller.cpp:290] Extracting layer tar ball '/tmp/3l8ZBv/store/staging/5tw8bD/456/layer.tar to rootfs '/tmp/3l8ZBv/store/staging/5tw8bD/456/rootfs'
../../src/tests/containerizer/provisioner_docker_tests.cpp:183: Failure
(imageInfo).failure(): Collect failed: Subprocess 'tar, tar, -x, -f, /tmp/3l8ZBv/store/staging/5tw8bD/456/layer.tar, -C, /tmp/3l8ZBv/store/staging/5tw8bD/456/rootfs' failed: tar: This does not look like a tar archive
tar: Exiting with failure status due to previous errors

[  FAILED  ] ProvisionerDockerLocalStoreTest.LocalStoreTestWithTar (243 ms)
{code}",Bug,Major,vinodkone,,1,Open,New,ProvisionerDockerLocalStoreTest.LocalStoreTestWithTar is flaky,2016-04-30T16:36:46.000+0000,MESOS-5139,2.0,mesos,Mesosphere Sprint 33
klueska,2016-04-07T01:40:57.000+0000,klueska,An update to master the day after all of the Nvidia GPU stuff landed has a build error in the Nvidia GPU tests. The namespace that MasterDetector lives in has changed and the test needs to be updated to pull in the class from the proper namespace now.,Bug,Major,klueska,2016-04-08T21:20:40.000+0000,5,Resolved,Complete,Fix Nvidia GPU test build for namespace change of MasterDetector,2016-04-08T21:20:40.000+0000,MESOS-5138,1.0,mesos,Mesosphere Sprint 32
klueska,2016-04-07T01:25:01.000+0000,klueska,This file is no longer in use anywhere.,Task,Major,klueska,2016-04-08T19:39:08.000+0000,5,Resolved,Complete,Remove 'dashboard.js' from the webui.,2016-04-08T19:39:08.000+0000,MESOS-5137,1.0,mesos,Mesosphere Sprint 32
klueska,2016-04-07T01:21:14.000+0000,klueska,"The default JSON representation of a Resource currently lists a value of ""0"" if no value is set on a first class SCALAR resource (i.e. cpus, mem, disk).  We should add GPUs in here as well.  ",Task,Major,klueska,2016-04-08T21:22:14.000+0000,5,Resolved,Complete,Update the default JSON representation of a Resource to include GPUs,2016-04-08T21:22:14.000+0000,MESOS-5136,1.0,mesos,Mesosphere Sprint 32
klueska,2016-04-07T01:16:51.000+0000,klueska,"Specifically, the documentation in the following files should be udated:

{noformat}
docs/attributes-resources.md
docs/monitoring.md
{noformat}",Task,Major,klueska,2016-04-08T21:23:46.000+0000,5,Resolved,Complete,Update existing documentation to Include references to GPUs as a first class resource.,2016-04-08T21:23:46.000+0000,MESOS-5135,1.0,mesos,Mesosphere Sprint 32
,2016-04-06T22:20:52.000+0000,greggomann,It would be helpful if the TaskStatus lists provided by the master's {{/state}} endpoint included the {{source}} and {{reason}} associated with the status message. The JSON modeling function for TaskStatus should be extended to include these fields.,Improvement,Major,greggomann,,1,Open,New,Expose TaskStatus source & reason in master's '/state' output,2016-04-06T22:20:52.000+0000,MESOS-5133,1.0,mesos,
mcypark,2016-04-06T21:40:22.000+0000,mcypark,"In verbose mode (i.e., {{git commit --verbose}}), the commit message includes the diff of the commit at the bottom, delimited by the following lines:

{code}
# ------------------------ >8 ------------------------
# Do not touch the line above.
# Everything below will be removed.
{code}

We should {{break}} once we encounter such a line.",Bug,Major,mcypark,2016-04-06T23:09:08.000+0000,5,Resolved,Complete,Commit message hook lints the diff in verbose mode.,2016-04-06T23:09:08.000+0000,MESOS-5132,2.0,mesos,Mesosphere Sprint 32
avinash@mesosphere.io,2016-04-06T15:48:10.000+0000,avinash@mesosphere.io,"Currently there are no default `network` isolators for `MesosContainerizer`. With the development of the `network/cni` isolator we have an interface to run Mesos on multitude of IP networks. Given that its based on an open standard (the CNI spec) which is gathering a lot of traction from vendors (calico, weave, coreOS) and already works on some default networks (bridge, ipvlan, macvlan) it makes sense to make it as the default network isolator.",Task,Major,avinash@mesosphere.io,2016-04-14T16:19:21.000+0000,5,Resolved,Complete,Enable `newtork/cni` isolator in `MesosContainerizer` as the default `network` isolator.,2016-04-14T16:19:21.000+0000,MESOS-5130,1.0,mesos,Mesosphere Sprint 32
neilc,2016-04-06T05:19:18.000+0000,greggomann,"Observed on ASF CI:

{code}
[ RUN      ] DiskResource/PersistentVolumeTest.AccessPersistentVolume/0
I0405 17:29:19.134435 31837 cluster.cpp:139] Creating default 'local' authorizer
I0405 17:29:19.251143 31837 leveldb.cpp:174] Opened db in 116.386403ms
I0405 17:29:19.310050 31837 leveldb.cpp:181] Compacted db in 58.80688ms
I0405 17:29:19.310180 31837 leveldb.cpp:196] Created db iterator in 37145ns
I0405 17:29:19.310199 31837 leveldb.cpp:202] Seeked to beginning of db in 4212ns
I0405 17:29:19.310210 31837 leveldb.cpp:271] Iterated through 0 keys in the db in 410ns
I0405 17:29:19.310279 31837 replica.cpp:779] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned
I0405 17:29:19.311069 31861 recover.cpp:447] Starting replica recovery
I0405 17:29:19.311362 31861 recover.cpp:473] Replica is in EMPTY status
I0405 17:29:19.312641 31861 replica.cpp:673] Replica in EMPTY status received a broadcasted recover request from (14359)@172.17.0.4:43972
I0405 17:29:19.313045 31860 recover.cpp:193] Received a recover response from a replica in EMPTY status
I0405 17:29:19.313608 31860 recover.cpp:564] Updating replica status to STARTING
I0405 17:29:19.316416 31867 master.cpp:376] Master 9565ff6f-f1b6-4259-8430-690e635c391f (4090d10eba90) started on 172.17.0.4:43972
I0405 17:29:19.316470 31867 master.cpp:378] Flags at startup: --acls="""" --allocation_interval=""1secs"" --allocator=""HierarchicalDRF"" --authenticate=""true"" --authenticate_http=""true"" --authenticate_slaves=""true"" --authenticators=""crammd5"" --authorizers=""local"" --credentials=""/tmp/0A9ELu/credentials"" --framework_sorter=""drf"" --help=""false"" --hostname_lookup=""true"" --http_authenticators=""basic"" --initialize_driver_logging=""true"" --log_auto_initialize=""true"" --logbufsecs=""0"" --logging_level=""INFO"" --max_completed_frameworks=""50"" --max_completed_tasks_per_framework=""1000"" --max_slave_ping_timeouts=""5"" --quiet=""false"" --recovery_slave_removal_limit=""100%"" --registry=""replicated_log"" --registry_fetch_timeout=""1mins"" --registry_store_timeout=""100secs"" --registry_strict=""true"" --root_submissions=""true"" --slave_ping_timeout=""15secs"" --slave_reregister_timeout=""10mins"" --user_sorter=""drf"" --version=""false"" --webui_dir=""/mesos/mesos-0.29.0/_inst/share/mesos/webui"" --work_dir=""/tmp/0A9ELu/master"" --zk_session_timeout=""10secs""
I0405 17:29:19.316938 31867 master.cpp:427] Master only allowing authenticated frameworks to register
I0405 17:29:19.316951 31867 master.cpp:432] Master only allowing authenticated agents to register
I0405 17:29:19.316961 31867 credentials.hpp:37] Loading credentials for authentication from '/tmp/0A9ELu/credentials'
I0405 17:29:19.317402 31867 master.cpp:474] Using default 'crammd5' authenticator
I0405 17:29:19.317643 31867 master.cpp:545] Using default 'basic' HTTP authenticator
I0405 17:29:19.317854 31867 master.cpp:583] Authorization enabled
I0405 17:29:19.318081 31864 whitelist_watcher.cpp:77] No whitelist given
I0405 17:29:19.318079 31861 hierarchical.cpp:144] Initialized hierarchical allocator process
I0405 17:29:19.320838 31864 master.cpp:1826] The newly elected leader is master@172.17.0.4:43972 with id 9565ff6f-f1b6-4259-8430-690e635c391f
I0405 17:29:19.320888 31864 master.cpp:1839] Elected as the leading master!
I0405 17:29:19.320909 31864 master.cpp:1526] Recovering from registrar
I0405 17:29:19.321218 31871 registrar.cpp:331] Recovering registrar
I0405 17:29:19.347045 31860 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 33.164133ms
I0405 17:29:19.347126 31860 replica.cpp:320] Persisted replica status to STARTING
I0405 17:29:19.347611 31869 recover.cpp:473] Replica is in STARTING status
I0405 17:29:19.349215 31871 replica.cpp:673] Replica in STARTING status received a broadcasted recover request from (14361)@172.17.0.4:43972
I0405 17:29:19.349653 31870 recover.cpp:193] Received a recover response from a replica in STARTING status
I0405 17:29:19.350236 31866 recover.cpp:564] Updating replica status to VOTING
I0405 17:29:19.388882 31864 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 38.38299ms
I0405 17:29:19.388993 31864 replica.cpp:320] Persisted replica status to VOTING
I0405 17:29:19.389369 31856 recover.cpp:578] Successfully joined the Paxos group
I0405 17:29:19.389735 31856 recover.cpp:462] Recover process terminated
I0405 17:29:19.390476 31868 log.cpp:659] Attempting to start the writer
I0405 17:29:19.392125 31862 replica.cpp:493] Replica received implicit promise request from (14362)@172.17.0.4:43972 with proposal 1
I0405 17:29:19.430706 31862 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 38.505062ms
I0405 17:29:19.430816 31862 replica.cpp:342] Persisted promised to 1
I0405 17:29:19.431918 31856 coordinator.cpp:238] Coordinator attempting to fill missing positions
I0405 17:29:19.433725 31861 replica.cpp:388] Replica received explicit promise request from (14363)@172.17.0.4:43972 for position 0 with proposal 2
I0405 17:29:19.472491 31861 leveldb.cpp:341] Persisting action (8 bytes) to leveldb took 38.659492ms
I0405 17:29:19.472595 31861 replica.cpp:712] Persisted action at 0
I0405 17:29:19.474556 31864 replica.cpp:537] Replica received write request for position 0 from (14364)@172.17.0.4:43972
I0405 17:29:19.474652 31864 leveldb.cpp:436] Reading position from leveldb took 49423ns
I0405 17:29:19.528175 31864 leveldb.cpp:341] Persisting action (14 bytes) to leveldb took 53.443616ms
I0405 17:29:19.528300 31864 replica.cpp:712] Persisted action at 0
I0405 17:29:19.529389 31865 replica.cpp:691] Replica received learned notice for position 0 from @0.0.0.0:0
I0405 17:29:19.571137 31865 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 41.676495ms
I0405 17:29:19.571254 31865 replica.cpp:712] Persisted action at 0
I0405 17:29:19.571302 31865 replica.cpp:697] Replica learned NOP action at position 0
I0405 17:29:19.572322 31856 log.cpp:675] Writer started with ending position 0
I0405 17:29:19.574060 31861 leveldb.cpp:436] Reading position from leveldb took 83200ns
I0405 17:29:19.575417 31864 registrar.cpp:364] Successfully fetched the registry (0B) in 0ns
I0405 17:29:19.575565 31864 registrar.cpp:463] Applied 1 operations in 46419ns; attempting to update the 'registry'
I0405 17:29:19.576517 31857 log.cpp:683] Attempting to append 170 bytes to the log
I0405 17:29:19.576849 31857 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 1
I0405 17:29:19.578390 31857 replica.cpp:537] Replica received write request for position 1 from (14365)@172.17.0.4:43972
I0405 17:29:19.780277 31857 leveldb.cpp:341] Persisting action (189 bytes) to leveldb took 201.808617ms
I0405 17:29:19.780366 31857 replica.cpp:712] Persisted action at 1
I0405 17:29:19.782024 31857 replica.cpp:691] Replica received learned notice for position 1 from @0.0.0.0:0
I0405 17:29:19.823770 31857 leveldb.cpp:341] Persisting action (191 bytes) to leveldb took 41.667662ms
I0405 17:29:19.823851 31857 replica.cpp:712] Persisted action at 1
I0405 17:29:19.823889 31857 replica.cpp:697] Replica learned APPEND action at position 1
I0405 17:29:19.825701 31867 registrar.cpp:508] Successfully updated the 'registry' in 0ns
I0405 17:29:19.825929 31867 registrar.cpp:394] Successfully recovered registrar
I0405 17:29:19.826015 31857 log.cpp:702] Attempting to truncate the log to 1
I0405 17:29:19.826262 31867 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 2
I0405 17:29:19.827647 31867 replica.cpp:537] Replica received write request for position 2 from (14366)@172.17.0.4:43972
I0405 17:29:19.828018 31857 master.cpp:1634] Recovered 0 agents from the Registry (131B) ; allowing 10mins for agents to re-register
I0405 17:29:19.828065 31861 hierarchical.cpp:171] Skipping recovery of hierarchical allocator: nothing to recover
I0405 17:29:19.865555 31867 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 37.822178ms
I0405 17:29:19.865661 31867 replica.cpp:712] Persisted action at 2
I0405 17:29:19.866921 31867 replica.cpp:691] Replica received learned notice for position 2 from @0.0.0.0:0
I0405 17:29:19.907341 31867 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 40.356649ms
I0405 17:29:19.907531 31867 leveldb.cpp:399] Deleting ~1 keys from leveldb took 91109ns
I0405 17:29:19.907560 31867 replica.cpp:712] Persisted action at 2
I0405 17:29:19.907599 31867 replica.cpp:697] Replica learned TRUNCATE action at position 2
I0405 17:29:19.923305 31837 resources.cpp:572] Parsing resources as JSON failed: cpus:2;mem:2048
Trying semicolon-delimited string format instead
I0405 17:29:19.926491 31837 containerizer.cpp:155] Using isolation: posix/cpu,posix/mem,filesystem/posix
W0405 17:29:19.927836 31837 backend.cpp:66] Failed to create 'bind' backend: BindBackend requires root privileges
I0405 17:29:19.932029 31862 slave.cpp:200] Agent started on 441)@172.17.0.4:43972
I0405 17:29:19.932086 31862 slave.cpp:201] Flags at startup: --appc_simple_discovery_uri_prefix=""http://"" --appc_store_dir=""/tmp/mesos/store/appc"" --authenticate_http=""true"" --authenticatee=""crammd5"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --credential=""/tmp/DiskResource_PersistentVolumeTest_AccessPersistentVolume_0_fJS7AC/credential"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_kill_orphans=""true"" --docker_registry=""https://registry-1.docker.io"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/tmp/mesos/store/docker"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/tmp/DiskResource_PersistentVolumeTest_AccessPersistentVolume_0_fJS7AC/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --hostname_lookup=""true"" --http_authenticators=""basic"" --http_credentials=""/tmp/DiskResource_PersistentVolumeTest_AccessPersistentVolume_0_fJS7AC/http_credentials"" --image_provisioner_backend=""copy"" --initialize_driver_logging=""true"" --isolation=""posix/cpu,posix/mem"" --launcher_dir=""/mesos/mesos-0.29.0/_build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""10ms"" --resources=""[{""name"":""cpus"",""role"":""*"",""scalar"":{""value"":2.0},""type"":""SCALAR""},{""name"":""mem"",""role"":""*"",""scalar"":{""value"":2048.0},""type"":""SCALAR""},{""name"":""disk"",""role"":""role1"",""scalar"":{""value"":4096.0},""type"":""SCALAR""}]"" --revocable_cpu_low_priority=""true"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --systemd_enable_support=""true"" --systemd_runtime_directory=""/run/systemd/system"" --version=""false"" --work_dir=""/tmp/DiskResource_PersistentVolumeTest_AccessPersistentVolume_0_fJS7AC""
I0405 17:29:19.932665 31862 credentials.hpp:86] Loading credential for authentication from '/tmp/DiskResource_PersistentVolumeTest_AccessPersistentVolume_0_fJS7AC/credential'
I0405 17:29:19.932934 31862 slave.cpp:338] Agent using credential for: test-principal
I0405 17:29:19.932968 31862 credentials.hpp:37] Loading credentials for authentication from '/tmp/DiskResource_PersistentVolumeTest_AccessPersistentVolume_0_fJS7AC/http_credentials'
I0405 17:29:19.933284 31862 slave.cpp:390] Using default 'basic' HTTP authenticator
I0405 17:29:19.934916 31837 sched.cpp:222] Version: 0.29.0
I0405 17:29:19.935566 31862 slave.cpp:589] Agent resources: cpus(*):2; mem(*):2048; disk(role1):4096; ports(*):[31000-32000]
I0405 17:29:19.935664 31862 slave.cpp:597] Agent attributes: [  ]
I0405 17:29:19.935679 31862 slave.cpp:602] Agent hostname: 4090d10eba90
I0405 17:29:19.938390 31864 state.cpp:57] Recovering state from '/tmp/DiskResource_PersistentVolumeTest_AccessPersistentVolume_0_fJS7AC/meta'
I0405 17:29:19.940608 31869 sched.cpp:326] New master detected at master@172.17.0.4:43972
I0405 17:29:19.940749 31869 sched.cpp:382] Authenticating with master master@172.17.0.4:43972
I0405 17:29:19.940773 31869 sched.cpp:389] Using default CRAM-MD5 authenticatee
I0405 17:29:19.942371 31869 authenticatee.cpp:121] Creating new client SASL connection
I0405 17:29:19.942873 31859 master.cpp:5679] Authenticating scheduler-bdf68f7f-d938-47ed-a132-bb3f218628bf@172.17.0.4:43972
I0405 17:29:19.943156 31859 authenticator.cpp:413] Starting authentication session for crammd5_authenticatee(896)@172.17.0.4:43972
I0405 17:29:19.943507 31863 authenticator.cpp:98] Creating new server SASL connection
I0405 17:29:19.943740 31859 authenticatee.cpp:212] Received SASL authentication mechanisms: CRAM-MD5
I0405 17:29:19.943783 31859 authenticatee.cpp:238] Attempting to authenticate with mechanism 'CRAM-MD5'
I0405 17:29:19.943892 31859 authenticator.cpp:203] Received SASL authentication start
I0405 17:29:19.943977 31859 authenticator.cpp:325] Authentication requires more steps
I0405 17:29:19.944066 31859 authenticatee.cpp:258] Received SASL authentication step
I0405 17:29:19.944164 31859 authenticator.cpp:231] Received SASL authentication step
I0405 17:29:19.944193 31859 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: '4090d10eba90' server FQDN: '4090d10eba90' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0405 17:29:19.944206 31859 auxprop.cpp:179] Looking up auxiliary property '*userPassword'
I0405 17:29:19.944268 31859 auxprop.cpp:179] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0405 17:29:19.944300 31859 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: '4090d10eba90' server FQDN: '4090d10eba90' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0405 17:29:19.944313 31859 auxprop.cpp:129] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0405 17:29:19.944321 31859 auxprop.cpp:129] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0405 17:29:19.944339 31859 authenticator.cpp:317] Authentication success
I0405 17:29:19.944541 31859 authenticatee.cpp:298] Authentication success
I0405 17:29:19.944655 31859 master.cpp:5709] Successfully authenticated principal 'test-principal' at scheduler-bdf68f7f-d938-47ed-a132-bb3f218628bf@172.17.0.4:43972
I0405 17:29:19.944737 31859 authenticator.cpp:431] Authentication session cleanup for crammd5_authenticatee(896)@172.17.0.4:43972
I0405 17:29:19.945111 31859 sched.cpp:472] Successfully authenticated with master master@172.17.0.4:43972
I0405 17:29:19.945132 31859 sched.cpp:777] Sending SUBSCRIBE call to master@172.17.0.4:43972
I0405 17:29:19.945591 31859 sched.cpp:810] Will retry registration in 372.80738ms if necessary
I0405 17:29:19.945744 31865 master.cpp:2346] Received SUBSCRIBE call for framework 'default' at scheduler-bdf68f7f-d938-47ed-a132-bb3f218628bf@172.17.0.4:43972
I0405 17:29:19.945838 31865 master.cpp:1865] Authorizing framework principal 'test-principal' to receive offers for role 'role1'
I0405 17:29:19.946194 31865 master.cpp:2417] Subscribing framework default with checkpointing disabled and capabilities [  ]
I0405 17:29:19.946866 31866 hierarchical.cpp:266] Added framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000
I0405 17:29:19.946974 31866 hierarchical.cpp:1490] No resources available to allocate!
I0405 17:29:19.947010 31866 hierarchical.cpp:1585] No inverse offers to send out!
I0405 17:29:19.947054 31865 sched.cpp:704] Framework registered with 9565ff6f-f1b6-4259-8430-690e635c391f-0000
I0405 17:29:19.947074 31866 hierarchical.cpp:1141] Performed allocation for 0 agents in 178242ns
I0405 17:29:19.947124 31865 sched.cpp:718] Scheduler::registered took 38907ns
I0405 17:29:19.948712 31866 status_update_manager.cpp:200] Recovering status update manager
I0405 17:29:19.948901 31866 containerizer.cpp:416] Recovering containerizer
I0405 17:29:19.951021 31866 provisioner.cpp:245] Provisioner recovery complete
I0405 17:29:19.951802 31866 slave.cpp:4773] Finished recovery
I0405 17:29:19.952518 31866 slave.cpp:4945] Querying resource estimator for oversubscribable resources
I0405 17:29:19.953248 31866 slave.cpp:928] New master detected at master@172.17.0.4:43972
I0405 17:29:19.953305 31865 status_update_manager.cpp:174] Pausing sending status updates
I0405 17:29:19.953626 31866 slave.cpp:991] Authenticating with master master@172.17.0.4:43972
I0405 17:29:19.953716 31866 slave.cpp:996] Using default CRAM-MD5 authenticatee
I0405 17:29:19.954074 31866 slave.cpp:964] Detecting new master
I0405 17:29:19.954167 31861 authenticatee.cpp:121] Creating new client SASL connection
I0405 17:29:19.954372 31866 slave.cpp:4959] Received oversubscribable resources  from the resource estimator
I0405 17:29:19.954756 31866 master.cpp:5679] Authenticating slave(441)@172.17.0.4:43972
I0405 17:29:19.954944 31861 authenticator.cpp:413] Starting authentication session for crammd5_authenticatee(897)@172.17.0.4:43972
I0405 17:29:19.955368 31863 authenticator.cpp:98] Creating new server SASL connection
I0405 17:29:19.955687 31861 authenticatee.cpp:212] Received SASL authentication mechanisms: CRAM-MD5
I0405 17:29:19.955801 31861 authenticatee.cpp:238] Attempting to authenticate with mechanism 'CRAM-MD5'
I0405 17:29:19.956075 31861 authenticator.cpp:203] Received SASL authentication start
I0405 17:29:19.956279 31861 authenticator.cpp:325] Authentication requires more steps
I0405 17:29:19.956455 31861 authenticatee.cpp:258] Received SASL authentication step
I0405 17:29:19.956676 31861 authenticator.cpp:231] Received SASL authentication step
I0405 17:29:19.956815 31861 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: '4090d10eba90' server FQDN: '4090d10eba90' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0405 17:29:19.956907 31861 auxprop.cpp:179] Looking up auxiliary property '*userPassword'
I0405 17:29:19.957044 31861 auxprop.cpp:179] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0405 17:29:19.957166 31861 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: '4090d10eba90' server FQDN: '4090d10eba90' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0405 17:29:19.957264 31861 auxprop.cpp:129] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0405 17:29:19.957353 31861 auxprop.cpp:129] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0405 17:29:19.957449 31861 authenticator.cpp:317] Authentication success
I0405 17:29:19.957664 31857 authenticatee.cpp:298] Authentication success
I0405 17:29:19.957813 31857 master.cpp:5709] Successfully authenticated principal 'test-principal' at slave(441)@172.17.0.4:43972
I0405 17:29:19.958008 31861 authenticator.cpp:431] Authentication session cleanup for crammd5_authenticatee(897)@172.17.0.4:43972
I0405 17:29:19.958732 31857 slave.cpp:1061] Successfully authenticated with master master@172.17.0.4:43972
I0405 17:29:19.958930 31857 slave.cpp:1457] Will retry registration in 18.568334ms if necessary
I0405 17:29:19.959262 31857 master.cpp:4390] Registering agent at slave(441)@172.17.0.4:43972 (4090d10eba90) with id 9565ff6f-f1b6-4259-8430-690e635c391f-S0
I0405 17:29:19.959934 31857 registrar.cpp:463] Applied 1 operations in 99197ns; attempting to update the 'registry'
I0405 17:29:19.961587 31857 log.cpp:683] Attempting to append 343 bytes to the log
I0405 17:29:19.961879 31857 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 3
I0405 17:29:19.963135 31857 replica.cpp:537] Replica received write request for position 3 from (14381)@172.17.0.4:43972
I0405 17:29:19.999408 31857 leveldb.cpp:341] Persisting action (362 bytes) to leveldb took 36.200109ms
I0405 17:29:19.999512 31857 replica.cpp:712] Persisted action at 3
I0405 17:29:20.001049 31869 replica.cpp:691] Replica received learned notice for position 3 from @0.0.0.0:0
I0405 17:29:20.038849 31869 leveldb.cpp:341] Persisting action (364 bytes) to leveldb took 37.709507ms
I0405 17:29:20.038930 31869 replica.cpp:712] Persisted action at 3
I0405 17:29:20.038965 31869 replica.cpp:697] Replica learned APPEND action at position 3
I0405 17:29:20.041484 31869 registrar.cpp:508] Successfully updated the 'registry' in 0ns
I0405 17:29:20.041785 31869 log.cpp:702] Attempting to truncate the log to 3
I0405 17:29:20.042364 31859 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 4
I0405 17:29:20.043767 31859 replica.cpp:537] Replica received write request for position 4 from (14382)@172.17.0.4:43972
I0405 17:29:20.044585 31869 master.cpp:4458] Registered agent 9565ff6f-f1b6-4259-8430-690e635c391f-S0 at slave(441)@172.17.0.4:43972 (4090d10eba90) with cpus(*):2; mem(*):2048; disk(role1):4096; ports(*):[31000-32000]
I0405 17:29:20.044910 31864 slave.cpp:1105] Registered with master master@172.17.0.4:43972; given agent ID 9565ff6f-f1b6-4259-8430-690e635c391f-S0
I0405 17:29:20.045075 31864 fetcher.cpp:81] Clearing fetcher cache
I0405 17:29:20.045140 31870 hierarchical.cpp:476] Added agent 9565ff6f-f1b6-4259-8430-690e635c391f-S0 (4090d10eba90) with cpus(*):2; mem(*):2048; disk(role1):4096; ports(*):[31000-32000] (allocated: )
I0405 17:29:20.045581 31864 slave.cpp:1128] Checkpointing SlaveInfo to '/tmp/DiskResource_PersistentVolumeTest_AccessPersistentVolume_0_fJS7AC/meta/slaves/9565ff6f-f1b6-4259-8430-690e635c391f-S0/slave.info'
I0405 17:29:20.045974 31864 slave.cpp:1165] Forwarding total oversubscribed resources 
I0405 17:29:20.046077 31864 slave.cpp:3664] Received ping from slave-observer(399)@172.17.0.4:43972
I0405 17:29:20.046193 31864 status_update_manager.cpp:181] Resuming sending status updates
I0405 17:29:20.046289 31870 hierarchical.cpp:1585] No inverse offers to send out!
I0405 17:29:20.046370 31870 hierarchical.cpp:1164] Performed allocation for agent 9565ff6f-f1b6-4259-8430-690e635c391f-S0 in 1.153377ms
I0405 17:29:20.046499 31864 master.cpp:4802] Received update of agent 9565ff6f-f1b6-4259-8430-690e635c391f-S0 at slave(441)@172.17.0.4:43972 (4090d10eba90) with total oversubscribed resources 
I0405 17:29:20.047142 31868 hierarchical.cpp:534] Agent 9565ff6f-f1b6-4259-8430-690e635c391f-S0 (4090d10eba90) updated with oversubscribed resources  (total: cpus(*):2; mem(*):2048; disk(role1):4096; ports(*):[31000-32000], allocated: disk(role1):4096; cpus(*):2; mem(*):2048; ports(*):[31000-32000])
I0405 17:29:20.047960 31868 hierarchical.cpp:1490] No resources available to allocate!
I0405 17:29:20.048009 31868 hierarchical.cpp:1585] No inverse offers to send out!
I0405 17:29:20.048065 31868 hierarchical.cpp:1164] Performed allocation for agent 9565ff6f-f1b6-4259-8430-690e635c391f-S0 in 866803ns
I0405 17:29:20.048591 31864 master.cpp:5508] Sending 1 offers to framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 (default) at scheduler-bdf68f7f-d938-47ed-a132-bb3f218628bf@172.17.0.4:43972
I0405 17:29:20.049188 31860 sched.cpp:874] Scheduler::resourceOffers took 114867ns
I0405 17:29:20.080921 31859 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 37.025538ms
I0405 17:29:20.081001 31859 replica.cpp:712] Persisted action at 4
I0405 17:29:20.082425 31859 replica.cpp:691] Replica received learned notice for position 4 from @0.0.0.0:0
I0405 17:29:20.106056 31859 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 23.583037ms
I0405 17:29:20.106205 31859 leveldb.cpp:399] Deleting ~2 keys from leveldb took 76995ns
I0405 17:29:20.106240 31859 replica.cpp:712] Persisted action at 4
I0405 17:29:20.106278 31859 replica.cpp:697] Replica learned TRUNCATE action at position 4
I0405 17:29:20.119488 31837 resources.cpp:572] Parsing resources as JSON failed: cpus:1;mem:128
Trying semicolon-delimited string format instead
I0405 17:29:20.121356 31859 master.cpp:3288] Processing ACCEPT call for offers: [ 9565ff6f-f1b6-4259-8430-690e635c391f-O0 ] on agent 9565ff6f-f1b6-4259-8430-690e635c391f-S0 at slave(441)@172.17.0.4:43972 (4090d10eba90) for framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 (default) at scheduler-bdf68f7f-d938-47ed-a132-bb3f218628bf@172.17.0.4:43972
I0405 17:29:20.121485 31859 master.cpp:3046] Authorizing principal 'test-principal' to create volumes
I0405 17:29:20.121692 31859 master.cpp:2891] Authorizing framework principal 'test-principal' to launch task 91050005-0b1d-4a37-9ea1-f8ae1ff3b542 as user 'mesos'
I0405 17:29:20.123877 31871 master.cpp:3617] Applying CREATE operation for volumes disk(role1)[id1:path1]:2048 from framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 (default) at scheduler-bdf68f7f-d938-47ed-a132-bb3f218628bf@172.17.0.4:43972 to agent 9565ff6f-f1b6-4259-8430-690e635c391f-S0 at slave(441)@172.17.0.4:43972 (4090d10eba90)
I0405 17:29:20.125424 31871 master.cpp:6747] Sending checkpointed resources disk(role1)[id1:path1]:2048 to agent 9565ff6f-f1b6-4259-8430-690e635c391f-S0 at slave(441)@172.17.0.4:43972 (4090d10eba90)
I0405 17:29:20.126397 31856 hierarchical.cpp:656] Updated allocation of framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 on agent 9565ff6f-f1b6-4259-8430-690e635c391f-S0 from disk(role1):4096; cpus(*):2; mem(*):2048; ports(*):[31000-32000] to disk(role1):2048; cpus(*):2; mem(*):2048; ports(*):[31000-32000]; disk(role1)[id1:path1]:2048
I0405 17:29:20.126667 31871 master.hpp:177] Adding task 91050005-0b1d-4a37-9ea1-f8ae1ff3b542 with resources cpus(*):1; mem(*):128; disk(role1)[id1:path1]:2048 on agent 9565ff6f-f1b6-4259-8430-690e635c391f-S0 (4090d10eba90)
I0405 17:29:20.126875 31871 master.cpp:3773] Launching task 91050005-0b1d-4a37-9ea1-f8ae1ff3b542 of framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 (default) at scheduler-bdf68f7f-d938-47ed-a132-bb3f218628bf@172.17.0.4:43972 with resources cpus(*):1; mem(*):128; disk(role1)[id1:path1]:2048 on agent 9565ff6f-f1b6-4259-8430-690e635c391f-S0 at slave(441)@172.17.0.4:43972 (4090d10eba90)
I0405 17:29:20.127390 31856 slave.cpp:2523] Updated checkpointed resources from  to disk(role1)[id1:path1]:2048
I0405 17:29:20.127615 31856 slave.cpp:1497] Got assigned task 91050005-0b1d-4a37-9ea1-f8ae1ff3b542 for framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000
I0405 17:29:20.127876 31856 resources.cpp:572] Parsing resources as JSON failed: cpus:0.1;mem:32
Trying semicolon-delimited string format instead
I0405 17:29:20.127841 31871 hierarchical.cpp:893] Recovered disk(role1):2048; cpus(*):1; mem(*):1920; ports(*):[31000-32000] (total: cpus(*):2; mem(*):2048; disk(role1):2048; ports(*):[31000-32000]; disk(role1)[id1:path1]:2048, allocated: disk(role1)[id1:path1]:2048; cpus(*):1; mem(*):128) on agent 9565ff6f-f1b6-4259-8430-690e635c391f-S0 from framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000
I0405 17:29:20.127913 31871 hierarchical.cpp:930] Framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 filtered agent 9565ff6f-f1b6-4259-8430-690e635c391f-S0 for 5secs
I0405 17:29:20.128667 31856 slave.cpp:1616] Launching task 91050005-0b1d-4a37-9ea1-f8ae1ff3b542 for framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000
I0405 17:29:20.128937 31856 resources.cpp:572] Parsing resources as JSON failed: cpus:0.1;mem:32
Trying semicolon-delimited string format instead
I0405 17:29:20.129776 31856 paths.cpp:528] Trying to chown '/tmp/DiskResource_PersistentVolumeTest_AccessPersistentVolume_0_fJS7AC/slaves/9565ff6f-f1b6-4259-8430-690e635c391f-S0/frameworks/9565ff6f-f1b6-4259-8430-690e635c391f-0000/executors/91050005-0b1d-4a37-9ea1-f8ae1ff3b542/runs/bc8b48e5-dd32-4283-a1a6-e1988c82ae09' to user 'mesos'
I0405 17:29:20.145324 31856 slave.cpp:5575] Launching executor 91050005-0b1d-4a37-9ea1-f8ae1ff3b542 of framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 with resources cpus(*):0.1; mem(*):32 in work directory '/tmp/DiskResource_PersistentVolumeTest_AccessPersistentVolume_0_fJS7AC/slaves/9565ff6f-f1b6-4259-8430-690e635c391f-S0/frameworks/9565ff6f-f1b6-4259-8430-690e635c391f-0000/executors/91050005-0b1d-4a37-9ea1-f8ae1ff3b542/runs/bc8b48e5-dd32-4283-a1a6-e1988c82ae09'
I0405 17:29:20.146057 31858 containerizer.cpp:675] Starting container 'bc8b48e5-dd32-4283-a1a6-e1988c82ae09' for executor '91050005-0b1d-4a37-9ea1-f8ae1ff3b542' of framework '9565ff6f-f1b6-4259-8430-690e635c391f-0000'
I0405 17:29:20.146078 31856 slave.cpp:1834] Queuing task '91050005-0b1d-4a37-9ea1-f8ae1ff3b542' for executor '91050005-0b1d-4a37-9ea1-f8ae1ff3b542' of framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000
I0405 17:29:20.146203 31856 slave.cpp:881] Successfully attached file '/tmp/DiskResource_PersistentVolumeTest_AccessPersistentVolume_0_fJS7AC/slaves/9565ff6f-f1b6-4259-8430-690e635c391f-S0/frameworks/9565ff6f-f1b6-4259-8430-690e635c391f-0000/executors/91050005-0b1d-4a37-9ea1-f8ae1ff3b542/runs/bc8b48e5-dd32-4283-a1a6-e1988c82ae09'
I0405 17:29:20.147619 31859 posix.cpp:206] Changing the ownership of the persistent volume at '/tmp/DiskResource_PersistentVolumeTest_AccessPersistentVolume_0_fJS7AC/volumes/roles/role1/id1' with uid 1000 and gid 1000
I0405 17:29:20.162421 31859 posix.cpp:250] Adding symlink from '/tmp/DiskResource_PersistentVolumeTest_AccessPersistentVolume_0_fJS7AC/volumes/roles/role1/id1' to '/tmp/DiskResource_PersistentVolumeTest_AccessPersistentVolume_0_fJS7AC/slaves/9565ff6f-f1b6-4259-8430-690e635c391f-S0/frameworks/9565ff6f-f1b6-4259-8430-690e635c391f-0000/executors/91050005-0b1d-4a37-9ea1-f8ae1ff3b542/runs/bc8b48e5-dd32-4283-a1a6-e1988c82ae09/path1' for persistent volume disk(role1)[id1:path1]:2048 of container bc8b48e5-dd32-4283-a1a6-e1988c82ae09
I0405 17:29:20.172133 31861 launcher.cpp:123] Forked child with pid '7927' for container 'bc8b48e5-dd32-4283-a1a6-e1988c82ae09'
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0405 17:29:20.376197  7941 process.cpp:986] libprocess is initialized on 172.17.0.4:50952 with 16 worker threads
I0405 17:29:20.378132  7941 logging.cpp:195] Logging to STDERR
I0405 17:29:20.380861  7941 exec.cpp:150] Version: 0.29.0
I0405 17:29:20.396257  7966 exec.cpp:200] Executor started at: executor(1)@172.17.0.4:50952 with pid 7941
I0405 17:29:20.399426 31860 slave.cpp:2825] Got registration for executor '91050005-0b1d-4a37-9ea1-f8ae1ff3b542' of framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 from executor(1)@172.17.0.4:50952
I0405 17:29:20.402995  7966 exec.cpp:225] Executor registered on agent 9565ff6f-f1b6-4259-8430-690e635c391f-S0
I0405 17:29:20.403014 31860 slave.cpp:1999] Sending queued task '91050005-0b1d-4a37-9ea1-f8ae1ff3b542' to executor '91050005-0b1d-4a37-9ea1-f8ae1ff3b542' of framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 at executor(1)@172.17.0.4:50952
I0405 17:29:20.405624  7966 exec.cpp:237] Executor::registered took 393272ns
I0405 17:29:20.406108  7966 exec.cpp:312] Executor asked to run task '91050005-0b1d-4a37-9ea1-f8ae1ff3b542'
Registered executor on 4090d10eba90
I0405 17:29:20.406708  7966 exec.cpp:321] Executor::launchTask took 568039ns
Starting task 91050005-0b1d-4a37-9ea1-f8ae1ff3b542
Forked command at 7972
sh -c 'echo abc > path1/file'
I0405 17:29:20.411375  7966 exec.cpp:535] Executor sending status update TASK_RUNNING (UUID: cf4f8fe9-44f2-43ce-8868-b3a09b7298cf) for task 91050005-0b1d-4a37-9ea1-f8ae1ff3b542 of framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000
I0405 17:29:20.413156 31857 slave.cpp:3184] Handling status update TASK_RUNNING (UUID: cf4f8fe9-44f2-43ce-8868-b3a09b7298cf) for task 91050005-0b1d-4a37-9ea1-f8ae1ff3b542 of framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 from executor(1)@172.17.0.4:50952
I0405 17:29:20.415714 31857 status_update_manager.cpp:320] Received status update TASK_RUNNING (UUID: cf4f8fe9-44f2-43ce-8868-b3a09b7298cf) for task 91050005-0b1d-4a37-9ea1-f8ae1ff3b542 of framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000
I0405 17:29:20.415788 31857 status_update_manager.cpp:497] Creating StatusUpdate stream for task 91050005-0b1d-4a37-9ea1-f8ae1ff3b542 of framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000
I0405 17:29:20.416345 31857 status_update_manager.cpp:374] Forwarding update TASK_RUNNING (UUID: cf4f8fe9-44f2-43ce-8868-b3a09b7298cf) for task 91050005-0b1d-4a37-9ea1-f8ae1ff3b542 of framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 to the agent
I0405 17:29:20.416720 31870 slave.cpp:3582] Forwarding the update TASK_RUNNING (UUID: cf4f8fe9-44f2-43ce-8868-b3a09b7298cf) for task 91050005-0b1d-4a37-9ea1-f8ae1ff3b542 of framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 to master@172.17.0.4:43972
I0405 17:29:20.416954 31870 slave.cpp:3476] Status update manager successfully handled status update TASK_RUNNING (UUID: cf4f8fe9-44f2-43ce-8868-b3a09b7298cf) for task 91050005-0b1d-4a37-9ea1-f8ae1ff3b542 of framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000
I0405 17:29:20.416997 31870 slave.cpp:3492] Sending acknowledgement for status update TASK_RUNNING (UUID: cf4f8fe9-44f2-43ce-8868-b3a09b7298cf) for task 91050005-0b1d-4a37-9ea1-f8ae1ff3b542 of framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 to executor(1)@172.17.0.4:50952
I0405 17:29:20.417505 31870 master.cpp:4947] Status update TASK_RUNNING (UUID: cf4f8fe9-44f2-43ce-8868-b3a09b7298cf) for task 91050005-0b1d-4a37-9ea1-f8ae1ff3b542 of framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 from agent 9565ff6f-f1b6-4259-8430-690e635c391f-S0 at slave(441)@172.17.0.4:43972 (4090d10eba90)
I0405 17:29:20.417549 31870 master.cpp:4995] Forwarding status update TASK_RUNNING (UUID: cf4f8fe9-44f2-43ce-8868-b3a09b7298cf) for task 91050005-0b1d-4a37-9ea1-f8ae1ff3b542 of framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000
I0405 17:29:20.417724 31870 master.cpp:6608] Updating the state of task 91050005-0b1d-4a37-9ea1-f8ae1ff3b542 of framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 (latest state: TASK_RUNNING, status update state: TASK_RUNNING)
I0405 17:29:20.417943  7960 exec.cpp:358] Executor received status update acknowledgement cf4f8fe9-44f2-43ce-8868-b3a09b7298cf for task 91050005-0b1d-4a37-9ea1-f8ae1ff3b542 of framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000
I0405 17:29:20.418002 31870 sched.cpp:982] Scheduler::statusUpdate took 105225ns
I0405 17:29:20.418623 31870 master.cpp:4102] Processing ACKNOWLEDGE call cf4f8fe9-44f2-43ce-8868-b3a09b7298cf for task 91050005-0b1d-4a37-9ea1-f8ae1ff3b542 of framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 (default) at scheduler-bdf68f7f-d938-47ed-a132-bb3f218628bf@172.17.0.4:43972 on agent 9565ff6f-f1b6-4259-8430-690e635c391f-S0
I0405 17:29:20.419181 31860 status_update_manager.cpp:392] Received status update acknowledgement (UUID: cf4f8fe9-44f2-43ce-8868-b3a09b7298cf) for task 91050005-0b1d-4a37-9ea1-f8ae1ff3b542 of framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000
I0405 17:29:20.419816 31860 slave.cpp:2594] Status update manager successfully handled status update acknowledgement (UUID: cf4f8fe9-44f2-43ce-8868-b3a09b7298cf) for task 91050005-0b1d-4a37-9ea1-f8ae1ff3b542 of framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000
I0405 17:29:20.513465  7969 exec.cpp:535] Executor sending status update TASK_FINISHED (UUID: 128eb7af-a662-4cbb-9401-125dca38f719) for task 91050005-0b1d-4a37-9ea1-f8ae1ff3b542 of framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000
Command exited with status 0 (pid: 7972)
I0405 17:29:20.515449 31870 slave.cpp:3184] Handling status update TASK_FINISHED (UUID: 128eb7af-a662-4cbb-9401-125dca38f719) for task 91050005-0b1d-4a37-9ea1-f8ae1ff3b542 of framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 from executor(1)@172.17.0.4:50952
I0405 17:29:20.516875 31860 slave.cpp:5885] Terminating task 91050005-0b1d-4a37-9ea1-f8ae1ff3b542
I0405 17:29:20.517496 31867 posix.cpp:156] Removing symlink '/tmp/DiskResource_PersistentVolumeTest_AccessPersistentVolume_0_fJS7AC/slaves/9565ff6f-f1b6-4259-8430-690e635c391f-S0/frameworks/9565ff6f-f1b6-4259-8430-690e635c391f-0000/executors/91050005-0b1d-4a37-9ea1-f8ae1ff3b542/runs/bc8b48e5-dd32-4283-a1a6-e1988c82ae09/path1' for persistent volume disk(role1)[id1:path1]:2048 of container bc8b48e5-dd32-4283-a1a6-e1988c82ae09
I0405 17:29:20.519361 31864 status_update_manager.cpp:320] Received status update TASK_FINISHED (UUID: 128eb7af-a662-4cbb-9401-125dca38f719) for task 91050005-0b1d-4a37-9ea1-f8ae1ff3b542 of framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000
I0405 17:29:20.519850 31864 status_update_manager.cpp:374] Forwarding update TASK_FINISHED (UUID: 128eb7af-a662-4cbb-9401-125dca38f719) for task 91050005-0b1d-4a37-9ea1-f8ae1ff3b542 of framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 to the agent
I0405 17:29:20.520678 31870 slave.cpp:3582] Forwarding the update TASK_FINISHED (UUID: 128eb7af-a662-4cbb-9401-125dca38f719) for task 91050005-0b1d-4a37-9ea1-f8ae1ff3b542 of framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 to master@172.17.0.4:43972
I0405 17:29:20.520901 31870 slave.cpp:3476] Status update manager successfully handled status update TASK_FINISHED (UUID: 128eb7af-a662-4cbb-9401-125dca38f719) for task 91050005-0b1d-4a37-9ea1-f8ae1ff3b542 of framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000
I0405 17:29:20.520949 31870 slave.cpp:3492] Sending acknowledgement for status update TASK_FINISHED (UUID: 128eb7af-a662-4cbb-9401-125dca38f719) for task 91050005-0b1d-4a37-9ea1-f8ae1ff3b542 of framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 to executor(1)@172.17.0.4:50952
I0405 17:29:20.521550 31864 master.cpp:4947] Status update TASK_FINISHED (UUID: 128eb7af-a662-4cbb-9401-125dca38f719) for task 91050005-0b1d-4a37-9ea1-f8ae1ff3b542 of framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 from agent 9565ff6f-f1b6-4259-8430-690e635c391f-S0 at slave(441)@172.17.0.4:43972 (4090d10eba90)
I0405 17:29:20.521610 31864 master.cpp:4995] Forwarding status update TASK_FINISHED (UUID: 128eb7af-a662-4cbb-9401-125dca38f719) for task 91050005-0b1d-4a37-9ea1-f8ae1ff3b542 of framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000
I0405 17:29:20.522099 31871 sched.cpp:982] Scheduler::statusUpdate took 102502ns
I0405 17:29:20.522367 31864 master.cpp:6608] Updating the state of task 91050005-0b1d-4a37-9ea1-f8ae1ff3b542 of framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 (latest state: TASK_FINISHED, status update state: TASK_FINISHED)
I0405 17:29:20.524288 31871 hierarchical.cpp:1676] Filtered offer with disk(role1):2048; cpus(*):1; mem(*):1920; ports(*):[31000-32000] on agent 9565ff6f-f1b6-4259-8430-690e635c391f-S0 for framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000
I0405 17:29:20.524379 31871 hierarchical.cpp:1490] No resources available to allocate!
I0405 17:29:20.524451 31871 hierarchical.cpp:1585] No inverse offers to send out!
I0405 17:29:20.524551 31871 hierarchical.cpp:1141] Performed allocation for 1 agents in 961746ns
I0405 17:29:20.525182 31858 hierarchical.cpp:893] Recovered cpus(*):1; mem(*):128; disk(role1)[id1:path1]:2048 (total: cpus(*):2; mem(*):2048; disk(role1):2048; ports(*):[31000-32000]; disk(role1)[id1:path1]:2048, allocated: ) on agent 9565ff6f-f1b6-4259-8430-690e635c391f-S0 from framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000
I0405 17:29:20.525197 31864 master.cpp:4102] Processing ACKNOWLEDGE call 128eb7af-a662-4cbb-9401-125dca38f719 for task 91050005-0b1d-4a37-9ea1-f8ae1ff3b542 of framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 (default) at scheduler-bdf68f7f-d938-47ed-a132-bb3f218628bf@172.17.0.4:43972 on agent 9565ff6f-f1b6-4259-8430-690e635c391f-S0
I0405 17:29:20.525380 31864 master.cpp:6674] Removing task 91050005-0b1d-4a37-9ea1-f8ae1ff3b542 with resources cpus(*):1; mem(*):128; disk(role1)[id1:path1]:2048 of framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 on agent 9565ff6f-f1b6-4259-8430-690e635c391f-S0 at slave(441)@172.17.0.4:43972 (4090d10eba90)
I0405 17:29:20.526067 31864 status_update_manager.cpp:392] Received status update acknowledgement (UUID: 128eb7af-a662-4cbb-9401-125dca38f719) for task 91050005-0b1d-4a37-9ea1-f8ae1ff3b542 of framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000
I0405 17:29:20.526425 31864 status_update_manager.cpp:528] Cleaning up status update stream for task 91050005-0b1d-4a37-9ea1-f8ae1ff3b542 of framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000
I0405 17:29:20.526917 31864 slave.cpp:2594] Status update manager successfully handled status update acknowledgement (UUID: 128eb7af-a662-4cbb-9401-125dca38f719) for task 91050005-0b1d-4a37-9ea1-f8ae1ff3b542 of framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000
I0405 17:29:20.527048 31864 slave.cpp:5926] Completing task 91050005-0b1d-4a37-9ea1-f8ae1ff3b542
I0405 17:29:20.527732  7964 exec.cpp:358] Executor received status update acknowledgement 128eb7af-a662-4cbb-9401-125dca38f719 for task 91050005-0b1d-4a37-9ea1-f8ae1ff3b542 of framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000
I0405 17:29:21.527920 31859 slave.cpp:3710] executor(1)@172.17.0.4:50952 exited
../../src/tests/persistent_volume_tests.cpp:825: Failure
Failed to wait 15secs for offers
I0405 17:29:35.542609 31856 master.cpp:1269] Framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 (default) at scheduler-bdf68f7f-d938-47ed-a132-bb3f218628bf@172.17.0.4:43972 disconnected
I0405 17:29:35.542811 31856 master.cpp:2642] Disconnecting framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 (default) at scheduler-bdf68f7f-d938-47ed-a132-bb3f218628bf@172.17.0.4:43972
I0405 17:29:35.542994 31856 master.cpp:2666] Deactivating framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 (default) at scheduler-bdf68f7f-d938-47ed-a132-bb3f218628bf@172.17.0.4:43972
I0405 17:29:35.543349 31860 hierarchical.cpp:378] Deactivated framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000
I0405 17:29:35.543501 31856 master.cpp:1293] Giving framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 (default) at scheduler-bdf68f7f-d938-47ed-a132-bb3f218628bf@172.17.0.4:43972 0ns to failover
I0405 17:29:35.543903 31868 master.cpp:5360] Framework failover timeout, removing framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 (default) at scheduler-bdf68f7f-d938-47ed-a132-bb3f218628bf@172.17.0.4:43972
I0405 17:29:35.543936 31868 master.cpp:6093] Removing framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 (default) at scheduler-bdf68f7f-d938-47ed-a132-bb3f218628bf@172.17.0.4:43972
I0405 17:29:35.544337 31861 slave.cpp:2215] Asked to shut down framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 by master@172.17.0.4:43972
I0405 17:29:35.544381 31861 slave.cpp:2240] Shutting down framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000
I0405 17:29:35.544456 31861 slave.cpp:4398] Shutting down executor '91050005-0b1d-4a37-9ea1-f8ae1ff3b542' of framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 at executor(1)@172.17.0.4:50952
I0405 17:29:35.544960 31872 poll_socket.cpp:110] Socket error while connecting
I0405 17:29:35.545013 31872 process.cpp:1650] Failed to send 'mesos.internal.ShutdownExecutorMessage' to '172.17.0.4:50952', connect: Socket error while connecting
E0405 17:29:35.545106 31872 process.cpp:1958] Failed to shutdown socket with fd 27: Transport endpoint is not connected
I0405 17:29:35.545474 31864 hierarchical.cpp:329] Removed framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000
../../src/tests/persistent_volume_tests.cpp:819: Failure
Actual function call count doesn't match EXPECT_CALL(sched, resourceOffers(&driver, _))...
         Expected: to be called at least once
           Actual: never called - unsatisfied and active
I0405 17:29:35.558538 31858 containerizer.cpp:1432] Destroying container 'bc8b48e5-dd32-4283-a1a6-e1988c82ae09'
../../src/tests/cluster.cpp:453: Failure
Failed to wait 15secs for wait
I0405 17:29:50.565403 31870 slave.cpp:800] Agent terminating
I0405 17:29:50.565512 31870 slave.cpp:2215] Asked to shut down framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 by @0.0.0.0:0
W0405 17:29:50.565544 31870 slave.cpp:2236] Ignoring shutdown framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 because it is terminating
I0405 17:29:50.574620 31866 master.cpp:1230] Agent 9565ff6f-f1b6-4259-8430-690e635c391f-S0 at slave(441)@172.17.0.4:43972 (4090d10eba90) disconnected
I0405 17:29:50.574766 31866 master.cpp:2701] Disconnecting agent 9565ff6f-f1b6-4259-8430-690e635c391f-S0 at slave(441)@172.17.0.4:43972 (4090d10eba90)
I0405 17:29:50.575003 31866 master.cpp:2720] Deactivating agent 9565ff6f-f1b6-4259-8430-690e635c391f-S0 at slave(441)@172.17.0.4:43972 (4090d10eba90)
I0405 17:29:50.575294 31865 hierarchical.cpp:563] Agent 9565ff6f-f1b6-4259-8430-690e635c391f-S0 deactivated
I0405 17:29:50.605787 31837 master.cpp:1083] Master terminating
I0405 17:29:50.606533 31866 hierarchical.cpp:508] Removed agent 9565ff6f-f1b6-4259-8430-690e635c391f-S0
[  FAILED  ] DiskResource/PersistentVolumeTest.AccessPersistentVolume/0, where GetParam() = 0 (31491 ms)
{code}",Bug,Major,greggomann,2016-04-06T23:21:07.000+0000,5,Resolved,Complete,PersistentVolumeTest.AccessPersistentVolume is flaky,2016-04-06T23:21:07.000+0000,MESOS-5128,3.0,mesos,Mesosphere Sprint 32
avinash@mesosphere.io,2016-04-06T04:10:24.000+0000,avinash@mesosphere.io,"Currently the `LIBPROCESS_IP` environment variable was being set to
    the Agent IP if the environment variable has not be defined by the
    `Framework`. For containers having their own IP address (as with
    containers on CNI networks) this becomes a problem since the command
    executor tries to bind to the `LIBPROCESS_IP` that does not exist in
    its network namespace, and fails. Thus, for containers launched on CNI
    networks the `LIBPROCESS_IP` should not be set, or rather is set to
    ""0.0.0.0"", allowing the container to bind to the IP address provided
    by the CNI network.",Bug,Major,avinash@mesosphere.io,2016-04-06T18:42:04.000+0000,5,Resolved,Complete,Reset `LIBPROCESS_IP` in `network\cni` isolator.,2016-04-06T18:42:04.000+0000,MESOS-5127,1.0,mesos,Mesosphere Sprint 32
mcypark,2016-04-05T19:47:03.000+0000,mcypark,"Currently, the commit message hook iterates over the commented lines.
For example, if there is a modified file for which its path is longer than 72 characters, the commit hook errors out. We should skip over the commented lines.",Bug,Major,mcypark,2016-04-06T21:41:49.000+0000,5,Resolved,Complete,Commit message hook iterates over the commented lines.,2016-04-06T21:41:49.000+0000,MESOS-5126,2.0,mesos,Mesosphere Sprint 32
mcypark,2016-04-05T19:44:17.000+0000,mcypark,"{{for LINE in $COMMIT_MESSAGE}} iterates over one word at a time, rather than one line at a time. We should use the following pattern instead:
{code}
while read LINE;
do
  ...
done <<< ""$COMMIT_MESSAGE""
{code}",Bug,Major,mcypark,2016-04-06T21:41:30.000+0000,5,Resolved,Complete,"Commit message hook iterates over words, rather than lines.",2016-04-06T21:41:30.000+0000,MESOS-5125,2.0,mesos,Mesosphere Sprint 32
alexr,2016-04-05T15:36:24.000+0000,alexr,Recently {{TASK_KILLING}} state (MESOS-4547) have been introduced to Mesos. We should add support for this feature to {{mesos-execute}}.,Improvement,Major,alexr,2016-04-12T10:47:25.000+0000,5,Resolved,Complete,TASK_KILLING is not supported by mesos-execute.,2016-04-12T10:47:25.000+0000,MESOS-5124,3.0,mesos,Mesosphere Sprint 33
chenzhiwei,2016-04-05T07:00:17.000+0000,chenzhiwei,"When compile on ppc64le, it will through error message: src/linux/fs.cpp:443:2: error: #error ""pivot_root is not available""

The current code logic in src/linux/fs.cpp is:

{code}
#ifdef __NR_pivot_root
  int ret = ::syscall(__NR_pivot_root, newRoot.c_str(), putOld.c_str());
#elif __x86_64__
  // A workaround for systems that have an old glib but have a new
  // kernel. The magic number '155' is the syscall number for
  // 'pivot_root' on the x86_64 architecture, see
  // arch/x86/syscalls/syscall_64.tbl
  int ret = ::syscall(155, newRoot.c_str(), putOld.c_str());
#else
#error ""pivot_root is not available""
#endif
{code}

There is no old glib version and the new kernel version, it will never run code in *#ifdef __NR_pivot_root* condition, and when I build on Ubuntu 16.04(It has the latest linux kernel and glibc), it still can't step into the *#ifdef __NR_pivot_root* condition.

For powerpc case, I added another condition:

{code}
#elif __powerpc__ || __ppc__ || __powerpc64__ || __ppc64__
  // A workaround for powerpc. The magic number '203' is the syscall
  // number for 'pivot_root' on the powerpc architecture, see
  // https://w3challs.com/syscalls/?arch=powerpc_64
  int ret = ::syscall(203, newRoot.c_str(), putOld.c_str());
{code}",Bug,Major,chenzhiwei,2016-04-05T17:08:21.000+0000,5,Resolved,Complete,pivot_root is not available on PowerPC,2016-04-24T20:21:28.000+0000,MESOS-5121,1.0,mesos,Mesosphere Sprint 32
klueska,2016-04-04T22:17:22.000+0000,klueska," Calls to 'nvidia-smi'  fail inside a container even if access to a GPU has been granted. Moreover, access to /dev/nvidiactl is actually required for a container to do anything useful with a GPU even if it has access to it.
    
We should grant/revoke access to /dev/nvidiactl and /dev/nvidia-uvm as GPUs are added and removed from a container in the Nvidia GPU isolator.",Bug,Major,klueska,2016-04-06T00:47:56.000+0000,5,Resolved,Complete,Grant access to /dev/nvidiactl and /dev/nvidia-uvm in the Nvidia GPU isolator.,2016-04-06T00:48:12.000+0000,MESOS-5115,2.0,mesos,Mesosphere Sprint 32
mcypark,2016-04-04T21:01:33.000+0000,clehene,"A missing default for quorum size has generated the following master config 
{code}
MESOS_WORK_DIR=""/var/lib/mesos/master""
MESOS_ZK=""zk://zk1:2181,zk2:2181,zk3:2181/mesos""
MESOS_QUORUM=

MESOS_PORT=5050
MESOS_CLUSTER=""mesos""
MESOS_LOG_DIR=""/var/log/mesos""
MESOS_LOGBUFSECS=1
MESOS_LOGGING_LEVEL=""INFO""
{code}

This was causing each elected leader to attempt replica recovery.

E.g. {{group.cpp:700] Trying to get '/mesos/log_replicas/0000000012' in ZooKeeper}}

And eventually:
{{master.cpp:1458] Recovery failed: Failed to recover registrar: Failed to perform fetch within 1mins}}

Full log on one of the masters https://gist.github.com/clehene/09a9ddfe49b92a5deb4c1b421f63479e

All masters and zk nodes were reachable over the network. 
Also once the quorum was configured the master recovery protocol finished gracefully. 
",Bug,Major,clehene,2016-04-05T17:56:19.000+0000,5,Resolved,Complete,Flags::parse does not handle empty string correctly.,2016-04-05T20:10:08.000+0000,MESOS-5114,2.0,mesos,Mesosphere Sprint 32
avinash@mesosphere.io,2016-04-04T20:19:19.000+0000,avinash@mesosphere.io,"If we start the agent with the --isolation='network/cni' but do not specify the --network_cni_plugins_dir flag, the agent crashes with the following stack dump:
0x00007ffff2324cc9 in __GI_raise (sig=sig@entry=6) at ../nptl/sysdeps/unix/sysv/linux/raise.c:56
56      ../nptl/sysdeps/unix/sysv/linux/raise.c: No such file or directory.
(gdb) bt
#0  0x00007ffff2324cc9 in __GI_raise (sig=sig@entry=6) at ../nptl/sysdeps/unix/sysv/linux/raise.c:56
#1  0x00007ffff23280d8 in __GI_abort () at abort.c:89
#2  0x00007ffff231db86 in __assert_fail_base (fmt=0x7ffff246e830 ""%s%s%s:%u: %s%sAssertion `%s' failed.\n%n"", assertion=assertion@entry=0x451f5c ""isSome()"",
    file=file@entry=0x451f65 ""../../3rdparty/libprocess/3rdparty/stout/include/stout/option.hpp"", line=line@entry=111,
    function=function@entry=0x45294a ""const T &Option<std::basic_string<char> >::get() const & [T = std::basic_string<char>]"") at assert.c:92
#3  0x00007ffff231dc32 in __GI___assert_fail (assertion=0x451f5c ""isSome()"", file=0x451f65 ""../../3rdparty/libprocess/3rdparty/stout/include/stout/option.hpp"", line=111,
    function=0x45294a ""const T &Option<std::basic_string<char> >::get() const & [T = std::basic_string<char>]"") at assert.c:101
#4  0x0000000000432c0d in Option<std::string>::get() const & (this=0x6c1ea8) at ../../3rdparty/libprocess/3rdparty/stout/include/stout/option.hpp:111
Python Exception <class 'IndexError'> list index out of range:
#5  0x00007ffff63ef7cc in mesos::internal::slave::NetworkCniIsolatorProcess::recover (this=0x6c1e70, states=empty std::list, orphans=...) at ../../src/slave/containerizer/mesos/isolators/network/cni/cni.cpp:331
#6  0x00007ffff60cddd8 in operator() (this=0x7fffc0001e00, process=0x6c1ef8) at ../../3rdparty/libprocess/include/process/dispatch.hpp:239
#7  0x00007ffff60cd972 in std::_Function_handler<void (process::ProcessBase*), process::Future<Nothing> process::dispatch<Nothing, mesos::internal::slave::MesosIsolatorProcess, std::list<mesos::slave::ContainerState, std::allocator<mesos::slave::ContainerState> > const&, hashset<mesos::ContainerID, std::hash<mesos::ContainerID>, std::equal_to<mesos::ContainerID> > const&, std::list<mesos::slave::ContainerState, std::allocator<mesos::slave::ContainerState> >, hashset<mesos::ContainerID, std::hash<mesos::ContainerID>, std::equal_to<mesos::ContainerID> > >(process::PID<mesos::internal::slave::MesosIsolatorProcess> const&, process::Future<Nothing> (mesos::internal::slave::MesosIsolatorProcess::*)(std::list<mesos::slave::ContainerState, std::allocator<mesos::slave::ContainerState> > const&, hashset<mesos::ContainerID, std::hash<mesos::ContainerID>, std::equal_to<mesos::ContainerID> > const&), std::list<mesos::slave::ContainerState, std::allocator<mesos::slave::ContainerState> >, hashset<mesos::ContainerID, std::hash<mesos::ContainerID>, std::equal_to<mesos::ContainerID> >)::{lambda(process::ProcessBase*)#1}>::_M_invoke(std::_Any_data const&, process::ProcessBase*) (__functor=..., __args=0x6c1ef8) at /usr/bin/../lib/gcc/x86_64-linux-gnu/4.8/../../../../include/c++/4.8/functional:2071
#8  0x00007ffff6a6bf38 in std::function<void (process::ProcessBase*)>::operator()(process::ProcessBase*) const (this=0x7fffc0001d70, __args=0x6c1ef8)
    at /usr/bin/../lib/gcc/x86_64-linux-gnu/4.8/../../../../include/c++/4.8/functional:2471
#9  0x00007ffff6a561b4 in process::ProcessBase::visit (this=0x6c1ef8, event=...) at ../../../3rdparty/libprocess/src/process.cpp:3130
#10 0x00007ffff6aac5fe in process::DispatchEvent::visit (this=0x7fffc0001570, visitor=0x6c1ef8) at ../../../3rdparty/libprocess/include/process/event.hpp:161
#11 0x00007ffff55e9c91 in process::ProcessBase::serve (this=0x6c1ef8, event=...) at ../../3rdparty/libprocess/include/process/process.hpp:82
#12 0x00007ffff6a53ed4 in process::ProcessManager::resume (this=0x67cca0, process=0x6c1ef8) at ../../../3rdparty/libprocess/src/process.cpp:2570
#13 0x00007ffff6a5bff5 in operator() (this=0x697d70, joining=...) at ../../../3rdparty/libprocess/src/process.cpp:2218
#14 0x00007ffff6a5bf33 in std::_Bind<process::ProcessManager::init_threads()::$_1 (std::reference_wrapper<std::atomic_bool const>)>::__call<void, , 0ul>(std::tuple<>&&, std::_Index_tuple<0ul>) (this=0x697d70,
    __args=<unknown type in /home/vagrant/mesosphere/mesos/build/src/.libs/libmesos-0.29.0.so, CU 0x45bb552, DIE 0x469efe5>) at /usr/bin/../lib/gcc/x86_64-linux-gnu/4.8/../../../../include/c++/4.8/functional:1295
#15 0x00007ffff6a5bee6 in std::_Bind<process::ProcessManager::init_threads()::$_1 (std::reference_wrapper<std::atomic_bool const>)>::operator()<, void>() (this=0x697d70)
    at /usr/bin/../lib/gcc/x86_64-linux-gnu/4.8/../../../../include/c++/4.8/functional:1353
#16 0x00007ffff6a5be95 in std::_Bind_simple<std::_Bind<process::ProcessManager::init_threads()::$_1 (std::reference_wrapper<std::atomic_bool const>)> ()>::_M_invoke<>(std::_Index_tuple<>) (this=0x697d70)
    at /usr/bin/../lib/gcc/x86_64-linux-gnu/4.8/../../../../include/c++/4.8/functional:1731
#17 0x00007ffff6a5be65 in std::_Bind_simple<std::_Bind<process::ProcessManager::init_threads()::$_1 (std::reference_wrapper<std::atomic_bool const>)> ()>::operator()() (this=0x697d70)
    at /usr/bin/../lib/gcc/x86_64-linux-gnu/4.8/../../../../include/c++/4.8/functional:1720
#18 0x00007ffff6a5be3c in std::thread::_Impl<std::_Bind_simple<std::_Bind<process::ProcessManager::init_threads()::$_1 (std::reference_wrapper<std::atomic_bool const>)> ()> >::_M_run() (this=0x697d58)
    at /usr/bin/../lib/gcc/x86_64-linux-gnu/4.8/../../../../include/c++/4.8/thread:115
#19 0x00007ffff2b98a60 in ?? () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6
#20 0x00007ffff26bb182 in start_thread (arg=0x7fffeb92d700) at pthread_create.c:312
#21 0x00007ffff23e847d in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:111
(gdb) frame 4
#4  0x0000000000432c0d in Option<std::string>::get() const & (this=0x6c1ea8) at ../../3rdparty/libprocess/3rdparty/stout/include/stout/option.hpp:111",Bug,Major,avinash@mesosphere.io,2016-04-05T17:35:17.000+0000,5,Resolved,Complete,`network/cni` isolator crashes when launched without the --network_cni_plugins_dir flag,2016-04-05T17:35:17.000+0000,MESOS-5113,1.0,mesos,Mesosphere Sprint 32
mcypark,2016-04-04T20:19:04.000+0000,mcypark,"{{WindowsError}} invokes {{::GetLastError}} to retrieve the error code. Windows has a {{::WSAGetLastError}} function which at the interface level, is intended for failed socket operations. We should introduce a {{WindowsSocketError}} which invokes {{::WSAGetLastError}} and use them accordingly.",Task,Major,mcypark,2016-04-05T18:41:01.000+0000,5,Resolved,Complete,Introduce `WindowsSocketError`.,2016-04-05T18:41:01.000+0000,MESOS-5112,2.0,mesos,Mesosphere Sprint 32
mcypark,2016-04-04T20:12:51.000+0000,mcypark,"{{network::connect}} function returns a {{Try<int>}} currently and the caller is required to inspect the state of {{errno}} out-of-band. {{network::connect}} should really return something like a {{Try<int, ErrnoError>}}.",Task,Major,mcypark,2016-04-05T18:41:34.000+0000,5,Resolved,Complete,Update `network::connect` to use the typed error state of `Try`.,2016-04-05T18:41:34.000+0000,MESOS-5111,2.0,mesos,Mesosphere Sprint 32
mcypark,2016-04-04T20:09:22.000+0000,mcypark,"Add an additional template parameter {{E}} to the {{Try}} class template.

{code}
template <typename T, typename E = Error>
class Try {
  /* ... */
};
{code}",Task,Major,mcypark,2016-04-05T18:40:00.000+0000,5,Resolved,Complete,Introduce an additional template parameter to `Try` for typed error.,2016-04-05T18:40:00.000+0000,MESOS-5110,3.0,mesos,Mesosphere Sprint 32
mcypark,2016-04-04T20:08:25.000+0000,mcypark,"The {{ErrnoError}} and {{WindowsError}} classes simply construct the error string via a mechanism such as {{strerror}}. They should also capture the error code, as it is an essential piece of information for such an error type.",Task,Major,mcypark,2016-04-05T18:40:34.000+0000,5,Resolved,Complete,Capture the error code in `ErrnoError` and `WindowsError`.,2016-04-05T18:40:34.000+0000,MESOS-5109,2.0,mesos,Mesosphere Sprint 32
jlarriba,2016-04-03T16:14:33.000+0000,jlarriba,Add the CMake build system to docker_build.sh to automatically test the build on Jenkins alongside gcc and clang.,Improvement,Major,jlarriba,,10006,Reviewable,New,Add CMake build to docker_build.sh,2016-04-27T16:13:18.000+0000,MESOS-5101,2.0,mesos,Mesosphere Sprint 33
klueska,2016-04-02T21:32:40.000+0000,klueska,"There appears to be a discrepancy between clang and gcc, which allows
clang to accept `using` declarations of the form `using ns_name::name;`
that contain nested classes, structs, and enums after the `name` field
in the declaration (e.g. `using ns_name::name::enum;`).

The language for describing this functionality is ambiguous in the
C++11 specification as referenced here:
http://en.cppreference.com/w/cpp/language/namespace#Using-declarations",Bug,Major,klueska,2016-04-05T22:21:36.000+0000,5,Resolved,Complete,Fix a bug in the Nvidia GPU device isolator that exposes a discrepancy between clang and gcc in 'using' declarations,2016-04-05T22:21:36.000+0000,MESOS-5082,1.0,mesos,Mesosphere Sprint 32
,2016-04-01T16:47:30.000+0000,greggomann,We should document the possible {{reason}} values that can be found in the {{TaskStatus}} message.,Documentation,Major,greggomann,,1,Open,New,Document TaskStatus reasons,2016-04-01T16:47:30.000+0000,MESOS-5078,1.0,mesos,
js84,2016-03-31T10:11:29.000+0000,js84,"The clone option in subprocess is only used (at least in the Mesos codebase) to specify custom namespace flags to clone. It feels having the clone function in the subprocess interface is too explicit for this functionality.
",Improvement,Major,js84,,10006,Reviewable,New,Refactor the clone option to subprocess.,2016-04-12T16:18:51.000+0000,MESOS-5071,2.0,mesos,Mesosphere Sprint 32
js84,2016-03-31T09:54:16.000+0000,js84,"We introduced a number of parameters to the subprocess interface with MESOS-5049.
Adding all options explicitly to the subprocess interface makes it inflexible. 
We should investigate a flexible options, which still prevents arbitrary code to be executed.",Improvement,Major,js84,,10006,Reviewable,New,Introduce more flexible subprocess interface for child options.,2016-04-12T16:18:43.000+0000,MESOS-5070,2.0,mesos,Mesosphere Sprint 32
gilbert,2016-03-30T15:40:28.000+0000,gilbert,"For docker private registry with authentication, docker containerizer should support using a default .docker/config.json file (or the old .dockercfg file) locally, which is pre-handled by operators. The default docker config file should be exposed by a new agent flag `--docker_config`. ",Task,Major,gilbert,2016-04-06T18:24:54.000+0000,5,Resolved,Complete,Support docker private registry default docker config.,2016-04-06T18:24:54.000+0000,MESOS-5065,3.0,mesos,Mesosphere Sprint 32
greggomann,2016-03-29T23:28:47.000+0000,hartem,"Following a crash report from the user we need to be more explicit about the dangers of using {{/tmp}} as agent {{work_dir}}. In addition, we can remove the default value for the {{\-\-work_dir}} flag, forcing users to explicitly set the work directory for the agent.",Bug,Major,hartem,,10006,Reviewable,New,Remove default value for the agent `work_dir`,2016-05-07T01:31:28.000+0000,MESOS-5064,2.0,mesos,Mesosphere Sprint 32
kaysoky,2016-03-29T18:46:45.000+0000,kaysoky,"There are a couple of problems with the long-lived framework that prevent it from being deployed (easily) on an actual cluster:

* The framework will greedily accept all offers; it runs one executor per agent in the cluster.
* The framework assumes the {{long-lived-executor}} binary is available on each agent.  This is generally only true in the build environment or in single-agent test environments.
* The framework does not specify an resources with the executor.  This is required by many isolators.
* The framework has no metrics.",Improvement,Major,kaysoky,2016-04-12T19:24:59.000+0000,5,Resolved,Complete,Update the long-lived-framework example to run on test clusters,2016-04-12T19:24:59.000+0000,MESOS-5062,3.0,mesos,Mesosphere Sprint 32
dongdong,2016-03-29T05:50:47.000+0000,dongdong,"This is a sub ticket of MESOS-3780. In this ticket, we will update all the slave to agent in the error messages and other strings in the code",Task,Major,dongdong,2016-04-18T02:36:13.000+0000,5,Resolved,Complete,Slave/Agent Rename Phase I - Update strings in error messages and other strings,2016-04-27T01:26:25.000+0000,MESOS-5057,3.0,mesos,Mesosphere Sprint 33
dongdong,2016-03-29T05:49:20.000+0000,dongdong,"This is a sub ticket of MESOS-3780. In this ticket, we will rename slave to agent in the shell script outputs",Task,Major,dongdong,2016-04-25T09:04:57.000+0000,5,Resolved,Complete,Replace Master/Slave Terminology Phase I - Update strings in the shell scripts outputs,2016-04-25T09:04:57.000+0000,MESOS-5056,1.0,mesos,
dongdong,2016-03-29T05:46:56.000+0000,dongdong,"This is a sub ticket of MESOS-3780. In this ticket, we will rename all the slave to agent in the log messages and standard output.",Task,Major,dongdong,2016-04-04T22:33:05.000+0000,5,Resolved,Complete,Slave/Agent Rename Phase I - Update strings in the log message and standard output,2016-04-27T01:25:06.000+0000,MESOS-5055,2.0,mesos,
a10gupta,2016-03-28T23:12:57.000+0000,greggomann,"A recent name collision occurred when updating the 3rdparty http-parser library: https://github.com/apache/mesos/commit/94df63f72146501872a06c6487e94bdfd0f23025

We should put stout's {{flags}} namespace within another suitable namespace (perhaps {{stout::flags}}) to avoid such collisions.",Improvement,Major,greggomann,,10020,Accepted,In Progress,Namespace the stout flags,2016-03-30T06:58:19.000+0000,MESOS-5054,2.0,mesos,
jojy,2016-03-28T17:13:34.000+0000,jieyu,"These helpers can either based on some existing library (e.g. libcap), or use system calls directly.",Task,Major,jieyu,,10006,Reviewable,New,Create helpers for manipulating Linux capabilities.,2016-04-27T16:13:15.000+0000,MESOS-5051,5.0,mesos,Mesosphere Sprint 32
jojy,2016-03-28T17:08:04.000+0000,jieyu,"We should at least support the following cases:
1) A root user has reduced capability
2) A non-root user has the capability of CAP_NET_ADMIN (to do e.g., tcpdump)",Task,Major,jieyu,2016-04-11T22:01:09.000+0000,5,Resolved,Complete,Design Linux capability support for Mesos containerizer,2016-04-11T22:01:10.000+0000,MESOS-5050,5.0,mesos,Mesosphere Sprint 32
js84,2016-03-28T16:49:02.000+0000,js84,"Executing arbitrary setup functions while creating new processes is
dangerous as all functions called have to be async safe. As setup
functions are used for only very few purposes (setsid, chdir, monitoring
and killing a process (see upcoming review) it makes sense to support
them safely via parameters to subprocess. 
Another common use of child setup are is to block the child while doing some work in the parent. This pattern can be more cleanly expressed with parentHooks. ",Improvement,Major,js84,2016-03-28T17:00:35.000+0000,5,Resolved,Complete,Refactore subproces setup functions.,2016-03-29T05:04:22.000+0000,MESOS-5049,3.0,mesos,Mesosphere Sprint 31
jvanremoortere,2016-03-26T20:08:54.000+0000,gilbert,"Currently in mesos test, we have the temporary directories created by `environment->mkdtemp()` cleaned up until the end of the test suite, which can be problematic. For instance, if we have many tests in a test suite, each of those tests is performing large size disk read/write in its temp dir, which may lead to out of disk issue on some resource limited machines. 

We should have these temp dir created by `environment->mkdtemp` cleaned up during each test teardown. Currently we only clean up the sandbox for each test.",Improvement,Major,gilbert,2016-04-04T23:04:22.000+0000,5,Resolved,Complete,Temporary directories created by environment->mkdtemp cleanup can be problematic.,2016-04-04T23:04:22.000+0000,MESOS-5044,1.0,mesos,Mesosphere Sprint 32
,2016-03-25T09:03:08.000+0000,adam-mesos,"Currently two formats of credentials are supported: JSON

{code}
  ""credentials"": [
    {
      ""principal"": ""sherman"",
      ""secret"": ""kitesurf""
    }
{code}

And a deprecated new line file:
{code}
principal1 secret1
pricipal2 secret2
{code}

We deprecated the new line format in 0.29, and should remove it after the deprecation cycle ends.",Improvement,Minor,cmaloney,,10020,Accepted,In Progress,Remove plain text Credential format (after deprecation cycle),2016-03-25T09:04:36.000+0000,MESOS-5032,3.0,mesos,
yongtang,2016-03-25T07:57:13.000+0000,adam-mesos,"We need to make the Action enum optional in authorization::Request, and add an `UNKNOWN = 0;` enum value. See MESOS-4997 for details.",Bug,Major,adam-mesos,2016-03-30T20:54:21.000+0000,5,Resolved,Complete,Authorization Action enum does not support upgrades.,2016-04-22T21:02:19.000+0000,MESOS-5031,2.0,mesos,Mesosphere Sprint 32
gilbert,2016-03-24T20:19:51.000+0000,zhitao,"I'm trying to play with the new image provisioner on our custom docker images, but one of layer failed to get copied, possibly due to a dangling symlink.

Error log with Glog_v=1:

{quote}
I0324 05:42:48.926678 15067 copy.cpp:127] Copying layer path '/tmp/mesos/store/docker/layers/5df0888641196b88dcc1b97d04c74839f02a73b8a194a79e134426d6a8fcb0f1/rootfs' to rootfs '/var/lib/mesos/provisioner/containers/5f05be6c-c970-4539-aa64-fd0eef2ec7ae/backends/copy/rootfses/507173f3-e316-48a3-a96e-5fdea9ffe9f6'
E0324 05:42:49.028506 15062 slave.cpp:3773] Container '5f05be6c-c970-4539-aa64-fd0eef2ec7ae' for executor 'test' of framework 75932a89-1514-4011-bafe-beb6a208bb2d-0004 failed to start: Collect failed: Collect failed: Failed to copy layer: cp: cannot overwrite directory ‘/var/lib/mesos/provisioner/containers/5f05be6c-c970-4539-aa64-fd0eef2ec7ae/backends/copy/rootfses/507173f3-e316-48a3-a96e-5fdea9ffe9f6/etc/apt’ with non-directory
{quote}

Content of _/tmp/mesos/store/docker/layers/5df0888641196b88dcc1b97d04c74839f02a73b8a194a79e134426d6a8fcb0f1/rootfs/etc/apt_ points to a non-existing absolute path (cannot provide exact path but it's a result of us trying to mount apt keys into docker container at build time).

I believe what happened is that we executed a script at build time, which contains equivalent of:
{quote}
rm -rf /etc/apt/* && ln -sf /build-mount-point/ /etc/apt
{quote}
",Bug,Major,zhitao,,10020,Accepted,In Progress,Copy provisioner cannot replace directory with symlink,2016-04-30T16:43:50.000+0000,MESOS-5028,3.0,mesos,Mesosphere Sprint 32
haosdent@gmail.com,2016-03-24T18:58:55.000+0000,greggomann,"The webui hits a number of endpoints to get the data that it displays: {{/state}}, {{/metrics/snapshot}}, {{/files/browse}}, {{/files/read}}, and maybe others? Once authentication is enabled on these endpoints, we need to add a login prompt to the webui so that users can provide credentials.",Improvement,Major,greggomann,,10020,Accepted,In Progress,Enable authenticated login in the webui,2016-05-02T14:32:30.000+0000,MESOS-5027,2.0,mesos,
gilbert,2016-03-24T14:33:43.000+0000,alexr,"Observed on the Apache Jenkins.

{noformat}
[ RUN      ] MesosContainerizerProvisionerTest.ProvisionFailed
I0324 13:38:56.284261  2948 containerizer.cpp:666] Starting container 'test_container' for executor 'executor' of framework ''
I0324 13:38:56.285825  2939 containerizer.cpp:1421] Destroying container 'test_container'
I0324 13:38:56.285854  2939 containerizer.cpp:1424] Waiting for the provisioner to complete for container 'test_container'
[       OK ] MesosContainerizerProvisionerTest.ProvisionFailed (7 ms)
[ RUN      ] MesosContainerizerProvisionerTest.DestroyWhileProvisioning
I0324 13:38:56.291187  2944 containerizer.cpp:666] Starting container 'c2316963-c6cb-4c7f-a3b9-17ca5931e5b2' for executor 'executor' of framework ''
I0324 13:38:56.292157  2944 containerizer.cpp:1421] Destroying container 'c2316963-c6cb-4c7f-a3b9-17ca5931e5b2'
I0324 13:38:56.292179  2944 containerizer.cpp:1424] Waiting for the provisioner to complete for container 'c2316963-c6cb-4c7f-a3b9-17ca5931e5b2'
F0324 13:38:56.292899  2944 containerizer.cpp:752] Check failed: containers_.contains(containerId)
*** Check failure stack trace: ***
    @     0x2ac9973d0ae4  google::LogMessage::Fail()
    @     0x2ac9973d0a30  google::LogMessage::SendToLog()
    @     0x2ac9973d0432  google::LogMessage::Flush()
    @     0x2ac9973d3346  google::LogMessageFatal::~LogMessageFatal()
    @     0x2ac996af897c  mesos::internal::slave::MesosContainerizerProcess::_launch()
    @     0x2ac996b1f18a  _ZZN7process8dispatchIbN5mesos8internal5slave25MesosContainerizerProcessERKNS1_11ContainerIDERK6OptionINS1_8TaskInfoEERKNS1_12ExecutorInfoERKSsRKS8_ISsERKNS1_7SlaveIDERKNS_3PIDINS3_5SlaveEEEbRKS8_INS3_13ProvisionInfoEES5_SA_SD_SsSI_SL_SQ_bSU_EENS_6FutureIT_EERKNSO_IT0_EEMS10_FSZ_T1_T2_T3_T4_T5_T6_T7_T8_T9_ET10_T11_T12_T13_T14_T15_T16_T17_T18_ENKUlPNS_11ProcessBaseEE_clES1P_
    @     0x2ac996b479d9  _ZNSt17_Function_handlerIFvPN7process11ProcessBaseEEZNS0_8dispatchIbN5mesos8internal5slave25MesosContainerizerProcessERKNS5_11ContainerIDERK6OptionINS5_8TaskInfoEERKNS5_12ExecutorInfoERKSsRKSC_ISsERKNS5_7SlaveIDERKNS0_3PIDINS7_5SlaveEEEbRKSC_INS7_13ProvisionInfoEES9_SE_SH_SsSM_SP_SU_bSY_EENS0_6FutureIT_EERKNSS_IT0_EEMS14_FS13_T1_T2_T3_T4_T5_T6_T7_T8_T9_ET10_T11_T12_T13_T14_T15_T16_T17_T18_EUlS2_E_E9_M_invokeERKSt9_Any_dataS2_
    @     0x2ac997334fef  std::function<>::operator()()
    @     0x2ac99731b1c7  process::ProcessBase::visit()
    @     0x2ac997321154  process::DispatchEvent::visit()
    @           0x9a699c  process::ProcessBase::serve()
    @     0x2ac9973173c0  process::ProcessManager::resume()
    @     0x2ac99731445a  _ZZN7process14ProcessManager12init_threadsEvENKUlRKSt11atomic_boolE_clES3_
    @     0x2ac997320916  _ZNSt5_BindIFZN7process14ProcessManager12init_threadsEvEUlRKSt11atomic_boolE_St17reference_wrapperIS3_EEE6__callIvIEILm0EEEET_OSt5tupleIIDpT0_EESt12_Index_tupleIIXspT1_EEE
    @     0x2ac9973208c6  _ZNSt5_BindIFZN7process14ProcessManager12init_threadsEvEUlRKSt11atomic_boolE_St17reference_wrapperIS3_EEEclIIEvEET0_DpOT_
    @     0x2ac997320858  _ZNSt12_Bind_simpleIFSt5_BindIFZN7process14ProcessManager12init_threadsEvEUlRKSt11atomic_boolE_St17reference_wrapperIS4_EEEvEE9_M_invokeIIEEEvSt12_Index_tupleIIXspT_EEE
    @     0x2ac9973207af  _ZNSt12_Bind_simpleIFSt5_BindIFZN7process14ProcessManager12init_threadsEvEUlRKSt11atomic_boolE_St17reference_wrapperIS4_EEEvEEclEv
    @     0x2ac997320748  _ZNSt6thread5_ImplISt12_Bind_simpleIFSt5_BindIFZN7process14ProcessManager12init_threadsEvEUlRKSt11atomic_boolE_St17reference_wrapperIS6_EEEvEEE6_M_runEv
    @     0x2ac9989aea60  (unknown)
    @     0x2ac999125182  start_thread
    @     0x2ac99943547d  (unknown)
make[4]: Leaving directory `/mesos/mesos-0.29.0/_build/src'
make[4]: *** [check-local] Aborted
make[3]: *** [check-am] Error 2
make[3]: Leaving directory `/mesos/mesos-0.29.0/_build/src'
make[2]: *** [check] Error 2
make[2]: Leaving directory `/mesos/mesos-0.29.0/_build/src'
make[1]: *** [check-recursive] Error 1
make[1]: Leaving directory `/mesos/mesos-0.29.0/_build'
make: *** [distcheck] Error 1
Build step 'Execute shell' marked build as failure
{noformat}",Bug,Critical,alexr,2016-03-30T00:54:26.000+0000,5,Resolved,Complete,MesosContainerizerProvisionerTest.DestroyWhileProvisioning is flaky.,2016-03-30T00:54:26.000+0000,MESOS-5023,2.0,mesos,Mesosphere Sprint 32
,2016-03-23T18:04:36.000+0000,vinodkone,A reconnect() method on the library would allow the scheduler to force a reconnection (disconnect and reconnect) by the library. This might be used by the scheduler to react to lack of HEARTBEATs.,Improvement,Major,vinodkone,2016-03-23T19:08:41.000+0000,5,Resolved,Complete,Add a reconnect() method to the C++ scheduler library,2016-03-23T19:08:41.000+0000,MESOS-5016,3.0,mesos,
yongtang,2016-03-23T18:01:16.000+0000,vinodkone,Having a 'required' Type enum has backwards compatibility issues when adding new enum types. See MESOS-4997 for details.,Improvement,Major,vinodkone,2016-03-25T21:48:55.000+0000,5,Resolved,Complete,Call and Event Type enums in executor.proto should be optional,2016-04-22T21:02:59.000+0000,MESOS-5015,2.0,mesos,
yongtang,2016-03-23T18:00:56.000+0000,vinodkone,Having a 'required' Type enum has backwards compatibility issues when adding new enum types. See MESOS-4997 for details.,Improvement,Major,vinodkone,2016-03-25T21:10:44.000+0000,5,Resolved,Complete,Call and Event Type enums in scheduler.proto should be optional,2016-04-22T21:02:55.000+0000,MESOS-5014,2.0,mesos,
gyliu,2016-03-23T16:00:03.000+0000,gyliu,"The isolator will interact with Docker Volume Driver Plugins to mount and unmount external volumes to container.
",Bug,Major,gyliu,2016-04-24T19:05:09.000+0000,5,Resolved,Complete,Add docker volume driver isolator for Mesos containerizer.,2016-04-25T07:06:12.000+0000,MESOS-5013,8.0,mesos,Mesosphere Sprint 33
bbannier,2016-03-22T22:43:46.000+0000,bbannier,"The installation of mesos python package is incomplete, i.e., the files {{cli.py}}, {{futures.py}}, and {{http.py}} are not installed.

{code}
% ../configure --enable-python
% make install DESTDIR=$PWD/D
% PYTHONPATH=$PWD/D/usr/local/lib/python2.7/site-packages:$PYTHONPATH python -c 'from mesos import http'

Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
ImportError: cannot import name http
{code}

This appears to be first broken with {{d1d70b9}} (MESOS-3969, [Upgraded bundled pip to 7.1.2.|https://reviews.apache.org/r/40630]). Bisecting in {{pip}}-land shows that our install becomes broken for {{pip-6.0.1}} and later (we are using {{pip-7.1.2}}).
",Bug,Major,bbannier,2016-04-11T12:32:33.000+0000,5,Resolved,Complete,Installation of mesos python package is incomplete,2016-04-11T12:32:57.000+0000,MESOS-5010,2.0,mesos,Mesosphere Sprint 32
,2016-03-22T18:46:31.000+0000,jojy,Example usage for Appc flags and images needs to be added to container-image.md.,Documentation,Major,jojy,,1,Open,New,Add example for mesos-execute usage of Appc images in container-image.md.,2016-03-23T09:04:26.000+0000,MESOS-5006,3.0,mesos,
greggomann,2016-03-22T18:35:44.000+0000,greggomann,"Currently, we require that {{ReservationInfo.principal}} be equal to the principal provided for authentication, which means that when HTTP authentication is disabled this field cannot be set. Based on comments in 'mesos.proto', the original intention was to enforce this same constraint for {{Persistence.principal}}, but it seems that we don't enforce it. This should be changed to make the two fields equivalent.",Bug,Major,greggomann,,10020,Accepted,In Progress,Enforce that DiskInfo principal is equal to framework/operator principal,2016-05-06T19:44:31.000+0000,MESOS-5005,3.0,mesos,Mesosphere Sprint 32
,2016-03-22T18:29:19.000+0000,greggomann,"For both reservations and persistent volume creation, the behavior of the HTTP endpoints differs slightly from that of the framework operations. Due to the implementation of HTTP authentication, it is not possible for a framework/operator to provide a principal when HTTP authentication is disabled. This means that when HTTP authentication is disabled, the endpoint handlers will _always_ receive {{None()}} as the principal associated with the request, and thus if authorization is enabled, the request will only succeed if the NONE principal is authorized to do stuff.

The docs should be updated to explain this behavior explicitly.",Documentation,Major,greggomann,2016-04-25T23:56:40.000+0000,5,Resolved,Complete,Clarify docs on '/reserve' and '/create-volumes' without authentication,2016-04-25T23:56:40.000+0000,MESOS-5004,1.0,mesos,
js84,2016-03-22T08:20:33.000+0000,js84,"Creating a new subprocess in mesos involves forking/cloning a new process. In most cases (executors, perf, ..) the parent of the new process is the agent/slave process. This can lead to problematic behavior especially when creating several new processes at the same time.

The problem here is that the normal fork() (or clone syscall used by libprocess) provides a copy-on-write (cow) view of the parents address space until the child execs its new binary. Note that during the time between fork and exec Mesos does several setup actions such as placing the new processes in systemd units or assigning them to the freezer cgroup.
This cow property of the address space implies that existing memory is marked as read-only and any write will trigger a page-fault and a newly created page. Note this behavior also extends to the parent process and hence any write will be very costly.

We simulated the number of pagefaults when forking/cloning new processes by this benchmark:
https://github.com/joerg84/forking-benchmark

Results can be seen here: 
https://docs.google.com/presentation/d/1SUjKAVHdrutLPpFJy3Q1yhinG5FOMw3HbbEdzuhZ7A8",Epic,Major,js84,,3,In Progress,In Progress,Problematic fork/clone performance at high load.,2016-03-31T09:54:16.000+0000,MESOS-4998,8.0,mesos,
,2016-03-21T16:50:48.000+0000,skonto,"The SandBox uri of a framework does not work if i just copy paste it to the browser.

For example the following sandbox uri:
http://172.17.0.1:5050/#/slaves/50f87c73-79ef-4f2a-95f0-b2b4062b2de6-S0/frameworks/50f87c73-79ef-4f2a-95f0-b2b4062b2de6-0009/executors/driver-20160321155016-0001/browse

should redirect to:
http://172.17.0.1:5050/#/slaves/50f87c73-79ef-4f2a-95f0-b2b4062b2de6-S0/browse?path=%2Ftmp%2Fmesos%2Fslaves%2F50f87c73-79ef-4f2a-95f0-b2b4062b2de6-S0%2Fframeworks%2F50f87c73-79ef-4f2a-95f0-b2b4062b2de6-0009%2Fexecutors%2Fdriver-20160321155016-0001%2Fruns%2F60533483-31fb-4353-987d-f3393911cc80

yet it fails with the message:
""Failed to find slaves.
Navigate to the slave's sandbox via the Mesos UI.""
and redirects to:
http://172.17.0.1:5050/#/

It is an issue for me because im working on expanding the mesos spark ui with sandbox uri, The other option is to get the slave info and parse the json file there and get executor paths not so straightforward or elegant though.

Moreover i dont see the runs/container_id in the Mesos Proto Api. I guess this is hidden info, this is the needed piece of info to re-write the uri without redirection.
",Bug,Major,skonto,,10020,Accepted,In Progress,sandbox uri does not work outisde mesos http server,2016-04-20T16:59:05.000+0000,MESOS-4992,3.0,mesos,
gilbert,2016-03-18T23:21:02.000+0000,jieyu,"Here is the possible sequence of events:
1) containerizer->launch
2) provisioner->provision is called. it is fetching the image
3) executor registration timed out
4) containerizer->destroy is called
5) container->state is still in PREPARING
6) provisioner->destroy is called

So we can be calling provisioner->destory while provisioner->provision hasn't finished yet. provisioner->destroy might just skip since there's no information about the container yet, and later, provisioner will prepare the root filesystem. This root filesystem will not be destroyed as destroy already finishes.",Bug,Critical,jieyu,2016-03-23T22:39:07.000+0000,5,Resolved,Complete,Destroy a container while it's provisioning can lead to leaked provisioned directories.,2016-03-23T22:40:37.000+0000,MESOS-4985,3.0,mesos,Mesosphere Sprint 31
anandmazumdar,2016-03-18T22:45:23.000+0000,neilc,"Observed on Arch Linux with GCC 6, running in a virtualbox VM:

[ RUN      ] MasterTest.SlavesEndpointTwoSlaves
/mesos-2/src/tests/master_tests.cpp:1710: Failure
Value of: array.get().values.size()
  Actual: 1
Expected: 2u
Which is: 2
[  FAILED  ] MasterTest.SlavesEndpointTwoSlaves (86 ms)

Seems to fail non-deterministically, perhaps more often when there is concurrent CPU load on the machine.",Bug,Major,neilc,2016-03-24T06:51:13.000+0000,5,Resolved,Complete,MasterTest.SlavesEndpointTwoSlaves is flaky,2016-03-24T06:51:13.000+0000,MESOS-4984,2.0,mesos,Mesosphere Sprint 31
anandmazumdar,2016-03-18T20:04:05.000+0000,anandmazumdar,"We need to modify the long running test framework similar to {{src/examples/long_lived_framework.cpp}} to use the v1 API.

This would allow us to vet the v1 API and the scheduler library in test clusters.",Task,Major,anandmazumdar,2016-04-12T19:24:13.000+0000,5,Resolved,Complete,Update example long running to use v1 API.,2016-04-12T19:24:13.000+0000,MESOS-4982,5.0,mesos,Mesosphere Sprint 32
jojy,2016-03-18T17:36:44.000+0000,jojy,mesos-execute cli application currently does not have support for Appc images. Adding support would make integration tests easier.,Bug,Major,jojy,2016-04-08T20:16:01.000+0000,5,Resolved,Complete,Update mesos-execute with Appc changes.,2016-04-08T20:16:01.000+0000,MESOS-4978,3.0,mesos,Mesosphere Sprint 31
,2016-03-17T20:19:47.000+0000,greggomann,"The configuration documentation currently only shows examples of scalar resource types in JSON format. The structures of JSON resources are a bit complicated, so it would be very helpful to include examples of ranges, sets, and text resource types as well.",Documentation,Major,greggomann,,10020,Accepted,In Progress,Add more examples of JSON resources to docs,2016-03-17T20:19:54.000+0000,MESOS-4970,1.0,mesos,
vinodkone,2016-03-16T20:19:31.000+0000,vinodkone,"As part of Mesos reaching 1.0, we need to formalize the policy of supporting Mesos releases.

Some specific questions we need to answer:

--> What fixes should we backports to older releases.

--> How many old releases are supported.

--> Should we have a LTS version?

--> What is the cadence of major, minor and patch releases?",Task,Major,vinodkone,2016-04-06T21:42:21.000+0000,5,Resolved,Complete,Support for Mesos releases,2016-04-08T20:23:51.000+0000,MESOS-4962,8.0,mesos,Mesosphere Sprint 31
kaysoky,2016-03-16T15:16:17.000+0000,kaysoky,"The logger subprocesses may exit before we reach the {{waitpid}} in the test.  If this happens, {{waitpid}} will return a {{-1}} as the process no longer exists.

Verbose logs:
{code}
[ RUN      ] ContainerLoggerTest.LOGROTATE_RotateInSandbox
I0316 14:28:51.329337  1242 cluster.cpp:139] Creating default 'local' authorizer
I0316 14:28:51.332823  1242 leveldb.cpp:174] Opened db in 3.079559ms
I0316 14:28:51.333916  1242 leveldb.cpp:181] Compacted db in 1.054247ms
I0316 14:28:51.333979  1242 leveldb.cpp:196] Created db iterator in 21450ns
I0316 14:28:51.334005  1242 leveldb.cpp:202] Seeked to beginning of db in 2205ns
I0316 14:28:51.334025  1242 leveldb.cpp:271] Iterated through 0 keys in the db in 410ns
I0316 14:28:51.334089  1242 replica.cpp:779] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned
I0316 14:28:51.334661  1275 recover.cpp:447] Starting replica recovery
I0316 14:28:51.335044  1275 recover.cpp:473] Replica is in EMPTY status
I0316 14:28:51.336207  1262 replica.cpp:673] Replica in EMPTY status received a broadcasted recover request from (484)@172.17.0.3:45919
I0316 14:28:51.336730  1270 recover.cpp:193] Received a recover response from a replica in EMPTY status
I0316 14:28:51.337257  1275 recover.cpp:564] Updating replica status to STARTING
I0316 14:28:51.338001  1267 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 537200ns
I0316 14:28:51.338032  1267 replica.cpp:320] Persisted replica status to STARTING
I0316 14:28:51.338183  1261 master.cpp:376] Master c7653f60-33e9-4406-9f62-dc74c906bf83 (2cbb23302fe5) started on 172.17.0.3:45919
I0316 14:28:51.338295  1263 recover.cpp:473] Replica is in STARTING status
I0316 14:28:51.338213  1261 master.cpp:378] Flags at startup: --acls="""" --allocation_interval=""1secs"" --allocator=""HierarchicalDRF"" --authenticate=""true"" --authenticate_http=""true"" --authenticate_slaves=""true"" --authenticators=""crammd5"" --authorizers=""local"" --credentials=""/tmp/XtqwkS/credentials"" --framework_sorter=""drf"" --help=""false"" --hostname_lookup=""true"" --http_authenticators=""basic"" --initialize_driver_logging=""true"" --log_auto_initialize=""true"" --logbufsecs=""0"" --logging_level=""INFO"" --max_completed_frameworks=""50"" --max_completed_tasks_per_framework=""1000"" --max_slave_ping_timeouts=""5"" --quiet=""false"" --recovery_slave_removal_limit=""100%"" --registry=""replicated_log"" --registry_fetch_timeout=""1mins"" --registry_store_timeout=""100secs"" --registry_strict=""true"" --root_submissions=""true"" --slave_ping_timeout=""15secs"" --slave_reregister_timeout=""10mins"" --user_sorter=""drf"" --version=""false"" --webui_dir=""/mesos/mesos-0.29.0/_inst/share/mesos/webui"" --work_dir=""/tmp/XtqwkS/master"" --zk_session_timeout=""10secs""
I0316 14:28:51.338562  1261 master.cpp:423] Master only allowing authenticated frameworks to register
I0316 14:28:51.338572  1261 master.cpp:428] Master only allowing authenticated slaves to register
I0316 14:28:51.338580  1261 credentials.hpp:35] Loading credentials for authentication from '/tmp/XtqwkS/credentials'
I0316 14:28:51.338877  1261 master.cpp:468] Using default 'crammd5' authenticator
I0316 14:28:51.339030  1262 replica.cpp:673] Replica in STARTING status received a broadcasted recover request from (485)@172.17.0.3:45919
I0316 14:28:51.339246  1261 master.cpp:537] Using default 'basic' HTTP authenticator
I0316 14:28:51.339393  1261 master.cpp:571] Authorization enabled
I0316 14:28:51.339390  1266 recover.cpp:193] Received a recover response from a replica in STARTING status
I0316 14:28:51.339606  1271 whitelist_watcher.cpp:77] No whitelist given
I0316 14:28:51.339607  1275 hierarchical.cpp:144] Initialized hierarchical allocator process
I0316 14:28:51.340077  1268 recover.cpp:564] Updating replica status to VOTING
I0316 14:28:51.340533  1270 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 331558ns
I0316 14:28:51.340558  1270 replica.cpp:320] Persisted replica status to VOTING
I0316 14:28:51.340672  1270 recover.cpp:578] Successfully joined the Paxos group
I0316 14:28:51.340827  1270 recover.cpp:462] Recover process terminated
I0316 14:28:51.341684  1270 master.cpp:1806] The newly elected leader is master@172.17.0.3:45919 with id c7653f60-33e9-4406-9f62-dc74c906bf83
I0316 14:28:51.341717  1270 master.cpp:1819] Elected as the leading master!
I0316 14:28:51.341740  1270 master.cpp:1508] Recovering from registrar
I0316 14:28:51.341954  1263 registrar.cpp:307] Recovering registrar
I0316 14:28:51.342499  1273 log.cpp:659] Attempting to start the writer
I0316 14:28:51.343616  1266 replica.cpp:493] Replica received implicit promise request from (487)@172.17.0.3:45919 with proposal 1
I0316 14:28:51.344183  1266 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 536941ns
I0316 14:28:51.344208  1266 replica.cpp:342] Persisted promised to 1
I0316 14:28:51.344825  1267 coordinator.cpp:238] Coordinator attempting to fill missing positions
I0316 14:28:51.346009  1276 replica.cpp:388] Replica received explicit promise request from (488)@172.17.0.3:45919 for position 0 with proposal 2
I0316 14:28:51.346371  1276 leveldb.cpp:341] Persisting action (8 bytes) to leveldb took 327890ns
I0316 14:28:51.346393  1276 replica.cpp:712] Persisted action at 0
I0316 14:28:51.347363  1267 replica.cpp:537] Replica received write request for position 0 from (489)@172.17.0.3:45919
I0316 14:28:51.347414  1267 leveldb.cpp:436] Reading position from leveldb took 24861ns
I0316 14:28:51.347774  1267 leveldb.cpp:341] Persisting action (14 bytes) to leveldb took 323654ns
I0316 14:28:51.347796  1267 replica.cpp:712] Persisted action at 0
I0316 14:28:51.348323  1276 replica.cpp:691] Replica received learned notice for position 0 from @0.0.0.0:0
I0316 14:28:51.348714  1276 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 361981ns
I0316 14:28:51.348738  1276 replica.cpp:712] Persisted action at 0
I0316 14:28:51.348760  1276 replica.cpp:697] Replica learned NOP action at position 0
I0316 14:28:51.349318  1274 log.cpp:675] Writer started with ending position 0
I0316 14:28:51.350275  1267 leveldb.cpp:436] Reading position from leveldb took 23849ns
I0316 14:28:51.351171  1271 registrar.cpp:340] Successfully fetched the registry (0B) in 9.173248ms
I0316 14:28:51.351300  1271 registrar.cpp:439] Applied 1 operations in 32119ns; attempting to update the 'registry'
I0316 14:28:51.351989  1272 log.cpp:683] Attempting to append 170 bytes to the log
I0316 14:28:51.352108  1266 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 1
I0316 14:28:51.352802  1263 replica.cpp:537] Replica received write request for position 1 from (490)@172.17.0.3:45919
I0316 14:28:51.353313  1263 leveldb.cpp:341] Persisting action (189 bytes) to leveldb took 474854ns
I0316 14:28:51.353338  1263 replica.cpp:712] Persisted action at 1
I0316 14:28:51.354101  1273 replica.cpp:691] Replica received learned notice for position 1 from @0.0.0.0:0
I0316 14:28:51.354483  1273 leveldb.cpp:341] Persisting action (191 bytes) to leveldb took 338210ns
I0316 14:28:51.354507  1273 replica.cpp:712] Persisted action at 1
I0316 14:28:51.354529  1273 replica.cpp:697] Replica learned APPEND action at position 1
I0316 14:28:51.355444  1275 registrar.cpp:484] Successfully updated the 'registry' in 4.084224ms
I0316 14:28:51.355569  1275 registrar.cpp:370] Successfully recovered registrar
I0316 14:28:51.355697  1268 log.cpp:702] Attempting to truncate the log to 1
I0316 14:28:51.355870  1269 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 2
I0316 14:28:51.356016  1274 master.cpp:1616] Recovered 0 slaves from the Registry (131B) ; allowing 10mins for slaves to re-register
I0316 14:28:51.356032  1272 hierarchical.cpp:171] Skipping recovery of hierarchical allocator: nothing to recover
I0316 14:28:51.356761  1273 replica.cpp:537] Replica received write request for position 2 from (491)@172.17.0.3:45919
I0316 14:28:51.357203  1273 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 406053ns
I0316 14:28:51.357226  1273 replica.cpp:712] Persisted action at 2
I0316 14:28:51.357718  1270 replica.cpp:691] Replica received learned notice for position 2 from @0.0.0.0:0
I0316 14:28:51.358093  1270 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 345370ns
I0316 14:28:51.358175  1270 leveldb.cpp:399] Deleting ~1 keys from leveldb took 57us
I0316 14:28:51.358201  1270 replica.cpp:712] Persisted action at 2
I0316 14:28:51.358220  1270 replica.cpp:697] Replica learned TRUNCATE action at position 2
I0316 14:28:51.368399  1242 containerizer.cpp:149] Using isolation: posix/cpu,posix/mem,filesystem/posix
W0316 14:28:51.406371  1242 backend.cpp:66] Failed to create 'bind' backend: BindBackend requires root privileges
I0316 14:28:51.410480  1266 slave.cpp:193] Slave started on 12)@172.17.0.3:45919
I0316 14:28:51.410518  1266 slave.cpp:194] Flags at startup: --appc_simple_discovery_uri_prefix=""http://"" --appc_store_dir=""/tmp/mesos/store/appc"" --authenticatee=""crammd5"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --container_logger=""org_apache_mesos_LogrotateContainerLogger"" --containerizers=""mesos"" --credential=""/tmp/ContainerLoggerTest_LOGROTATE_RotateInSandbox_JHP0gy/credential"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_kill_orphans=""true"" --docker_registry=""https://registry-1.docker.io"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/tmp/mesos/store/docker"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/tmp/ContainerLoggerTest_LOGROTATE_RotateInSandbox_JHP0gy/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --hostname_lookup=""true"" --image_provisioner_backend=""copy"" --initialize_driver_logging=""true"" --isolation=""posix/cpu,posix/mem"" --launcher_dir=""/mesos/mesos-0.29.0/_build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""10ms"" --resources=""cpus:2;mem:1024;disk:1024;ports:[31000-32000]"" --revocable_cpu_low_priority=""true"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --systemd_enable_support=""true"" --systemd_runtime_directory=""/run/systemd/system"" --version=""false"" --work_dir=""/tmp/ContainerLoggerTest_LOGROTATE_RotateInSandbox_JHP0gy""
I0316 14:28:51.411118  1266 credentials.hpp:83] Loading credential for authentication from '/tmp/ContainerLoggerTest_LOGROTATE_RotateInSandbox_JHP0gy/credential'
I0316 14:28:51.411381  1266 slave.cpp:324] Slave using credential for: test-principal
I0316 14:28:51.411696  1266 resources.cpp:572] Parsing resources as JSON failed: cpus:2;mem:1024;disk:1024;ports:[31000-32000]
Trying semicolon-delimited string format instead
I0316 14:28:51.412075  1266 slave.cpp:464] Slave resources: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I0316 14:28:51.412148  1266 slave.cpp:472] Slave attributes: [  ]
I0316 14:28:51.412160  1266 slave.cpp:477] Slave hostname: 2cbb23302fe5
I0316 14:28:51.413516  1263 state.cpp:58] Recovering state from '/tmp/ContainerLoggerTest_LOGROTATE_RotateInSandbox_JHP0gy/meta'
I0316 14:28:51.413774  1266 status_update_manager.cpp:200] Recovering status update manager
I0316 14:28:51.414029  1276 containerizer.cpp:407] Recovering containerizer
I0316 14:28:51.415222  1269 provisioner.cpp:245] Provisioner recovery complete
I0316 14:28:51.415650  1268 slave.cpp:4565] Finished recovery
I0316 14:28:51.416115  1268 slave.cpp:4737] Querying resource estimator for oversubscribable resources
I0316 14:28:51.416365  1268 slave.cpp:796] New master detected at master@172.17.0.3:45919
I0316 14:28:51.416448  1276 status_update_manager.cpp:174] Pausing sending status updates
I0316 14:28:51.416445  1268 slave.cpp:859] Authenticating with master master@172.17.0.3:45919
I0316 14:28:51.416522  1268 slave.cpp:864] Using default CRAM-MD5 authenticatee
I0316 14:28:51.416671  1268 slave.cpp:832] Detecting new master
I0316 14:28:51.416731  1275 authenticatee.cpp:121] Creating new client SASL connection
I0316 14:28:51.416807  1268 slave.cpp:4751] Received oversubscribable resources  from the resource estimator
I0316 14:28:51.417006  1263 master.cpp:5659] Authenticating slave(12)@172.17.0.3:45919
I0316 14:28:51.417103  1262 authenticator.cpp:413] Starting authentication session for crammd5_authenticatee(38)@172.17.0.3:45919
I0316 14:28:51.417348  1273 authenticator.cpp:98] Creating new server SASL connection
I0316 14:28:51.417548  1266 authenticatee.cpp:212] Received SASL authentication mechanisms: CRAM-MD5
I0316 14:28:51.417582  1266 authenticatee.cpp:238] Attempting to authenticate with mechanism 'CRAM-MD5'
I0316 14:28:51.417696  1264 authenticator.cpp:203] Received SASL authentication start
I0316 14:28:51.417753  1264 authenticator.cpp:325] Authentication requires more steps
I0316 14:28:51.417948  1265 authenticatee.cpp:258] Received SASL authentication step
I0316 14:28:51.418107  1267 authenticator.cpp:231] Received SASL authentication step
I0316 14:28:51.418159  1267 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: '2cbb23302fe5' server FQDN: '2cbb23302fe5' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0316 14:28:51.418180  1267 auxprop.cpp:179] Looking up auxiliary property '*userPassword'
I0316 14:28:51.418233  1267 auxprop.cpp:179] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0316 14:28:51.418270  1267 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: '2cbb23302fe5' server FQDN: '2cbb23302fe5' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0316 14:28:51.418289  1267 auxprop.cpp:129] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0316 14:28:51.418300  1267 auxprop.cpp:129] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0316 14:28:51.418323  1267 authenticator.cpp:317] Authentication success
I0316 14:28:51.418414  1264 authenticatee.cpp:298] Authentication success
I0316 14:28:51.418473  1269 master.cpp:5689] Successfully authenticated principal 'test-principal' at slave(12)@172.17.0.3:45919
I0316 14:28:51.418514  1275 authenticator.cpp:431] Authentication session cleanup for crammd5_authenticatee(38)@172.17.0.3:45919
I0316 14:28:51.418781  1276 slave.cpp:927] Successfully authenticated with master master@172.17.0.3:45919
I0316 14:28:51.418937  1276 slave.cpp:1321] Will retry registration in 1.983001ms if necessary
I0316 14:28:51.419108  1262 master.cpp:4370] Registering slave at slave(12)@172.17.0.3:45919 (2cbb23302fe5) with id c7653f60-33e9-4406-9f62-dc74c906bf83-S0
I0316 14:28:51.419643  1266 registrar.cpp:439] Applied 1 operations in 75642ns; attempting to update the 'registry'
I0316 14:28:51.420670  1272 log.cpp:683] Attempting to append 339 bytes to the log
I0316 14:28:51.420820  1269 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 3
I0316 14:28:51.421495  1270 slave.cpp:1321] Will retry registration in 1.437257ms if necessary
I0316 14:28:51.421716  1275 master.cpp:4358] Ignoring register slave message from slave(12)@172.17.0.3:45919 (2cbb23302fe5) as admission is already in progress
I0316 14:28:51.422107  1267 replica.cpp:537] Replica received write request for position 3 from (505)@172.17.0.3:45919
I0316 14:28:51.423033  1267 leveldb.cpp:341] Persisting action (358 bytes) to leveldb took 762815ns
I0316 14:28:51.423066  1267 replica.cpp:712] Persisted action at 3
I0316 14:28:51.424069  1267 replica.cpp:691] Replica received learned notice for position 3 from @0.0.0.0:0
I0316 14:28:51.424232  1264 slave.cpp:1321] Will retry registration in 66.01292ms if necessary
I0316 14:28:51.424342  1269 master.cpp:4358] Ignoring register slave message from slave(12)@172.17.0.3:45919 (2cbb23302fe5) as admission is already in progress
I0316 14:28:51.424686  1267 leveldb.cpp:341] Persisting action (360 bytes) to leveldb took 574743ns
I0316 14:28:51.424757  1267 replica.cpp:712] Persisted action at 3
I0316 14:28:51.424792  1267 replica.cpp:697] Replica learned APPEND action at position 3
I0316 14:28:51.426441  1272 registrar.cpp:484] Successfully updated the 'registry' in 6.721024ms
I0316 14:28:51.426677  1262 log.cpp:702] Attempting to truncate the log to 3
I0316 14:28:51.426808  1264 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 4
I0316 14:28:51.427584  1261 slave.cpp:3482] Received ping from slave-observer(11)@172.17.0.3:45919
I0316 14:28:51.428213  1262 hierarchical.cpp:473] Added slave c7653f60-33e9-4406-9f62-dc74c906bf83-S0 (2cbb23302fe5) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (allocated: )
I0316 14:28:51.427865  1266 master.cpp:4438] Registered slave c7653f60-33e9-4406-9f62-dc74c906bf83-S0 at slave(12)@172.17.0.3:45919 (2cbb23302fe5) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I0316 14:28:51.428270  1267 slave.cpp:971] Registered with master master@172.17.0.3:45919; given slave ID c7653f60-33e9-4406-9f62-dc74c906bf83-S0
I0316 14:28:51.428412  1265 replica.cpp:537] Replica received write request for position 4 from (506)@172.17.0.3:45919
I0316 14:28:51.428443  1267 fetcher.cpp:81] Clearing fetcher cache
I0316 14:28:51.428503  1262 hierarchical.cpp:1453] No resources available to allocate!
I0316 14:28:51.428535  1262 hierarchical.cpp:1150] Performed allocation for slave c7653f60-33e9-4406-9f62-dc74c906bf83-S0 in 205421ns
I0316 14:28:51.428750  1273 status_update_manager.cpp:181] Resuming sending status updates
I0316 14:28:51.429157  1265 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 695258ns
I0316 14:28:51.429225  1267 slave.cpp:994] Checkpointing SlaveInfo to '/tmp/ContainerLoggerTest_LOGROTATE_RotateInSandbox_JHP0gy/meta/slaves/c7653f60-33e9-4406-9f62-dc74c906bf83-S0/slave.info'
I0316 14:28:51.429275  1265 replica.cpp:712] Persisted action at 4
I0316 14:28:51.429759  1267 slave.cpp:1030] Forwarding total oversubscribed resources 
I0316 14:28:51.430055  1265 master.cpp:4782] Received update of slave c7653f60-33e9-4406-9f62-dc74c906bf83-S0 at slave(12)@172.17.0.3:45919 (2cbb23302fe5) with total oversubscribed resources 
I0316 14:28:51.430614  1271 replica.cpp:691] Replica received learned notice for position 4 from @0.0.0.0:0
I0316 14:28:51.430891  1242 sched.cpp:222] Version: 0.29.0
I0316 14:28:51.431043  1265 hierarchical.cpp:531] Slave c7653f60-33e9-4406-9f62-dc74c906bf83-S0 (2cbb23302fe5) updated with oversubscribed resources  (total: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000], allocated: )
I0316 14:28:51.431236  1271 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 536892ns
I0316 14:28:51.431267  1265 hierarchical.cpp:1453] No resources available to allocate!
I0316 14:28:51.431584  1271 leveldb.cpp:399] Deleting ~2 keys from leveldb took 66904ns
I0316 14:28:51.431538  1273 sched.cpp:326] New master detected at master@172.17.0.3:45919
I0316 14:28:51.431622  1271 replica.cpp:712] Persisted action at 4
I0316 14:28:51.431623  1265 hierarchical.cpp:1150] Performed allocation for slave c7653f60-33e9-4406-9f62-dc74c906bf83-S0 in 518588ns
I0316 14:28:51.431660  1271 replica.cpp:697] Replica learned TRUNCATE action at position 4
I0316 14:28:51.431711  1273 sched.cpp:382] Authenticating with master master@172.17.0.3:45919
I0316 14:28:51.431737  1273 sched.cpp:389] Using default CRAM-MD5 authenticatee
I0316 14:28:51.431982  1266 authenticatee.cpp:121] Creating new client SASL connection
I0316 14:28:51.432369  1261 master.cpp:5659] Authenticating scheduler-96f85a94-b6a8-4363-bc3b-b8a233b90e79@172.17.0.3:45919
I0316 14:28:51.432509  1263 authenticator.cpp:413] Starting authentication session for crammd5_authenticatee(39)@172.17.0.3:45919
I0316 14:28:51.432868  1267 authenticator.cpp:98] Creating new server SASL connection
I0316 14:28:51.433135  1276 authenticatee.cpp:212] Received SASL authentication mechanisms: CRAM-MD5
I0316 14:28:51.433233  1276 authenticatee.cpp:238] Attempting to authenticate with mechanism 'CRAM-MD5'
I0316 14:28:51.433423  1276 authenticator.cpp:203] Received SASL authentication start
I0316 14:28:51.433502  1276 authenticator.cpp:325] Authentication requires more steps
I0316 14:28:51.433606  1274 authenticatee.cpp:258] Received SASL authentication step
I0316 14:28:51.433744  1273 authenticator.cpp:231] Received SASL authentication step
I0316 14:28:51.433785  1273 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: '2cbb23302fe5' server FQDN: '2cbb23302fe5' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0316 14:28:51.433801  1273 auxprop.cpp:179] Looking up auxiliary property '*userPassword'
I0316 14:28:51.433861  1273 auxprop.cpp:179] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0316 14:28:51.433897  1273 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: '2cbb23302fe5' server FQDN: '2cbb23302fe5' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0316 14:28:51.433912  1273 auxprop.cpp:129] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0316 14:28:51.433924  1273 auxprop.cpp:129] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0316 14:28:51.433944  1273 authenticator.cpp:317] Authentication success
I0316 14:28:51.434037  1274 authenticatee.cpp:298] Authentication success
I0316 14:28:51.434108  1268 master.cpp:5689] Successfully authenticated principal 'test-principal' at scheduler-96f85a94-b6a8-4363-bc3b-b8a233b90e79@172.17.0.3:45919
I0316 14:28:51.434211  1272 authenticator.cpp:431] Authentication session cleanup for crammd5_authenticatee(39)@172.17.0.3:45919
I0316 14:28:51.434512  1274 sched.cpp:471] Successfully authenticated with master master@172.17.0.3:45919
I0316 14:28:51.434535  1274 sched.cpp:776] Sending SUBSCRIBE call to master@172.17.0.3:45919
I0316 14:28:51.434648  1274 sched.cpp:809] Will retry registration in 356.547014ms if necessary
I0316 14:28:51.434819  1266 master.cpp:2326] Received SUBSCRIBE call for framework 'default' at scheduler-96f85a94-b6a8-4363-bc3b-b8a233b90e79@172.17.0.3:45919
I0316 14:28:51.434905  1266 master.cpp:1845] Authorizing framework principal 'test-principal' to receive offers for role '*'
I0316 14:28:51.435464  1265 master.cpp:2397] Subscribing framework default with checkpointing disabled and capabilities [  ]
I0316 14:28:51.435979  1269 hierarchical.cpp:265] Added framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000
I0316 14:28:51.436213  1272 sched.cpp:703] Framework registered with c7653f60-33e9-4406-9f62-dc74c906bf83-0000
I0316 14:28:51.436316  1272 sched.cpp:717] Scheduler::registered took 73782ns
I0316 14:28:51.436928  1269 hierarchical.cpp:1548] No inverse offers to send out!
I0316 14:28:51.436978  1269 hierarchical.cpp:1130] Performed allocation for 1 slaves in 970638ns
I0316 14:28:51.437278  1272 master.cpp:5488] Sending 1 offers to framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000 (default) at scheduler-96f85a94-b6a8-4363-bc3b-b8a233b90e79@172.17.0.3:45919
I0316 14:28:51.437782  1262 sched.cpp:873] Scheduler::resourceOffers took 129952ns
I0316 14:28:51.440006  1274 master.cpp:3268] Processing ACCEPT call for offers: [ c7653f60-33e9-4406-9f62-dc74c906bf83-O0 ] on slave c7653f60-33e9-4406-9f62-dc74c906bf83-S0 at slave(12)@172.17.0.3:45919 (2cbb23302fe5) for framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000 (default) at scheduler-96f85a94-b6a8-4363-bc3b-b8a233b90e79@172.17.0.3:45919
I0316 14:28:51.440094  1274 master.cpp:2871] Authorizing framework principal 'test-principal' to launch task 864698ee-117b-4b95-b8d7-4c3ec6e0b917 as user 'mesos'
I0316 14:28:51.442152  1274 master.hpp:177] Adding task 864698ee-117b-4b95-b8d7-4c3ec6e0b917 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave c7653f60-33e9-4406-9f62-dc74c906bf83-S0 (2cbb23302fe5)
I0316 14:28:51.442348  1274 master.cpp:3753] Launching task 864698ee-117b-4b95-b8d7-4c3ec6e0b917 of framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000 (default) at scheduler-96f85a94-b6a8-4363-bc3b-b8a233b90e79@172.17.0.3:45919 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave c7653f60-33e9-4406-9f62-dc74c906bf83-S0 at slave(12)@172.17.0.3:45919 (2cbb23302fe5)
I0316 14:28:51.442749  1265 slave.cpp:1361] Got assigned task 864698ee-117b-4b95-b8d7-4c3ec6e0b917 for framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000
I0316 14:28:51.443006  1265 resources.cpp:572] Parsing resources as JSON failed: cpus:0.1;mem:32
Trying semicolon-delimited string format instead
I0316 14:28:51.443624  1265 slave.cpp:1480] Launching task 864698ee-117b-4b95-b8d7-4c3ec6e0b917 for framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000
I0316 14:28:51.443730  1265 resources.cpp:572] Parsing resources as JSON failed: cpus:0.1;mem:32
Trying semicolon-delimited string format instead
I0316 14:28:51.444629  1265 paths.cpp:528] Trying to chown '/tmp/ContainerLoggerTest_LOGROTATE_RotateInSandbox_JHP0gy/slaves/c7653f60-33e9-4406-9f62-dc74c906bf83-S0/frameworks/c7653f60-33e9-4406-9f62-dc74c906bf83-0000/executors/864698ee-117b-4b95-b8d7-4c3ec6e0b917/runs/6e2770ca-32d3-47ad-b4fe-7d9f26489621' to user 'mesos'
I0316 14:28:51.449493  1265 slave.cpp:5367] Launching executor 864698ee-117b-4b95-b8d7-4c3ec6e0b917 of framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000 with resources cpus(*):0.1; mem(*):32 in work directory '/tmp/ContainerLoggerTest_LOGROTATE_RotateInSandbox_JHP0gy/slaves/c7653f60-33e9-4406-9f62-dc74c906bf83-S0/frameworks/c7653f60-33e9-4406-9f62-dc74c906bf83-0000/executors/864698ee-117b-4b95-b8d7-4c3ec6e0b917/runs/6e2770ca-32d3-47ad-b4fe-7d9f26489621'
I0316 14:28:51.450256  1261 containerizer.cpp:666] Starting container '6e2770ca-32d3-47ad-b4fe-7d9f26489621' for executor '864698ee-117b-4b95-b8d7-4c3ec6e0b917' of framework 'c7653f60-33e9-4406-9f62-dc74c906bf83-0000'
I0316 14:28:51.450299  1265 slave.cpp:1698] Queuing task '864698ee-117b-4b95-b8d7-4c3ec6e0b917' for executor '864698ee-117b-4b95-b8d7-4c3ec6e0b917' of framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000
I0316 14:28:51.450428  1265 slave.cpp:749] Successfully attached file '/tmp/ContainerLoggerTest_LOGROTATE_RotateInSandbox_JHP0gy/slaves/c7653f60-33e9-4406-9f62-dc74c906bf83-S0/frameworks/c7653f60-33e9-4406-9f62-dc74c906bf83-0000/executors/864698ee-117b-4b95-b8d7-4c3ec6e0b917/runs/6e2770ca-32d3-47ad-b4fe-7d9f26489621'
I0316 14:28:51.459421  1268 launcher.cpp:147] Forked child with pid '1453' for container '6e2770ca-32d3-47ad-b4fe-7d9f26489621'
I0316 14:28:51.613296  1274 slave.cpp:2643] Got registration for executor '864698ee-117b-4b95-b8d7-4c3ec6e0b917' of framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000 from executor(1)@172.17.0.3:56062
I0316 14:28:51.615416  1271 slave.cpp:1863] Sending queued task '864698ee-117b-4b95-b8d7-4c3ec6e0b917' to executor '864698ee-117b-4b95-b8d7-4c3ec6e0b917' of framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000 at executor(1)@172.17.0.3:56062
I0316 14:28:51.622187  1272 slave.cpp:3002] Handling status update TASK_RUNNING (UUID: aee0de1c-8acd-46eb-8723-d26cd203228f) for task 864698ee-117b-4b95-b8d7-4c3ec6e0b917 of framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000 from executor(1)@172.17.0.3:56062
I0316 14:28:51.623610  1275 status_update_manager.cpp:320] Received status update TASK_RUNNING (UUID: aee0de1c-8acd-46eb-8723-d26cd203228f) for task 864698ee-117b-4b95-b8d7-4c3ec6e0b917 of framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000
I0316 14:28:51.623646  1275 status_update_manager.cpp:497] Creating StatusUpdate stream for task 864698ee-117b-4b95-b8d7-4c3ec6e0b917 of framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000
I0316 14:28:51.624053  1275 status_update_manager.cpp:374] Forwarding update TASK_RUNNING (UUID: aee0de1c-8acd-46eb-8723-d26cd203228f) for task 864698ee-117b-4b95-b8d7-4c3ec6e0b917 of framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000 to the slave
I0316 14:28:51.624423  1274 slave.cpp:3400] Forwarding the update TASK_RUNNING (UUID: aee0de1c-8acd-46eb-8723-d26cd203228f) for task 864698ee-117b-4b95-b8d7-4c3ec6e0b917 of framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000 to master@172.17.0.3:45919
I0316 14:28:51.624621  1274 slave.cpp:3294] Status update manager successfully handled status update TASK_RUNNING (UUID: aee0de1c-8acd-46eb-8723-d26cd203228f) for task 864698ee-117b-4b95-b8d7-4c3ec6e0b917 of framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000
I0316 14:28:51.624677  1274 slave.cpp:3310] Sending acknowledgement for status update TASK_RUNNING (UUID: aee0de1c-8acd-46eb-8723-d26cd203228f) for task 864698ee-117b-4b95-b8d7-4c3ec6e0b917 of framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000 to executor(1)@172.17.0.3:56062
I0316 14:28:51.624836  1270 master.cpp:4927] Status update TASK_RUNNING (UUID: aee0de1c-8acd-46eb-8723-d26cd203228f) for task 864698ee-117b-4b95-b8d7-4c3ec6e0b917 of framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000 from slave c7653f60-33e9-4406-9f62-dc74c906bf83-S0 at slave(12)@172.17.0.3:45919 (2cbb23302fe5)
I0316 14:28:51.624881  1270 master.cpp:4975] Forwarding status update TASK_RUNNING (UUID: aee0de1c-8acd-46eb-8723-d26cd203228f) for task 864698ee-117b-4b95-b8d7-4c3ec6e0b917 of framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000
I0316 14:28:51.625077  1270 master.cpp:6588] Updating the state of task 864698ee-117b-4b95-b8d7-4c3ec6e0b917 of framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000 (latest state: TASK_RUNNING, status update state: TASK_RUNNING)
I0316 14:28:51.625355  1269 sched.cpp:981] Scheduler::statusUpdate took 141149ns
I0316 14:28:51.625671  1266 master.cpp:4082] Processing ACKNOWLEDGE call aee0de1c-8acd-46eb-8723-d26cd203228f for task 864698ee-117b-4b95-b8d7-4c3ec6e0b917 of framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000 (default) at scheduler-96f85a94-b6a8-4363-bc3b-b8a233b90e79@172.17.0.3:45919 on slave c7653f60-33e9-4406-9f62-dc74c906bf83-S0
I0316 14:28:51.625977  1267 status_update_manager.cpp:392] Received status update acknowledgement (UUID: aee0de1c-8acd-46eb-8723-d26cd203228f) for task 864698ee-117b-4b95-b8d7-4c3ec6e0b917 of framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000
I0316 14:28:51.626369  1265 slave.cpp:2412] Status update manager successfully handled status update acknowledgement (UUID: aee0de1c-8acd-46eb-8723-d26cd203228f) for task 864698ee-117b-4b95-b8d7-4c3ec6e0b917 of framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000
I0316 14:28:52.340801  1266 hierarchical.cpp:1453] No resources available to allocate!
I0316 14:28:52.340884  1266 hierarchical.cpp:1548] No inverse offers to send out!
I0316 14:28:52.340922  1266 hierarchical.cpp:1130] Performed allocation for 1 slaves in 350313ns
I0316 14:28:53.342003  1263 hierarchical.cpp:1453] No resources available to allocate!
I0316 14:28:53.342077  1263 hierarchical.cpp:1548] No inverse offers to send out!
I0316 14:28:53.342110  1263 hierarchical.cpp:1130] Performed allocation for 1 slaves in 332715ns
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0316 14:28:53.619144  1451 process.cpp:986] libprocess is initialized on 172.17.0.3:40885 for 16 cpus
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0316 14:28:53.790701  1452 process.cpp:986] libprocess is initialized on 172.17.0.3:50144 for 16 cpus
I0316 14:28:53.939643  1268 slave.cpp:3002] Handling status update TASK_FINISHED (UUID: a873c6e2-442e-439e-a13f-54bb19df1881) for task 864698ee-117b-4b95-b8d7-4c3ec6e0b917 of framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000 from executor(1)@172.17.0.3:56062
I0316 14:28:53.940950  1267 slave.cpp:5677] Terminating task 864698ee-117b-4b95-b8d7-4c3ec6e0b917
I0316 14:28:53.942181  1275 status_update_manager.cpp:320] Received status update TASK_FINISHED (UUID: a873c6e2-442e-439e-a13f-54bb19df1881) for task 864698ee-117b-4b95-b8d7-4c3ec6e0b917 of framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000
I0316 14:28:53.942358  1275 status_update_manager.cpp:374] Forwarding update TASK_FINISHED (UUID: a873c6e2-442e-439e-a13f-54bb19df1881) for task 864698ee-117b-4b95-b8d7-4c3ec6e0b917 of framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000 to the slave
I0316 14:28:53.942715  1265 slave.cpp:3400] Forwarding the update TASK_FINISHED (UUID: a873c6e2-442e-439e-a13f-54bb19df1881) for task 864698ee-117b-4b95-b8d7-4c3ec6e0b917 of framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000 to master@172.17.0.3:45919
I0316 14:28:53.942919  1265 slave.cpp:3294] Status update manager successfully handled status update TASK_FINISHED (UUID: a873c6e2-442e-439e-a13f-54bb19df1881) for task 864698ee-117b-4b95-b8d7-4c3ec6e0b917 of framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000
I0316 14:28:53.942961  1265 slave.cpp:3310] Sending acknowledgement for status update TASK_FINISHED (UUID: a873c6e2-442e-439e-a13f-54bb19df1881) for task 864698ee-117b-4b95-b8d7-4c3ec6e0b917 of framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000 to executor(1)@172.17.0.3:56062
I0316 14:28:53.943159  1273 master.cpp:4927] Status update TASK_FINISHED (UUID: a873c6e2-442e-439e-a13f-54bb19df1881) for task 864698ee-117b-4b95-b8d7-4c3ec6e0b917 of framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000 from slave c7653f60-33e9-4406-9f62-dc74c906bf83-S0 at slave(12)@172.17.0.3:45919 (2cbb23302fe5)
I0316 14:28:53.943218  1273 master.cpp:4975] Forwarding status update TASK_FINISHED (UUID: a873c6e2-442e-439e-a13f-54bb19df1881) for task 864698ee-117b-4b95-b8d7-4c3ec6e0b917 of framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000
I0316 14:28:53.943392  1273 master.cpp:6588] Updating the state of task 864698ee-117b-4b95-b8d7-4c3ec6e0b917 of framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000 (latest state: TASK_FINISHED, status update state: TASK_FINISHED)
I0316 14:28:53.944248  1275 sched.cpp:981] Scheduler::statusUpdate took 172957ns
I0316 14:28:53.944351  1262 hierarchical.cpp:890] Recovered cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (total: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000], allocated: ) on slave c7653f60-33e9-4406-9f62-dc74c906bf83-S0 from framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000
I0316 14:28:53.944548  1242 sched.cpp:1903] Asked to stop the driver
I0316 14:28:53.944672  1275 sched.cpp:1143] Stopping framework 'c7653f60-33e9-4406-9f62-dc74c906bf83-0000'
I0316 14:28:53.944736  1263 master.cpp:4082] Processing ACKNOWLEDGE call a873c6e2-442e-439e-a13f-54bb19df1881 for task 864698ee-117b-4b95-b8d7-4c3ec6e0b917 of framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000 (default) at scheduler-96f85a94-b6a8-4363-bc3b-b8a233b90e79@172.17.0.3:45919 on slave c7653f60-33e9-4406-9f62-dc74c906bf83-S0
I0316 14:28:53.944795  1263 master.cpp:6654] Removing task 864698ee-117b-4b95-b8d7-4c3ec6e0b917 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] of framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000 on slave c7653f60-33e9-4406-9f62-dc74c906bf83-S0 at slave(12)@172.17.0.3:45919 (2cbb23302fe5)
I0316 14:28:53.945226  1263 master.cpp:6061] Processing TEARDOWN call for framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000 (default) at scheduler-96f85a94-b6a8-4363-bc3b-b8a233b90e79@172.17.0.3:45919
I0316 14:28:53.945253  1263 master.cpp:6073] Removing framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000 (default) at scheduler-96f85a94-b6a8-4363-bc3b-b8a233b90e79@172.17.0.3:45919
I0316 14:28:53.945324  1275 status_update_manager.cpp:392] Received status update acknowledgement (UUID: a873c6e2-442e-439e-a13f-54bb19df1881) for task 864698ee-117b-4b95-b8d7-4c3ec6e0b917 of framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000
I0316 14:28:53.945412  1274 hierarchical.cpp:375] Deactivated framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000
I0316 14:28:53.945462  1276 slave.cpp:2079] Asked to shut down framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000 by master@172.17.0.3:45919
I0316 14:28:53.945579  1276 slave.cpp:2104] Shutting down framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000
I0316 14:28:53.945669  1276 slave.cpp:4198] Shutting down executor '864698ee-117b-4b95-b8d7-4c3ec6e0b917' of framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000 at executor(1)@172.17.0.3:56062
I0316 14:28:53.945714  1275 status_update_manager.cpp:528] Cleaning up status update stream for task 864698ee-117b-4b95-b8d7-4c3ec6e0b917 of framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000
I0316 14:28:53.945818  1274 hierarchical.cpp:326] Removed framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000
I0316 14:28:53.946151  1265 slave.cpp:2412] Status update manager successfully handled status update acknowledgement (UUID: a873c6e2-442e-439e-a13f-54bb19df1881) for task 864698ee-117b-4b95-b8d7-4c3ec6e0b917 of framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000
I0316 14:28:53.946213  1265 slave.cpp:5718] Completing task 864698ee-117b-4b95-b8d7-4c3ec6e0b917
I0316 14:28:54.343000  1263 hierarchical.cpp:1453] No resources available to allocate!
I0316 14:28:54.343056  1263 hierarchical.cpp:1130] Performed allocation for 1 slaves in 213036ns
I0316 14:28:54.943627  1261 slave.cpp:3528] executor(1)@172.17.0.3:56062 exited
I0316 14:28:54.944002  1274 containerizer.cpp:1608] Executor for container '6e2770ca-32d3-47ad-b4fe-7d9f26489621' has exited
I0316 14:28:54.944205  1274 containerizer.cpp:1392] Destroying container '6e2770ca-32d3-47ad-b4fe-7d9f26489621'
I0316 14:28:54.949076  1276 provisioner.cpp:306] Ignoring destroy request for unknown container 6e2770ca-32d3-47ad-b4fe-7d9f26489621
I0316 14:28:54.949502  1276 slave.cpp:3886] Executor '864698ee-117b-4b95-b8d7-4c3ec6e0b917' of framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000 exited with status 0
I0316 14:28:54.949556  1276 slave.cpp:3990] Cleaning up executor '864698ee-117b-4b95-b8d7-4c3ec6e0b917' of framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000 at executor(1)@172.17.0.3:56062
I0316 14:28:54.949807  1270 gc.cpp:55] Scheduling '/tmp/ContainerLoggerTest_LOGROTATE_RotateInSandbox_JHP0gy/slaves/c7653f60-33e9-4406-9f62-dc74c906bf83-S0/frameworks/c7653f60-33e9-4406-9f62-dc74c906bf83-0000/executors/864698ee-117b-4b95-b8d7-4c3ec6e0b917/runs/6e2770ca-32d3-47ad-b4fe-7d9f26489621' for gc 6.99998900785778days in the future
I0316 14:28:54.949931  1276 slave.cpp:4078] Cleaning up framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000
I0316 14:28:54.950188  1276 status_update_manager.cpp:282] Closing status update streams for framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000
I0316 14:28:54.950196  1270 gc.cpp:55] Scheduling '/tmp/ContainerLoggerTest_LOGROTATE_RotateInSandbox_JHP0gy/slaves/c7653f60-33e9-4406-9f62-dc74c906bf83-S0/frameworks/c7653f60-33e9-4406-9f62-dc74c906bf83-0000/executors/864698ee-117b-4b95-b8d7-4c3ec6e0b917' for gc 6.99998900606519days in the future
I0316 14:28:54.950458  1270 gc.cpp:55] Scheduling '/tmp/ContainerLoggerTest_LOGROTATE_RotateInSandbox_JHP0gy/slaves/c7653f60-33e9-4406-9f62-dc74c906bf83-S0/frameworks/c7653f60-33e9-4406-9f62-dc74c906bf83-0000' for gc 6.99998900418963days in the future
../../src/tests/container_logger_tests.cpp:461: Failure
Value of: waitpid(pstree.process.pid, __null, 0)
  Actual: -1
Expected: pstree.process.pid
Which is: 1453
I0316 14:28:54.952739  1264 slave.cpp:668] Slave terminating
I0316 14:28:54.952980  1275 master.cpp:1212] Slave c7653f60-33e9-4406-9f62-dc74c906bf83-S0 at slave(12)@172.17.0.3:45919 (2cbb23302fe5) disconnected
I0316 14:28:54.953069  1275 master.cpp:2681] Disconnecting slave c7653f60-33e9-4406-9f62-dc74c906bf83-S0 at slave(12)@172.17.0.3:45919 (2cbb23302fe5)
I0316 14:28:54.953172  1275 master.cpp:2700] Deactivating slave c7653f60-33e9-4406-9f62-dc74c906bf83-S0 at slave(12)@172.17.0.3:45919 (2cbb23302fe5)
I0316 14:28:54.953404  1269 hierarchical.cpp:560] Slave c7653f60-33e9-4406-9f62-dc74c906bf83-S0 deactivated
I0316 14:28:54.957495  1274 master.cpp:1065] Master terminating
I0316 14:28:54.958026  1276 hierarchical.cpp:505] Removed slave c7653f60-33e9-4406-9f62-dc74c906bf83-S0
[  FAILED  ] ContainerLoggerTest.LOGROTATE_RotateInSandbox (3635 ms)
{code}",Bug,Major,kaysoky,2016-03-29T20:04:45.000+0000,5,Resolved,Complete,ContainerLoggerTest.LOGROTATE_RotateInSandbox is flaky,2016-03-29T20:04:45.000+0000,MESOS-4961,1.0,mesos,Mesosphere Sprint 32
greggomann,2016-03-16T09:58:33.000+0000,adam-mesos,"To protect access (authz) to master/agent logs as well as executor sandboxes, we need authentication on the /files endpoints.

Adding HTTP authentication to these endpoints is a bit complicated since they are defined in code that is shared by the master and agent.

While working on MESOS-4850, it became apparent that since our tests use the same instance of libprocess for both master and agent, different default authentication realms must be used for master/agent so that HTTP authentication can be independently enabled/disabled for each.

We should establish a mechanism for making an endpoint authenticated that allows us to:
1) Install an endpoint like {{/files}}, whose code is shared by the master and agent, with different authentication realms for the master and agent
2) Avoid hard-coding a default authentication realm into libprocess, to permit the use of different authentication realms for the master and agent and to keep application-level concerns from leaking into libprocess

Another option would be to use a single default authentication realm and always enable or disable HTTP authentication for *both* the master and agent in tests. However, this wouldn't allow us to test scenarios where HTTP authentication is enabled on one but disabled on the other.",Improvement,Major,greggomann,2016-03-28T09:51:52.000+0000,5,Resolved,Complete,Add authentication to /files endpoints,2016-03-28T09:51:52.000+0000,MESOS-4956,5.0,mesos,Mesosphere Sprint 31
greggomann,2016-03-15T20:53:17.000+0000,greggomann,"To prepare for MESOS-4902, the Mesos master and agent need a way to pass the desired authentication realm to libprocess. Since some endpoints (like {{/profiler/*}}) get installed in libprocess, the master/agent should be able to specify during initialization what authentication realm the libprocess-level endpoints will be authenticated under.",Improvement,Major,greggomann,2016-04-26T20:30:37.000+0000,5,Resolved,Complete,Enable actors to pass an authentication realm to libprocess,2016-04-27T13:12:53.000+0000,MESOS-4951,2.0,mesos,Mesosphere Sprint 33
anandmazumdar,2016-03-15T20:29:43.000+0000,anandmazumdar,"Currently, there is no way for the schedulers to force a reconnection attempt with the master using the scheduler library {{src/scheduler/scheduler.cpp}}. It is specifically useful in scenarios where there is a one way network partition with the master. Due to this, the scheduler has not received any {{HEARTBEAT}} events from the master. In this case, the scheduler might want to force a reconnection attempt with the master instead of relying on the {{disconnected}} callback.",Bug,Major,anandmazumdar,2016-03-30T19:02:47.000+0000,5,Resolved,Complete,Implement reconnect funtionality in the scheduler library.,2016-03-30T19:02:47.000+0000,MESOS-4950,3.0,mesos,Mesosphere Sprint 32
alexr,2016-03-15T14:20:22.000+0000,alexr,"Currently, executor shutdown grace period is specified by an agent flag, which is propagated to executors via the {{MESOS_EXECUTOR_SHUTDOWN_GRACE_PERIOD}} environment variable. There is no way to adjust this timeout for the needs of a particular executor.

To tackle this problem, we propose to introduce an optional {{shutdown_grace_period}} field in {{ExecutorInfo}}.",Improvement,Major,alexr,2016-03-24T17:31:44.000+0000,5,Resolved,Complete,Executor shutdown grace period should be configurable.,2016-03-24T17:33:20.000+0000,MESOS-4949,3.0,mesos,Mesosphere Sprint 31
lins05,2016-03-15T01:25:39.000+0000,jieyu,"Currently, the overlay backend will provision a read-only FS. We can use an empty directory from the container sandbox to act as the upper layer so that it's writable.",Task,Major,jieyu,2016-04-11T16:20:10.000+0000,5,Resolved,Complete,Improve overlay backend so that it's writable,2016-04-11T16:21:17.000+0000,MESOS-4944,5.0,mesos,Mesosphere Sprint 32
gilbert,2016-03-15T00:15:24.000+0000,gilbert,"Currently slave working directory is used as docker store dir and archive dir, which is problematic. Because slave work dir is exactly `environment->mkdtemp()`, it will get cleaned up until the end of the whole test. But the runtime isolator local puller tests cp the host's rootfs, which size is relatively big. Cleanup has to be done by each test tear down. ",Bug,Major,gilbert,2016-03-15T21:42:18.000+0000,5,Resolved,Complete,Docker runtime isolator tests may cause disk issue.,2016-03-15T21:42:18.000+0000,MESOS-4942,2.0,mesos,Mesosphere Sprint 31
zhitao,2016-03-14T21:01:22.000+0000,zhitao,"We want to support updating an existing quota without the cycle of delete and recreate. This avoids the possible starvation risk of losing the quota between delete and recreate, and also makes the interface friendly.

Design doc:
https://docs.google.com/document/d/1c8fJY9_N0W04FtUQ_b_kZM6S0eePU7eYVyfUP14dSys",Improvement,Major,zhitao,,3,In Progress,In Progress,Support update existing quota.,2016-04-27T16:29:59.000+0000,MESOS-4941,8.0,mesos,Mesosphere Sprint 33
gilbert,2016-03-14T17:31:36.000+0000,jieyu,"Currently, we only support a per agent flag to specify the docker registry. We should instead, allow people to specify the registry as part of the docker image name (like `docker pull` does).",Task,Major,jieyu,2016-04-05T17:41:32.000+0000,5,Resolved,Complete,Support specifying per-container docker registry.,2016-04-05T17:41:32.000+0000,MESOS-4939,3.0,mesos,Mesosphere Sprint 31
jojy,2016-03-14T17:17:47.000+0000,jieyu,"We should investigate the following to improve the container security for Mesos containerizer and come up with a list of features that we want to support in MVP.

1) Capabilities
2) User namespace
3) Seccomp
4) SELinux
5) AppArmor

We should investigate what other container systems are doing regarding security:
1) [k8s| https://github.com/kubernetes/kubernetes/blob/master/pkg/api/v1/types.go#L2905]
2) [docker|https://docs.docker.com/engine/security/security/]
3) [oci|https://github.com/opencontainers/specs/blob/master/config.md]",Task,Major,jieyu,2016-04-01T21:25:36.000+0000,5,Resolved,Complete,Investigate container security options for Mesos containerizer,2016-04-01T21:25:36.000+0000,MESOS-4937,5.0,mesos,Mesosphere Sprint 31
js84,2016-03-14T12:20:34.000+0000,js84,As we enable authentication for more and more endpoints we should document which endpoints support authentication and which ones don't.,Task,Major,js84,2016-03-18T21:38:13.000+0000,5,Resolved,Complete,Enable HELP to include authentication status of endpoint.,2016-03-18T21:38:13.000+0000,MESOS-4934,2.0,mesos,Mesosphere Sprint 31
nfnt,2016-03-14T11:07:04.000+0000,js84,"Now that the master (and agents in progress) provide http authentication the registrar should do the same. 

See http://mesos.apache.org/documentation/latest/endpoints/registrar/registry/",Task,Major,js84,2016-04-01T09:58:11.000+0000,5,Resolved,Complete,Registrar HTTP Authentication.,2016-04-01T09:58:11.000+0000,MESOS-4933,3.0,mesos,Mesosphere Sprint 32
js84,2016-03-14T11:01:11.000+0000,js84,"The design doc can be found here:
https://docs.google.com/document/d/1M27S7OTSfJ8afZCklOz00g_wcVrL32i9Lyl6g22GWeY",Task,Major,js84,2016-04-22T15:04:28.000+0000,5,Resolved,Complete,Propose Design for Authorization based filtering for endpoints.,2016-04-22T15:04:29.000+0000,MESOS-4932,5.0,mesos,Mesosphere Sprint 31
klueska,2016-03-14T07:33:43.000+0000,klueska,"When possible, {{.get()}} calls should be replaced by {{->}} for {{Option}} / {{Try}} variables.  This ticket only proposes a blanket change for this in the resource abstraction files, not the code base as a whole.  This is in preparation for introducing the new GPU resource.  Without this change, I would need to use the old {{.get()}} calls.  Instead, I propose to fix the old code surrounding it so that consistency has me doing it the right way.  ",Improvement,Major,klueska,2016-03-29T18:28:42.000+0000,5,Resolved,Complete,Remove all '.get().' calls on Option / Try variables in the resources abstraction.,2016-03-29T18:28:42.000+0000,MESOS-4928,1.0,mesos,Mesosphere Sprint 31
klueska,2016-03-14T07:27:52.000+0000,klueska,The title says it all.,Improvement,Major,klueska,2016-03-15T20:30:00.000+0000,5,Resolved,Complete,"The flag parser for `hashmap<string, string>` should live in stout, not mesos.",2016-03-15T20:30:00.000+0000,MESOS-4927,1.0,mesos,
klueska,2016-03-14T07:26:07.000+0000,klueska,Some flags require lists of integers to be passed in.  We should have an explicit parser for this instead of relying on ad hoc solutions.,Improvement,Major,klueska,2016-03-29T18:46:59.000+0000,5,Resolved,Complete,Add a list parser for comma separated integers in flags.,2016-03-29T18:47:00.000+0000,MESOS-4926,2.0,mesos,Mesosphere Sprint 31
avinash@mesosphere.io,2016-03-13T00:58:12.000+0000,qianzhang,"The network/cni isolator needs to properly setup /etc/hostname and /etc/hosts for the container with a hostname (e.g., randomly generated) and the assigned IP returned by CNI plugin.
We should consider the following cases:
1) container is using host filesystem
2) container is using a different filesystem
3) custom executor and command executor",Bug,Major,qianzhang,2016-04-14T16:19:46.000+0000,5,Resolved,Complete,"Setup proper /etc/hostname, /etc/hosts and /etc/resolv.conf for containers in network/cni isolator.",2016-04-14T16:19:46.000+0000,MESOS-4922,5.0,mesos,Mesosphere Sprint 32
,2016-03-11T07:09:52.000+0000,karya,"Since the module managers are allowed to load the same module multiple times, we should be caching the module manifests to avoid cases where the module tries to trick the module manager by changing `ModuleBase` fields before the next call to `ModuleManager::load`.",Task,Major,karya,,1,Open,New,Cache module manifests while loading in ModuleManager.,2016-03-11T07:09:52.000+0000,MESOS-4918,3.0,mesos,
,2016-03-11T06:49:19.000+0000,karya,"A module might be instantiated multiple time (e.g., multiple schedulers in the same Java process instantiating an authenticator module) within the same process. The current mechanism doesn't provide a way through the module API to forbid multiple instantiations. It is up to the module to check and return error on prior instantiation.

Along similar lines, a module should be able to express thread-safety concerns. Typically, a module running in Master/Agent doesn't have to be concerned about thread safety if it uses libprocess API. However, we should investigate how it plays in the scheduler environment.",Task,Major,karya,,1,Open,New,Allow modules to express if they are multi-instantiable and thread safe.,2016-03-11T06:56:22.000+0000,MESOS-4916,8.0,mesos,
klueska,2016-03-11T01:30:13.000+0000,klueska,"Currently, the delegate field in the ProcessManager is just a string type. We check for 'existence' of a delegate by comparing (delegate != """"). Using an Option is the preferred method for things like this.",Improvement,Minor,klueska,2016-03-11T02:41:55.000+0000,5,Resolved,Complete,"ProcessorManager delegate should be an Option<string>, not just a string.",2016-03-11T02:41:55.000+0000,MESOS-4914,1.0,mesos,Mesosphere Sprint 30
gilbert,2016-03-10T15:37:08.000+0000,bernd-mesos,"Observed on our CI:
{noformat}
[09:34:15] :	 [Step 11/11] [ RUN      ] LinuxFilesystemIsolatorTest.ROOT_MultipleContainers
[09:34:19]W:	 [Step 11/11] I0309 09:34:19.906719  2357 linux.cpp:81] Making '/tmp/MLVLnv' a shared mount
[09:34:19]W:	 [Step 11/11] I0309 09:34:19.923548  2357 linux_launcher.cpp:101] Using /sys/fs/cgroup/freezer as the freezer hierarchy for the Linux launcher
[09:34:19]W:	 [Step 11/11] I0309 09:34:19.924705  2376 containerizer.cpp:666] Starting container 'da610f7f-a709-4de8-94d3-74f4a520619b' for executor 'test_executor1' of framework ''
[09:34:19]W:	 [Step 11/11] I0309 09:34:19.925355  2371 provisioner.cpp:285] Provisioning image rootfs '/tmp/MLVLnv/provisioner/containers/da610f7f-a709-4de8-94d3-74f4a520619b/backends/copy/rootfses/0d7e047a-50f1-490b-bb58-00e9c49628d0' for container da610f7f-a709-4de8-94d3-74f4a520619b
[09:34:19]W:	 [Step 11/11] I0309 09:34:19.925881  2377 copy.cpp:127] Copying layer path '/tmp/MLVLnv/test_image1' to rootfs '/tmp/MLVLnv/provisioner/containers/da610f7f-a709-4de8-94d3-74f4a520619b/backends/copy/rootfses/0d7e047a-50f1-490b-bb58-00e9c49628d0'
[09:34:30]W:	 [Step 11/11] I0309 09:34:30.835127  2376 linux.cpp:355] Bind mounting work directory from '/tmp/MLVLnv/slaves/test_slave/frameworks/executors/test_executor1/runs/da610f7f-a709-4de8-94d3-74f4a520619b' to '/tmp/MLVLnv/provisioner/containers/da610f7f-a709-4de8-94d3-74f4a520619b/backends/copy/rootfses/0d7e047a-50f1-490b-bb58-00e9c49628d0/mnt/mesos/sandbox' for container da610f7f-a709-4de8-94d3-74f4a520619b
[09:34:30]W:	 [Step 11/11] I0309 09:34:30.835392  2376 linux.cpp:683] Changing the ownership of the persistent volume at '/tmp/MLVLnv/volumes/roles/test_role/persistent_volume_id' with uid 0 and gid 0
[09:34:30]W:	 [Step 11/11] I0309 09:34:30.840425  2376 linux.cpp:723] Mounting '/tmp/MLVLnv/volumes/roles/test_role/persistent_volume_id' to '/tmp/MLVLnv/provisioner/containers/da610f7f-a709-4de8-94d3-74f4a520619b/backends/copy/rootfses/0d7e047a-50f1-490b-bb58-00e9c49628d0/mnt/mesos/sandbox/volume' for persistent volume disk(test_role)[persistent_volume_id:volume]:32 of container da610f7f-a709-4de8-94d3-74f4a520619b
[09:34:30]W:	 [Step 11/11] I0309 09:34:30.843878  2374 linux_launcher.cpp:304] Cloning child process with flags = CLONE_NEWNS
[09:34:30]W:	 [Step 11/11] I0309 09:34:30.848302  2371 containerizer.cpp:666] Starting container 'fe4729c5-1e63-4cc6-a2e3-fe5006ffe087' for executor 'test_executor2' of framework ''
[09:34:30]W:	 [Step 11/11] I0309 09:34:30.848758  2371 containerizer.cpp:1392] Destroying container 'da610f7f-a709-4de8-94d3-74f4a520619b'
[09:34:30]W:	 [Step 11/11] I0309 09:34:30.848865  2373 provisioner.cpp:285] Provisioning image rootfs '/tmp/MLVLnv/provisioner/containers/fe4729c5-1e63-4cc6-a2e3-fe5006ffe087/backends/copy/rootfses/518b2464-43dd-47b0-9648-e78aedde6917' for container fe4729c5-1e63-4cc6-a2e3-fe5006ffe087
[09:34:30]W:	 [Step 11/11] I0309 09:34:30.849449  2375 copy.cpp:127] Copying layer path '/tmp/MLVLnv/test_image2' to rootfs '/tmp/MLVLnv/provisioner/containers/fe4729c5-1e63-4cc6-a2e3-fe5006ffe087/backends/copy/rootfses/518b2464-43dd-47b0-9648-e78aedde6917'
[09:34:30]W:	 [Step 11/11] I0309 09:34:30.854038  2374 cgroups.cpp:2427] Freezing cgroup /sys/fs/cgroup/freezer/mesos/da610f7f-a709-4de8-94d3-74f4a520619b
[09:34:30]W:	 [Step 11/11] I0309 09:34:30.856693  2372 cgroups.cpp:1409] Successfully froze cgroup /sys/fs/cgroup/freezer/mesos/da610f7f-a709-4de8-94d3-74f4a520619b after 2.608128ms
[09:34:30]W:	 [Step 11/11] I0309 09:34:30.859237  2377 cgroups.cpp:2445] Thawing cgroup /sys/fs/cgroup/freezer/mesos/da610f7f-a709-4de8-94d3-74f4a520619b
[09:34:30]W:	 [Step 11/11] I0309 09:34:30.861454  2377 cgroups.cpp:1438] Successfullly thawed cgroup /sys/fs/cgroup/freezer/mesos/da610f7f-a709-4de8-94d3-74f4a520619b after 2176us
[09:34:30]W:	 [Step 11/11] I0309 09:34:30.934608  2378 containerizer.cpp:1608] Executor for container 'da610f7f-a709-4de8-94d3-74f4a520619b' has exited
[09:34:30]W:	 [Step 11/11] I0309 09:34:30.937692  2372 linux.cpp:798] Unmounting volume '/tmp/MLVLnv/provisioner/containers/da610f7f-a709-4de8-94d3-74f4a520619b/backends/copy/rootfses/0d7e047a-50f1-490b-bb58-00e9c49628d0/mnt/mesos/sandbox/volume' for container da610f7f-a709-4de8-94d3-74f4a520619b
[09:34:30]W:	 [Step 11/11] I0309 09:34:30.937742  2372 linux.cpp:817] Unmounting sandbox/work directory '/tmp/MLVLnv/provisioner/containers/da610f7f-a709-4de8-94d3-74f4a520619b/backends/copy/rootfses/0d7e047a-50f1-490b-bb58-00e9c49628d0/mnt/mesos/sandbox' for container da610f7f-a709-4de8-94d3-74f4a520619b
[09:34:30]W:	 [Step 11/11] I0309 09:34:30.938129  2375 provisioner.cpp:330] Destroying container rootfs at '/tmp/MLVLnv/provisioner/containers/da610f7f-a709-4de8-94d3-74f4a520619b/backends/copy/rootfses/0d7e047a-50f1-490b-bb58-00e9c49628d0' for container da610f7f-a709-4de8-94d3-74f4a520619b
[09:34:45] :	 [Step 11/11] ../../src/tests/containerizer/filesystem_isolator_tests.cpp:1318: Failure
[09:34:45] :	 [Step 11/11] Failed to wait 15secs for wait1
[09:34:48] :	 [Step 11/11] [  FAILED  ] LinuxFilesystemIsolatorTest.ROOT_MultipleContainers (32341 ms)
{noformat}",Bug,Major,bernd-mesos,2016-03-16T00:23:03.000+0000,5,Resolved,Complete,LinuxFilesystemIsolatorTest.ROOT_MultipleContainers fails.,2016-04-12T14:26:24.000+0000,MESOS-4912,3.0,mesos,Mesosphere Sprint 31
alexr,2016-03-10T15:22:01.000+0000,alexr,"Executor shutdown grace period, configured on the agent, is
propagated to executors via the `MESOS_EXECUTOR_SHUTDOWN_GRACE_PERIOD`
environment variable. The executor driver must use this timeout to delay
the hard shutdown of the related executor.",Bug,Major,alexr,2016-03-19T10:35:37.000+0000,5,Resolved,Complete,Executor driver does not respect executor shutdown grace period.,2016-03-19T10:36:02.000+0000,MESOS-4911,1.0,mesos,Mesosphere Sprint 31
alexr,2016-03-10T14:55:20.000+0000,alexr,"Instead, a combination of {{executor_shutdown_grace_period}}
agent flag and optionally task kill policies should be used.",Improvement,Major,alexr,2016-03-31T15:13:23.000+0000,5,Resolved,Complete,Deprecate the --docker_stop_timeout agent flag.,2016-03-31T15:13:31.000+0000,MESOS-4910,1.0,mesos,Mesosphere Sprint 31
alexr,2016-03-10T14:53:04.000+0000,alexr,A task may require some time to clean up or even a special mechanism to issue a kill request (currently it's a SIGTERM followed by SIGKILL). Introducing kill policies per task will help address these issue.,Improvement,Major,alexr,2016-03-31T15:15:40.000+0000,5,Resolved,Complete,Introduce kill policy for tasks.,2016-03-31T15:15:40.000+0000,MESOS-4909,5.0,mesos,Mesosphere Sprint 31
alexr,2016-03-10T12:40:07.000+0000,alexr,"Currently there is no way for a scheduler to instruct the executor to kill a certain task immediately, skipping any possible timeouts and / or kill policies. This may be desirable in cases like, e.g., the kill policy is 10 minutes but something went wrong, so the scheduler decides to issue a forceful kill.",Improvement,Major,alexr,,3,In Progress,In Progress,Tasks cannot be killed forcefully.,2016-04-27T16:13:18.000+0000,MESOS-4908,5.0,mesos,Mesosphere Sprint 33
karya,2016-03-09T20:46:19.000+0000,karya,"The ModuleManager::load() is designed to be called exactly once during a process lifetime. This works well for Master/Agent environments. However, it can fail in Scheduler environments. For example, a single Scheduler binary might implement multiple scheduler drivers causing multiple calls to ModuleManager::load() leading to a failure.",Bug,Blocker,karya,2016-03-11T10:31:13.000+0000,5,Resolved,Complete,Allow multiple loads of module manifests,2016-03-11T10:31:13.000+0000,MESOS-4903,3.0,mesos,Mesosphere Sprint 30
greggomann,2016-03-09T18:48:49.000+0000,greggomann,"In addition to the endpoints addressed by MESOS-4850 and MESOS-5152, the following endpoints would also benefit from HTTP authentication:
* {{/profiler/*}}
* {{/logging/toggle}}
* {{/metrics/snapshot}}

Adding HTTP authentication to these endpoints is a bit more complicated because they are defined at the libprocess level.

While working on MESOS-4850, it became apparent that since our tests use the same instance of libprocess for both master and agent, different default authentication realms must be used for master/agent so that HTTP authentication can be independently enabled/disabled for each.

We should establish a mechanism for making an endpoint authenticated that allows us to:
1) Install an endpoint like {{/files}}, whose code is shared by the master and agent, with different authentication realms for the master and agent
2) Avoid hard-coding a default authentication realm into libprocess, to permit the use of different authentication realms for the master and agent and to keep application-level concerns from leaking into libprocess

Another option would be to use a single default authentication realm and always enable or disable HTTP authentication for *both* the master and agent in tests. However, this wouldn't allow us to test scenarios where HTTP authentication is enabled on one but disabled on the other.",Improvement,Major,greggomann,2016-04-27T13:07:27.000+0000,5,Resolved,Complete,Add authentication to libprocess endpoints,2016-04-27T13:07:27.000+0000,MESOS-4902,5.0,mesos,Mesosphere Sprint 33
guoger,2016-03-07T22:48:00.000+0000,jieyu,"This endpoint will be similar to /monitor/statistics.json endpoint, but it'll also contain the 'container_status' about the container (see ContainerStatus in mesos.proto). We'll eventually deprecate the /monitor/statistics.json endpoint.",Improvement,Major,jieyu,2016-04-22T00:50:16.000+0000,5,Resolved,Complete,Add a '/containers' endpoint to the agent to list all the active containers.,2016-05-02T19:32:59.000+0000,MESOS-4891,8.0,mesos,
gilbert,2016-03-07T21:28:12.000+0000,gilbert,"There different cases in docker runtime isolator. Some special cases should be tested with unique test case, to verify the docker runtime isolator logic is correct.",Task,Major,gilbert,2016-03-14T17:25:36.000+0000,5,Resolved,Complete,Implement runtime isolator tests.,2016-03-14T17:25:36.000+0000,MESOS-4889,5.0,mesos,
gilbert,2016-03-07T21:16:52.000+0000,gilbert,"When mesos containerizer launch a container using a docker image, which only container default Cmd. The executable command is is a incorrect sequence. For example:

If an image default entrypoint is null, cmd is ""sh"", user defines shell=false, value is none, and arguments as [-c, echo 'hello world']. The executable command is `[sh, -c, echo 'hello world', sh]`, which is incorrect. It should be `[sh, sh, -c, echo 'hello world']` instead.

This problem is only exposed for the case: sh=0, value=0, argv=1, entrypoint=0, cmd=1. ",Bug,Major,gilbert,2016-03-09T17:58:47.000+0000,5,Resolved,Complete,Default cmd is executed as an incorrect command.,2016-03-11T03:43:49.000+0000,MESOS-4888,2.0,mesos,Mesosphere Sprint 30
gyliu,2016-03-07T08:04:43.000+0000,gyliu,"{{CommandInfo}} protobuf support two kinds of command:
{code}
// There are two ways to specify the command:
  // 1) If 'shell == true', the command will be launched via shell
  //		(i.e., /bin/sh -c 'value'). The 'value' specified will be
  //		treated as the shell command. The 'arguments' will be ignored.
  // 2) If 'shell == false', the command will be launched by passing
  //		arguments to an executable. The 'value' specified will be
  //		treated as the filename of the executable. The 'arguments'
  //		will be treated as the arguments to the executable. This is
  //		similar to how POSIX exec families launch processes (i.e.,
  //		execlp(value, arguments(0), arguments(1), ...)).
{code}

The mesos-execute cannot handle 2) now, enabling 2) can help with testing and running one off tasks.",Improvement,Major,gyliu,,3,In Progress,In Progress,Add support for command and arguments to mesos-execute.,2016-04-27T16:30:04.000+0000,MESOS-4882,5.0,mesos,Mesosphere Sprint 33
chenzhiwei,2016-03-07T01:58:15.000+0000,chenzhiwei,This is a part of PowerPC LE porting,Improvement,Major,chenzhiwei,2016-03-23T17:43:26.000+0000,5,Resolved,Complete,Update glog patch to support PowerPC LE,2016-04-13T09:07:29.000+0000,MESOS-4879,1.0,mesos,Mesosphere Sprint 31
gilbert,2016-03-06T16:44:06.000+0000,lins05,"This can be demonstrated with the {{mesos-execute}} command:

# Docker containerizer with image {{alpine}}: success
{code}
sudo ./build/src/mesos-execute --docker_image=alpine --containerizer=docker --name=just-a-test --command=""sleep 1000"" --master=localhost:5050
{code}
# Mesos containerizer with image {{alpine}}: failure
{code}
sudo ./build/src/mesos-execute --docker_image=alpine --containerizer=mesos --name=just-a-test --command=""sleep 1000"" --master=localhost:5050
{code}
# Mesos containerizer with image {{library/alpine}}: success
{code}
sudo ./build/src/mesos-execute --docker_image=library/alpine --containerizer=mesos --name=just-a-test --command=""sleep 1000"" --master=localhost:5050
{code}

In the slave logs:

{code}
ea-4460-83
9c-838da86af34c-0007'
I0306 16:32:41.418269  3403 metadata_manager.cpp:159] Looking for image 'alpine:latest'
I0306 16:32:41.418699  3403 registry_puller.cpp:194] Pulling image 'alpine:latest' from 'docker-manifest://registry-1.docker.io:443alpine?latest#https' to '/tmp/mesos-test
/store/docker/staging/ka7MlQ'
E0306 16:32:43.098131  3400 slave.cpp:3773] Container '4bf9132d-9a57-4baa-a78c-e7164e93ace6' for executor 'just-a-test' of framework 4f055c6f-1bea-4460-839c-838da86af34c-0
007 failed to start: Collect failed: Unexpected HTTP response '401 Unauthorized
{code}

curl command executed:

{code}
$ sudo sysdig -A -p ""*%evt.time %proc.cmdline"" evt.type=execve and proc.name=curl                                                                   16:42:53.198998042 curl -s -S -L -D - https://registry-1.docker.io:443/v2/alpine/manifests/latest
16:42:53.784958541 curl -s -S -L -D - https://auth.docker.io/token?service=registry.docker.io&scope=repository:alpine:pull
16:42:54.294192024 curl -s -S -L -D - -H Authorization: Bearer eyJhbGciOiJFUzI1NiIsInR5cCI6IkpXVCIsIng1YyI6WyJNSUlDTHpDQ0FkU2dBd0lCQWdJQkFEQUtCZ2dxaGtqT1BRUURBakJHTVVRd1FnWURWUVFERXp0Uk5Gb3pPa2RYTjBrNldGUlFSRHBJVFRSUk9rOVVWRmc2TmtGRlF6cFNUVE5ET2tGU01rTTZUMFkzTnpwQ1ZrVkJPa2xHUlVrNlExazFTekFlRncweE5UQTJNalV4T1RVMU5EWmFGdzB4TmpBMk1qUXhPVFUxTkRaYU1FWXhSREJDQmdOVkJBTVRPMGhHU1UwNldGZFZWam8yUVZkSU9sWlpUVEk2TTFnMVREcFNWREkxT2s5VFNrbzZTMVExUmpwWVRsSklPbFJMTmtnNlMxUkxOanBCUVV0VU1Ga3dFd1lIS29aSXpqMENBUVlJS29aSXpqMERBUWNEUWdBRXl2UzIvdEI3T3JlMkVxcGRDeFdtS1NqV1N2VmJ2TWUrWGVFTUNVMDByQjI0akNiUVhreFdmOSs0MUxQMlZNQ29BK0RMRkIwVjBGZGdwajlOWU5rL2pxT0JzakNCcnpBT0JnTlZIUThCQWY4RUJBTUNBSUF3RHdZRFZSMGxCQWd3QmdZRVZSMGxBREJFQmdOVkhRNEVQUVE3U0VaSlRUcFlWMVZXT2paQlYwZzZWbGxOTWpveldEVk1PbEpVTWpVNlQxTktTanBMVkRWR09saE9Va2c2VkVzMlNEcExWRXMyT2tGQlMxUXdSZ1lEVlIwakJEOHdQWUE3VVRSYU16cEhWemRKT2xoVVVFUTZTRTAwVVRwUFZGUllPalpCUlVNNlVrMHpRenBCVWpKRE9rOUdOemM2UWxaRlFUcEpSa1ZKT2tOWk5Vc3dDZ1lJS29aSXpqMEVBd0lEU1FBd1JnSWhBTXZiT2h4cHhrTktqSDRhMFBNS0lFdXRmTjZtRDFvMWs4ZEJOVGxuWVFudkFpRUF0YVJGSGJSR2o4ZlVSSzZ4UVJHRURvQm1ZZ3dZelR3Z3BMaGJBZzNOUmFvPSJdfQ.eyJhY2Nlc3MiOltdLCJhdWQiOiJyZWdpc3RyeS5kb2NrZXIuaW8iLCJleHAiOjE0NTcyODI4NzQsImlhdCI6MTQ1NzI4MjU3NCwiaXNzIjoiYXV0aC5kb2NrZXIuaW8iLCJqdGkiOiJaOGtyNXZXNEJMWkNIRS1IcVJIaCIsIm5iZiI6MTQ1NzI4MjU3NCwic3ViIjoiIn0.C2wtJq_P-m0buPARhmQjDfh6ztIAhcvgN3tfWIZEClSgXlVQ_sAQXAALNZKwAQL2Chj7NpHX--0GW-aeL_28Aw https://registry-1.docker.io:443/v2/alpine/manifests/latest
{code}

Also got the same result with {{ubuntu}} docker image.",Bug,Major,lins05,2016-03-22T16:20:10.000+0000,5,Resolved,Complete,"Mesos containerizer can't handle top level docker image like ""alpine"" (must use ""library/alpine"")",2016-03-22T16:20:10.000+0000,MESOS-4877,3.0,mesos,Mesosphere Sprint 31
kaysoky,2016-03-05T00:37:42.000+0000,kaysoky,"[~bernd-mesos] added this logic for extra info about a rare flaky test:
https://github.com/apache/mesos/blob/d26baee1f377aedb148ad04cc004bb38b85ee4f6/src/tests/fetcher_cache_tests.cpp#L249-L259

This information is useful regardless of the test type and should be generalized for {{cluster::Slave}}.  i.e. 
# When a {{cluster::Slave}} is destructed, it can detect if the test has failed.  
# If so, navigate through its own {{work_dir}} and print sandboxes and/or other useful debugging info.
Also see the refactor in [MESOS-4634].",Improvement,Major,kaysoky,,1,Open,New,Dump the contents of the sandbox when a test fails,2016-03-05T00:39:14.000+0000,MESOS-4872,3.0,mesos,
yongtang,2016-03-04T19:39:28.000+0000,kaysoky,"The {{PersistentVolumeTest}} s have a custom helper for setting up ACLs in the {{master::Flags}}:
{code}
ACLs acls;
    hashset<string> roles;

    foreach (const FrameworkInfo& framework, frameworks) {
      mesos::ACL::RegisterFramework* acl = acls.add_register_frameworks();
      acl->mutable_principals()->add_values(framework.principal());
      acl->mutable_roles()->add_values(framework.role());

      roles.insert(framework.role());
    }

    flags.acls = acls;
    flags.roles = strings::join("","", roles);
{code}

This is no longer necessary with implicit roles.",Improvement,Major,kaysoky,2016-03-09T08:57:18.000+0000,5,Resolved,Complete,PersistentVolumeTests do not need to set up ACLs.,2016-03-09T08:57:18.000+0000,MESOS-4868,1.0,mesos,
klueska,2016-03-04T01:04:29.000+0000,klueska,"We will add ""gpus"" as an explicitly recognized resource in Mesos, akin to cpus, memory, ports, and disk.  In the containerizer, we will verify that the number of GPU resources passed in via the --resources flag matches the list of GPUs passed in via the --nvidia_gpus flag.  In the future we will add autodiscovery so this matching is unnecessary.  However, we will always have to pass ""gpus"" as a resource to make any GPU available on the system (unlike for cpus and memory, where the default is probed).",Task,Major,klueska,2016-03-30T00:37:11.000+0000,5,Resolved,Complete,Add GPUs as an explicit resource.,2016-03-30T00:37:11.000+0000,MESOS-4865,3.0,mesos,Mesosphere Sprint 31
klueska,2016-03-04T00:58:26.000+0000,klueska,"In the initial GPU support we will not do auto-discovery of GPUs on an agent.  As such, an operator will need to specify a flag on the command line, listing all of the GPUs available on the system.",Task,Major,klueska,2016-03-30T00:36:54.000+0000,5,Resolved,Complete,Add flag to specify available Nvidia GPUs on an agent's command line.,2016-03-30T00:36:54.000+0000,MESOS-4864,3.0,mesos,Mesosphere Sprint 31
klueska,2016-03-04T00:55:29.000+0000,klueska,"We need to be able to run unit tests that verify GPU isolation, as well as run full blown tests that actually exercise the GPUs.

These tests should only build when the proper configure flags are set for enabling nvidia GPU support.",Task,Major,klueska,2016-04-06T00:48:36.000+0000,5,Resolved,Complete,Add Nvidia GPU isolator tests.,2016-04-06T00:48:36.000+0000,MESOS-4863,2.0,mesos,Mesosphere Sprint 31
klueska,2016-03-04T00:51:47.000+0000,klueska,"The configure flags can be used to enable Nvidia GPU support, as well as specify the installation directories of the nvml header and library files if not already installed in standard include/library paths on the system.

They will also be used to conditionally build support for Nvidia GPUs into Mesos.",Task,Major,klueska,2016-03-29T23:26:24.000+0000,5,Resolved,Complete,Add configure flags to build with Nvidia GPU support.,2016-03-29T23:26:24.000+0000,MESOS-4861,2.0,mesos,Mesosphere Sprint 31
klueska,2016-03-04T00:48:40.000+0000,klueska,"This script can be used to install the Nvidia GDK for Cuda 7.5 on a
mesos development machine. The purpose of the Nvidia GDK is to provide
all the necessary header files (nvml.h) and library files
(libnvidia-ml.so) necessary to build mesos with Nvidia GPU support.

If the machine on which Mesos is being compiled doesn't have any GPUs,
then libnvidia-ml.so consists only of stubs, allowing Mesos to build
and run, but not actually do anything useful under the hood. This
enables us to build a GPU-enabled mesos on a development machine
without GPUs and then deploy it to a production machine with GPUs and
be reasonably sure it will work.",Task,Minor,klueska,2016-03-29T18:59:05.000+0000,5,Resolved,Complete,Add a script to install the Nvidia GDK on a host.,2016-03-29T18:59:05.000+0000,MESOS-4860,2.0,mesos,Mesosphere Sprint 31
,2016-03-03T23:39:58.000+0000,greggomann,"The documentation currently contains per-version upgrade guidelines, which for recent releases only outlines the upgrade concerns for that version, without detailing explicit upgrade instructions.

We should add explicit upgrade instructions to the top of the upgrades documentation, which can be supplemented by the per-version concerns.

This is done within the upgrade docs for some early versions, with text like:

{code}
In order to upgrade a running cluster:

Install the new master binaries and restart the masters.
Upgrade the schedulers by linking the latest native library and mesos jar (if necessary).
Restart the schedulers.
Install the new slave binaries and restart the slaves.
Upgrade the executors by linking the latest native library and mesos jar (if necessary).
{code}

Instructions to this effect should be featured prominently in the doc.",Improvement,Major,greggomann,,1,Open,New,Add explicit upgrade instructions to the docs,2016-03-04T21:44:22.000+0000,MESOS-4859,1.0,mesos,
anandmazumdar,2016-03-03T21:24:46.000+0000,anandmazumdar,"While implementing pipelining changes for the scheduler library (MESOS-3570), we noticed a couple of small bugs that we would like to fix in the executor library:

- Don't pass {{Connection}} objects to {{defer}} callbacks as they can sometimes lead to deadlocks.
- Minor cleanups around not accepting {{SUBSCRIBE}} call if one is currently in progress.
- Create a random UUID (connectionId) before we initiate a connection to the agent, as in some scenarios, we can accept connection attempts from stale connections.",Task,Major,anandmazumdar,2016-03-13T00:32:52.000+0000,5,Resolved,Complete,Make changes to executor v1 library around managing connections.,2016-03-13T01:56:36.000+0000,MESOS-4858,3.0,mesos,Mesosphere Sprint 30
avinash@mesosphere.io,2016-03-03T19:13:53.000+0000,avinash@mesosphere.io,Need to update the CHANGELOG for 0.28 release.,Documentation,Blocker,avinash@mesosphere.io,2016-03-03T19:34:57.000+0000,5,Resolved,Complete,Update CHANGELOG with net_cls isolator,2016-03-03T19:34:57.000+0000,MESOS-4854,1.0,mesos,Mesosphere Sprint 30
greggomann,2016-03-03T09:30:24.000+0000,adam-mesos,"The {{/state}} and {{/flags}} endpoints are installed in {{src/slave/slave.cpp}}, and thus are straightforward to make authenticated. Other agent endpoints require a bit more consideration, and are tracked in MESOS-4902.

For more information on agent endpoints, see http://mesos.apache.org/documentation/latest/endpoints/
or search for `route(` in the source code:
{code}
$ grep -rn ""route("" src/ |grep -v master |grep -v tests |grep -v json
src/version/version.cpp:75:  route(""/"", VERSION_HELP(), &VersionProcess::version);
src/files/files.cpp:150:  route(""/browse"",
src/files/files.cpp:153:  route(""/read"",
src/files/files.cpp:156:  route(""/download"",
src/files/files.cpp:159:  route(""/debug"",
src/slave/slave.cpp:580:  route(""/api/v1/executor"",
src/slave/slave.cpp:595:  route(""/state"",
src/slave/slave.cpp:601:  route(""/flags"",
src/slave/slave.cpp:607:  route(""/health"",
src/slave/monitor.cpp:100:    route(""/statistics"",
$ grep -rn ""route("" 3rdparty/ |grep -v tests |grep -v README |grep -v examples |grep -v help |grep -v ""process..pp""
3rdparty/libprocess/include/process/profiler.hpp:34:    route(""/start"", START_HELP(), &Profiler::start);
3rdparty/libprocess/include/process/profiler.hpp:35:    route(""/stop"", STOP_HELP(), &Profiler::stop);
3rdparty/libprocess/include/process/system.hpp:70:    route(""/stats.json"", statsHelp(), &System::stats);
3rdparty/libprocess/include/process/logging.hpp:44:    route(""/toggle"", TOGGLE_HELP(), &This::toggle);
{code}",Task,Major,adam-mesos,2016-03-24T06:19:41.000+0000,5,Resolved,Complete,Add authentication to agent endpoints /state and /flags,2016-03-24T06:19:41.000+0000,MESOS-4850,3.0,mesos,Mesosphere Sprint 30
greggomann,2016-03-03T09:21:20.000+0000,adam-mesos,"Flags should be added to the agent to:
1. Enable HTTP authentication ({{--authenticate_http}})
2. Specify credentials ({{--http_credentials}})
3. Specify HTTP authenticators ({{--authenticators}})",Task,Major,adam-mesos,2016-03-24T06:18:08.000+0000,5,Resolved,Complete,Add agent flags for HTTP authentication,2016-03-24T06:18:08.000+0000,MESOS-4849,2.0,mesos,Mesosphere Sprint 30
greggomann,2016-03-03T09:16:19.000+0000,adam-mesos,"Research the master authentication flags to see what changes will be necessary for agent http authentication.
Write up a 1-2 page summary/design doc.",Task,Major,adam-mesos,2016-03-13T11:13:14.000+0000,5,Resolved,Complete,Agent Authn Research Spike,2016-03-13T11:13:14.000+0000,MESOS-4848,2.0,mesos,Mesosphere Sprint 30
js84,2016-03-03T06:19:51.000+0000,adam-mesos,"Before we can add authorization around operator endpoints, we need to add authentication support, so that unauthenticated requests are denied when --authenticate_http is enabled, and so that the principal is passed into `route()`.",Task,Major,adam-mesos,2016-03-14T10:23:09.000+0000,5,Resolved,Complete,Add authentication to master endpoints,2016-03-14T10:23:09.000+0000,MESOS-4844,2.0,mesos,Mesosphere Sprint 30
arojas,2016-03-02T17:46:45.000+0000,arojas,{{ShutdownFramework}} acl was deprecated a couple of versions ago in favor of the {{TeardownFramework}} message. Its deprecation cycle came with 0.27. That means we should remove the message and its references in the code base.,Task,Minor,arojas,2016-03-08T02:28:52.000+0000,5,Resolved,Complete,Remove internal usage of deprecated ShutdownFramework ACL,2016-03-08T02:28:52.000+0000,MESOS-4840,2.0,mesos,Mesosphere Sprint 30
js84,2016-03-02T16:15:21.000+0000,js84,"The Linux Launcher places new processes into the freezer cgroup.
This is currently done by a combination of childSetup function (blocking the new process until parent is done) and the parent (placing child process into the cgroup and then signaling child to continue).
ParentHooks support this behavior (blocking child until some work is done in the parent) in a much cleaner way. 
",Improvement,Major,js84,2016-03-26T13:00:07.000+0000,5,Resolved,Complete,Move placement new processes into the freezer cgroup into a parent hook.,2016-03-26T13:00:07.000+0000,MESOS-4839,3.0,mesos,Mesosphere Sprint 31
hausdorff,2016-03-02T01:45:33.000+0000,vinodkone,This is due to a bug in MESOS-4415 that landed for 0.27.0.,Bug,Major,vinodkone,2016-03-02T07:58:23.000+0000,5,Resolved,Complete,Fix rmdir for windows,2016-03-02T07:58:23.000+0000,MESOS-4836,1.0,mesos,Mesosphere Sprint 30
,2016-03-02T01:22:56.000+0000,kaysoky,"Verbose logs: 
{code}
[ RUN      ] CgroupsAnyHierarchyWithFreezerTest.ROOT_CGROUPS_DestroyTracedProcess
I0302 00:43:14.127846 11755 cgroups.cpp:2427] Freezing cgroup /sys/fs/cgroup/freezer/mesos_test
I0302 00:43:14.267411 11758 cgroups.cpp:1409] Successfully froze cgroup /sys/fs/cgroup/freezer/mesos_test after 139.46496ms
I0302 00:43:14.409395 11751 cgroups.cpp:2445] Thawing cgroup /sys/fs/cgroup/freezer/mesos_test
I0302 00:43:14.551304 11751 cgroups.cpp:1438] Successfullly thawed cgroup /sys/fs/cgroup/freezer/mesos_test after 141.811968ms
../../src/tests/containerizer/cgroups_tests.cpp:949: Failure
Value of: ::waitpid(pid, &status, 0)
  Actual: 23809
Expected: -1
../../src/tests/containerizer/cgroups_tests.cpp:950: Failure
Value of: (*__errno_location ())
  Actual: 0
Expected: 10
[  FAILED  ] CgroupsAnyHierarchyWithFreezerTest.ROOT_CGROUPS_DestroyTracedProcess (1055 ms)
{code}",Bug,Major,kaysoky,,1,Open,New,CgroupsAnyHierarchyWithFreezerTest.ROOT_CGROUPS_DestroyTracedProcess is flaky,2016-03-18T21:08:19.000+0000,MESOS-4835,2.0,mesos,
jojy,2016-03-02T00:11:53.000+0000,jojy,"Add support for ""file"" based URI fetcher. This could be useful for container image provisioning from local file system.",Task,Major,jojy,2016-03-03T02:07:53.000+0000,5,Resolved,Complete,Add 'file' fetcher plugin.,2016-03-03T02:07:53.000+0000,MESOS-4834,2.0,mesos,Mesosphere Sprint 30
neilc,2016-03-01T21:34:04.000+0000,neilc,"Modifying the {{HierarchicalAllocator_BENCHMARK_Test.ResourceLabels}} benchmark from https://reviews.apache.org/r/43686/ to use distinct labels between different slaves, performance regresses from ~2 seconds to ~3 minutes. The culprit seems to be the way in which the allocator merges together resources; reserved resource labels (or persistent volume IDs) inhibit merging, which causes performance to be much worse.",Bug,Blocker,neilc,2016-03-03T01:20:19.000+0000,5,Resolved,Complete,Poor allocator performance with labeled resources and/or persistent volumes,2016-03-03T01:20:28.000+0000,MESOS-4833,5.0,mesos,Mesosphere Sprint 30
jieyu,2016-03-01T21:22:16.000+0000,kaysoky,"If the {{/tmp}} directory (where Mesos tests create temporary directories) is a bind mount, the test suite will exit here:
{code}
[ RUN      ] DockerContainerizerTest.ROOT_DOCKER_RecoverOrphanedPersistentVolumes
I0226 03:17:26.722806  1097 leveldb.cpp:174] Opened db in 12.587676ms
I0226 03:17:26.723496  1097 leveldb.cpp:181] Compacted db in 636999ns
I0226 03:17:26.723536  1097 leveldb.cpp:196] Created db iterator in 18271ns
I0226 03:17:26.723547  1097 leveldb.cpp:202] Seeked to beginning of db in 1555ns
I0226 03:17:26.723554  1097 leveldb.cpp:271] Iterated through 0 keys in the db in 363ns
I0226 03:17:26.723593  1097 replica.cpp:779] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned
I0226 03:17:26.724128  1117 recover.cpp:447] Starting replica recovery
I0226 03:17:26.724367  1117 recover.cpp:473] Replica is in EMPTY status
I0226 03:17:26.725237  1117 replica.cpp:673] Replica in EMPTY status received a broadcasted recover request from (13810)@172.30.2.151:51934
I0226 03:17:26.725744  1114 recover.cpp:193] Received a recover response from a replica in EMPTY status
I0226 03:17:26.726356  1111 master.cpp:376] Master 5cc57c0e-f1ad-4107-893f-420ed1a1db1a (ip-172-30-2-151.mesosphere.io) started on 172.30.2.151:51934
I0226 03:17:26.726369  1118 recover.cpp:564] Updating replica status to STARTING
I0226 03:17:26.726378  1111 master.cpp:378] Flags at startup: --acls="""" --allocation_interval=""1secs"" --allocator=""HierarchicalDRF"" --authenticate=""true"" --authenticate_http=""true"" --authenticate_slaves=""true"" --authenticators=""crammd5"" --authorizers=""local"" --credentials=""/tmp/djHTVQ/credentials"" --framework_sorter=""drf"" --help=""false"" --hostname_lookup=""true"" --http_authenticators=""basic"" --initialize_driver_logging=""true"" --log_auto_initialize=""true"" --logbufsecs=""0"" --logging_level=""INFO"" --max_completed_frameworks=""50"" --max_completed_tasks_per_framework=""1000"" --max_slave_ping_timeouts=""5"" --quiet=""false"" --recovery_slave_removal_limit=""100%"" --registry=""replicated_log"" --registry_fetch_timeout=""1mins"" --registry_store_timeout=""100secs"" --registry_strict=""true"" --root_submissions=""true"" --slave_ping_timeout=""15secs"" --slave_reregister_timeout=""10mins"" --user_sorter=""drf"" --version=""false"" --webui_dir=""/usr/local/share/mesos/webui"" --work_dir=""/tmp/djHTVQ/master"" --zk_session_timeout=""10secs""
I0226 03:17:26.726605  1111 master.cpp:423] Master only allowing authenticated frameworks to register
I0226 03:17:26.726616  1111 master.cpp:428] Master only allowing authenticated slaves to register
I0226 03:17:26.726632  1111 credentials.hpp:35] Loading credentials for authentication from '/tmp/djHTVQ/credentials'
I0226 03:17:26.726860  1111 master.cpp:468] Using default 'crammd5' authenticator
I0226 03:17:26.726977  1111 master.cpp:537] Using default 'basic' HTTP authenticator
I0226 03:17:26.727092  1111 master.cpp:571] Authorization enabled
I0226 03:17:26.727243  1118 hierarchical.cpp:144] Initialized hierarchical allocator process
I0226 03:17:26.727285  1116 whitelist_watcher.cpp:77] No whitelist given
I0226 03:17:26.728852  1114 master.cpp:1712] The newly elected leader is master@172.30.2.151:51934 with id 5cc57c0e-f1ad-4107-893f-420ed1a1db1a
I0226 03:17:26.728876  1114 master.cpp:1725] Elected as the leading master!
I0226 03:17:26.728891  1114 master.cpp:1470] Recovering from registrar
I0226 03:17:26.728977  1117 registrar.cpp:307] Recovering registrar
I0226 03:17:26.731503  1112 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 4.977811ms
I0226 03:17:26.731539  1112 replica.cpp:320] Persisted replica status to STARTING
I0226 03:17:26.731711  1111 recover.cpp:473] Replica is in STARTING status
I0226 03:17:26.732501  1114 replica.cpp:673] Replica in STARTING status received a broadcasted recover request from (13812)@172.30.2.151:51934
I0226 03:17:26.732862  1111 recover.cpp:193] Received a recover response from a replica in STARTING status
I0226 03:17:26.733264  1117 recover.cpp:564] Updating replica status to VOTING
I0226 03:17:26.733836  1118 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 388246ns
I0226 03:17:26.733855  1118 replica.cpp:320] Persisted replica status to VOTING
I0226 03:17:26.733979  1113 recover.cpp:578] Successfully joined the Paxos group
I0226 03:17:26.734149  1113 recover.cpp:462] Recover process terminated
I0226 03:17:26.734478  1111 log.cpp:659] Attempting to start the writer
I0226 03:17:26.735523  1114 replica.cpp:493] Replica received implicit promise request from (13813)@172.30.2.151:51934 with proposal 1
I0226 03:17:26.736130  1114 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 576451ns
I0226 03:17:26.736150  1114 replica.cpp:342] Persisted promised to 1
I0226 03:17:26.736709  1115 coordinator.cpp:238] Coordinator attempting to fill missing positions
I0226 03:17:26.737771  1114 replica.cpp:388] Replica received explicit promise request from (13814)@172.30.2.151:51934 for position 0 with proposal 2
I0226 03:17:26.738386  1114 leveldb.cpp:341] Persisting action (8 bytes) to leveldb took 583184ns
I0226 03:17:26.738404  1114 replica.cpp:712] Persisted action at 0
I0226 03:17:26.739312  1118 replica.cpp:537] Replica received write request for position 0 from (13815)@172.30.2.151:51934
I0226 03:17:26.739367  1118 leveldb.cpp:436] Reading position from leveldb took 26157ns
I0226 03:17:26.740638  1118 leveldb.cpp:341] Persisting action (14 bytes) to leveldb took 1.238477ms
I0226 03:17:26.740669  1118 replica.cpp:712] Persisted action at 0
I0226 03:17:26.741158  1118 replica.cpp:691] Replica received learned notice for position 0 from @0.0.0.0:0
I0226 03:17:26.742878  1118 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 1.697254ms
I0226 03:17:26.742902  1118 replica.cpp:712] Persisted action at 0
I0226 03:17:26.742916  1118 replica.cpp:697] Replica learned NOP action at position 0
I0226 03:17:26.743393  1117 log.cpp:675] Writer started with ending position 0
I0226 03:17:26.744370  1112 leveldb.cpp:436] Reading position from leveldb took 34329ns
I0226 03:17:26.745240  1117 registrar.cpp:340] Successfully fetched the registry (0B) in 16.21888ms
I0226 03:17:26.745350  1117 registrar.cpp:439] Applied 1 operations in 30460ns; attempting to update the 'registry'
I0226 03:17:26.746016  1111 log.cpp:683] Attempting to append 210 bytes to the log
I0226 03:17:26.746119  1116 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 1
I0226 03:17:26.746798  1114 replica.cpp:537] Replica received write request for position 1 from (13816)@172.30.2.151:51934
I0226 03:17:26.747251  1114 leveldb.cpp:341] Persisting action (229 bytes) to leveldb took 411333ns
I0226 03:17:26.747269  1114 replica.cpp:712] Persisted action at 1
I0226 03:17:26.747808  1113 replica.cpp:691] Replica received learned notice for position 1 from @0.0.0.0:0
I0226 03:17:26.749511  1113 leveldb.cpp:341] Persisting action (231 bytes) to leveldb took 1.673488ms
I0226 03:17:26.749534  1113 replica.cpp:712] Persisted action at 1
I0226 03:17:26.749550  1113 replica.cpp:697] Replica learned APPEND action at position 1
I0226 03:17:26.750422  1111 registrar.cpp:484] Successfully updated the 'registry' in 5.021952ms
I0226 03:17:26.750560  1111 registrar.cpp:370] Successfully recovered registrar
I0226 03:17:26.750635  1112 log.cpp:702] Attempting to truncate the log to 1
I0226 03:17:26.750751  1113 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 2
I0226 03:17:26.751096  1116 master.cpp:1522] Recovered 0 slaves from the Registry (171B) ; allowing 10mins for slaves to re-register
I0226 03:17:26.751126  1111 hierarchical.cpp:171] Skipping recovery of hierarchical allocator: nothing to recover
I0226 03:17:26.751561  1118 replica.cpp:537] Replica received write request for position 2 from (13817)@172.30.2.151:51934
I0226 03:17:26.751999  1118 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 406823ns
I0226 03:17:26.752018  1118 replica.cpp:712] Persisted action at 2
I0226 03:17:26.752521  1113 replica.cpp:691] Replica received learned notice for position 2 from @0.0.0.0:0
I0226 03:17:26.754161  1113 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 1.614888ms
I0226 03:17:26.754210  1113 leveldb.cpp:399] Deleting ~1 keys from leveldb took 26384ns
I0226 03:17:26.754225  1113 replica.cpp:712] Persisted action at 2
I0226 03:17:26.754240  1113 replica.cpp:697] Replica learned TRUNCATE action at position 2
I0226 03:17:26.765103  1115 slave.cpp:193] Slave started on 399)@172.30.2.151:51934
I0226 03:17:26.765130  1115 slave.cpp:194] Flags at startup: --appc_simple_discovery_uri_prefix=""http://"" --appc_store_dir=""/tmp/mesos/store/appc"" --authenticatee=""crammd5"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --credential=""/tmp/DockerContainerizerTest_ROOT_DOCKER_RecoverOrphanedPersistentVolumes_aJOesP/credential"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_kill_orphans=""true"" --docker_registry=""https://registry-1.docker.io"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/tmp/mesos/store/docker"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/tmp/DockerContainerizerTest_ROOT_DOCKER_RecoverOrphanedPersistentVolumes_aJOesP/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --hostname_lookup=""true"" --image_provisioner_backend=""copy"" --initialize_driver_logging=""true"" --isolation=""posix/cpu,posix/mem"" --launcher_dir=""/mnt/teamcity/work/4240ba9ddd0997c3/build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""10ms"" --resources=""cpu:2;mem:2048;disk(role1):2048"" --revocable_cpu_low_priority=""true"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --systemd_enable_support=""true"" --systemd_runtime_directory=""/run/systemd/system"" --version=""false"" --work_dir=""/tmp/DockerContainerizerTest_ROOT_DOCKER_RecoverOrphanedPersistentVolumes_aJOesP""
I0226 03:17:26.765403  1115 credentials.hpp:83] Loading credential for authentication from '/tmp/DockerContainerizerTest_ROOT_DOCKER_RecoverOrphanedPersistentVolumes_aJOesP/credential'
I0226 03:17:26.765573  1115 slave.cpp:324] Slave using credential for: test-principal
I0226 03:17:26.765733  1115 resources.cpp:576] Parsing resources as JSON failed: cpu:2;mem:2048;disk(role1):2048
Trying semicolon-delimited string format instead
I0226 03:17:26.766185  1115 slave.cpp:464] Slave resources: cpu(*):2; mem(*):2048; disk(role1):2048; cpus(*):8; ports(*):[31000-32000]
I0226 03:17:26.766242  1115 slave.cpp:472] Slave attributes: [  ]
I0226 03:17:26.766250  1115 slave.cpp:477] Slave hostname: ip-172-30-2-151.mesosphere.io
I0226 03:17:26.767325  1097 sched.cpp:222] Version: 0.28.0
I0226 03:17:26.767390  1111 state.cpp:58] Recovering state from '/tmp/DockerContainerizerTest_ROOT_DOCKER_RecoverOrphanedPersistentVolumes_aJOesP/meta'
I0226 03:17:26.767603  1115 status_update_manager.cpp:200] Recovering status update manager
I0226 03:17:26.767865  1113 docker.cpp:726] Recovering Docker containers
I0226 03:17:26.767971  1111 sched.cpp:326] New master detected at master@172.30.2.151:51934
I0226 03:17:26.768045  1111 sched.cpp:382] Authenticating with master master@172.30.2.151:51934
I0226 03:17:26.768059  1111 sched.cpp:389] Using default CRAM-MD5 authenticatee
I0226 03:17:26.768070  1118 slave.cpp:4565] Finished recovery
I0226 03:17:26.768273  1112 authenticatee.cpp:121] Creating new client SASL connection
I0226 03:17:26.768435  1118 slave.cpp:4737] Querying resource estimator for oversubscribable resources
I0226 03:17:26.768565  1111 master.cpp:5526] Authenticating scheduler-c59020d6-385e-48a3-8a10-9e5c3f1dbd92@172.30.2.151:51934
I0226 03:17:26.768661  1118 slave.cpp:796] New master detected at master@172.30.2.151:51934
I0226 03:17:26.768659  1115 authenticator.cpp:413] Starting authentication session for crammd5_authenticatee(839)@172.30.2.151:51934
I0226 03:17:26.768679  1113 status_update_manager.cpp:174] Pausing sending status updates
I0226 03:17:26.768728  1118 slave.cpp:859] Authenticating with master master@172.30.2.151:51934
I0226 03:17:26.768743  1118 slave.cpp:864] Using default CRAM-MD5 authenticatee
I0226 03:17:26.768865  1118 slave.cpp:832] Detecting new master
I0226 03:17:26.768868  1112 authenticator.cpp:98] Creating new server SASL connection
I0226 03:17:26.768908  1114 authenticatee.cpp:121] Creating new client SASL connection
I0226 03:17:26.769003  1118 slave.cpp:4751] Received oversubscribable resources  from the resource estimator
I0226 03:17:26.769103  1115 authenticatee.cpp:212] Received SASL authentication mechanisms: CRAM-MD5
I0226 03:17:26.769131  1115 authenticatee.cpp:238] Attempting to authenticate with mechanism 'CRAM-MD5'
I0226 03:17:26.769209  1116 master.cpp:5526] Authenticating slave(399)@172.30.2.151:51934
I0226 03:17:26.769253  1114 authenticator.cpp:203] Received SASL authentication start
I0226 03:17:26.769295  1115 authenticator.cpp:413] Starting authentication session for crammd5_authenticatee(840)@172.30.2.151:51934
I0226 03:17:26.769307  1114 authenticator.cpp:325] Authentication requires more steps
I0226 03:17:26.769403  1117 authenticatee.cpp:258] Received SASL authentication step
I0226 03:17:26.769495  1114 authenticator.cpp:98] Creating new server SASL connection
I0226 03:17:26.769531  1115 authenticator.cpp:231] Received SASL authentication step
I0226 03:17:26.769554  1115 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: 'ip-172-30-2-151.mesosphere.io' server FQDN: 'ip-172-30-2-151.mesosphere.io' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0226 03:17:26.769562  1115 auxprop.cpp:179] Looking up auxiliary property '*userPassword'
I0226 03:17:26.769608  1115 auxprop.cpp:179] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0226 03:17:26.769629  1115 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: 'ip-172-30-2-151.mesosphere.io' server FQDN: 'ip-172-30-2-151.mesosphere.io' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0226 03:17:26.769637  1115 auxprop.cpp:129] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0226 03:17:26.769642  1115 auxprop.cpp:129] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0226 03:17:26.769654  1115 authenticator.cpp:317] Authentication success
I0226 03:17:26.769728  1117 authenticatee.cpp:298] Authentication success
I0226 03:17:26.769769  1112 authenticatee.cpp:212] Received SASL authentication mechanisms: CRAM-MD5
I0226 03:17:26.769767  1118 master.cpp:5556] Successfully authenticated principal 'test-principal' at scheduler-c59020d6-385e-48a3-8a10-9e5c3f1dbd92@172.30.2.151:51934
I0226 03:17:26.769803  1112 authenticatee.cpp:238] Attempting to authenticate with mechanism 'CRAM-MD5'
I0226 03:17:26.769798  1114 authenticator.cpp:431] Authentication session cleanup for crammd5_authenticatee(839)@172.30.2.151:51934
I0226 03:17:26.769881  1112 authenticator.cpp:203] Received SASL authentication start
I0226 03:17:26.769932  1112 authenticator.cpp:325] Authentication requires more steps
I0226 03:17:26.769981  1117 sched.cpp:471] Successfully authenticated with master master@172.30.2.151:51934
I0226 03:17:26.770004  1117 sched.cpp:776] Sending SUBSCRIBE call to master@172.30.2.151:51934
I0226 03:17:26.770064  1118 authenticatee.cpp:258] Received SASL authentication step
I0226 03:17:26.770102  1117 sched.cpp:809] Will retry registration in 1.937819802secs if necessary
I0226 03:17:26.770165  1115 authenticator.cpp:231] Received SASL authentication step
I0226 03:17:26.770193  1115 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: 'ip-172-30-2-151.mesosphere.io' server FQDN: 'ip-172-30-2-151.mesosphere.io' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0226 03:17:26.770207  1115 auxprop.cpp:179] Looking up auxiliary property '*userPassword'
I0226 03:17:26.770213  1116 master.cpp:2280] Received SUBSCRIBE call for framework 'default' at scheduler-c59020d6-385e-48a3-8a10-9e5c3f1dbd92@172.30.2.151:51934
I0226 03:17:26.770241  1115 auxprop.cpp:179] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0226 03:17:26.770274  1115 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: 'ip-172-30-2-151.mesosphere.io' server FQDN: 'ip-172-30-2-151.mesosphere.io' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0226 03:17:26.770277  1116 master.cpp:1751] Authorizing framework principal 'test-principal' to receive offers for role 'role1'
I0226 03:17:26.770298  1115 auxprop.cpp:129] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0226 03:17:26.770331  1115 auxprop.cpp:129] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0226 03:17:26.770349  1115 authenticator.cpp:317] Authentication success
I0226 03:17:26.770428  1118 authenticatee.cpp:298] Authentication success
I0226 03:17:26.770442  1116 master.cpp:5556] Successfully authenticated principal 'test-principal' at slave(399)@172.30.2.151:51934
I0226 03:17:26.770547  1116 authenticator.cpp:431] Authentication session cleanup for crammd5_authenticatee(840)@172.30.2.151:51934
I0226 03:17:26.770846  1116 master.cpp:2351] Subscribing framework default with checkpointing enabled and capabilities [  ]
I0226 03:17:26.770866  1118 slave.cpp:927] Successfully authenticated with master master@172.30.2.151:51934
I0226 03:17:26.770966  1118 slave.cpp:1321] Will retry registration in 1.453415ms if necessary
I0226 03:17:26.771225  1115 hierarchical.cpp:265] Added framework 5cc57c0e-f1ad-4107-893f-420ed1a1db1a-0000
I0226 03:17:26.771275  1118 sched.cpp:703] Framework registered with 5cc57c0e-f1ad-4107-893f-420ed1a1db1a-0000
I0226 03:17:26.771299  1115 hierarchical.cpp:1434] No resources available to allocate!
I0226 03:17:26.771328  1115 hierarchical.cpp:1529] No inverse offers to send out!
I0226 03:17:26.771344  1118 sched.cpp:717] Scheduler::registered took 50146ns
I0226 03:17:26.771356  1116 master.cpp:4240] Registering slave at slave(399)@172.30.2.151:51934 (ip-172-30-2-151.mesosphere.io) with id 5cc57c0e-f1ad-4107-893f-420ed1a1db1a-S0
I0226 03:17:26.771348  1115 hierarchical.cpp:1127] Performed allocation for 0 slaves in 101438ns
I0226 03:17:26.771860  1114 registrar.cpp:439] Applied 1 operations in 59672ns; attempting to update the 'registry'
I0226 03:17:26.772645  1117 log.cpp:683] Attempting to append 423 bytes to the log
I0226 03:17:26.772758  1112 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 3
I0226 03:17:26.773435  1117 replica.cpp:537] Replica received write request for position 3 from (13824)@172.30.2.151:51934
I0226 03:17:26.773586  1111 slave.cpp:1321] Will retry registration in 2.74261ms if necessary
I0226 03:17:26.773682  1115 master.cpp:4228] Ignoring register slave message from slave(399)@172.30.2.151:51934 (ip-172-30-2-151.mesosphere.io) as admission is already in progress
I0226 03:17:26.773937  1117 leveldb.cpp:341] Persisting action (442 bytes) to leveldb took 469969ns
I0226 03:17:26.773957  1117 replica.cpp:712] Persisted action at 3
I0226 03:17:26.774605  1114 replica.cpp:691] Replica received learned notice for position 3 from @0.0.0.0:0
I0226 03:17:26.775961  1114 leveldb.cpp:341] Persisting action (444 bytes) to leveldb took 1.329435ms
I0226 03:17:26.775986  1114 replica.cpp:712] Persisted action at 3
I0226 03:17:26.776008  1114 replica.cpp:697] Replica learned APPEND action at position 3
I0226 03:17:26.777228  1115 slave.cpp:1321] Will retry registration in 41.5608ms if necessary
I0226 03:17:26.777300  1112 registrar.cpp:484] Successfully updated the 'registry' in 5.378048ms
I0226 03:17:26.777361  1114 master.cpp:4228] Ignoring register slave message from slave(399)@172.30.2.151:51934 (ip-172-30-2-151.mesosphere.io) as admission is already in progress
I0226 03:17:26.777505  1113 log.cpp:702] Attempting to truncate the log to 3
I0226 03:17:26.777616  1111 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 4
I0226 03:17:26.778062  1114 slave.cpp:3482] Received ping from slave-observer(369)@172.30.2.151:51934
I0226 03:17:26.778139  1118 master.cpp:4308] Registered slave 5cc57c0e-f1ad-4107-893f-420ed1a1db1a-S0 at slave(399)@172.30.2.151:51934 (ip-172-30-2-151.mesosphere.io) with cpu(*):2; mem(*):2048; disk(role1):2048; cpus(*):8; ports(*):[31000-32000]
I0226 03:17:26.778213  1113 replica.cpp:537] Replica received write request for position 4 from (13825)@172.30.2.151:51934
I0226 03:17:26.778291  1114 slave.cpp:971] Registered with master master@172.30.2.151:51934; given slave ID 5cc57c0e-f1ad-4107-893f-420ed1a1db1a-S0
I0226 03:17:26.778316  1114 fetcher.cpp:81] Clearing fetcher cache
I0226 03:17:26.778367  1116 hierarchical.cpp:473] Added slave 5cc57c0e-f1ad-4107-893f-420ed1a1db1a-S0 (ip-172-30-2-151.mesosphere.io) with cpu(*):2; mem(*):2048; disk(role1):2048; cpus(*):8; ports(*):[31000-32000] (allocated: )
I0226 03:17:26.778447  1117 status_update_manager.cpp:181] Resuming sending status updates
I0226 03:17:26.778617  1113 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 375414ns
I0226 03:17:26.778635  1113 replica.cpp:712] Persisted action at 4
I0226 03:17:26.778650  1114 slave.cpp:994] Checkpointing SlaveInfo to '/tmp/DockerContainerizerTest_ROOT_DOCKER_RecoverOrphanedPersistentVolumes_aJOesP/meta/slaves/5cc57c0e-f1ad-4107-893f-420ed1a1db1a-S0/slave.info'
I0226 03:17:26.778900  1114 slave.cpp:1030] Forwarding total oversubscribed resources 
I0226 03:17:26.779109  1114 master.cpp:4649] Received update of slave 5cc57c0e-f1ad-4107-893f-420ed1a1db1a-S0 at slave(399)@172.30.2.151:51934 (ip-172-30-2-151.mesosphere.io) with total oversubscribed resources 
I0226 03:17:26.779139  1112 replica.cpp:691] Replica received learned notice for position 4 from @0.0.0.0:0
I0226 03:17:26.779331  1116 hierarchical.cpp:1529] No inverse offers to send out!
I0226 03:17:26.779369  1116 hierarchical.cpp:1147] Performed allocation for slave 5cc57c0e-f1ad-4107-893f-420ed1a1db1a-S0 in 969593ns
I0226 03:17:26.779645  1113 master.cpp:5355] Sending 1 offers to framework 5cc57c0e-f1ad-4107-893f-420ed1a1db1a-0000 (default) at scheduler-c59020d6-385e-48a3-8a10-9e5c3f1dbd92@172.30.2.151:51934
I0226 03:17:26.779700  1116 hierarchical.cpp:531] Slave 5cc57c0e-f1ad-4107-893f-420ed1a1db1a-S0 (ip-172-30-2-151.mesosphere.io) updated with oversubscribed resources  (total: cpu(*):2; mem(*):2048; disk(role1):2048; cpus(*):8; ports(*):[31000-32000], allocated: disk(role1):2048; cpu(*):2; mem(*):2048; cpus(*):8; ports(*):[31000-32000])
I0226 03:17:26.779819  1116 hierarchical.cpp:1434] No resources available to allocate!
I0226 03:17:26.779847  1116 hierarchical.cpp:1529] No inverse offers to send out!
I0226 03:17:26.779865  1116 hierarchical.cpp:1147] Performed allocation for slave 5cc57c0e-f1ad-4107-893f-420ed1a1db1a-S0 in 133437ns
I0226 03:17:26.780025  1118 sched.cpp:873] Scheduler::resourceOffers took 102165ns
I0226 03:17:26.780372  1097 resources.cpp:576] Parsing resources as JSON failed: cpus:1;mem:64;
Trying semicolon-delimited string format instead
I0226 03:17:26.780882  1112 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 1.715066ms
I0226 03:17:26.780938  1112 leveldb.cpp:399] Deleting ~2 keys from leveldb took 32370ns
I0226 03:17:26.780953  1112 replica.cpp:712] Persisted action at 4
I0226 03:17:26.780971  1112 replica.cpp:697] Replica learned TRUNCATE action at position 4
I0226 03:17:26.781693  1117 master.cpp:3138] Processing ACCEPT call for offers: [ 5cc57c0e-f1ad-4107-893f-420ed1a1db1a-O0 ] on slave 5cc57c0e-f1ad-4107-893f-420ed1a1db1a-S0 at slave(399)@172.30.2.151:51934 (ip-172-30-2-151.mesosphere.io) for framework 5cc57c0e-f1ad-4107-893f-420ed1a1db1a-0000 (default) at scheduler-c59020d6-385e-48a3-8a10-9e5c3f1dbd92@172.30.2.151:51934
I0226 03:17:26.781731  1117 master.cpp:2926] Authorizing principal 'test-principal' to create volumes
I0226 03:17:26.781801  1117 master.cpp:2825] Authorizing framework principal 'test-principal' to launch task 1 as user 'root'
I0226 03:17:26.782827  1114 master.cpp:3467] Applying CREATE operation for volumes disk(role1)[id1:path1]:64 from framework 5cc57c0e-f1ad-4107-893f-420ed1a1db1a-0000 (default) at scheduler-c59020d6-385e-48a3-8a10-9e5c3f1dbd92@172.30.2.151:51934 to slave 5cc57c0e-f1ad-4107-893f-420ed1a1db1a-S0 at slave(399)@172.30.2.151:51934 (ip-172-30-2-151.mesosphere.io)
I0226 03:17:26.783136  1114 master.cpp:6589] Sending checkpointed resources disk(role1)[id1:path1]:64 to slave 5cc57c0e-f1ad-4107-893f-420ed1a1db1a-S0 at slave(399)@172.30.2.151:51934 (ip-172-30-2-151.mesosphere.io)
I0226 03:17:26.783641  1111 slave.cpp:2341] Updated checkpointed resources from  to disk(role1)[id1:path1]:64
I0226 03:17:26.783911  1114 master.hpp:176] Adding task 1 with resources cpus(*):1; mem(*):64; disk(role1)[id1:path1]:64 on slave 5cc57c0e-f1ad-4107-893f-420ed1a1db1a-S0 (ip-172-30-2-151.mesosphere.io)
I0226 03:17:26.784056  1114 master.cpp:3623] Launching task 1 of framework 5cc57c0e-f1ad-4107-893f-420ed1a1db1a-0000 (default) at scheduler-c59020d6-385e-48a3-8a10-9e5c3f1dbd92@172.30.2.151:51934 with resources cpus(*):1; mem(*):64; disk(role1)[id1:path1]:64 on slave 5cc57c0e-f1ad-4107-893f-420ed1a1db1a-S0 at slave(399)@172.30.2.151:51934 (ip-172-30-2-151.mesosphere.io)
I0226 03:17:26.784397  1115 slave.cpp:1361] Got assigned task 1 for framework 5cc57c0e-f1ad-4107-893f-420ed1a1db1a-0000
I0226 03:17:26.784557  1115 slave.cpp:5287] Checkpointing FrameworkInfo to '/tmp/DockerContainerizerTest_ROOT_DOCKER_RecoverOrphanedPersistentVolumes_aJOesP/meta/slaves/5cc57c0e-f1ad-4107-893f-420ed1a1db1a-S0/frameworks/5cc57c0e-f1ad-4107-893f-420ed1a1db1a-0000/framework.info'
I0226 03:17:26.784739  1116 hierarchical.cpp:653] Updated allocation of framework 5cc57c0e-f1ad-4107-893f-420ed1a1db1a-0000 on slave 5cc57c0e-f1ad-4107-893f-420ed1a1db1a-S0 from disk(role1):2048; cpu(*):2; mem(*):2048; cpus(*):8; ports(*):[31000-32000] to disk(role1):1984; cpu(*):2; mem(*):2048; cpus(*):8; ports(*):[31000-32000]; disk(role1)[id1:path1]:64
I0226 03:17:26.784848  1115 slave.cpp:5298] Checkpointing framework pid 'scheduler-c59020d6-385e-48a3-8a10-9e5c3f1dbd92@172.30.2.151:51934' to '/tmp/DockerContainerizerTest_ROOT_DOCKER_RecoverOrphanedPersistentVolumes_aJOesP/meta/slaves/5cc57c0e-f1ad-4107-893f-420ed1a1db1a-S0/frameworks/5cc57c0e-f1ad-4107-893f-420ed1a1db1a-0000/framework.pid'
I0226 03:17:26.785078  1115 resources.cpp:576] Parsing resources as JSON failed: cpus:0.1;mem:32
Trying semicolon-delimited string format instead
I0226 03:17:26.785322  1116 hierarchical.cpp:892] Recovered disk(role1):1984; cpu(*):2; mem(*):1984; cpus(*):7; ports(*):[31000-32000] (total: cpu(*):2; mem(*):2048; disk(role1):1984; cpus(*):8; ports(*):[31000-32000]; disk(role1)[id1:path1]:64, allocated: disk(role1)[id1:path1]:64; cpus(*):1; mem(*):64) on slave 5cc57c0e-f1ad-4107-893f-420ed1a1db1a-S0 from framework 5cc57c0e-f1ad-4107-893f-420ed1a1db1a-0000
I0226 03:17:26.785658  1115 slave.cpp:1480] Launching task 1 for framework 5cc57c0e-f1ad-4107-893f-420ed1a1db1a-0000
I0226 03:17:26.785719  1115 resources.cpp:576] Parsing resources as JSON failed: cpus:0.1;mem:32
Trying semicolon-delimited string format instead
I0226 03:17:26.786197  1115 paths.cpp:474] Trying to chown '/tmp/DockerContainerizerTest_ROOT_DOCKER_RecoverOrphanedPersistentVolumes_aJOesP/slaves/5cc57c0e-f1ad-4107-893f-420ed1a1db1a-S0/frameworks/5cc57c0e-f1ad-4107-893f-420ed1a1db1a-0000/executors/1/runs/bcc90102-163d-4ff6-a3fc-a1b2e3fc3b7c' to user 'root'
I0226 03:17:26.791122  1115 slave.cpp:5739] Checkpointing ExecutorInfo to '/tmp/DockerContainerizerTest_ROOT_DOCKER_RecoverOrphanedPersistentVolumes_aJOesP/meta/slaves/5cc57c0e-f1ad-4107-893f-420ed1a1db1a-S0/frameworks/5cc57c0e-f1ad-4107-893f-420ed1a1db1a-0000/executors/1/executor.info'
I0226 03:17:26.791543  1115 slave.cpp:5367] Launching executor 1 of framework 5cc57c0e-f1ad-4107-893f-420ed1a1db1a-0000 with resources cpus(*):0.1; mem(*):32 in work directory '/tmp/DockerContainerizerTest_ROOT_DOCKER_RecoverOrphanedPersistentVolumes_aJOesP/slaves/5cc57c0e-f1ad-4107-893f-420ed1a1db1a-S0/frameworks/5cc57c0e-f1ad-4107-893f-420ed1a1db1a-0000/executors/1/runs/bcc90102-163d-4ff6-a3fc-a1b2e3fc3b7c'
I0226 03:17:26.792325  1115 slave.cpp:5762] Checkpointing TaskInfo to '/tmp/DockerContainerizerTest_ROOT_DOCKER_RecoverOrphanedPersistentVolumes_aJOesP/meta/slaves/5cc57c0e-f1ad-4107-893f-420ed1a1db1a-S0/frameworks/5cc57c0e-f1ad-4107-893f-420ed1a1db1a-0000/executors/1/runs/bcc90102-163d-4ff6-a3fc-a1b2e3fc3b7c/tasks/1/task.info'
I0226 03:17:26.794337  1115 slave.cpp:1698] Queuing task '1' for executor '1' of framework 5cc57c0e-f1ad-4107-893f-420ed1a1db1a-0000
I0226 03:17:26.794478  1115 slave.cpp:749] Successfully attached file '/tmp/DockerContainerizerTest_ROOT_DOCKER_RecoverOrphanedPersistentVolumes_aJOesP/slaves/5cc57c0e-f1ad-4107-893f-420ed1a1db1a-S0/frameworks/5cc57c0e-f1ad-4107-893f-420ed1a1db1a-0000/executors/1/runs/bcc90102-163d-4ff6-a3fc-a1b2e3fc3b7c'
I0226 03:17:26.797106  1116 docker.cpp:1023] Starting container 'bcc90102-163d-4ff6-a3fc-a1b2e3fc3b7c' for task '1' (and executor '1') of framework '5cc57c0e-f1ad-4107-893f-420ed1a1db1a-0000'
I0226 03:17:26.797462  1116 docker.cpp:1053] Running docker -H unix:///var/run/docker.sock inspect alpine:latest
I0226 03:17:26.910549  1111 docker.cpp:394] Docker pull alpine completed
I0226 03:17:26.910800  1111 docker.cpp:483] Changing the ownership of the persistent volume at '/tmp/DockerContainerizerTest_ROOT_DOCKER_RecoverOrphanedPersistentVolumes_aJOesP/volumes/roles/role1/id1' with uid 0 and gid 0
I0226 03:17:26.915712  1111 docker.cpp:504] Mounting '/tmp/DockerContainerizerTest_ROOT_DOCKER_RecoverOrphanedPersistentVolumes_aJOesP/volumes/roles/role1/id1' to '/tmp/DockerContainerizerTest_ROOT_DOCKER_RecoverOrphanedPersistentVolumes_aJOesP/slaves/5cc57c0e-f1ad-4107-893f-420ed1a1db1a-S0/frameworks/5cc57c0e-f1ad-4107-893f-420ed1a1db1a-0000/executors/1/runs/bcc90102-163d-4ff6-a3fc-a1b2e3fc3b7c/path1' for persistent volume disk(role1)[id1:path1]:64 of container bcc90102-163d-4ff6-a3fc-a1b2e3fc3b7c
I0226 03:17:26.919000  1117 docker.cpp:576] Checkpointing pid 9568 to '/tmp/DockerContainerizerTest_ROOT_DOCKER_RecoverOrphanedPersistentVolumes_aJOesP/meta/slaves/5cc57c0e-f1ad-4107-893f-420ed1a1db1a-S0/frameworks/5cc57c0e-f1ad-4107-893f-420ed1a1db1a-0000/executors/1/runs/bcc90102-163d-4ff6-a3fc-a1b2e3fc3b7c/pids/forked.pid'
I0226 03:17:26.974776  1114 slave.cpp:2643] Got registration for executor '1' of framework 5cc57c0e-f1ad-4107-893f-420ed1a1db1a-0000 from executor(1)@172.30.2.151:46052
I0226 03:17:26.975217  1114 slave.cpp:2729] Checkpointing executor pid 'executor(1)@172.30.2.151:46052' to '/tmp/DockerContainerizerTest_ROOT_DOCKER_RecoverOrphanedPersistentVolumes_aJOesP/meta/slaves/5cc57c0e-f1ad-4107-893f-420ed1a1db1a-S0/frameworks/5cc57c0e-f1ad-4107-893f-420ed1a1db1a-0000/executors/1/runs/bcc90102-163d-4ff6-a3fc-a1b2e3fc3b7c/pids/libprocess.pid'
I0226 03:17:26.976177  1113 docker.cpp:1303] Ignoring updating container 'bcc90102-163d-4ff6-a3fc-a1b2e3fc3b7c' with resources passed to update is identical to existing resources
I0226 03:17:26.976492  1115 slave.cpp:1863] Sending queued task '1' to executor '1' of framework 5cc57c0e-f1ad-4107-893f-420ed1a1db1a-0000 at executor(1)@172.30.2.151:46052
I0226 03:17:27.691769  1111 slave.cpp:3002] Handling status update TASK_RUNNING (UUID: 9f75a4e5-9ff4-4ca9-8623-8b2574796229) for task 1 of framework 5cc57c0e-f1ad-4107-893f-420ed1a1db1a-0000 from executor(1)@172.30.2.151:46052
I0226 03:17:27.692291  1116 status_update_manager.cpp:320] Received status update TASK_RUNNING (UUID: 9f75a4e5-9ff4-4ca9-8623-8b2574796229) for task 1 of framework 5cc57c0e-f1ad-4107-893f-420ed1a1db1a-0000
I0226 03:17:27.692327  1116 status_update_manager.cpp:497] Creating StatusUpdate stream for task 1 of framework 5cc57c0e-f1ad-4107-893f-420ed1a1db1a-0000
I0226 03:17:27.692773  1116 status_update_manager.cpp:824] Checkpointing UPDATE for status update TASK_RUNNING (UUID: 9f75a4e5-9ff4-4ca9-8623-8b2574796229) for task 1 of framework 5cc57c0e-f1ad-4107-893f-420ed1a1db1a-0000
I0226 03:17:27.700090  1116 status_update_manager.cpp:374] Forwarding update TASK_RUNNING (UUID: 9f75a4e5-9ff4-4ca9-8623-8b2574796229) for task 1 of framework 5cc57c0e-f1ad-4107-893f-420ed1a1db1a-0000 to the slave
I0226 03:17:27.700389  1113 slave.cpp:3400] Forwarding the update TASK_RUNNING (UUID: 9f75a4e5-9ff4-4ca9-8623-8b2574796229) for task 1 of framework 5cc57c0e-f1ad-4107-893f-420ed1a1db1a-0000 to master@172.30.2.151:51934
I0226 03:17:27.700606  1113 slave.cpp:3294] Status update manager successfully handled status update TASK_RUNNING (UUID: 9f75a4e5-9ff4-4ca9-8623-8b2574796229) for task 1 of framework 5cc57c0e-f1ad-4107-893f-420ed1a1db1a-0000
I0226 03:17:27.700644  1113 slave.cpp:3310] Sending acknowledgement for status update TASK_RUNNING (UUID: 9f75a4e5-9ff4-4ca9-8623-8b2574796229) for task 1 of framework 5cc57c0e-f1ad-4107-893f-420ed1a1db1a-0000 to executor(1)@172.30.2.151:46052
I0226 03:17:27.700742  1117 master.cpp:4794] Status update TASK_RUNNING (UUID: 9f75a4e5-9ff4-4ca9-8623-8b2574796229) for task 1 of framework 5cc57c0e-f1ad-4107-893f-420ed1a1db1a-0000 from slave 5cc57c0e-f1ad-4107-893f-420ed1a1db1a-S0 at slave(399)@172.30.2.151:51934 (ip-172-30-2-151.mesosphere.io)
I0226 03:17:27.700775  1117 master.cpp:4842] Forwarding status update TASK_RUNNING (UUID: 9f75a4e5-9ff4-4ca9-8623-8b2574796229) for task 1 of framework 5cc57c0e-f1ad-4107-893f-420ed1a1db1a-0000
I0226 03:17:27.700923  1117 master.cpp:6450] Updating the state of task 1 of framework 5cc57c0e-f1ad-4107-893f-420ed1a1db1a-0000 (latest state: TASK_RUNNING, status update state: TASK_RUNNING)
I0226 03:17:27.701145  1118 sched.cpp:981] Scheduler::statusUpdate took 107222ns
I0226 03:17:27.701550  1112 master.cpp:3952] Processing ACKNOWLEDGE call 9f75a4e5-9ff4-4ca9-8623-8b2574796229 for task 1 of framework 5cc57c0e-f1ad-4107-893f-420ed1a1db1a-0000 (default) at scheduler-c59020d6-385e-48a3-8a10-9e5c3f1dbd92@172.30.2.151:51934 on slave 5cc57c0e-f1ad-4107-893f-420ed1a1db1a-S0
I0226 03:17:27.701828  1114 status_update_manager.cpp:392] Received status update acknowledgement (UUID: 9f75a4e5-9ff4-4ca9-8623-8b2574796229) for task 1 of framework 5cc57c0e-f1ad-4107-893f-420ed1a1db1a-0000
I0226 03:17:27.701962  1114 status_update_manager.cpp:824] Checkpointing ACK for status update TASK_RUNNING (UUID: 9f75a4e5-9ff4-4ca9-8623-8b2574796229) for task 1 of framework 5cc57c0e-f1ad-4107-893f-420ed1a1db1a-0000
I0226 03:17:27.701987  1112 slave.cpp:668] Slave terminating
I0226 03:17:27.702256  1117 master.cpp:1174] Slave 5cc57c0e-f1ad-4107-893f-420ed1a1db1a-S0 at slave(399)@172.30.2.151:51934 (ip-172-30-2-151.mesosphere.io) disconnected
I0226 03:17:27.702275  1117 master.cpp:2635] Disconnecting slave 5cc57c0e-f1ad-4107-893f-420ed1a1db1a-S0 at slave(399)@172.30.2.151:51934 (ip-172-30-2-151.mesosphere.io)
I0226 03:17:27.702335  1117 master.cpp:2654] Deactivating slave 5cc57c0e-f1ad-4107-893f-420ed1a1db1a-S0 at slave(399)@172.30.2.151:51934 (ip-172-30-2-151.mesosphere.io)
I0226 03:17:27.702492  1111 hierarchical.cpp:560] Slave 5cc57c0e-f1ad-4107-893f-420ed1a1db1a-S0 deactivated
I0226 03:17:27.707713  1115 slave.cpp:193] Slave started on 400)@172.30.2.151:51934
I0226 03:17:27.707739  1115 slave.cpp:194] Flags at startup: --appc_simple_discovery_uri_prefix=""http://"" --appc_store_dir=""/tmp/mesos/store/appc"" --authenticatee=""crammd5"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --credential=""/tmp/DockerContainerizerTest_ROOT_DOCKER_RecoverOrphanedPersistentVolumes_aJOesP/credential"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_kill_orphans=""true"" --docker_registry=""https://registry-1.docker.io"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/tmp/mesos/store/docker"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/tmp/DockerContainerizerTest_ROOT_DOCKER_RecoverOrphanedPersistentVolumes_aJOesP/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --hostname_lookup=""true"" --image_provisioner_backend=""copy"" --initialize_driver_logging=""true"" --isolation=""posix/cpu,posix/mem"" --launcher_dir=""/mnt/teamcity/work/4240ba9ddd0997c3/build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""10ms"" --resources=""cpu:2;mem:2048;disk(role1):2048"" --revocable_cpu_low_priority=""true"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --systemd_enable_support=""true"" --systemd_runtime_directory=""/run/systemd/system"" --version=""false"" --work_dir=""/tmp/DockerContainerizerTest_ROOT_DOCKER_RecoverOrphanedPersistentVolumes_aJOesP""
I0226 03:17:27.708133  1115 credentials.hpp:83] Loading credential for authentication from '/tmp/DockerContainerizerTest_ROOT_DOCKER_RecoverOrphanedPersistentVolumes_aJOesP/credential'
I0226 03:17:27.708282  1115 slave.cpp:324] Slave using credential for: test-principal
I0226 03:17:27.708407  1115 resources.cpp:576] Parsing resources as JSON failed: cpu:2;mem:2048;disk(role1):2048
Trying semicolon-delimited string format instead
I0226 03:17:27.708874  1115 slave.cpp:464] Slave resources: cpu(*):2; mem(*):2048; disk(role1):2048; cpus(*):8; ports(*):[31000-32000]
I0226 03:17:27.708931  1115 slave.cpp:472] Slave attributes: [  ]
I0226 03:17:27.708941  1115 slave.cpp:477] Slave hostname: ip-172-30-2-151.mesosphere.io
I0226 03:17:27.710033  1113 state.cpp:58] Recovering state from '/tmp/DockerContainerizerTest_ROOT_DOCKER_RecoverOrphanedPersistentVolumes_aJOesP/meta'
I0226 03:17:27.711252  1114 fetcher.cpp:81] Clearing fetcher cache
I0226 03:17:27.711447  1116 status_update_manager.cpp:200] Recovering status update manager
I0226 03:17:27.711727  1111 docker.cpp:726] Recovering Docker containers
I0226 03:17:27.711839  1111 docker.cpp:885] Running docker -H unix:///var/run/docker.sock ps -a
I0226 03:17:27.728170  1117 hierarchical.cpp:1434] No resources available to allocate!
I0226 03:17:27.728235  1117 hierarchical.cpp:1529] No inverse offers to send out!
I0226 03:17:27.728268  1117 hierarchical.cpp:1127] Performed allocation for 1 slaves in 296715ns
I0226 03:17:27.817551  1113 docker.cpp:766] Running docker -H unix:///var/run/docker.sock inspect mesos-5cc57c0e-f1ad-4107-893f-420ed1a1db1a-S0.bcc90102-163d-4ff6-a3fc-a1b2e3fc3b7c
I0226 03:17:27.923014  1112 docker.cpp:932] Checking if Docker container named '/mesos-5cc57c0e-f1ad-4107-893f-420ed1a1db1a-S0.bcc90102-163d-4ff6-a3fc-a1b2e3fc3b7c' was started by Mesos
I0226 03:17:27.923071  1112 docker.cpp:942] Checking if Mesos container with ID 'bcc90102-163d-4ff6-a3fc-a1b2e3fc3b7c' has been orphaned
I0226 03:17:27.923122  1112 docker.cpp:678] Running docker -H unix:///var/run/docker.sock stop -t 0 0a10ad8641f8e85227324a979817933322dc901706cb4430eab0bcaf979835d1
I0226 03:17:28.023885  1116 docker.cpp:727] Running docker -H unix:///var/run/docker.sock rm -v 0a10ad8641f8e85227324a979817933322dc901706cb4430eab0bcaf979835d1

I0226 03:17:28.127876  1114 docker.cpp:912] Unmounting volume for container 'bcc90102-163d-4ff6-a3fc-a1b2e3fc3b7c'
../../3rdparty/libprocess/include/process/gmock.hpp:214: ERROR: this mock object (used in test DockerContainerizerTest.ROOT_DOCKER_RecoverOrphanedPersistentVolumes) should be deleted but never is. Its address is @0x5781dd8.
I0226 03:17:28.127957  1114 docker.cpp:912] Unmounting volume for container 'bcc90102-163d-4ff6-a3fc-a1b2e3fc3b7c'
../../src/tests/mesos.cpp:673: ERROR: this mock object (used in test DockerContainerizerTest.ROOT_DOCKER_RecoverOrphanedPersistentVolumes) should be deleted but never is. Its address is @0x5a03260.
../../src/tests/mesos.hpp:1357: ERROR: this mock object (used in test DockerContainerizerTest.ROOT_DOCKER_RecoverOrphanedPersistentVolumes) should be deleted but never is. Its address is @0x5b477c0.
Failed to perform recovery: Unable to unmount volumes for Docker container 'bcc90102-163d-4ff6-a3fc-a1b2e3fc3b7c': Failed to unmount volume '/tmp/DockerContainerizerTest_ROOT_DOCKER_RecoverOrphanedPersistentVolumes_aJOesP/slaves/5cc57c0e-f1ad-4107-893f-420ed1a1db1a-S0/frameworks/5cc57c0e-f1ad-4107-893f-420ed1a1db1a-0000/executors/1/runs/bcc90102-163d-4ff6-a3fc-a1b2e3fc3b7c/path1': Failed to unmount '/tmp/DockerContainerizerTest_ROOT_DOCKER_RecoverOrphanedPersistentVolumes_aJOesP/slaves/5cc57c0e-f1ad-4107-893f-420ed1a1db1a-S0/frameworks/5cc57c0e-f1ad-4107-893f-420ed1a1db1a-0000/executors/1/runs/bcc90102-163d-4ff6-a3fc-a1b2e3fc3b7c/path1': Invalid argument
../../src/tests/containerizer/docker_containerizer_tests.cpp:1650: ERROR: this mock object (used in test DockerContainerizerTest.ROOT_DOCKER_RecoverOrphanedPersistentVolumes) should be deleted but never is. Its address is @0x7ffe75a8d310.
To remedy this do as follows:
ERROR: 4 leaked mock objects found at program exit.
Step 1: rm -f /tmp/DockerContainerizerTest_ROOT_DOCKER_RecoverOrphanedPersistentVolumes_aJOesP/meta/slaves/latest
        This ensures slave doesn't recover old live executors.
Step 2: Restart the slave.
Process exited with code 1
{code}

There appear to be two problems:
1) The docker containerizer should not exit on failure to clean up orphans.  The MesosContainerizer does not do this (see [MESOS-2367]).
2) Unmounting the orphan persistent volume fails for some reason.",Bug,Major,kaysoky,2016-03-02T01:38:12.000+0000,5,Resolved,Complete,DockerContainerizerTest.ROOT_DOCKER_RecoverOrphanedPersistentVolumes exits when the /tmp directory is bind-mounted,2016-03-02T01:38:12.000+0000,MESOS-4832,2.0,mesos,Mesosphere Sprint 30
gilbert,2016-03-01T19:43:47.000+0000,gilbert,"If image provider is specified as `docker` but docker/runtime is not set, it would be not meaningful, because of no executables. A check should be added to make sure docker runtime isolator is on if using docker as image provider.",Bug,Major,gilbert,2016-03-01T21:16:09.000+0000,5,Resolved,Complete,Bind docker runtime isolator with docker image provider.,2016-03-01T21:16:09.000+0000,MESOS-4830,1.0,mesos,Mesosphere Sprint 30
anandmazumdar,2016-03-01T18:48:31.000+0000,anandmazumdar,"There are two ways in which a shutdown of executor can be triggered:
1. If it receives an explicit `Shutdown` message from the agent.
2. If the recovery timeout period has elapsed, and the executor still hasn’t been able to (re-)connect with the agent.

Currently, the executor library relies on the field `grace_period_seconds` having a default value of 5 seconds to handle the second scenario. https://github.com/apache/mesos/blob/master/src/executor/executor.cpp#L608

The driver used to trigger the grace period via a constant defined in src/slave/constants.cpp. https://github.com/apache/mesos/blob/master/src/exec/exec.cpp#L92

The agent may want to force a shorter shutdown grace period (e.g. oversubscription eviction may have shorter deadline) in the future. For now, we can just read the value via an environment variable.",Task,Major,anandmazumdar,2016-03-01T23:40:24.000+0000,5,Resolved,Complete,Remove `grace_period_seconds` field from Shutdown event v1 protobuf.,2016-03-01T23:40:24.000+0000,MESOS-4829,3.0,mesos,Mesosphere Sprint 30
klaus1982,2016-03-01T02:03:29.000+0000,jvanremoortere,The master's logic for reregistering a slave does not update the version field if the slave re-registers with a new version.,Bug,Blocker,jvanremoortere,2016-03-03T02:02:55.000+0000,5,Resolved,Complete,Master's slave reregister logic does not update version field,2016-03-03T02:02:55.000+0000,MESOS-4825,1.0,mesos,
kaysoky,2016-03-01T00:28:16.000+0000,kaysoky,"A persistent volume can be orphaned when:
# A framework registers with checkpointing enabled.
# The framework starts a task + a persistent volume.
# The agent exits.  The task continues to run.
# Something wipes the agent's {{meta}} directory.  This removes the checkpointed framework info from the agent.
# The agent comes back and recovers.  The framework for the task is not found, so the task is considered orphaned now.

The agent currently does not unmount the persistent volume, saying (with {{GLOG_v=1}}) 
{code}
I0229 23:55:42.078940  5635 linux.cpp:711] Ignoring cleanup request for unknown container: a35189d3-85d5-4d02-b568-67f675b6dc97
{code}

Test implemented here: https://reviews.apache.org/r/44122/",Bug,Blocker,kaysoky,2016-03-02T15:43:09.000+0000,5,Resolved,Complete,"""filesystem/linux"" isolator does not unmount orphaned persistent volumes",2016-03-02T15:43:09.000+0000,MESOS-4824,2.0,mesos,Mesosphere Sprint 30
avinash@mesosphere.io,2016-02-29T20:12:20.000+0000,avinash@mesosphere.io,"Most docker and appc images wish to expose ports that micro-services are listening on, to the outside world. When containers are running on bridged (or ptp) networking this can be achieved by installing port forwarding rules on the agent (using iptables). This can be done in the `network/cni` isolator. 

The reason we would like this functionality to be implemented in the `network/cni` isolator, and not a CNI plugin, is that the specifications currently do not support specifying port forwarding rules. Further, to install these rules the isolator needs two pieces of information, the exposed ports and the IP address associated with the container. Bother are available to the isolator.",Task,Critical,avinash@mesosphere.io,,10020,Accepted,In Progress,Implement port forwarding in `network/cni` isolator,2016-04-06T15:49:14.000+0000,MESOS-4823,2.0,mesos,Mesosphere Sprint 30
jojy,2016-02-29T20:07:37.000+0000,jojy,Currently Appc image provisioner supports http(s) fetching. It would be valuable to add support for local file path(URI) based  fetching.,Task,Major,jojy,2016-03-03T02:07:32.000+0000,5,Resolved,Complete,Add support for local image fetching in Appc provisioner.,2016-03-03T02:07:32.000+0000,MESOS-4822,2.0,mesos,Mesosphere Sprint 30
avinash@mesosphere.io,2016-02-29T20:06:19.000+0000,avinash@mesosphere.io,Networking isolators such as `network/cni` need to learn about ports that a container wishes to be exposed to the outside world. This can be achieved by adding a field to the `ImageManifest` protobuf and allowing the `ImageProvisioner` to set these fields to inform the isolator of the ports that the container wishes to be exposed. ,Task,Major,avinash@mesosphere.io,2016-03-30T16:54:03.000+0000,5,Resolved,Complete,Introduce a port field in `ImageManifest` in order to set exposed ports for a container.,2016-03-30T16:54:03.000+0000,MESOS-4821,1.0,mesos,Mesosphere Sprint 30
avinash@mesosphere.io,2016-02-29T20:02:54.000+0000,avinash@mesosphere.io,"Most docker images have an `EXPOSE` command associated with them. This tells the container run-time the TCP ports that the micro-service ""wishes"" to expose to the outside world. 

With the `Unified containerizer` project since `MesosContainerizer` is going to natively support docker images it is imperative that the Mesos container run time have a mechanism to expose ports listed in a Docker image. The first step to achieve this is to extract this information from the `Docker` image and set in the `ContainerConfig` . The `ContainerConfig` can then be used to pass this information to any isolator (for e.g. `network/cni` isolator) that will install port forwarding rules to expose the desired ports.",Task,Critical,avinash@mesosphere.io,2016-03-30T16:56:05.000+0000,5,Resolved,Complete,Need to set `EXPOSED` ports from docker images into `ContainerConfig`,2016-03-30T16:56:05.000+0000,MESOS-4820,1.0,mesos,Mesosphere Sprint 30
,2016-02-29T19:56:41.000+0000,jojy,"Add documentation for the Appc image discovery feature that covers:

- Use case
- Implementation detail (Simple discovery).",Documentation,Major,jojy,,10020,Accepted,In Progress,Add documentation for Appc image discovery.,2016-03-14T18:10:53.000+0000,MESOS-4819,3.0,mesos,
jojy,2016-02-29T19:52:58.000+0000,jojy,"Add tests that covers integration test of the Appc provisioner feature with mesos containerizer.
 ",Task,Major,jojy,2016-03-23T22:45:45.000+0000,5,Resolved,Complete,Add end to end testing for Appc images.,2016-03-23T22:45:45.000+0000,MESOS-4818,3.0,mesos,Mesosphere Sprint 30
js84,2016-02-29T19:34:49.000+0000,js84,"We still use the deprecated *.json internally (UI, tests, documentation). ",Task,Major,js84,2016-03-01T21:39:05.000+0000,5,Resolved,Complete,Remove internal usage of deprecated *.json endpoints.,2016-03-01T21:39:05.000+0000,MESOS-4817,3.0,mesos,Mesosphere Sprint 30
gilbert,2016-02-29T18:33:30.000+0000,gilbert,Using command line executor to test shell commands with local docker images.,Task,Major,gilbert,2016-03-14T17:21:00.000+0000,5,Resolved,Complete,Implement base tests for unified container using local puller.,2016-03-14T17:21:01.000+0000,MESOS-4813,2.0,mesos,Mesosphere Sprint 30
jieyu,2016-02-29T10:42:41.000+0000,bernd-mesos,"{noformat}
[09:46:46] :	 [Step 11/11] [ RUN      ] ProvisionerDockerRegistryPullerTest.ROOT_INTERNET_CURL_ShellCommand
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.628413  1166 leveldb.cpp:174] Opened db in 4.242882ms
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.629926  1166 leveldb.cpp:181] Compacted db in 1.483621ms
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.629966  1166 leveldb.cpp:196] Created db iterator in 15498ns
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.629977  1166 leveldb.cpp:202] Seeked to beginning of db in 1405ns
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.629984  1166 leveldb.cpp:271] Iterated through 0 keys in the db in 239ns
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.630015  1166 replica.cpp:779] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.630470  1183 recover.cpp:447] Starting replica recovery
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.630702  1180 recover.cpp:473] Replica is in EMPTY status
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.631767  1182 replica.cpp:673] Replica in EMPTY status received a broadcasted recover request from (14567)@172.30.2.124:37431
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.632115  1183 recover.cpp:193] Received a recover response from a replica in EMPTY status
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.632450  1186 recover.cpp:564] Updating replica status to STARTING
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.633476  1186 master.cpp:375] Master 3fbb2fb0-4f18-498b-a440-9acbf6923a13 (ip-172-30-2-124.mesosphere.io) started on 172.30.2.124:37431
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.633491  1186 master.cpp:377] Flags at startup: --acls="""" --allocation_interval=""1secs"" --allocator=""HierarchicalDRF"" --authenticate=""true"" --authenticate_http=""true"" --authenticate_slaves=""true"" --authenticators=""crammd5"" --authorizers=""local"" --credentials=""/tmp/4UxXoW/credentials"" --framework_sorter=""drf"" --help=""false"" --hostname_lookup=""true"" --http_authenticators=""basic"" --initialize_driver_logging=""true"" --log_auto_initialize=""true"" --logbufsecs=""0"" --logging_level=""INFO"" --max_completed_frameworks=""50"" --max_completed_tasks_per_framework=""1000"" --max_slave_ping_timeouts=""5"" --quiet=""false"" --recovery_slave_removal_limit=""100%"" --registry=""replicated_log"" --registry_fetch_timeout=""1mins"" --registry_store_timeout=""100secs"" --registry_strict=""true"" --root_submissions=""true"" --slave_ping_timeout=""15secs"" --slave_reregister_timeout=""10mins"" --user_sorter=""drf"" --version=""false"" --webui_dir=""/usr/local/share/mesos/webui"" --work_dir=""/tmp/4UxXoW/master"" --zk_session_timeout=""10secs""
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.633677  1186 master.cpp:422] Master only allowing authenticated frameworks to register
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.633685  1186 master.cpp:427] Master only allowing authenticated slaves to register
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.633692  1186 credentials.hpp:35] Loading credentials for authentication from '/tmp/4UxXoW/credentials'
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.633851  1183 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 1.191043ms
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.633873  1183 replica.cpp:320] Persisted replica status to STARTING
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.633894  1186 master.cpp:467] Using default 'crammd5' authenticator
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.634003  1186 master.cpp:536] Using default 'basic' HTTP authenticator
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.634062  1184 recover.cpp:473] Replica is in STARTING status
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.634109  1186 master.cpp:570] Authorization enabled
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.634249  1187 whitelist_watcher.cpp:77] No whitelist given
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.634255  1184 hierarchical.cpp:144] Initialized hierarchical allocator process
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.634884  1187 replica.cpp:673] Replica in STARTING status received a broadcasted recover request from (14569)@172.30.2.124:37431
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.635278  1181 recover.cpp:193] Received a recover response from a replica in STARTING status
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.635742  1187 recover.cpp:564] Updating replica status to VOTING
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.636391  1180 master.cpp:1711] The newly elected leader is master@172.30.2.124:37431 with id 3fbb2fb0-4f18-498b-a440-9acbf6923a13
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.636415  1180 master.cpp:1724] Elected as the leading master!
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.636430  1180 master.cpp:1469] Recovering from registrar
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.636554  1187 registrar.cpp:307] Recovering registrar
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.637111  1181 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 1.120322ms
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.637133  1181 replica.cpp:320] Persisted replica status to VOTING
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.637218  1186 recover.cpp:578] Successfully joined the Paxos group
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.637354  1186 recover.cpp:462] Recover process terminated
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.637715  1182 log.cpp:659] Attempting to start the writer
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.638617  1184 replica.cpp:493] Replica received implicit promise request from (14570)@172.30.2.124:37431 with proposal 1
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.639700  1184 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 1.057386ms
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.639722  1184 replica.cpp:342] Persisted promised to 1
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.640251  1184 coordinator.cpp:238] Coordinator attempting to fill missing positions
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.641274  1185 replica.cpp:388] Replica received explicit promise request from (14571)@172.30.2.124:37431 for position 0 with proposal 2
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.642371  1185 leveldb.cpp:341] Persisting action (8 bytes) to leveldb took 1.061574ms
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.642396  1185 replica.cpp:712] Persisted action at 0
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.643299  1186 replica.cpp:537] Replica received write request for position 0 from (14572)@172.30.2.124:37431
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.643349  1186 leveldb.cpp:436] Reading position from leveldb took 21735ns
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.644448  1186 leveldb.cpp:341] Persisting action (14 bytes) to leveldb took 1.06671ms
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.644469  1186 replica.cpp:712] Persisted action at 0
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.645077  1181 replica.cpp:691] Replica received learned notice for position 0 from @0.0.0.0:0
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.646174  1181 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 1.069097ms
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.646198  1181 replica.cpp:712] Persisted action at 0
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.646211  1181 replica.cpp:697] Replica learned NOP action at position 0
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.646716  1182 log.cpp:675] Writer started with ending position 0
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.647538  1183 leveldb.cpp:436] Reading position from leveldb took 21456ns
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.648298  1186 registrar.cpp:340] Successfully fetched the registry (0B) in 11.71072ms
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.648388  1186 registrar.cpp:439] Applied 1 operations in 21138ns; attempting to update the 'registry'
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.648947  1187 log.cpp:683] Attempting to append 210 bytes to the log
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.649050  1183 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 1
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.649655  1187 replica.cpp:537] Replica received write request for position 1 from (14573)@172.30.2.124:37431
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.650725  1187 leveldb.cpp:341] Persisting action (229 bytes) to leveldb took 1.041938ms
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.650748  1187 replica.cpp:712] Persisted action at 1
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.651198  1181 replica.cpp:691] Replica received learned notice for position 1 from @0.0.0.0:0
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.652312  1181 leveldb.cpp:341] Persisting action (231 bytes) to leveldb took 1.092268ms
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.652335  1181 replica.cpp:712] Persisted action at 1
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.652349  1181 replica.cpp:697] Replica learned APPEND action at position 1
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.653095  1187 registrar.cpp:484] Successfully updated the 'registry' in 4.664064ms
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.653236  1187 registrar.cpp:370] Successfully recovered registrar
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.653306  1181 log.cpp:702] Attempting to truncate the log to 1
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.653476  1184 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 2
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.653642  1183 master.cpp:1521] Recovered 0 slaves from the Registry (171B) ; allowing 10mins for slaves to re-register
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.653659  1181 hierarchical.cpp:171] Skipping recovery of hierarchical allocator: nothing to recover
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.654270  1181 replica.cpp:537] Replica received write request for position 2 from (14574)@172.30.2.124:37431
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.655357  1181 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 1.055267ms
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.655378  1181 replica.cpp:712] Persisted action at 2
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.655850  1184 replica.cpp:691] Replica received learned notice for position 2 from @0.0.0.0:0
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.657009  1184 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 1.137223ms
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.657059  1184 leveldb.cpp:399] Deleting ~1 keys from leveldb took 26459ns
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.657074  1184 replica.cpp:712] Persisted action at 2
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.657089  1184 replica.cpp:697] Replica learned TRUNCATE action at position 2
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.665710  1166 containerizer.cpp:149] Using isolation: docker/runtime,filesystem/linux
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.672399  1166 linux_launcher.cpp:101] Using /sys/fs/cgroup/freezer as the freezer hierarchy for the Linux launcher
[09:46:46]W:	 [Step 11/11] E0229 09:46:46.676822  1166 shell.hpp:93] Command 'hadoop version 2>&1' failed; this is the output:
[09:46:46]W:	 [Step 11/11] sh: hadoop: command not found
[09:46:46]W:	 [Step 11/11] E0229 09:46:46.676851  1166 fetcher.cpp:58] Failed to create URI fetcher plugin 'hadoop': Failed to create HDFS client: Failed to execute 'hadoop version 2>&1'; the command was either not found or exited with a non-zero exit status: 127
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.678383  1166 linux.cpp:81] Making '/tmp/ProvisionerDockerRegistryPullerTest_ROOT_INTERNET_CURL_ShellCommand_5BWCfv' a shared mount
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.687223  1180 slave.cpp:193] Slave started on 422)@172.30.2.124:37431
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.687248  1180 slave.cpp:194] Flags at startup: --appc_simple_discovery_uri_prefix=""http://"" --appc_store_dir=""/tmp/mesos/store/appc"" --authenticatee=""crammd5"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --credential=""/tmp/ProvisionerDockerRegistryPullerTest_ROOT_INTERNET_CURL_ShellCommand_5BWCfv/credential"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_kill_orphans=""true"" --docker_registry=""https://registry-1.docker.io"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/tmp/mesos/store/docker"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/tmp/ProvisionerDockerRegistryPullerTest_ROOT_INTERNET_CURL_ShellCommand_5BWCfv/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --hostname_lookup=""true"" --image_providers=""docker"" --image_provisioner_backend=""copy"" --initialize_driver_logging=""true"" --isolation=""docker/runtime,filesystem/linux"" --launcher_dir=""/mnt/teamcity/work/4240ba9ddd0997c3/build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""10ms"" --resources=""cpus:2;mem:1024;disk:1024;ports:[31000-32000]"" --revocable_cpu_low_priority=""true"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --systemd_enable_support=""true"" --systemd_runtime_directory=""/run/systemd/system"" --version=""false"" --work_dir=""/tmp/ProvisionerDockerRegistryPullerTest_ROOT_INTERNET_CURL_ShellCommand_5BWCfv""
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.687531  1180 credentials.hpp:83] Loading credential for authentication from '/tmp/ProvisionerDockerRegistryPullerTest_ROOT_INTERNET_CURL_ShellCommand_5BWCfv/credential'
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.687666  1180 slave.cpp:324] Slave using credential for: test-principal
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.687798  1180 resources.cpp:572] Parsing resources as JSON failed: cpus:2;mem:1024;disk:1024;ports:[31000-32000]
[09:46:46]W:	 [Step 11/11] Trying semicolon-delimited string format instead
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.688151  1180 slave.cpp:464] Slave resources: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.688207  1180 slave.cpp:472] Slave attributes: [  ]
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.688217  1180 slave.cpp:477] Slave hostname: ip-172-30-2-124.mesosphere.io
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.689259  1187 state.cpp:58] Recovering state from '/tmp/ProvisionerDockerRegistryPullerTest_ROOT_INTERNET_CURL_ShellCommand_5BWCfv/meta'
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.689394  1166 sched.cpp:222] Version: 0.28.0
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.689497  1180 status_update_manager.cpp:200] Recovering status update manager
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.689798  1182 containerizer.cpp:407] Recovering containerizer
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.690021  1186 sched.cpp:326] New master detected at master@172.30.2.124:37431
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.690146  1186 sched.cpp:382] Authenticating with master master@172.30.2.124:37431
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.690162  1186 sched.cpp:389] Using default CRAM-MD5 authenticatee
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.690378  1181 authenticatee.cpp:121] Creating new client SASL connection
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.690688  1186 master.cpp:5540] Authenticating scheduler-52603476-875a-49a8-85d4-c98d102cdfab@172.30.2.124:37431
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.690801  1184 authenticator.cpp:413] Starting authentication session for crammd5_authenticatee(877)@172.30.2.124:37431
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.691025  1181 authenticator.cpp:98] Creating new server SASL connection
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.691314  1180 authenticatee.cpp:212] Received SASL authentication mechanisms: CRAM-MD5
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.691339  1180 authenticatee.cpp:238] Attempting to authenticate with mechanism 'CRAM-MD5'
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.691437  1180 authenticator.cpp:203] Received SASL authentication start
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.691490  1180 authenticator.cpp:325] Authentication requires more steps
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.691581  1180 authenticatee.cpp:258] Received SASL authentication step
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.691684  1180 authenticator.cpp:231] Received SASL authentication step
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.691712  1180 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: 'ip-172-30-2-124.mesosphere.io' server FQDN: 'ip-172-30-2-124.mesosphere.io' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.691726  1180 auxprop.cpp:179] Looking up auxiliary property '*userPassword'
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.691768  1180 auxprop.cpp:179] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.691802  1180 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: 'ip-172-30-2-124.mesosphere.io' server FQDN: 'ip-172-30-2-124.mesosphere.io' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.691817  1180 auxprop.cpp:129] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.691829  1180 auxprop.cpp:129] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.691848  1180 authenticator.cpp:317] Authentication success
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.691944  1186 authenticatee.cpp:298] Authentication success
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.692011  1185 master.cpp:5570] Successfully authenticated principal 'test-principal' at scheduler-52603476-875a-49a8-85d4-c98d102cdfab@172.30.2.124:37431
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.692056  1187 authenticator.cpp:431] Authentication session cleanup for crammd5_authenticatee(877)@172.30.2.124:37431
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.692308  1184 sched.cpp:471] Successfully authenticated with master master@172.30.2.124:37431
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.692325  1184 sched.cpp:776] Sending SUBSCRIBE call to master@172.30.2.124:37431
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.692399  1184 sched.cpp:809] Will retry registration in 954.231367ms if necessary
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.692505  1183 master.cpp:2279] Received SUBSCRIBE call for framework 'default' at scheduler-52603476-875a-49a8-85d4-c98d102cdfab@172.30.2.124:37431
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.692553  1183 master.cpp:1750] Authorizing framework principal 'test-principal' to receive offers for role '*'
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.692836  1184 master.cpp:2350] Subscribing framework default with checkpointing disabled and capabilities [  ]
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.692942  1183 metadata_manager.cpp:188] No images to load from disk. Docker provisioner image storage path '/tmp/mesos/store/docker/storedImages' does not exist
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.693208  1180 provisioner.cpp:245] Provisioner recovery complete
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.693295  1186 hierarchical.cpp:265] Added framework 3fbb2fb0-4f18-498b-a440-9acbf6923a13-0000
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.693357  1186 hierarchical.cpp:1437] No resources available to allocate!
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.693397  1186 hierarchical.cpp:1532] No inverse offers to send out!
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.693424  1186 hierarchical.cpp:1130] Performed allocation for 0 slaves in 111679ns
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.693442  1187 sched.cpp:703] Framework registered with 3fbb2fb0-4f18-498b-a440-9acbf6923a13-0000
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.693476  1187 sched.cpp:717] Scheduler::registered took 15735ns
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.693604  1183 slave.cpp:4565] Finished recovery
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.693872  1183 slave.cpp:4737] Querying resource estimator for oversubscribable resources
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.694072  1183 slave.cpp:796] New master detected at master@172.30.2.124:37431
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.694078  1182 status_update_manager.cpp:174] Pausing sending status updates
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.694133  1183 slave.cpp:859] Authenticating with master master@172.30.2.124:37431
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.694159  1183 slave.cpp:864] Using default CRAM-MD5 authenticatee
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.694279  1183 slave.cpp:832] Detecting new master
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.694320  1180 authenticatee.cpp:121] Creating new client SASL connection
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.694438  1183 slave.cpp:4751] Received oversubscribable resources  from the resource estimator
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.694577  1183 master.cpp:5540] Authenticating slave(422)@172.30.2.124:37431
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.694659  1181 authenticator.cpp:413] Starting authentication session for crammd5_authenticatee(878)@172.30.2.124:37431
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.694840  1182 authenticator.cpp:98] Creating new server SASL connection
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.695081  1187 authenticatee.cpp:212] Received SASL authentication mechanisms: CRAM-MD5
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.695109  1187 authenticatee.cpp:238] Attempting to authenticate with mechanism 'CRAM-MD5'
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.695215  1186 authenticator.cpp:203] Received SASL authentication start
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.695257  1186 authenticator.cpp:325] Authentication requires more steps
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.695322  1186 authenticatee.cpp:258] Received SASL authentication step
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.695423  1185 authenticator.cpp:231] Received SASL authentication step
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.695446  1185 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: 'ip-172-30-2-124.mesosphere.io' server FQDN: 'ip-172-30-2-124.mesosphere.io' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.695453  1185 auxprop.cpp:179] Looking up auxiliary property '*userPassword'
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.695477  1185 auxprop.cpp:179] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.695497  1185 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: 'ip-172-30-2-124.mesosphere.io' server FQDN: 'ip-172-30-2-124.mesosphere.io' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.695504  1185 auxprop.cpp:129] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.695510  1185 auxprop.cpp:129] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.695520  1185 authenticator.cpp:317] Authentication success
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.695588  1180 authenticatee.cpp:298] Authentication success
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.695633  1186 master.cpp:5570] Successfully authenticated principal 'test-principal' at slave(422)@172.30.2.124:37431
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.695675  1180 authenticator.cpp:431] Authentication session cleanup for crammd5_authenticatee(878)@172.30.2.124:37431
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.695933  1187 slave.cpp:927] Successfully authenticated with master master@172.30.2.124:37431
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.696039  1187 slave.cpp:1321] Will retry registration in 6.094985ms if necessary
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.696171  1183 master.cpp:4254] Registering slave at slave(422)@172.30.2.124:37431 (ip-172-30-2-124.mesosphere.io) with id 3fbb2fb0-4f18-498b-a440-9acbf6923a13-S0
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.696535  1181 registrar.cpp:439] Applied 1 operations in 48295ns; attempting to update the 'registry'
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.697289  1182 log.cpp:683] Attempting to append 396 bytes to the log
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.697402  1183 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 3
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.698032  1181 replica.cpp:537] Replica received write request for position 3 from (14593)@172.30.2.124:37431
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.699445  1181 leveldb.cpp:341] Persisting action (415 bytes) to leveldb took 1.381647ms
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.699467  1181 replica.cpp:712] Persisted action at 3
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.699934  1181 replica.cpp:691] Replica received learned notice for position 3 from @0.0.0.0:0
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.701073  1181 leveldb.cpp:341] Persisting action (417 bytes) to leveldb took 1.117397ms
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.701095  1181 replica.cpp:712] Persisted action at 3
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.701110  1181 replica.cpp:697] Replica learned APPEND action at position 3
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.702229  1185 registrar.cpp:484] Successfully updated the 'registry' in 5.643008ms
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.702409  1182 log.cpp:702] Attempting to truncate the log to 3
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.702441  1180 slave.cpp:1321] Will retry registration in 33.795772ms if necessary
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.702523  1181 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 4
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.702775  1182 slave.cpp:3482] Received ping from slave-observer(389)@172.30.2.124:37431
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.702837  1184 master.cpp:4322] Registered slave 3fbb2fb0-4f18-498b-a440-9acbf6923a13-S0 at slave(422)@172.30.2.124:37431 (ip-172-30-2-124.mesosphere.io) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.702922  1182 slave.cpp:971] Registered with master master@172.30.2.124:37431; given slave ID 3fbb2fb0-4f18-498b-a440-9acbf6923a13-S0
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.702947  1182 fetcher.cpp:81] Clearing fetcher cache
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.703011  1181 hierarchical.cpp:473] Added slave 3fbb2fb0-4f18-498b-a440-9acbf6923a13-S0 (ip-172-30-2-124.mesosphere.io) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (allocated: )
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.703053  1184 master.cpp:4224] Slave 3fbb2fb0-4f18-498b-a440-9acbf6923a13-S0 at slave(422)@172.30.2.124:37431 (ip-172-30-2-124.mesosphere.io) already registered, resending acknowledgement
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.703060  1186 status_update_manager.cpp:181] Resuming sending status updates
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.703213  1184 replica.cpp:537] Replica received write request for position 4 from (14594)@172.30.2.124:37431
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.703228  1182 slave.cpp:994] Checkpointing SlaveInfo to '/tmp/ProvisionerDockerRegistryPullerTest_ROOT_INTERNET_CURL_ShellCommand_5BWCfv/meta/slaves/3fbb2fb0-4f18-498b-a440-9acbf6923a13-S0/slave.info'
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.703416  1182 slave.cpp:1030] Forwarding total oversubscribed resources 
[09:46:46]W:	 [Step 11/11] W0229 09:46:46.703513  1182 slave.cpp:1016] Already registered with master master@172.30.2.124:37431
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.703531  1182 slave.cpp:1030] Forwarding total oversubscribed resources 
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.703559  1185 master.cpp:4663] Received update of slave 3fbb2fb0-4f18-498b-a440-9acbf6923a13-S0 at slave(422)@172.30.2.124:37431 (ip-172-30-2-124.mesosphere.io) with total oversubscribed resources 
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.703564  1181 hierarchical.cpp:1532] No inverse offers to send out!
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.703614  1181 hierarchical.cpp:1150] Performed allocation for slave 3fbb2fb0-4f18-498b-a440-9acbf6923a13-S0 in 572661ns
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.703939  1185 master.cpp:5369] Sending 1 offers to framework 3fbb2fb0-4f18-498b-a440-9acbf6923a13-0000 (default) at scheduler-52603476-875a-49a8-85d4-c98d102cdfab@172.30.2.124:37431
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.703972  1186 hierarchical.cpp:531] Slave 3fbb2fb0-4f18-498b-a440-9acbf6923a13-S0 (ip-172-30-2-124.mesosphere.io) updated with oversubscribed resources  (total: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000], allocated: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000])
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.704087  1186 hierarchical.cpp:1437] No resources available to allocate!
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.704113  1185 master.cpp:4663] Received update of slave 3fbb2fb0-4f18-498b-a440-9acbf6923a13-S0 at slave(422)@172.30.2.124:37431 (ip-172-30-2-124.mesosphere.io) with total oversubscribed resources 
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.704123  1186 hierarchical.cpp:1532] No inverse offers to send out!
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.704169  1186 hierarchical.cpp:1150] Performed allocation for slave 3fbb2fb0-4f18-498b-a440-9acbf6923a13-S0 in 162818ns
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.704421  1184 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 1.177949ms
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.704442  1187 sched.cpp:873] Scheduler::resourceOffers took 146551ns
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.704452  1184 replica.cpp:712] Persisted action at 4
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.704747  1166 resources.cpp:572] Parsing resources as JSON failed: cpus:1;mem:128
[09:46:46]W:	 [Step 11/11] Trying semicolon-delimited string format instead
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.704737  1185 hierarchical.cpp:531] Slave 3fbb2fb0-4f18-498b-a440-9acbf6923a13-S0 (ip-172-30-2-124.mesosphere.io) updated with oversubscribed resources  (total: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000], allocated: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000])
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.704888  1185 hierarchical.cpp:1437] No resources available to allocate!
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.704931  1185 hierarchical.cpp:1532] No inverse offers to send out!
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.704958  1185 hierarchical.cpp:1150] Performed allocation for slave 3fbb2fb0-4f18-498b-a440-9acbf6923a13-S0 in 172983ns
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.705059  1185 replica.cpp:691] Replica received learned notice for position 4 from @0.0.0.0:0
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.705976  1184 master.cpp:3152] Processing ACCEPT call for offers: [ 3fbb2fb0-4f18-498b-a440-9acbf6923a13-O0 ] on slave 3fbb2fb0-4f18-498b-a440-9acbf6923a13-S0 at slave(422)@172.30.2.124:37431 (ip-172-30-2-124.mesosphere.io) for framework 3fbb2fb0-4f18-498b-a440-9acbf6923a13-0000 (default) at scheduler-52603476-875a-49a8-85d4-c98d102cdfab@172.30.2.124:37431
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.706009  1184 master.cpp:2824] Authorizing framework principal 'test-principal' to launch task cd81ece8-93b2-4e8a-a4b0-b566038bf281 as user 'root'
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.706212  1185 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 1.125309ms
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.706269  1185 leveldb.cpp:399] Deleting ~2 keys from leveldb took 32428ns
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.706284  1185 replica.cpp:712] Persisted action at 4
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.706298  1185 replica.cpp:697] Replica learned TRUNCATE action at position 4
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.707129  1184 master.hpp:176] Adding task cd81ece8-93b2-4e8a-a4b0-b566038bf281 with resources cpus(*):1; mem(*):128 on slave 3fbb2fb0-4f18-498b-a440-9acbf6923a13-S0 (ip-172-30-2-124.mesosphere.io)
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.707231  1184 master.cpp:3637] Launching task cd81ece8-93b2-4e8a-a4b0-b566038bf281 of framework 3fbb2fb0-4f18-498b-a440-9acbf6923a13-0000 (default) at scheduler-52603476-875a-49a8-85d4-c98d102cdfab@172.30.2.124:37431 with resources cpus(*):1; mem(*):128 on slave 3fbb2fb0-4f18-498b-a440-9acbf6923a13-S0 at slave(422)@172.30.2.124:37431 (ip-172-30-2-124.mesosphere.io)
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.707516  1182 slave.cpp:1361] Got assigned task cd81ece8-93b2-4e8a-a4b0-b566038bf281 for framework 3fbb2fb0-4f18-498b-a440-9acbf6923a13-0000
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.707669  1182 resources.cpp:572] Parsing resources as JSON failed: cpus:0.1;mem:32
[09:46:46]W:	 [Step 11/11] Trying semicolon-delimited string format instead
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.707772  1183 hierarchical.cpp:890] Recovered cpus(*):1; mem(*):896; disk(*):1024; ports(*):[31000-32000] (total: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000], allocated: cpus(*):1; mem(*):128) on slave 3fbb2fb0-4f18-498b-a440-9acbf6923a13-S0 from framework 3fbb2fb0-4f18-498b-a440-9acbf6923a13-0000
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.707814  1183 hierarchical.cpp:927] Framework 3fbb2fb0-4f18-498b-a440-9acbf6923a13-0000 filtered slave 3fbb2fb0-4f18-498b-a440-9acbf6923a13-S0 for 5secs
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.708055  1182 slave.cpp:1480] Launching task cd81ece8-93b2-4e8a-a4b0-b566038bf281 for framework 3fbb2fb0-4f18-498b-a440-9acbf6923a13-0000
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.708122  1182 resources.cpp:572] Parsing resources as JSON failed: cpus:0.1;mem:32
[09:46:46]W:	 [Step 11/11] Trying semicolon-delimited string format instead
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.708601  1182 paths.cpp:474] Trying to chown '/tmp/ProvisionerDockerRegistryPullerTest_ROOT_INTERNET_CURL_ShellCommand_5BWCfv/slaves/3fbb2fb0-4f18-498b-a440-9acbf6923a13-S0/frameworks/3fbb2fb0-4f18-498b-a440-9acbf6923a13-0000/executors/cd81ece8-93b2-4e8a-a4b0-b566038bf281/runs/7f271a3f-bada-4dfb-a37f-f6c6d7aefd89' to user 'root'
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.713331  1182 slave.cpp:5367] Launching executor cd81ece8-93b2-4e8a-a4b0-b566038bf281 of framework 3fbb2fb0-4f18-498b-a440-9acbf6923a13-0000 with resources cpus(*):0.1; mem(*):32 in work directory '/tmp/ProvisionerDockerRegistryPullerTest_ROOT_INTERNET_CURL_ShellCommand_5BWCfv/slaves/3fbb2fb0-4f18-498b-a440-9acbf6923a13-S0/frameworks/3fbb2fb0-4f18-498b-a440-9acbf6923a13-0000/executors/cd81ece8-93b2-4e8a-a4b0-b566038bf281/runs/7f271a3f-bada-4dfb-a37f-f6c6d7aefd89'
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.713762  1185 containerizer.cpp:666] Starting container '7f271a3f-bada-4dfb-a37f-f6c6d7aefd89' for executor 'cd81ece8-93b2-4e8a-a4b0-b566038bf281' of framework '3fbb2fb0-4f18-498b-a440-9acbf6923a13-0000'
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.713769  1182 slave.cpp:1698] Queuing task 'cd81ece8-93b2-4e8a-a4b0-b566038bf281' for executor 'cd81ece8-93b2-4e8a-a4b0-b566038bf281' of framework 3fbb2fb0-4f18-498b-a440-9acbf6923a13-0000
[09:46:46]W:	 [Step 11/11] I0229 09:46:46.713860  1182 slave.cpp:749] Successfully attached file '/tmp/ProvisionerDockerRegistryPullerTest_ROOT_INTERNET_CURL_ShellCommand_5BWCfv/slaves/3fbb2fb0-4f18-498b-a440-9acbf6923a13-S0/frameworks/3fbb2fb0-4f18-498b-a440-9acbf6923a13-0000/executors/cd81ece8-93b2-4e8a-a4b0-b566038bf281/runs/7f271a3f-bada-4dfb-a37f-f6c6d7aefd89'
[09:46:47]W:	 [Step 11/11] I0229 09:46:47.601546  1180 registry_puller.cpp:210] The manifest for image 'library/alpine' is '{
[09:46:47]W:	 [Step 11/11]    ""schemaVersion"": 1,
[09:46:47]W:	 [Step 11/11]    ""name"": ""library/alpine"",
[09:46:47]W:	 [Step 11/11]    ""tag"": ""latest"",
[09:46:47]W:	 [Step 11/11]    ""architecture"": ""amd64"",
[09:46:47]W:	 [Step 11/11]    ""fsLayers"": [
[09:46:47]W:	 [Step 11/11]       {
[09:46:47]W:	 [Step 11/11]          ""blobSum"": ""sha256:ee54741ab35b188477c19fddc30356317b091177966da94c2e9391de49fc7f43""
[09:46:47]W:	 [Step 11/11]       }
[09:46:47]W:	 [Step 11/11]    ],
[09:46:47]W:	 [Step 11/11]    ""history"": [
[09:46:47]W:	 [Step 11/11]       {
[09:46:47]W:	 [Step 11/11]          ""v1Compatibility"": ""{\""id\"":\""9d710148acd0066166bf3ce04894072b2f3caed24d0295ae2fa136fb7f602605\"",\""created\"":\""2016-02-17T15:51:37.348814441Z\"",\""container\"":\""1c7d9aa5eff83e7f7e563f36c01ba975b90a4a6e17fa6024f4a998f5f0a43b28\"",\""container_config\"":{\""Hostname\"":\""1c7d9aa5eff8\"",\""Domainname\"":\""\"",\""User\"":\""\"",\""AttachStdin\"":false,\""AttachStdout\"":false,\""AttachStderr\"":false,\""Tty\"":false,\""OpenStdin\"":false,\""StdinOnce\"":false,\""Env\"":null,\""Cmd\"":[\""/bin/sh\"",\""-c\"",\""#(nop) ADD file:0f9cfb2e848f093649aca9cc67927e4d04a74e150e0d92f4ad18ee583a287bf2 in /\""],\""Image\"":\""\"",\""Volumes\"":null,\""WorkingDir\"":\""\"",\""Entrypoint\"":null,\""OnBuild\"":null,\""Labels\"":null},\""docker_version\"":\""1.9.1\"",\""config\"":{\""Hostname\"":\""1c7d9aa5eff8\"",\""Domainname\"":\""\"",\""User\"":\""\"",\""AttachStdin\"":false,\""AttachStdout\"":false,\""AttachStderr\"":false,\""Tty\"":false,\""OpenStdin\"":false,\""StdinOnce\"":false,\""Env\"":null,\""Cmd\"":null,\""Image\"":\""\"",\""Volumes\"":null,\""WorkingDir\"":\""\"",\""Entrypoint\"":null,\""OnBuild\"":null,\""Labels\"":null},\""architecture\"":\""amd64\"",\""os\"":\""linux\"",\""Size\"":4793867}""
[09:46:47]W:	 [Step 11/11]       }
[09:46:47]W:	 [Step 11/11]    ],
[09:46:47]W:	 [Step 11/11]    ""signatures"": [
[09:46:47]W:	 [Step 11/11]       {
[09:46:47]W:	 [Step 11/11]          ""header"": {
[09:46:47]W:	 [Step 11/11]             ""jwk"": {
[09:46:47]W:	 [Step 11/11]                ""crv"": ""P-256"",
[09:46:47]W:	 [Step 11/11]                ""kid"": ""OOI5:SI3T:LC7D:O7DX:FY6S:IAYW:WDRN:VQEM:BCFL:OIST:Q3LO:GTQQ"",
[09:46:47]W:	 [Step 11/11]                ""kty"": ""EC"",
[09:46:47]W:	 [Step 11/11]                ""x"": ""J2N5ePGhlblMI2cdsR6NrAG_xbNC_X7s1HRtk5GXvzM"",
[09:46:47]W:	 [Step 11/11]                ""y"": ""Idr-tEBjnNnfq6_71aeXBi3Z9ah_rrE209l4wiaohk0""
[09:46:47]W:	 [Step 11/11]             },
[09:46:47]W:	 [Step 11/11]             ""alg"": ""ES256""
[09:46:47]W:	 [Step 11/11]          },
[09:46:47]W:	 [Step 11/11]          ""signature"": ""gFqNfRROAFCbMmm7sCjaNFjy18vu3IWQUrFQbhCwrpNuNbMc7ImdW636Pz1IrVfGTzalAZftluLsiHcMPU2jBQ"",
[09:46:47]W:	 [Step 11/11]          ""protected"": ""eyJmb3JtYXRMZW5ndGgiOjEzNzUsImZvcm1hdFRhaWwiOiJDbjAiLCJ0aW1lIjoiMjAxNi0wMi0yM1QxOTowMjowMFoifQ""
[09:46:47]W:	 [Step 11/11]       }
[09:46:47]W:	 [Step 11/11]    ]
[09:46:47]W:	 [Step 11/11] }'
[09:46:47]W:	 [Step 11/11] I0229 09:46:47.601771  1180 registry_puller.cpp:317] Fetching blob 'sha256:ee54741ab35b188477c19fddc30356317b091177966da94c2e9391de49fc7f43' for layer '9d710148acd0066166bf3ce04894072b2f3caed24d0295ae2fa136fb7f602605' of image 'library/alpine'
[09:46:47]W:	 [Step 11/11] I0229 09:46:47.635748  1182 hierarchical.cpp:1623] Filtered offer with cpus(*):1; mem(*):896; disk(*):1024; ports(*):[31000-32000] on slave 3fbb2fb0-4f18-498b-a440-9acbf6923a13-S0 for framework 3fbb2fb0-4f18-498b-a440-9acbf6923a13-0000
[09:46:47]W:	 [Step 11/11] I0229 09:46:47.635797  1182 hierarchical.cpp:1437] No resources available to allocate!
[09:46:47]W:	 [Step 11/11] I0229 09:46:47.635829  1182 hierarchical.cpp:1532] No inverse offers to send out!
[09:46:47]W:	 [Step 11/11] I0229 09:46:47.635854  1182 hierarchical.cpp:1130] Performed allocation for 1 slaves in 573296ns
[09:46:48]W:	 [Step 11/11] I0229 09:46:48.299258  1180 provisioner.cpp:285] Provisioning image rootfs '/tmp/ProvisionerDockerRegistryPullerTest_ROOT_INTERNET_CURL_ShellCommand_5BWCfv/provisioner/containers/7f271a3f-bada-4dfb-a37f-f6c6d7aefd89/backends/copy/rootfses/6f8be9d5-12fe-4b66-9ff3-fda1efbb2519' for container 7f271a3f-bada-4dfb-a37f-f6c6d7aefd89
[09:46:48]W:	 [Step 11/11] I0229 09:46:48.299828  1181 copy.cpp:127] Copying layer path '/tmp/mesos/store/docker/layers/9d710148acd0066166bf3ce04894072b2f3caed24d0295ae2fa136fb7f602605/rootfs' to rootfs '/tmp/ProvisionerDockerRegistryPullerTest_ROOT_INTERNET_CURL_ShellCommand_5BWCfv/provisioner/containers/7f271a3f-bada-4dfb-a37f-f6c6d7aefd89/backends/copy/rootfses/6f8be9d5-12fe-4b66-9ff3-fda1efbb2519'
[09:46:48]W:	 [Step 11/11] I0229 09:46:48.410997  1187 linux_launcher.cpp:304] Cloning child process with flags = CLONE_NEWNS
[09:46:48]W:	 [Step 11/11] + /mnt/teamcity/work/4240ba9ddd0997c3/build/src/mesos-containerizer mount --help=false --operation=make-rslave --path=/
[09:46:48]W:	 [Step 11/11] + grep -E /tmp/ProvisionerDockerRegistryPullerTest_ROOT_INTERNET_CURL_ShellCommand_5BWCfv/.+ /proc/self/mountinfo
[09:46:48]W:	 [Step 11/11] + grep -v 7f271a3f-bada-4dfb-a37f-f6c6d7aefd89
[09:46:48]W:	 [Step 11/11] + cut '-d ' -f5
[09:46:48]W:	 [Step 11/11] + xargs --no-run-if-empty umount -l
[09:46:48]W:	 [Step 11/11] + mount -n --rbind /tmp/ProvisionerDockerRegistryPullerTest_ROOT_INTERNET_CURL_ShellCommand_5BWCfv/provisioner/containers/7f271a3f-bada-4dfb-a37f-f6c6d7aefd89/backends/copy/rootfses/6f8be9d5-12fe-4b66-9ff3-fda1efbb2519 /tmp/ProvisionerDockerRegistryPullerTest_ROOT_INTERNET_CURL_ShellCommand_5BWCfv/slaves/3fbb2fb0-4f18-498b-a440-9acbf6923a13-S0/frameworks/3fbb2fb0-4f18-498b-a440-9acbf6923a13-0000/executors/cd81ece8-93b2-4e8a-a4b0-b566038bf281/runs/7f271a3f-bada-4dfb-a37f-f6c6d7aefd89/.rootfs
[09:46:48]W:	 [Step 11/11] WARNING: Logging before InitGoogleLogging() is written to STDERR
[09:46:48]W:	 [Step 11/11] I0229 09:46:48.550132 12320 process.cpp:991] libprocess is initialized on 172.30.2.124:39586 for 8 cpus
[09:46:48]W:	 [Step 11/11] I0229 09:46:48.550712 12320 logging.cpp:193] Logging to STDERR
[09:46:48]W:	 [Step 11/11] I0229 09:46:48.552098 12320 exec.cpp:143] Version: 0.28.0
[09:46:48]W:	 [Step 11/11] I0229 09:46:48.557407 12370 exec.cpp:193] Executor started at: executor(1)@172.30.2.124:39586 with pid 12320
[09:46:48]W:	 [Step 11/11] I0229 09:46:48.559065  1180 slave.cpp:2643] Got registration for executor 'cd81ece8-93b2-4e8a-a4b0-b566038bf281' of framework 3fbb2fb0-4f18-498b-a440-9acbf6923a13-0000 from executor(1)@172.30.2.124:39586
[09:46:48]W:	 [Step 11/11] I0229 09:46:48.560705 12374 exec.cpp:217] Executor registered on slave 3fbb2fb0-4f18-498b-a440-9acbf6923a13-S0
[09:46:48]W:	 [Step 11/11] I0229 09:46:48.560752  1180 slave.cpp:1863] Sending queued task 'cd81ece8-93b2-4e8a-a4b0-b566038bf281' to executor 'cd81ece8-93b2-4e8a-a4b0-b566038bf281' of framework 3fbb2fb0-4f18-498b-a440-9acbf6923a13-0000 at executor(1)@172.30.2.124:39586
[09:46:48]W:	 [Step 11/11] I0229 09:46:48.562134 12374 exec.cpp:229] Executor::registered took 256564ns
[09:46:48]W:	 [Step 11/11] I0229 09:46:48.562368 12374 exec.cpp:304] Executor asked to run task 'cd81ece8-93b2-4e8a-a4b0-b566038bf281'
[09:46:48]W:	 [Step 11/11] I0229 09:46:48.562463 12374 exec.cpp:313] Executor::launchTask took 75896ns
[09:46:48] :	 [Step 11/11] Registered executor on ip-172-30-2-124.mesosphere.io
[09:46:48] :	 [Step 11/11] Starting task cd81ece8-93b2-4e8a-a4b0-b566038bf281
[09:46:48] :	 [Step 11/11] Forked command at 12377
[09:46:48]W:	 [Step 11/11] I0229 09:46:48.566723 12369 exec.cpp:526] Executor sending status update TASK_RUNNING (UUID: 78b0b15b-22c8-479b-b2ee-bb02a7466964) for task cd81ece8-93b2-4e8a-a4b0-b566038bf281 of framework 3fbb2fb0-4f18-498b-a440-9acbf6923a13-0000
[09:46:48] :	 [Step 11/11] sh -c 'ls -al /'
[09:46:48]W:	 [Step 11/11] I0229 09:46:48.567494  1187 slave.cpp:3002] Handling status update TASK_RUNNING (UUID: 78b0b15b-22c8-479b-b2ee-bb02a7466964) for task cd81ece8-93b2-4e8a-a4b0-b566038bf281 of framework 3fbb2fb0-4f18-498b-a440-9acbf6923a13-0000 from executor(1)@172.30.2.124:39586
[09:46:48]W:	 [Step 11/11] Failed to exec: No such file or directory
[09:46:48]W:	 [Step 11/11] I0229 09:46:48.568670  1186 status_update_manager.cpp:320] Received status update TASK_RUNNING (UUID: 78b0b15b-22c8-479b-b2ee-bb02a7466964) for task cd81ece8-93b2-4e8a-a4b0-b566038bf281 of framework 3fbb2fb0-4f18-498b-a440-9acbf6923a13-0000
[09:46:48]W:	 [Step 11/11] I0229 09:46:48.568704  1186 status_update_manager.cpp:497] Creating StatusUpdate stream for task cd81ece8-93b2-4e8a-a4b0-b566038bf281 of framework 3fbb2fb0-4f18-498b-a440-9acbf6923a13-0000
[09:46:48]W:	 [Step 11/11] I0229 09:46:48.569011  1186 status_update_manager.cpp:374] Forwarding update TASK_RUNNING (UUID: 78b0b15b-22c8-479b-b2ee-bb02a7466964) for task cd81ece8-93b2-4e8a-a4b0-b566038bf281 of framework 3fbb2fb0-4f18-498b-a440-9acbf6923a13-0000 to the slave
[09:46:48]W:	 [Step 11/11] I0229 09:46:48.569222  1183 slave.cpp:3400] Forwarding the update TASK_RUNNING (UUID: 78b0b15b-22c8-479b-b2ee-bb02a7466964) for task cd81ece8-93b2-4e8a-a4b0-b566038bf281 of framework 3fbb2fb0-4f18-498b-a440-9acbf6923a13-0000 to master@172.30.2.124:37431
[09:46:48]W:	 [Step 11/11] I0229 09:46:48.569411  1183 slave.cpp:3294] Status update manager successfully handled status update TASK_RUNNING (UUID: 78b0b15b-22c8-479b-b2ee-bb02a7466964) for task cd81ece8-93b2-4e8a-a4b0-b566038bf281 of framework 3fbb2fb0-4f18-498b-a440-9acbf6923a13-0000
[09:46:48]W:	 [Step 11/11] I0229 09:46:48.569447  1183 slave.cpp:3310] Sending acknowledgement for status update TASK_RUNNING (UUID: 78b0b15b-22c8-479b-b2ee-bb02a7466964) for task cd81ece8-93b2-4e8a-a4b0-b566038bf281 of framework 3fbb2fb0-4f18-498b-a440-9acbf6923a13-0000 to executor(1)@172.30.2.124:39586
[09:46:48]W:	 [Step 11/11] I0229 09:46:48.569512  1187 master.cpp:4808] Status update TASK_RUNNING (UUID: 78b0b15b-22c8-479b-b2ee-bb02a7466964) for task cd81ece8-93b2-4e8a-a4b0-b566038bf281 of framework 3fbb2fb0-4f18-498b-a440-9acbf6923a13-0000 from slave 3fbb2fb0-4f18-498b-a440-9acbf6923a13-S0 at slave(422)@172.30.2.124:37431 (ip-172-30-2-124.mesosphere.io)
[09:46:48]W:	 [Step 11/11] I0229 09:46:48.569546  1187 master.cpp:4856] Forwarding status update TASK_RUNNING (UUID: 78b0b15b-22c8-479b-b2ee-bb02a7466964) for task cd81ece8-93b2-4e8a-a4b0-b566038bf281 of framework 3fbb2fb0-4f18-498b-a440-9acbf6923a13-0000
[09:46:48]W:	 [Step 11/11] I0229 09:46:48.569679  1187 master.cpp:6464] Updating the state of task cd81ece8-93b2-4e8a-a4b0-b566038bf281 of framework 3fbb2fb0-4f18-498b-a440-9acbf6923a13-0000 (latest state: TASK_RUNNING, status update state: TASK_RUNNING)
[09:46:48]W:	 [Step 11/11] I0229 09:46:48.569871  1184 sched.cpp:981] Scheduler::statusUpdate took 110230ns
[09:46:48]W:	 [Step 11/11] I0229 09:46:48.569912 12374 exec.cpp:350] Executor received status update acknowledgement 78b0b15b-22c8-479b-b2ee-bb02a7466964 for task cd81ece8-93b2-4e8a-a4b0-b566038bf281 of framework 3fbb2fb0-4f18-498b-a440-9acbf6923a13-0000
[09:46:48]W:	 [Step 11/11] I0229 09:46:48.570202  1184 master.cpp:3966] Processing ACKNOWLEDGE call 78b0b15b-22c8-479b-b2ee-bb02a7466964 for task cd81ece8-93b2-4e8a-a4b0-b566038bf281 of framework 3fbb2fb0-4f18-498b-a440-9acbf6923a13-0000 (default) at scheduler-52603476-875a-49a8-85d4-c98d102cdfab@172.30.2.124:37431 on slave 3fbb2fb0-4f18-498b-a440-9acbf6923a13-S0
[09:46:48]W:	 [Step 11/11] I0229 09:46:48.570435  1186 status_update_manager.cpp:392] Received status update acknowledgement (UUID: 78b0b15b-22c8-479b-b2ee-bb02a7466964) for task cd81ece8-93b2-4e8a-a4b0-b566038bf281 of framework 3fbb2fb0-4f18-498b-a440-9acbf6923a13-0000
[09:46:48]W:	 [Step 11/11] I0229 09:46:48.570641  1183 slave.cpp:2412] Status update manager successfully handled status update acknowledgement (UUID: 78b0b15b-22c8-479b-b2ee-bb02a7466964) for task cd81ece8-93b2-4e8a-a4b0-b566038bf281 of framework 3fbb2fb0-4f18-498b-a440-9acbf6923a13-0000
[09:46:48]W:	 [Step 11/11] I0229 09:46:48.636754  1182 hierarchical.cpp:1623] Filtered offer with cpus(*):1; mem(*):896; disk(*):1024; ports(*):[31000-32000] on slave 3fbb2fb0-4f18-498b-a440-9acbf6923a13-S0 for framework 3fbb2fb0-4f18-498b-a440-9acbf6923a13-0000
[09:46:48]W:	 [Step 11/11] I0229 09:46:48.636795  1182 hierarchical.cpp:1437] No resources available to allocate!
[09:46:48]W:	 [Step 11/11] I0229 09:46:48.636827  1182 hierarchical.cpp:1532] No inverse offers to send out!
[09:46:48]W:	 [Step 11/11] I0229 09:46:48.636849  1182 hierarchical.cpp:1130] Performed allocation for 1 slaves in 503523ns
[09:46:48] :	 [Step 11/11] Command terminated with signal Aborted (pid: 12377)
[09:46:48]W:	 [Step 11/11] I0229 09:46:48.665805 12369 exec.cpp:526] Executor sending status update TASK_FAILED (UUID: 265863c0-80d5-48a4-ac87-6f0de02ddbcb) for task cd81ece8-93b2-4e8a-a4b0-b566038bf281 of framework 3fbb2fb0-4f18-498b-a440-9acbf6923a13-0000
[09:46:48]W:	 [Step 11/11] I0229 09:46:48.666326  1184 slave.cpp:3002] Handling status update TASK_FAILED (UUID: 265863c0-80d5-48a4-ac87-6f0de02ddbcb) for task cd81ece8-93b2-4e8a-a4b0-b566038bf281 of framework 3fbb2fb0-4f18-498b-a440-9acbf6923a13-0000 from executor(1)@172.30.2.124:39586
[09:46:48]W:	 [Step 11/11] I0229 09:46:48.667079  1183 slave.cpp:5677] Terminating task cd81ece8-93b2-4e8a-a4b0-b566038bf281
[09:46:48]W:	 [Step 11/11] I0229 09:46:48.667944  1180 status_update_manager.cpp:320] Received status update TASK_FAILED (UUID: 265863c0-80d5-48a4-ac87-6f0de02ddbcb) for task cd81ece8-93b2-4e8a-a4b0-b566038bf281 of framework 3fbb2fb0-4f18-498b-a440-9acbf6923a13-0000
[09:46:48]W:	 [Step 11/11] I0229 09:46:48.668077  1180 status_update_manager.cpp:374] Forwarding update TASK_FAILED (UUID: 265863c0-80d5-48a4-ac87-6f0de02ddbcb) for task cd81ece8-93b2-4e8a-a4b0-b566038bf281 of framework 3fbb2fb0-4f18-498b-a440-9acbf6923a13-0000 to the slave
[09:46:48]W:	 [Step 11/11] I0229 09:46:48.668303  1182 slave.cpp:3400] Forwarding the update TASK_FAILED (UUID: 265863c0-80d5-48a4-ac87-6f0de02ddbcb) for task cd81ece8-93b2-4e8a-a4b0-b566038bf281 of framework 3fbb2fb0-4f18-498b-a440-9acbf6923a13-0000 to master@172.30.2.124:37431
[09:46:48]W:	 [Step 11/11] I0229 09:46:48.668453  1182 slave.cpp:3294] Status update manager successfully handled status update TASK_FAILED (UUID: 265863c0-80d5-48a4-ac87-6f0de02ddbcb) for task cd81ece8-93b2-4e8a-a4b0-b566038bf281 of framework 3fbb2fb0-4f18-498b-a440-9acbf6923a13-0000
[09:46:48]W:	 [Step 11/11] I0229 09:46:48.668499  1182 slave.cpp:3310] Sending acknowledgement for status update TASK_FAILED (UUID: 265863c0-80d5-48a4-ac87-6f0de02ddbcb) for task cd81ece8-93b2-4e8a-a4b0-b566038bf281 of framework 3fbb2fb0-4f18-498b-a440-9acbf6923a13-0000 to executor(1)@172.30.2.124:39586
[09:46:48]W:	 [Step 11/11] I0229 09:46:48.668642  1181 master.cpp:4808] Status update TASK_FAILED (UUID: 265863c0-80d5-48a4-ac87-6f0de02ddbcb) for task cd81ece8-93b2-4e8a-a4b0-b566038bf281 of framework 3fbb2fb0-4f18-498b-a440-9acbf6923a13-0000 from slave 3fbb2fb0-4f18-498b-a440-9acbf6923a13-S0 at slave(422)@172.30.2.124:37431 (ip-172-30-2-124.mesosphere.io)
[09:46:48]W:	 [Step 11/11] I0229 09:46:48.668689  1181 master.cpp:4856] Forwarding status update TASK_FAILED (UUID: 265863c0-80d5-48a4-ac87-6f0de02ddbcb) for task cd81ece8-93b2-4e8a-a4b0-b566038bf281 of framework 3fbb2fb0-4f18-498b-a440-9acbf6923a13-0000
[09:46:48]W:	 [Step 11/11] I0229 09:46:48.668826  1181 master.cpp:6464] Updating the state of task cd81ece8-93b2-4e8a-a4b0-b566038bf281 of framework 3fbb2fb0-4f18-498b-a440-9acbf6923a13-0000 (latest state: TASK_FAILED, status update state: TASK_FAILED)
[09:46:48]W:	 [Step 11/11] I0229 09:46:48.668920 12373 exec.cpp:350] Executor received status update acknowledgement 265863c0-80d5-48a4-ac87-6f0de02ddbcb for task cd81ece8-93b2-4e8a-a4b0-b566038bf281 of framework 3fbb2fb0-4f18-498b-a440-9acbf6923a13-0000
[09:46:48]W:	 [Step 11/11] I0229 09:46:48.669082  1183 sched.cpp:981] Scheduler::statusUpdate took 143562ns
[09:46:48]W:	 [Step 11/11] I0229 09:46:48.669242  1186 hierarchical.cpp:890] Recovered cpus(*):1; mem(*):128 (total: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000], allocated: ) on slave 3fbb2fb0-4f18-498b-a440-9acbf6923a13-S0 from framework 3fbb2fb0-4f18-498b-a440-9acbf6923a13-0000
[09:46:48] :	 [Step 11/11] ../../src/tests/containerizer/provisioner_docker_tests.cpp:379: Failure
[09:46:48]W:	 [Step 11/11] I0229 09:46:48.669381  1186 master.cpp:3966] Processing ACKNOWLEDGE call 265863c0-80d5-48a4-ac87-6f0de02ddbcb for task cd81ece8-93b2-4e8a-a4b0-b566038bf281 of framework 3fbb2fb0-4f18-498b-a440-9acbf6923a13-0000 (default) at scheduler-52603476-875a-49a8-85d4-c98d102cdfab@172.30.2.124:37431 on slave 3fbb2fb0-4f18-498b-a440-9acbf6923a13-S0
[09:46:48] :	 [Step 11/11] Value of: statusFinished->state()
[09:46:48]W:	 [Step 11/11] I0229 09:46:48.669421  1166 sched.cpp:1903] Asked to stop the driver
[09:46:48] :	 [Step 11/11]   Actual: TASK_FAILED
[09:46:48] :	 [Step 11/11] Expected: TASK_FINISHED
[09:46:48]W:	 [Step 11/11] I0229 09:46:48.669423  1186 master.cpp:6530] Removing task cd81ece8-93b2-4e8a-a4b0-b566038bf281 with resources cpus(*):1; mem(*):128 of framework 3fbb2fb0-4f18-498b-a440-9acbf6923a13-0000 on slave 3fbb2fb0-4f18-498b-a440-9acbf6923a13-S0 at slave(422)@172.30.2.124:37431 (ip-172-30-2-124.mesosphere.io)
[09:46:48]W:	 [Step 11/11] I0229 09:46:48.669519  1181 sched.cpp:1143] Stopping framework '3fbb2fb0-4f18-498b-a440-9acbf6923a13-0000'
[09:46:48]W:	 [Step 11/11] I0229 09:46:48.669746  1186 master.cpp:5940] Processing TEARDOWN call for framework 3fbb2fb0-4f18-498b-a440-9acbf6923a13-0000 (default) at scheduler-52603476-875a-49a8-85d4-c98d102cdfab@172.30.2.124:37431
[09:46:48]W:	 [Step 11/11] I0229 09:46:48.669778  1186 master.cpp:5952] Removing framework 3fbb2fb0-4f18-498b-a440-9acbf6923a13-0000 (default) at scheduler-52603476-875a-49a8-85d4-c98d102cdfab@172.30.2.124:37431
[09:46:48]W:	 [Step 11/11] I0229 09:46:48.669850  1184 status_update_manager.cpp:392] Received status update acknowledgement (UUID: 265863c0-80d5-48a4-ac87-6f0de02ddbcb) for task cd81ece8-93b2-4e8a-a4b0-b566038bf281 of framework 3fbb2fb0-4f18-498b-a440-9acbf6923a13-0000
[09:46:48]W:	 [Step 11/11] I0229 09:46:48.669946  1187 hierarchical.cpp:375] Deactivated framework 3fbb2fb0-4f18-498b-a440-9acbf6923a13-0000
[09:46:48]W:	 [Step 11/11] I0229 09:46:48.670032  1184 status_update_manager.cpp:528] Cleaning up status update stream for task cd81ece8-93b2-4e8a-a4b0-b566038bf281 of framework 3fbb2fb0-4f18-498b-a440-9acbf6923a13-0000
[09:46:48]W:	 [Step 11/11] I0229 09:46:48.670030  1180 slave.cpp:2079] Asked to shut down framework 3fbb2fb0-4f18-498b-a440-9acbf6923a13-0000 by master@172.30.2.124:37431
[09:46:48]W:	 [Step 11/11] I0229 09:46:48.670080  1180 slave.cpp:2104] Shutting down framework 3fbb2fb0-4f18-498b-a440-9acbf6923a13-0000
[09:46:48]W:	 [Step 11/11] I0229 09:46:48.670140  1180 slave.cpp:4198] Shutting down executor 'cd81ece8-93b2-4e8a-a4b0-b566038bf281' of framework 3fbb2fb0-4f18-498b-a440-9acbf6923a13-0000 at executor(1)@172.30.2.124:39586
[09:46:48]W:	 [Step 11/11] I0229 09:46:48.670253  1186 master.cpp:1026] Master terminating
[09:46:48]W:	 [Step 11/11] I0229 09:46:48.670295  1187 hierarchical.cpp:326] Removed framework 3fbb2fb0-4f18-498b-a440-9acbf6923a13-0000
[09:46:48]W:	 [Step 11/11] I0229 09:46:48.670384  1180 slave.cpp:2412] Status update manager successfully handled status update acknowledgement (UUID: 265863c0-80d5-48a4-ac87-6f0de02ddbcb) for task cd81ece8-93b2-4e8a-a4b0-b566038bf281 of framework 3fbb2fb0-4f18-498b-a440-9acbf6923a13-0000
[09:46:48]W:	 [Step 11/11] I0229 09:46:48.670434  1180 slave.cpp:5718] Completing task cd81ece8-93b2-4e8a-a4b0-b566038bf281
[09:46:48]W:	 [Step 11/11] I0229 09:46:48.670588  1187 hierarchical.cpp:505] Removed slave 3fbb2fb0-4f18-498b-a440-9acbf6923a13-S0
[09:46:48]W:	 [Step 11/11] I0229 09:46:48.670605 12375 exec.cpp:390] Executor asked to shutdown
[09:46:48]W:	 [Step 11/11] I0229 09:46:48.670717 12375 exec.cpp:405] Executor::shutdown took 12737ns
[09:46:48]W:	 [Step 11/11] I0229 09:46:48.670728 12369 exec.cpp:87] Scheduling shutdown of the executor in 5secs
[09:46:48]W:	 [Step 11/11] I0229 09:46:48.670922  1183 slave.cpp:3528] master@172.30.2.124:37431 exited
[09:46:48]W:	 [Step 11/11] W0229 09:46:48.670940  1183 slave.cpp:3531] Master disconnected! Waiting for a new master to be elected
[09:46:48]W:	 [Step 11/11] I0229 09:46:48.675063  1186 containerizer.cpp:1378] Destroying container '7f271a3f-bada-4dfb-a37f-f6c6d7aefd89'
[09:46:48]W:	 [Step 11/11] I0229 09:46:48.677278  1182 cgroups.cpp:2427] Freezing cgroup /sys/fs/cgroup/freezer/mesos/7f271a3f-bada-4dfb-a37f-f6c6d7aefd89
[09:46:48]W:	 [Step 11/11] I0229 09:46:48.679386  1184 cgroups.cpp:1409] Successfully froze cgroup /sys/fs/cgroup/freezer/mesos/7f271a3f-bada-4dfb-a37f-f6c6d7aefd89 after 2.066176ms
[09:46:48]W:	 [Step 11/11] I0229 09:46:48.681586  1186 cgroups.cpp:2445] Thawing cgroup /sys/fs/cgroup/freezer/mesos/7f271a3f-bada-4dfb-a37f-f6c6d7aefd89
[09:46:48]W:	 [Step 11/11] I0229 09:46:48.683552  1186 cgroups.cpp:1438] Successfullly thawed cgroup /sys/fs/cgroup/freezer/mesos/7f271a3f-bada-4dfb-a37f-f6c6d7aefd89 after 1.926144ms
[09:46:48]W:	 [Step 11/11] I0229 09:46:48.696513  1186 slave.cpp:3528] executor(1)@172.30.2.124:39586 exited
[09:46:48]W:	 [Step 11/11] I0229 09:46:48.708107  1181 containerizer.cpp:1594] Executor for container '7f271a3f-bada-4dfb-a37f-f6c6d7aefd89' has exited
[09:46:48]W:	 [Step 11/11] I0229 09:46:48.710535  1186 linux.cpp:765] Ignoring unmounting sandbox/work directory for container 7f271a3f-bada-4dfb-a37f-f6c6d7aefd89
[09:46:48]W:	 [Step 11/11] I0229 09:46:48.710969  1187 provisioner.cpp:330] Destroying container rootfs at '/tmp/ProvisionerDockerRegistryPullerTest_ROOT_INTERNET_CURL_ShellCommand_5BWCfv/provisioner/containers/7f271a3f-bada-4dfb-a37f-f6c6d7aefd89/backends/copy/rootfses/6f8be9d5-12fe-4b66-9ff3-fda1efbb2519' for container 7f271a3f-bada-4dfb-a37f-f6c6d7aefd89
[09:46:48]W:	 [Step 11/11] I0229 09:46:48.809336  1183 slave.cpp:3886] Executor 'cd81ece8-93b2-4e8a-a4b0-b566038bf281' of framework 3fbb2fb0-4f18-498b-a440-9acbf6923a13-0000 terminated with signal Killed
[09:46:48]W:	 [Step 11/11] I0229 09:46:48.809378  1183 slave.cpp:3990] Cleaning up executor 'cd81ece8-93b2-4e8a-a4b0-b566038bf281' of framework 3fbb2fb0-4f18-498b-a440-9acbf6923a13-0000 at executor(1)@172.30.2.124:39586
[09:46:48]W:	 [Step 11/11] I0229 09:46:48.809614  1187 gc.cpp:54] Scheduling '/tmp/ProvisionerDockerRegistryPullerTest_ROOT_INTERNET_CURL_ShellCommand_5BWCfv/slaves/3fbb2fb0-4f18-498b-a440-9acbf6923a13-S0/frameworks/3fbb2fb0-4f18-498b-a440-9acbf6923a13-0000/executors/cd81ece8-93b2-4e8a-a4b0-b566038bf281/runs/7f271a3f-bada-4dfb-a37f-f6c6d7aefd89' for gc 6.9999906309837days in the future
[09:46:48]W:	 [Step 11/11] I0229 09:46:48.809703  1183 slave.cpp:4078] Cleaning up framework 3fbb2fb0-4f18-498b-a440-9acbf6923a13-0000
[09:46:48]W:	 [Step 11/11] I0229 09:46:48.809739  1187 gc.cpp:54] Scheduling '/tmp/ProvisionerDockerRegistryPullerTest_ROOT_INTERNET_CURL_ShellCommand_5BWCfv/slaves/3fbb2fb0-4f18-498b-a440-9acbf6923a13-S0/frameworks/3fbb2fb0-4f18-498b-a440-9acbf6923a13-0000/executors/cd81ece8-93b2-4e8a-a4b0-b566038bf281' for gc 6.99999062896889days in the future
[09:46:48]W:	 [Step 11/11] I0229 09:46:48.809801  1182 status_update_manager.cpp:282] Closing status update streams for framework 3fbb2fb0-4f18-498b-a440-9acbf6923a13-0000
[09:46:48]W:	 [Step 11/11] I0229 09:46:48.809854  1187 gc.cpp:54] Scheduling '/tmp/ProvisionerDockerRegistryPullerTest_ROOT_INTERNET_CURL_ShellCommand_5BWCfv/slaves/3fbb2fb0-4f18-498b-a440-9acbf6923a13-S0/frameworks/3fbb2fb0-4f18-498b-a440-9acbf6923a13-0000' for gc 6.99999062718519days in the future
[09:46:48]W:	 [Step 11/11] I0229 09:46:48.810493  1187 slave.cpp:668] Slave terminating
[09:46:48]W:	 [Step 11/11] Using temporary directory '/tmp/ContainerizerTest_ROOT_CGROUPS_BalloonFramework_e9Aoqv'
[09:46:48] :	 [Step 11/11] [  FAILED  ] ProvisionerDockerRegistryPullerTest.ROOT_INTERNET_CURL_ShellCommand (2193 ms)
{noformat}
",Bug,Major,bernd-mesos,2016-03-17T23:12:07.000+0000,5,Resolved,Complete,ProvisionerDockerPullerTest.ROOT_INTERNET_CURL_ShellCommand fails.,2016-03-17T23:12:07.000+0000,MESOS-4810,3.0,mesos,Mesosphere Sprint 30
yongtang,2016-02-29T08:38:17.000+0000,bbannier,"libprocess's {{IOTest.BufferedRead}} writes to the current directory. This is bad for a number of reasons, e.g.,

* should the test fail data might be leaked to random locations,
* the test cannot be executed from a write-only directory, or
* executing the same test in parallel would race on the existence of the created file, and show bogus behavior.

The test should probably be executed from a temporary directory, e.g., via stout's {{TemporaryDirectoryTest}} fixture.",Bug,Minor,bbannier,2016-03-15T20:21:09.000+0000,5,Resolved,Complete,IOTest.BufferedRead writes to the current directory,2016-03-15T20:21:48.000+0000,MESOS-4807,1.0,mesos,Mesosphere Sprint 31
chenzhiwei,2016-02-29T08:16:51.000+0000,qianzhang,"See https://github.com/nodejs/http-parser/releases/tag/v2.6.1.
The motivation is that nodejs/http-parser 2.6.1 has officially supported IBM Power (ppc64le), so this is needed by [MESOS-4312|https://issues.apache.org/jira/browse/MESOS-4312].",Improvement,Major,qianzhang,2016-03-28T18:50:01.000+0000,5,Resolved,Complete,Update ry-http-parser-1c3624a to nodejs/http-parser 2.6.1,2016-03-30T18:10:42.000+0000,MESOS-4805,3.0,mesos,Mesosphere Sprint 31
chenzhiwei,2016-02-29T08:07:11.000+0000,qianzhang,"The motivation is that libev 4.22 has officially supported IBM Power (ppc64le), so this is needed by [MESOS-4312|https://issues.apache.org/jira/browse/MESOS-4312].",Improvement,Major,qianzhang,2016-04-01T02:35:29.000+0000,5,Resolved,Complete,Update vendored libev to 4.22,2016-05-04T15:02:11.000+0000,MESOS-4803,3.0,mesos,Mesosphere Sprint 31
chenzhiwei,2016-02-29T07:54:41.000+0000,qianzhang,"See: https://github.com/google/leveldb/releases/tag/v1.18 for improvements / bug fixes.
The motivation is that leveldb 1.18 has officially supported IBM Power (ppc64le), so this is needed by [MESOS-4312|https://issues.apache.org/jira/browse/MESOS-4312].

Update: Since someone updated leveldb to 1.4, so I only update the patch file to support PowerPC LE. Because I don't think upgrade 3rdparty library frequently is a good thing.",Improvement,Major,qianzhang,2016-03-31T18:19:02.000+0000,5,Resolved,Complete,Update leveldb patch file to suport PowerPC LE,2016-03-31T18:19:02.000+0000,MESOS-4802,3.0,mesos,Mesosphere Sprint 31
anandmazumdar,2016-02-29T01:47:48.000+0000,anandmazumdar,"We need to migrate the existing tests in {{src/tests/scheduler_tests.cpp}} to use the new callback interface introduced in {{MESOS-3339}}. The changes to {{src/tests/master_maintenance_tests.cpp}} would be done when MESOS-4831 is resolved.

For an example see {{SchedulerTest.SchedulerFailover}} which already uses this new interface.",Task,Major,anandmazumdar,2016-03-04T00:11:28.000+0000,5,Resolved,Complete,Make existing scheduler library tests use the callback interface.,2016-03-04T00:11:28.000+0000,MESOS-4798,5.0,mesos,Mesosphere Sprint 30
kaysoky,2016-02-26T23:34:40.000+0000,kaysoky,"Support for persistent volumes was added to the docker containerizer in [MESOS-3413].  However, this does not work on CentOS 6.

On CentOS 6, the same {{docker run -v ...}} operation does not perform a recursive bind, whereas on every other OS supported by Mesos, docker does a recursive bind.

Docker has already [dropped support for CentOS 6|https://github.com/docker/docker/issues/14365], so we should add precautionary documentation in case anyone tries to use the docker containerizer on CentOS 6.",Documentation,Major,kaysoky,,10020,Accepted,In Progress,Add documentation around using the docker containerizer on CentOS 6.,2016-03-18T21:26:34.000+0000,MESOS-4794,1.0,mesos,
neilc,2016-02-26T21:31:40.000+0000,neilc,"src/master/constants.hpp contains:

{code}
// TODO(bmahler): It appears there may be a bug with gcc-4.1.2 in which the
// duration constants were not being initialized when having static linkage.
// This issue did not manifest in newer gcc's. Specifically, 4.2.1 was ok.
// So we've moved these to have external linkage but perhaps in the future
// we can revert this.
{code}

From commit 232a23b2a2e11f4e905b834aa2a11afe5bf6438a. We should investigate whether this is still necessary on supported compilers; it likely is not.",Improvement,Trivial,neilc,2016-03-14T18:19:00.000+0000,5,Resolved,Complete,Revert external linkage of symbols in master/constants.hpp,2016-03-14T18:19:00.000+0000,MESOS-4790,1.0,mesos,Mesosphere Sprint 30
klueska,2016-02-26T18:50:01.000+0000,neilc,"My understanding is that the recommended path for the v1 scheduler API is {{/api/v1/scheduler}}, but the HTTP endpoint [docs|http://mesos.apache.org/documentation/latest/endpoints/] for this endpoint list the path as {{/master/api/v1/scheduler}}; the filename of the doc page is also in the {{master}} subdirectory.

Similarly, we document the master state endpoint as {{/master/state}}, whereas the preferred name is now just {{/state}}, and so on for most of the other endpoints. Unlike we the V1 API, we might want to consider backward compatibility and document both forms -- not sure. But certainly it seems like we should encourage people to use the shorter paths, not the longer ones.",Documentation,Minor,neilc,2016-03-11T03:18:26.000+0000,5,Resolved,Complete,HTTP endpoint docs should use shorter paths,2016-03-11T03:53:32.000+0000,MESOS-4787,2.0,mesos,Mesosphere Sprint 30
arojas,2016-02-26T17:41:50.000+0000,greggomann,"The authorization documentation would benefit from a reorganization of the ACL subject/object descriptions. Instead of simple lists of the available subjects and objects, it would be nice to see a table showing which subject and object is used with each action.",Documentation,Major,greggomann,,10006,Reviewable,New,Reorganize ACL subject/object descriptions,2016-04-29T08:04:39.000+0000,MESOS-4785,1.0,mesos,Mesosphere Sprint 33
bbannier,2016-02-26T09:17:05.000+0000,bbannier,"The test attempts to observe a change in the {{slave/container_launch_errors}} metric, but does not wait for the triggering action to take place. Currently the test passes since hitting the endpoint blocks for some rate limit-related time which provides under many circumstances enough wait time for the action to take place. ",Bug,Major,bbannier,2016-02-29T09:02:32.000+0000,5,Resolved,Complete,SlaveTest.MetricsSlaveLaunchErrors test relies on implicit blocking behavior hitting the global metrics endpoint,2016-02-29T09:53:05.000+0000,MESOS-4784,1.0,mesos,Mesosphere Sprint 29
bbannier,2016-02-26T09:13:15.000+0000,bbannier,"Once we can optionally disable rate limiting in the global metrics endpoint with MESOS-4776 we should disable the rate limiting during the execution of mesos-tests.

* rate limiting makes it cumbersome to repeatedly hit the endpoint since one would not want to interfere with the rate limiting
* rate limiting might incur additional wait time which might slown down tests",Improvement,Major,bbannier,2016-03-03T05:33:36.000+0000,5,Resolved,Complete,Disable rate limiting of the global metrics endpoint for mesos-tests execution,2016-03-03T05:33:36.000+0000,MESOS-4783,3.0,mesos,
gilbert,2016-02-25T23:03:45.000+0000,gilbert,"Currently, command task inherits the env variables of the command executor. This is less ideal because the command executor environment variables include some Mesos internal env variables like MESOS_XXX and LIBPROCESS_XXX. Also, this behavior does not match what Docker containerizer does. We should construct the env variables from scratch for the command task, rather than relying on inheriting the env variables from the command executor.",Bug,Major,gilbert,,10006,Reviewable,New,Executor env variables should not be leaked to the command task.,2016-04-27T16:13:14.000+0000,MESOS-4781,3.0,mesos,Mesosphere Sprint 30
bbannier,2016-02-25T16:27:12.000+0000,bbannier,"Currently the {{/metrics/snapshot}} endpoint in libprocess has a [hard-coded|https://github.com/apache/mesos/blob/0.27.1/3rdparty/libprocess/include/process/metrics/metrics.hpp#L52] rate limit of 2 requests per second:

{code}
  MetricsProcess()
    : ProcessBase(""metrics""),
      limiter(2, Seconds(1)) {}
{code}

This should be configurable via a libprocess environment variable so that users can control this when initializing libprocess.",Improvement,Major,bbannier,2016-03-03T05:34:07.000+0000,5,Resolved,Complete,Libprocess metrics/snapshot endpoint rate limiting should be configurable.,2016-03-03T05:34:07.000+0000,MESOS-4776,2.0,mesos,
nfnt,2016-02-25T07:00:29.000+0000,adam-mesos,"We need a way to assign fine-grained ownership to tasks/executors so that multi-user frameworks can tell Mesos to associate the task with a user identity (rather than just the framework principal+role). Then, when an HTTP user requests to view the task's sandbox contents, or kill the task, or list all tasks, the authorizer can determine whether to allow/deny/filter the request based on finer-grained, user-level ownership.
Some systems may want TaskInfo.owner to represent a group rather than an individual user. That's fine as long as the framework sets the field to the group ID in such a way that a group-aware authorizer can interpret it.",Improvement,Major,adam-mesos,,3,In Progress,In Progress,TaskInfo/ExecutorInfo should include fine-grained ownership/namespacing,2016-04-11T07:16:20.000+0000,MESOS-4772,2.0,mesos,Mesosphere Sprint 30
avinash@mesosphere.io,2016-02-25T00:23:05.000+0000,jieyu,"We need to document this isolator in mesos-containerizer.md (e.g., how to configure it, what's the pre-requisite, etc.)",Task,Major,jieyu,,10020,Accepted,In Progress,Document the network/cni isolator.,2016-04-15T14:01:13.000+0000,MESOS-4771,3.0,mesos,
kaysoky,2016-02-24T22:57:24.000+0000,kaysoky,"[MESOS-4169] significantly sped up this test, but also surfaced some more flakiness.  This can be fixed in the same way as [MESOS-4059].

Verbose logs from ASF Centos7 build:
{code}
[ RUN      ] MasterMaintenanceTest.InverseOffers
I0224 22:35:53.714018  1948 leveldb.cpp:174] Opened db in 2.034387ms
I0224 22:35:53.714663  1948 leveldb.cpp:181] Compacted db in 608839ns
I0224 22:35:53.714709  1948 leveldb.cpp:196] Created db iterator in 19043ns
I0224 22:35:53.714844  1948 leveldb.cpp:202] Seeked to beginning of db in 2330ns
I0224 22:35:53.714956  1948 leveldb.cpp:271] Iterated through 0 keys in the db in 518ns
I0224 22:35:53.715092  1948 replica.cpp:779] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned
I0224 22:35:53.715646  1968 recover.cpp:447] Starting replica recovery
I0224 22:35:53.715915  1981 recover.cpp:473] Replica is in EMPTY status
I0224 22:35:53.717067  1972 replica.cpp:673] Replica in EMPTY status received a broadcasted recover request from (4533)@172.17.0.1:36678
I0224 22:35:53.717445  1981 recover.cpp:193] Received a recover response from a replica in EMPTY status
I0224 22:35:53.717888  1978 recover.cpp:564] Updating replica status to STARTING
I0224 22:35:53.718585  1979 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 525061ns
I0224 22:35:53.718618  1979 replica.cpp:320] Persisted replica status to STARTING
I0224 22:35:53.718827  1982 recover.cpp:473] Replica is in STARTING status
I0224 22:35:53.719728  1969 replica.cpp:673] Replica in STARTING status received a broadcasted recover request from (4534)@172.17.0.1:36678
I0224 22:35:53.719974  1971 recover.cpp:193] Received a recover response from a replica in STARTING status
I0224 22:35:53.720369  1970 recover.cpp:564] Updating replica status to VOTING
I0224 22:35:53.720789  1982 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 322308ns
I0224 22:35:53.720823  1982 replica.cpp:320] Persisted replica status to VOTING
I0224 22:35:53.720968  1982 recover.cpp:578] Successfully joined the Paxos group
I0224 22:35:53.721101  1982 recover.cpp:462] Recover process terminated
I0224 22:35:53.721698  1982 master.cpp:376] Master aab18b61-7811-4c43-a672-d1a63818c880 (4db5fa128d2d) started on 172.17.0.1:36678
I0224 22:35:53.721719  1982 master.cpp:378] Flags at startup: --acls="""" --allocation_interval=""1secs"" --allocator=""HierarchicalDRF"" --authenticate=""false"" --authenticate_http=""true"" --authenticate_slaves=""true"" --authenticators=""crammd5"" --authorizers=""local"" --credentials=""/tmp/MjbcWP/credentials"" --framework_sorter=""drf"" --help=""false"" --hostname_lookup=""true"" --http_authenticators=""basic"" --initialize_driver_logging=""true"" --log_auto_initialize=""true"" --logbufsecs=""0"" --logging_level=""INFO"" --max_completed_frameworks=""50"" --max_completed_tasks_per_framework=""1000"" --max_slave_ping_timeouts=""5"" --quiet=""false"" --recovery_slave_removal_limit=""100%"" --registry=""replicated_log"" --registry_fetch_timeout=""1mins"" --registry_store_timeout=""100secs"" --registry_strict=""true"" --root_submissions=""true"" --slave_ping_timeout=""15secs"" --slave_reregister_timeout=""10mins"" --user_sorter=""drf"" --version=""false"" --webui_dir=""/mesos/mesos-0.28.0/_inst/share/mesos/webui"" --work_dir=""/tmp/MjbcWP/master"" --zk_session_timeout=""10secs""
I0224 22:35:53.722039  1982 master.cpp:425] Master allowing unauthenticated frameworks to register
I0224 22:35:53.722053  1982 master.cpp:428] Master only allowing authenticated slaves to register
I0224 22:35:53.722061  1982 credentials.hpp:35] Loading credentials for authentication from '/tmp/MjbcWP/credentials'
I0224 22:35:53.722394  1982 master.cpp:468] Using default 'crammd5' authenticator
I0224 22:35:53.722525  1982 master.cpp:537] Using default 'basic' HTTP authenticator
I0224 22:35:53.722661  1982 master.cpp:571] Authorization enabled
I0224 22:35:53.722813  1968 hierarchical.cpp:144] Initialized hierarchical allocator process
I0224 22:35:53.722846  1980 whitelist_watcher.cpp:77] No whitelist given
I0224 22:35:53.724957  1977 master.cpp:1712] The newly elected leader is master@172.17.0.1:36678 with id aab18b61-7811-4c43-a672-d1a63818c880
I0224 22:35:53.725000  1977 master.cpp:1725] Elected as the leading master!
I0224 22:35:53.725023  1977 master.cpp:1470] Recovering from registrar
I0224 22:35:53.725306  1967 registrar.cpp:307] Recovering registrar
I0224 22:35:53.725808  1977 log.cpp:659] Attempting to start the writer
I0224 22:35:53.727145  1973 replica.cpp:493] Replica received implicit promise request from (4536)@172.17.0.1:36678 with proposal 1
I0224 22:35:53.727728  1973 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 424560ns
I0224 22:35:53.727828  1973 replica.cpp:342] Persisted promised to 1
I0224 22:35:53.729080  1973 coordinator.cpp:238] Coordinator attempting to fill missing positions
I0224 22:35:53.731009  1979 replica.cpp:388] Replica received explicit promise request from (4537)@172.17.0.1:36678 for position 0 with proposal 2
I0224 22:35:53.731580  1979 leveldb.cpp:341] Persisting action (8 bytes) to leveldb took 478479ns
I0224 22:35:53.731613  1979 replica.cpp:712] Persisted action at 0
I0224 22:35:53.734354  1979 replica.cpp:537] Replica received write request for position 0 from (4538)@172.17.0.1:36678
I0224 22:35:53.734485  1979 leveldb.cpp:436] Reading position from leveldb took 60879ns
I0224 22:35:53.735877  1979 leveldb.cpp:341] Persisting action (14 bytes) to leveldb took 1.324061ms
I0224 22:35:53.735930  1979 replica.cpp:712] Persisted action at 0
I0224 22:35:53.737061  1970 replica.cpp:691] Replica received learned notice for position 0 from @0.0.0.0:0
I0224 22:35:53.738881  1970 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 1.772814ms
I0224 22:35:53.738939  1970 replica.cpp:712] Persisted action at 0
I0224 22:35:53.738975  1970 replica.cpp:697] Replica learned NOP action at position 0
I0224 22:35:53.740136  1976 log.cpp:675] Writer started with ending position 0
I0224 22:35:53.741750  1976 leveldb.cpp:436] Reading position from leveldb took 74863ns
I0224 22:35:53.743479  1976 registrar.cpp:340] Successfully fetched the registry (0B) in 18.11968ms
I0224 22:35:53.743755  1976 registrar.cpp:439] Applied 1 operations in 56670ns; attempting to update the 'registry'
I0224 22:35:53.745604  1978 log.cpp:683] Attempting to append 170 bytes to the log
I0224 22:35:53.745905  1977 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 1
I0224 22:35:53.746968  1981 replica.cpp:537] Replica received write request for position 1 from (4539)@172.17.0.1:36678
I0224 22:35:53.747480  1981 leveldb.cpp:341] Persisting action (189 bytes) to leveldb took 456947ns
I0224 22:35:53.747609  1981 replica.cpp:712] Persisted action at 1
I0224 22:35:53.750448  1981 replica.cpp:691] Replica received learned notice for position 1 from @0.0.0.0:0
I0224 22:35:53.751158  1981 leveldb.cpp:341] Persisting action (191 bytes) to leveldb took 535163ns
I0224 22:35:53.751258  1981 replica.cpp:712] Persisted action at 1
I0224 22:35:53.751389  1981 replica.cpp:697] Replica learned APPEND action at position 1
I0224 22:35:53.753149  1979 registrar.cpp:484] Successfully updated the 'registry' in 9.228032ms
I0224 22:35:53.753324  1979 registrar.cpp:370] Successfully recovered registrar
I0224 22:35:53.753593  1979 log.cpp:702] Attempting to truncate the log to 1
I0224 22:35:53.753805  1979 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 2
I0224 22:35:53.754055  1981 master.cpp:1522] Recovered 0 slaves from the Registry (131B) ; allowing 10mins for slaves to re-register
I0224 22:35:53.754349  1979 hierarchical.cpp:171] Skipping recovery of hierarchical allocator: nothing to recover
I0224 22:35:53.755764  1977 replica.cpp:537] Replica received write request for position 2 from (4540)@172.17.0.1:36678
I0224 22:35:53.756459  1977 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 488559ns
I0224 22:35:53.756561  1977 replica.cpp:712] Persisted action at 2
I0224 22:35:53.757932  1972 replica.cpp:691] Replica received learned notice for position 2 from @0.0.0.0:0
I0224 22:35:53.758400  1972 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 343827ns
I0224 22:35:53.758539  1972 leveldb.cpp:399] Deleting ~1 keys from leveldb took 34231ns
I0224 22:35:53.758658  1972 replica.cpp:712] Persisted action at 2
I0224 22:35:53.758782  1972 replica.cpp:697] Replica learned TRUNCATE action at position 2
I0224 22:35:53.778059  1978 slave.cpp:193] Slave started on 115)@172.17.0.1:36678
I0224 22:35:53.778105  1978 slave.cpp:194] Flags at startup: --appc_simple_discovery_uri_prefix=""http://"" --appc_store_dir=""/tmp/mesos/store/appc"" --authenticatee=""crammd5"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --credential=""/tmp/MasterMaintenanceTest_InverseOffers_ywqvFF/credential"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_kill_orphans=""true"" --docker_registry=""https://registry-1.docker.io"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/tmp/mesos/store/docker"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/tmp/MasterMaintenanceTest_InverseOffers_ywqvFF/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --hostname=""maintenance-host"" --hostname_lookup=""true"" --image_provisioner_backend=""copy"" --initialize_driver_logging=""true"" --isolation=""posix/cpu,posix/mem"" --launcher_dir=""/mesos/mesos-0.28.0/_build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""10ms"" --resources=""cpus:2;mem:1024;disk:1024;ports:[31000-32000]"" --revocable_cpu_low_priority=""true"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --systemd_enable_support=""true"" --systemd_runtime_directory=""/run/systemd/system"" --version=""false"" --work_dir=""/tmp/MasterMaintenanceTest_InverseOffers_ywqvFF""
I0224 22:35:53.778609  1978 credentials.hpp:83] Loading credential for authentication from '/tmp/MasterMaintenanceTest_InverseOffers_ywqvFF/credential'
I0224 22:35:53.779175  1978 slave.cpp:324] Slave using credential for: test-principal
I0224 22:35:53.779520  1978 resources.cpp:576] Parsing resources as JSON failed: cpus:2;mem:1024;disk:1024;ports:[31000-32000]
Trying semicolon-delimited string format instead
I0224 22:35:53.780192  1978 slave.cpp:464] Slave resources: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I0224 22:35:53.780362  1978 slave.cpp:472] Slave attributes: [  ]
I0224 22:35:53.780483  1978 slave.cpp:477] Slave hostname: maintenance-host
I0224 22:35:53.782126  1967 state.cpp:58] Recovering state from '/tmp/MasterMaintenanceTest_InverseOffers_ywqvFF/meta'
I0224 22:35:53.782892  1969 status_update_manager.cpp:200] Recovering status update manager
I0224 22:35:53.783242  1969 slave.cpp:4565] Finished recovery
I0224 22:35:53.784001  1969 slave.cpp:4737] Querying resource estimator for oversubscribable resources
I0224 22:35:53.784678  1969 slave.cpp:796] New master detected at master@172.17.0.1:36678
I0224 22:35:53.784874  1967 status_update_manager.cpp:174] Pausing sending status updates
I0224 22:35:53.784808  1969 slave.cpp:859] Authenticating with master master@172.17.0.1:36678
I0224 22:35:53.784945  1969 slave.cpp:864] Using default CRAM-MD5 authenticatee
I0224 22:35:53.785181  1969 slave.cpp:832] Detecting new master
I0224 22:35:53.785326  1969 slave.cpp:4751] Received oversubscribable resources  from the resource estimator
I0224 22:35:53.785557  1969 authenticatee.cpp:121] Creating new client SASL connection
I0224 22:35:53.786227  1969 master.cpp:5526] Authenticating slave(115)@172.17.0.1:36678
I0224 22:35:53.786492  1969 authenticator.cpp:413] Starting authentication session for crammd5_authenticatee(298)@172.17.0.1:36678
I0224 22:35:53.786962  1969 authenticator.cpp:98] Creating new server SASL connection
I0224 22:35:53.787274  1969 authenticatee.cpp:212] Received SASL authentication mechanisms: CRAM-MD5
I0224 22:35:53.787308  1969 authenticatee.cpp:238] Attempting to authenticate with mechanism 'CRAM-MD5'
I0224 22:35:53.787400  1969 authenticator.cpp:203] Received SASL authentication start
I0224 22:35:53.787470  1969 authenticator.cpp:325] Authentication requires more steps
I0224 22:35:53.787884  1972 authenticatee.cpp:258] Received SASL authentication step
I0224 22:35:53.787992  1972 authenticator.cpp:231] Received SASL authentication step
I0224 22:35:53.788027  1972 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: '4db5fa128d2d' server FQDN: '4db5fa128d2d' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0224 22:35:53.788040  1972 auxprop.cpp:179] Looking up auxiliary property '*userPassword'
I0224 22:35:53.788090  1972 auxprop.cpp:179] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0224 22:35:53.788122  1972 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: '4db5fa128d2d' server FQDN: '4db5fa128d2d' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0224 22:35:53.788136  1972 auxprop.cpp:129] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0224 22:35:53.788146  1972 auxprop.cpp:129] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0224 22:35:53.788164  1972 authenticator.cpp:317] Authentication success
I0224 22:35:53.788331  1972 authenticatee.cpp:298] Authentication success
I0224 22:35:53.788439  1972 master.cpp:5556] Successfully authenticated principal 'test-principal' at slave(115)@172.17.0.1:36678
I0224 22:35:53.788529  1972 authenticator.cpp:431] Authentication session cleanup for crammd5_authenticatee(298)@172.17.0.1:36678
I0224 22:35:53.788988  1972 slave.cpp:927] Successfully authenticated with master master@172.17.0.1:36678
I0224 22:35:53.789139  1972 slave.cpp:1321] Will retry registration in 1.535786ms if necessary
I0224 22:35:53.789515  1972 master.cpp:4240] Registering slave at slave(115)@172.17.0.1:36678 (maintenance-host) with id aab18b61-7811-4c43-a672-d1a63818c880-S0
I0224 22:35:53.790577  1972 registrar.cpp:439] Applied 1 operations in 78745ns; attempting to update the 'registry'
I0224 22:35:53.791128  1971 process.cpp:3141] Handling HTTP event for process 'master' with path: '/master/maintenance/schedule'
I0224 22:35:53.791877  1971 http.cpp:501] HTTP POST for /master/maintenance/schedule from 172.17.0.1:45095
I0224 22:35:53.793313  1972 log.cpp:683] Attempting to append 343 bytes to the log
I0224 22:35:53.793586  1972 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 3
I0224 22:35:53.794533  1971 replica.cpp:537] Replica received write request for position 3 from (4547)@172.17.0.1:36678
I0224 22:35:53.794862  1971 leveldb.cpp:341] Persisting action (362 bytes) to leveldb took 283614ns
I0224 22:35:53.794893  1971 replica.cpp:712] Persisted action at 3
I0224 22:35:53.796646  1979 replica.cpp:691] Replica received learned notice for position 3 from @0.0.0.0:0
I0224 22:35:53.797102  1972 slave.cpp:1321] Will retry registration in 17.198963ms if necessary
I0224 22:35:53.797186  1979 leveldb.cpp:341] Persisting action (364 bytes) to leveldb took 498502ns
I0224 22:35:53.797230  1979 replica.cpp:712] Persisted action at 3
I0224 22:35:53.797260  1979 replica.cpp:697] Replica learned APPEND action at position 3
I0224 22:35:53.797417  1972 master.cpp:4228] Ignoring register slave message from slave(115)@172.17.0.1:36678 (maintenance-host) as admission is already in progress
I0224 22:35:53.799119  1978 registrar.cpp:484] Successfully updated the 'registry' in 8.45824ms
I0224 22:35:53.799613  1978 registrar.cpp:439] Applied 1 operations in 176193ns; attempting to update the 'registry'
I0224 22:35:53.800472  1972 master.cpp:4308] Registered slave aab18b61-7811-4c43-a672-d1a63818c880-S0 at slave(115)@172.17.0.1:36678 (maintenance-host) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I0224 22:35:53.800623  1978 log.cpp:702] Attempting to truncate the log to 3
I0224 22:35:53.801255  1969 hierarchical.cpp:473] Added slave aab18b61-7811-4c43-a672-d1a63818c880-S0 (maintenance-host) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (allocated: )
I0224 22:35:53.801301  1978 slave.cpp:971] Registered with master master@172.17.0.1:36678; given slave ID aab18b61-7811-4c43-a672-d1a63818c880-S0
I0224 22:35:53.801331  1978 fetcher.cpp:81] Clearing fetcher cache
I0224 22:35:53.801431  1969 hierarchical.cpp:1434] No resources available to allocate!
I0224 22:35:53.801466  1969 hierarchical.cpp:1147] Performed allocation for slave aab18b61-7811-4c43-a672-d1a63818c880-S0 in 162751ns
I0224 22:35:53.801532  1969 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 4
I0224 22:35:53.801867  1978 slave.cpp:994] Checkpointing SlaveInfo to '/tmp/MasterMaintenanceTest_InverseOffers_ywqvFF/meta/slaves/aab18b61-7811-4c43-a672-d1a63818c880-S0/slave.info'
I0224 22:35:53.801877  1969 status_update_manager.cpp:181] Resuming sending status updates
I0224 22:35:53.802898  1977 replica.cpp:537] Replica received write request for position 4 from (4548)@172.17.0.1:36678
I0224 22:35:53.803252  1978 slave.cpp:1030] Forwarding total oversubscribed resources 
I0224 22:35:53.803640  1970 master.cpp:4649] Received update of slave aab18b61-7811-4c43-a672-d1a63818c880-S0 at slave(115)@172.17.0.1:36678 (maintenance-host) with total oversubscribed resources 
I0224 22:35:53.803858  1977 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 912626ns
I0224 22:35:53.803889  1977 replica.cpp:712] Persisted action at 4
I0224 22:35:53.804144  1978 slave.cpp:3482] Received ping from slave-observer(117)@172.17.0.1:36678
I0224 22:35:53.804535  1971 hierarchical.cpp:531] Slave aab18b61-7811-4c43-a672-d1a63818c880-S0 (maintenance-host) updated with oversubscribed resources  (total: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000], allocated: )
I0224 22:35:53.804684  1971 hierarchical.cpp:1434] No resources available to allocate!
I0224 22:35:53.804714  1971 hierarchical.cpp:1147] Performed allocation for slave aab18b61-7811-4c43-a672-d1a63818c880-S0 in 131453ns
I0224 22:35:53.805541  1967 replica.cpp:691] Replica received learned notice for position 4 from @0.0.0.0:0
I0224 22:35:53.805941  1967 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 366444ns
I0224 22:35:53.806015  1967 leveldb.cpp:399] Deleting ~2 keys from leveldb took 42808ns
I0224 22:35:53.806041  1967 replica.cpp:712] Persisted action at 4
I0224 22:35:53.806066  1967 replica.cpp:697] Replica learned TRUNCATE action at position 4
I0224 22:35:53.807355  1978 log.cpp:683] Attempting to append 465 bytes to the log
I0224 22:35:53.807551  1978 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 5
I0224 22:35:53.809638  1979 replica.cpp:537] Replica received write request for position 5 from (4549)@172.17.0.1:36678
I0224 22:35:53.810858  1979 leveldb.cpp:341] Persisting action (484 bytes) to leveldb took 1.167663ms
I0224 22:35:53.810904  1979 replica.cpp:712] Persisted action at 5
I0224 22:35:53.811997  1979 replica.cpp:691] Replica received learned notice for position 5 from @0.0.0.0:0
I0224 22:35:53.812348  1979 leveldb.cpp:341] Persisting action (486 bytes) to leveldb took 318928ns
I0224 22:35:53.812376  1979 replica.cpp:712] Persisted action at 5
I0224 22:35:53.812397  1979 replica.cpp:697] Replica learned APPEND action at position 5
I0224 22:35:53.815132  1973 registrar.cpp:484] Successfully updated the 'registry' in 15.437312ms
I0224 22:35:53.815491  1976 log.cpp:702] Attempting to truncate the log to 5
I0224 22:35:53.815610  1973 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 6
I0224 22:35:53.815661  1968 master.cpp:4705] Updating unavailability of slave aab18b61-7811-4c43-a672-d1a63818c880-S0 at slave(115)@172.17.0.1:36678 (maintenance-host), starting at 2410.99235909694weeks
I0224 22:35:53.815845  1968 master.cpp:4705] Updating unavailability of slave aab18b61-7811-4c43-a672-d1a63818c880-S0 at slave(115)@172.17.0.1:36678 (maintenance-host), starting at 2410.99235909694weeks
I0224 22:35:53.816069  1975 hierarchical.cpp:1434] No resources available to allocate!
I0224 22:35:53.816103  1975 hierarchical.cpp:1147] Performed allocation for slave aab18b61-7811-4c43-a672-d1a63818c880-S0 in 175822ns
I0224 22:35:53.816272  1975 hierarchical.cpp:1434] No resources available to allocate!
I0224 22:35:53.816303  1975 hierarchical.cpp:1147] Performed allocation for slave aab18b61-7811-4c43-a672-d1a63818c880-S0 in 110913ns
I0224 22:35:53.817291  1972 replica.cpp:537] Replica received write request for position 6 from (4550)@172.17.0.1:36678
I0224 22:35:53.817908  1972 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 576032ns
I0224 22:35:53.817932  1972 replica.cpp:712] Persisted action at 6
I0224 22:35:53.818686  1980 replica.cpp:691] Replica received learned notice for position 6 from @0.0.0.0:0
I0224 22:35:53.819021  1980 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 305298ns
I0224 22:35:53.819095  1980 leveldb.cpp:399] Deleting ~2 keys from leveldb took 44332ns
I0224 22:35:53.819120  1980 replica.cpp:712] Persisted action at 6
I0224 22:35:53.819162  1980 replica.cpp:697] Replica learned TRUNCATE action at position 6
I0224 22:35:53.820662  1967 process.cpp:3141] Handling HTTP event for process 'master' with path: '/master/maintenance/status'
I0224 22:35:53.821190  1976 http.cpp:501] HTTP GET for /master/maintenance/status from 172.17.0.1:45096
I0224 22:35:53.823709  1948 scheduler.cpp:154] Version: 0.28.0
I0224 22:35:53.824424  1972 scheduler.cpp:236] New master detected at master@172.17.0.1:36678
I0224 22:35:53.825402  1982 scheduler.cpp:298] Sending SUBSCRIBE call to master@172.17.0.1:36678
I0224 22:35:53.827201  1978 process.cpp:3141] Handling HTTP event for process 'master' with path: '/master/api/v1/scheduler'
I0224 22:35:53.827636  1978 http.cpp:501] HTTP POST for /master/api/v1/scheduler from 172.17.0.1:45097
I0224 22:35:53.827922  1978 master.cpp:1974] Received subscription request for HTTP framework 'default'
I0224 22:35:53.827991  1978 master.cpp:1751] Authorizing framework principal 'test-principal' to receive offers for role '*'
I0224 22:35:53.828418  1982 master.cpp:2065] Subscribing framework 'default' with checkpointing disabled and capabilities [  ]
I0224 22:35:53.828943  1968 hierarchical.cpp:265] Added framework aab18b61-7811-4c43-a672-d1a63818c880-0000
I0224 22:35:53.829124  1982 master.hpp:1657] Sending heartbeat to aab18b61-7811-4c43-a672-d1a63818c880-0000
I0224 22:35:53.829987  1968 hierarchical.cpp:1127] Performed allocation for 1 slaves in 1.011356ms
I0224 22:35:53.830204  1982 master.cpp:5355] Sending 1 offers to framework aab18b61-7811-4c43-a672-d1a63818c880-0000 (default)
I0224 22:35:53.830801  1982 master.cpp:5445] Sending 1 inverse offers to framework aab18b61-7811-4c43-a672-d1a63818c880-0000 (default)
I0224 22:35:53.831132  1969 scheduler.cpp:457] Enqueuing event SUBSCRIBED received from master@172.17.0.1:36678
I0224 22:35:53.832396  1968 scheduler.cpp:457] Enqueuing event HEARTBEAT received from master@172.17.0.1:36678
I0224 22:35:53.833050  1976 master_maintenance_tests.cpp:177] Ignoring HEARTBEAT event
I0224 22:35:53.833256  1979 scheduler.cpp:457] Enqueuing event OFFERS received from master@172.17.0.1:36678
I0224 22:35:53.833775  1979 scheduler.cpp:457] Enqueuing event OFFERS received from master@172.17.0.1:36678
I0224 22:35:53.835662  1980 scheduler.cpp:298] Sending ACCEPT call to master@172.17.0.1:36678
I0224 22:35:53.837591  1967 process.cpp:3141] Handling HTTP event for process 'master' with path: '/master/api/v1/scheduler'
I0224 22:35:53.838021  1967 http.cpp:501] HTTP POST for /master/api/v1/scheduler from 172.17.0.1:45098
I0224 22:35:53.838851  1967 master.cpp:3138] Processing ACCEPT call for offers: [ aab18b61-7811-4c43-a672-d1a63818c880-O0 ] on slave aab18b61-7811-4c43-a672-d1a63818c880-S0 at slave(115)@172.17.0.1:36678 (maintenance-host) for framework aab18b61-7811-4c43-a672-d1a63818c880-0000 (default)
I0224 22:35:53.838946  1967 master.cpp:2825] Authorizing framework principal 'test-principal' to launch task 90bcae0c-9d40-40b7-9537-dae7e83479f6 as user 'mesos'
W0224 22:35:53.841048  1967 validation.cpp:404] Executor default for task 90bcae0c-9d40-40b7-9537-dae7e83479f6 uses less CPUs (None) than the minimum required (0.01). Please update your executor, as this will be mandatory in future releases.
W0224 22:35:53.841101  1967 validation.cpp:416] Executor default for task 90bcae0c-9d40-40b7-9537-dae7e83479f6 uses less memory (None) than the minimum required (32MB). Please update your executor, as this will be mandatory in future releases.
I0224 22:35:53.841624  1967 master.hpp:176] Adding task 90bcae0c-9d40-40b7-9537-dae7e83479f6 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave aab18b61-7811-4c43-a672-d1a63818c880-S0 (maintenance-host)
I0224 22:35:53.842157  1967 master.cpp:3623] Launching task 90bcae0c-9d40-40b7-9537-dae7e83479f6 of framework aab18b61-7811-4c43-a672-d1a63818c880-0000 (default) with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave aab18b61-7811-4c43-a672-d1a63818c880-S0 at slave(115)@172.17.0.1:36678 (maintenance-host)
I0224 22:35:53.842571  1980 slave.cpp:1361] Got assigned task 90bcae0c-9d40-40b7-9537-dae7e83479f6 for framework aab18b61-7811-4c43-a672-d1a63818c880-0000
I0224 22:35:53.843122  1980 slave.cpp:1480] Launching task 90bcae0c-9d40-40b7-9537-dae7e83479f6 for framework aab18b61-7811-4c43-a672-d1a63818c880-0000
I0224 22:35:53.843718  1980 paths.cpp:474] Trying to chown '/tmp/MasterMaintenanceTest_InverseOffers_ywqvFF/slaves/aab18b61-7811-4c43-a672-d1a63818c880-S0/frameworks/aab18b61-7811-4c43-a672-d1a63818c880-0000/executors/default/runs/a5a1e49d-20a8-4796-8ec0-5a1595e76159' to user 'mesos'
I0224 22:35:53.852052  1980 slave.cpp:5367] Launching executor default of framework aab18b61-7811-4c43-a672-d1a63818c880-0000 with resources  in work directory '/tmp/MasterMaintenanceTest_InverseOffers_ywqvFF/slaves/aab18b61-7811-4c43-a672-d1a63818c880-S0/frameworks/aab18b61-7811-4c43-a672-d1a63818c880-0000/executors/default/runs/a5a1e49d-20a8-4796-8ec0-5a1595e76159'
I0224 22:35:53.854452  1980 exec.cpp:143] Version: 0.28.0
I0224 22:35:53.854812  1967 exec.cpp:193] Executor started at: executor(47)@172.17.0.1:36678 with pid 1948
I0224 22:35:53.855108  1980 slave.cpp:1698] Queuing task '90bcae0c-9d40-40b7-9537-dae7e83479f6' for executor 'default' of framework aab18b61-7811-4c43-a672-d1a63818c880-0000
I0224 22:35:53.855264  1980 slave.cpp:749] Successfully attached file '/tmp/MasterMaintenanceTest_InverseOffers_ywqvFF/slaves/aab18b61-7811-4c43-a672-d1a63818c880-S0/frameworks/aab18b61-7811-4c43-a672-d1a63818c880-0000/executors/default/runs/a5a1e49d-20a8-4796-8ec0-5a1595e76159'
I0224 22:35:53.855362  1980 slave.cpp:2643] Got registration for executor 'default' of framework aab18b61-7811-4c43-a672-d1a63818c880-0000 from executor(47)@172.17.0.1:36678
I0224 22:35:53.855785  1974 exec.cpp:217] Executor registered on slave aab18b61-7811-4c43-a672-d1a63818c880-S0
I0224 22:35:53.855857  1974 exec.cpp:229] Executor::registered took 42512ns
I0224 22:35:53.856391  1980 slave.cpp:1863] Sending queued task '90bcae0c-9d40-40b7-9537-dae7e83479f6' to executor 'default' of framework aab18b61-7811-4c43-a672-d1a63818c880-0000 at executor(47)@172.17.0.1:36678
I0224 22:35:53.856720  1974 exec.cpp:304] Executor asked to run task '90bcae0c-9d40-40b7-9537-dae7e83479f6'
I0224 22:35:53.856812  1974 exec.cpp:313] Executor::launchTask took 65703ns
I0224 22:35:53.856922  1974 exec.cpp:526] Executor sending status update TASK_RUNNING (UUID: 249b169a-6b5f-4776-95c8-c897ba6b3f0b) for task 90bcae0c-9d40-40b7-9537-dae7e83479f6 of framework aab18b61-7811-4c43-a672-d1a63818c880-0000
I0224 22:35:53.857378  1980 slave.cpp:3002] Handling status update TASK_RUNNING (UUID: 249b169a-6b5f-4776-95c8-c897ba6b3f0b) for task 90bcae0c-9d40-40b7-9537-dae7e83479f6 of framework aab18b61-7811-4c43-a672-d1a63818c880-0000 from executor(47)@172.17.0.1:36678
I0224 22:35:53.858175  1980 status_update_manager.cpp:320] Received status update TASK_RUNNING (UUID: 249b169a-6b5f-4776-95c8-c897ba6b3f0b) for task 90bcae0c-9d40-40b7-9537-dae7e83479f6 of framework aab18b61-7811-4c43-a672-d1a63818c880-0000
I0224 22:35:53.858222  1980 status_update_manager.cpp:497] Creating StatusUpdate stream for task 90bcae0c-9d40-40b7-9537-dae7e83479f6 of framework aab18b61-7811-4c43-a672-d1a63818c880-0000
I0224 22:35:53.858687  1980 status_update_manager.cpp:374] Forwarding update TASK_RUNNING (UUID: 249b169a-6b5f-4776-95c8-c897ba6b3f0b) for task 90bcae0c-9d40-40b7-9537-dae7e83479f6 of framework aab18b61-7811-4c43-a672-d1a63818c880-0000 to the slave
I0224 22:35:53.859210  1980 slave.cpp:3400] Forwarding the update TASK_RUNNING (UUID: 249b169a-6b5f-4776-95c8-c897ba6b3f0b) for task 90bcae0c-9d40-40b7-9537-dae7e83479f6 of framework aab18b61-7811-4c43-a672-d1a63818c880-0000 to master@172.17.0.1:36678
I0224 22:35:53.859390  1980 slave.cpp:3294] Status update manager successfully handled status update TASK_RUNNING (UUID: 249b169a-6b5f-4776-95c8-c897ba6b3f0b) for task 90bcae0c-9d40-40b7-9537-dae7e83479f6 of framework aab18b61-7811-4c43-a672-d1a63818c880-0000
I0224 22:35:53.859436  1980 slave.cpp:3310] Sending acknowledgement for status update TASK_RUNNING (UUID: 249b169a-6b5f-4776-95c8-c897ba6b3f0b) for task 90bcae0c-9d40-40b7-9537-dae7e83479f6 of framework aab18b61-7811-4c43-a672-d1a63818c880-0000 to executor(47)@172.17.0.1:36678
I0224 22:35:53.859663  1980 exec.cpp:350] Executor received status update acknowledgement 249b169a-6b5f-4776-95c8-c897ba6b3f0b for task 90bcae0c-9d40-40b7-9537-dae7e83479f6 of framework aab18b61-7811-4c43-a672-d1a63818c880-0000
I0224 22:35:53.859657  1967 master.cpp:4794] Status update TASK_RUNNING (UUID: 249b169a-6b5f-4776-95c8-c897ba6b3f0b) for task 90bcae0c-9d40-40b7-9537-dae7e83479f6 of framework aab18b61-7811-4c43-a672-d1a63818c880-0000 from slave aab18b61-7811-4c43-a672-d1a63818c880-S0 at slave(115)@172.17.0.1:36678 (maintenance-host)
I0224 22:35:53.859851  1967 master.cpp:4842] Forwarding status update TASK_RUNNING (UUID: 249b169a-6b5f-4776-95c8-c897ba6b3f0b) for task 90bcae0c-9d40-40b7-9537-dae7e83479f6 of framework aab18b61-7811-4c43-a672-d1a63818c880-0000
I0224 22:35:53.860587  1967 master.cpp:6450] Updating the state of task 90bcae0c-9d40-40b7-9537-dae7e83479f6 of framework aab18b61-7811-4c43-a672-d1a63818c880-0000 (latest state: TASK_RUNNING, status update state: TASK_RUNNING)
I0224 22:35:53.862711  1967 scheduler.cpp:457] Enqueuing event UPDATE received from master@172.17.0.1:36678
I0224 22:35:53.866711  1976 scheduler.cpp:298] Sending ACKNOWLEDGE call to master@172.17.0.1:36678
I0224 22:35:53.870667  1972 process.cpp:3141] Handling HTTP event for process 'master' with path: '/master/api/v1/scheduler'
I0224 22:35:53.871269  1972 http.cpp:501] HTTP POST for /master/api/v1/scheduler from 172.17.0.1:45099
I0224 22:35:53.871459  1972 master.cpp:3952] Processing ACKNOWLEDGE call 249b169a-6b5f-4776-95c8-c897ba6b3f0b for task 90bcae0c-9d40-40b7-9537-dae7e83479f6 of framework aab18b61-7811-4c43-a672-d1a63818c880-0000 (default) on slave aab18b61-7811-4c43-a672-d1a63818c880-S0
I0224 22:35:53.872184  1972 status_update_manager.cpp:392] Received status update acknowledgement (UUID: 249b169a-6b5f-4776-95c8-c897ba6b3f0b) for task 90bcae0c-9d40-40b7-9537-dae7e83479f6 of framework aab18b61-7811-4c43-a672-d1a63818c880-0000
I0224 22:35:53.872537  1972 slave.cpp:2412] Status update manager successfully handled status update acknowledgement (UUID: 249b169a-6b5f-4776-95c8-c897ba6b3f0b) for task 90bcae0c-9d40-40b7-9537-dae7e83479f6 of framework aab18b61-7811-4c43-a672-d1a63818c880-0000
I0224 22:35:53.874407  1975 scheduler.cpp:298] Sending DECLINE call to master@172.17.0.1:36678
I0224 22:35:53.877537  1979 hierarchical.cpp:1434] No resources available to allocate!
I0224 22:35:53.877795  1979 hierarchical.cpp:1127] Performed allocation for 1 slaves in 482441ns
I0224 22:35:53.878082  1981 process.cpp:3141] Handling HTTP event for process 'master' with path: '/master/api/v1/scheduler'
I0224 22:35:53.878675  1978 http.cpp:501] HTTP POST for /master/api/v1/scheduler from 172.17.0.1:45100
I0224 22:35:53.878931  1978 master.cpp:3675] Processing DECLINE call for offers: [ aab18b61-7811-4c43-a672-d1a63818c880-O1 ] for framework aab18b61-7811-4c43-a672-d1a63818c880-0000 (default)
../../src/tests/master_maintenance_tests.cpp:1222: Failure
Failed to wait 15secs for event
I0224 22:36:08.881649  1948 master.cpp:1027] Master terminating
W0224 22:36:08.881925  1948 master.cpp:6502] Removing task 90bcae0c-9d40-40b7-9537-dae7e83479f6 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] of framework aab18b61-7811-4c43-a672-d1a63818c880-0000 on slave aab18b61-7811-4c43-a672-d1a63818c880-S0 at slave(115)@172.17.0.1:36678 (maintenance-host) in non-terminal state TASK_RUNNING
I0224 22:36:08.882961  1948 master.cpp:6545] Removing executor 'default' with resources  of framework aab18b61-7811-4c43-a672-d1a63818c880-0000 on slave aab18b61-7811-4c43-a672-d1a63818c880-S0 at slave(115)@172.17.0.1:36678 (maintenance-host)
I0224 22:36:08.884789  1969 hierarchical.cpp:505] Removed slave aab18b61-7811-4c43-a672-d1a63818c880-S0
I0224 22:36:08.887261  1969 hierarchical.cpp:326] Removed framework aab18b61-7811-4c43-a672-d1a63818c880-0000
I0224 22:36:08.916983  1976 slave.cpp:3528] master@172.17.0.1:36678 exited
W0224 22:36:08.917191  1976 slave.cpp:3531] Master disconnected! Waiting for a new master to be elected
I0224 22:36:08.934546  1975 slave.cpp:3528] executor(47)@172.17.0.1:36678 exited
I0224 22:36:08.934806  1974 slave.cpp:3886] Executor 'default' of framework aab18b61-7811-4c43-a672-d1a63818c880-0000 exited with status 0
I0224 22:36:08.935024  1974 slave.cpp:3002] Handling status update TASK_FAILED (UUID: 77d415df-58bd-4cf5-9c49-6106691d9599) for task 90bcae0c-9d40-40b7-9537-dae7e83479f6 of framework aab18b61-7811-4c43-a672-d1a63818c880-0000 from @0.0.0.0:0
I0224 22:36:08.935505  1974 slave.cpp:5677] Terminating task 90bcae0c-9d40-40b7-9537-dae7e83479f6
I0224 22:36:08.936190  1967 status_update_manager.cpp:320] Received status update TASK_FAILED (UUID: 77d415df-58bd-4cf5-9c49-6106691d9599) for task 90bcae0c-9d40-40b7-9537-dae7e83479f6 of framework aab18b61-7811-4c43-a672-d1a63818c880-0000
I0224 22:36:08.936368  1967 status_update_manager.cpp:374] Forwarding update TASK_FAILED (UUID: 77d415df-58bd-4cf5-9c49-6106691d9599) for task 90bcae0c-9d40-40b7-9537-dae7e83479f6 of framework aab18b61-7811-4c43-a672-d1a63818c880-0000 to the slave
I0224 22:36:08.936606  1974 slave.cpp:3400] Forwarding the update TASK_FAILED (UUID: 77d415df-58bd-4cf5-9c49-6106691d9599) for task 90bcae0c-9d40-40b7-9537-dae7e83479f6 of framework aab18b61-7811-4c43-a672-d1a63818c880-0000 to master@172.17.0.1:36678
I0224 22:36:08.936779  1974 slave.cpp:3294] Status update manager successfully handled status update TASK_FAILED (UUID: 77d415df-58bd-4cf5-9c49-6106691d9599) for task 90bcae0c-9d40-40b7-9537-dae7e83479f6 of framework aab18b61-7811-4c43-a672-d1a63818c880-0000
I0224 22:36:08.955370  1967 slave.cpp:668] Slave terminating
I0224 22:36:08.955499  1967 slave.cpp:2079] Asked to shut down framework aab18b61-7811-4c43-a672-d1a63818c880-0000 by @0.0.0.0:0
I0224 22:36:08.955538  1967 slave.cpp:2104] Shutting down framework aab18b61-7811-4c43-a672-d1a63818c880-0000
I0224 22:36:08.955606  1967 slave.cpp:3990] Cleaning up executor 'default' of framework aab18b61-7811-4c43-a672-d1a63818c880-0000 at executor(47)@172.17.0.1:36678
I0224 22:36:08.956053  1967 slave.cpp:4078] Cleaning up framework aab18b61-7811-4c43-a672-d1a63818c880-0000
I0224 22:36:08.956327  1967 gc.cpp:54] Scheduling '/tmp/MasterMaintenanceTest_InverseOffers_ywqvFF/slaves/aab18b61-7811-4c43-a672-d1a63818c880-S0/frameworks/aab18b61-7811-4c43-a672-d1a63818c880-0000/executors/default/runs/a5a1e49d-20a8-4796-8ec0-5a1595e76159' for gc 1.00002336880296weeks in the future
I0224 22:36:08.956495  1973 status_update_manager.cpp:282] Closing status update streams for framework aab18b61-7811-4c43-a672-d1a63818c880-0000
I0224 22:36:08.956524  1967 gc.cpp:54] Scheduling '/tmp/MasterMaintenanceTest_InverseOffers_ywqvFF/slaves/aab18b61-7811-4c43-a672-d1a63818c880-S0/frameworks/aab18b61-7811-4c43-a672-d1a63818c880-0000/executors/default' for gc 1.00002336880296weeks in the future
I0224 22:36:08.956549  1973 status_update_manager.cpp:528] Cleaning up status update stream for task 90bcae0c-9d40-40b7-9537-dae7e83479f6 of framework aab18b61-7811-4c43-a672-d1a63818c880-0000
I0224 22:36:08.956619  1967 gc.cpp:54] Scheduling '/tmp/MasterMaintenanceTest_InverseOffers_ywqvFF/slaves/aab18b61-7811-4c43-a672-d1a63818c880-S0/frameworks/aab18b61-7811-4c43-a672-d1a63818c880-0000' for gc 1.00002336880296weeks in the future
[  FAILED  ] MasterMaintenanceTest.InverseOffers (15258 ms)
{code}",Bug,Major,kaysoky,2016-02-25T07:34:12.000+0000,5,Resolved,Complete,MasterMaintenanceTest.InverseOffers is flaky,2016-02-25T07:34:12.000+0000,MESOS-4768,1.0,mesos,Mesosphere Sprint 29
qianzhang,2016-02-24T19:22:47.000+0000,jieyu,"In order for service discovery to work in some cases, the network/cni isolator needs to report the assigned IP address through the isolator->status() interface.",Task,Major,jieyu,2016-04-02T18:43:50.000+0000,5,Resolved,Complete,The network/cni isolator should report assigned IP address. ,2016-04-02T18:43:50.000+0000,MESOS-4764,3.0,mesos,Mesosphere Sprint 30
qianzhang,2016-02-24T19:20:14.000+0000,jieyu,"In order to test the network/cni isolator, we need to mock the behavior of an CNI plugin. One option is to write a mock script which acts as a CNI plugin. The isolator will talk to the mock script the same way it talks to an actual CNI plugin.

The mock script can just join the host network?",Task,Major,jieyu,2016-04-25T23:38:13.000+0000,5,Resolved,Complete,Add test mock for CNI plugins.,2016-04-25T23:39:11.000+0000,MESOS-4763,5.0,mesos,Mesosphere Sprint 30
avinash@mesosphere.io,2016-02-24T19:15:57.000+0000,jieyu,"Please get more context from the design doc (MESOS-4742).

The CNI plugin will return the DNS information about the network. The network/cni isolator needs to properly setup /etc/resolv.conf for the container. We should consider the following cases:
1) container is using host filesystem
2) container is using a different filesystem
3) custom executor and command executor",Task,Major,jieyu,2016-04-10T19:03:43.000+0000,5,Resolved,Complete,Setup proper DNS resolver for containers in network/cni isolator.,2016-04-10T19:03:43.000+0000,MESOS-4762,5.0,mesos,Mesosphere Sprint 30
qianzhang,2016-02-24T19:11:16.000+0000,jieyu,"According to design doc, we plan to add the following flags:

“--network_cni_plugins_dir”
Location of the CNI plugin binaries. The “network/cni” isolator will find CNI plugins under this directory so that it can execute the plugins to add/delete container from the CNI networks. It is the operator’s responsibility to install the CNI plugin binaries in the specified directory.

“--network_cni_config_dir”
Location of the CNI network configuration files. For each network that containers launched in Mesos agent can connect to, the operator should install a network configuration file in JSON format in the specified directory.",Task,Major,jieyu,2016-03-22T16:36:48.000+0000,5,Resolved,Complete,Add agent flags to allow operators to specify CNI plugin and config directories.,2016-03-22T16:36:48.000+0000,MESOS-4761,2.0,mesos,Mesosphere Sprint 30
mrbrowning,2016-02-24T19:08:12.000+0000,mrbrowning,"To evaluate the fetcher cache and calibrate the value of the fetcher_cache_size flag, it would be useful to have metrics and gauges on agents that expose operational statistics like cache hit rate, occupied cache size, and time spent downloading resources that were not present.",Improvement,Minor,mrbrowning,,1,Open,New,Expose metrics and gauges for fetcher cache usage and hit rate,2016-04-27T00:35:58.000+0000,MESOS-4760,2.0,mesos,
qianzhang,2016-02-24T19:07:52.000+0000,jieyu,"See the design doc for more context (MESOS-4742).

The isolator will interact with CNI plugins to create the network for the container to join.",Task,Major,jieyu,2016-04-08T20:16:45.000+0000,5,Resolved,Complete,Add network/cni isolator for Mesos containerizer.,2016-04-08T20:16:45.000+0000,MESOS-4759,8.0,mesos,Mesosphere Sprint 30
qianzhang,2016-02-24T19:05:06.000+0000,jieyu,"This allows the framework writer to specify the name of the network they want their container to join.

Why not using 'groups'? That's because there might be multiple groups under a single network (e.g., admin vs. user, public vs. private, etc.).",Task,Major,jieyu,2016-03-10T19:14:23.000+0000,5,Resolved,Complete,Add a 'name' field into NetworkInfo.,2016-03-11T01:42:01.000+0000,MESOS-4758,1.0,mesos,
jieyu,2016-02-24T18:47:27.000+0000,jieyu,"Currently, we call os::su(user) after pivot_root. This is problematic because /etc/passwd and /etc/group might be missing in container's root filesystem. We should instead, get the uid/gids before pivot_root, and call setuid/setgroups after pivot_root.",Bug,Major,jieyu,,10006,Reviewable,New,Mesos containerizer should get uid/gids before pivot_root.,2016-02-29T23:31:45.000+0000,MESOS-4757,3.0,mesos,Mesosphere Sprint 29
mcypark,2016-02-24T08:49:32.000+0000,mcypark,"In 0.26.0, the master's {{/state}} endpoint generated the following:

{code}
{
  /* ... */
  ""frameworks"": [
    {
      /* ... */
      ""executors"": [
        {
          ""command"": {
            ""argv"": [],
            ""uris"": [],
            ""value"": ""/Users/mpark/Projects/mesos/build/opt/src/long-lived-executor""
          },
          ""executor_id"": ""default"",
          ""framework_id"": ""0ea528a9-64ba-417f-98ea-9c4b8d418db6-0000"",
          ""name"": ""Long Lived Executor (C++)"",
          ""resources"": {
            ""cpus"": 0,
            ""disk"": 0,
            ""mem"": 0
          },
          ""slave_id"": ""8a513678-03a1-4cb5-9279-c3c0c591f1d8-S0""
        }
      ],
      /* ... */
    }
  ]
  /* ... */
}
{code}

In 0.27.1, the {{ExecutorInfo}} is mistakenly exposed in the raw protobuf schema:

{code}
{
  /* ... */
  ""frameworks"": [
    {
      /* ... */
      ""executors"": [
        {
          ""command"": {
            ""shell"": true,
            ""value"": ""/Users/mpark/Projects/mesos/build/opt/src/long-lived-executor""
          },
          ""executor_id"": {
            ""value"": ""default""
          },
          ""framework_id"": {
            ""value"": ""368a5a49-480b-41f6-a13b-24a69c92a72e-0000""
          },
          ""name"": ""Long Lived Executor (C++)"",
          ""slave_id"": ""8a513678-03a1-4cb5-9279-c3c0c591f1d8-S0"",
          ""source"": ""cpp_long_lived_framework""
        }
      ],
      /* ... */
    }
  ]
  /* ... */
}
{code}

This is a backwards incompatible API change.",Bug,Major,mcypark,2016-02-28T02:47:02.000+0000,5,Resolved,Complete,"The ""executors"" field is exposed under a backwards incompatible schema.",2016-02-28T02:47:02.000+0000,MESOS-4754,2.0,mesos,Mesosphere Sprint 29
nfnt,2016-02-23T21:04:52.000+0000,pawanufl,"I was trying to run Docker containers in a fully SSL-ized Mesos cluster but ran into problems because the executor was failing with a ""Failed to shutdown socket with fd 10: Transport endpoint is not connected"".

My understanding of why this is happening is because the executor was trying to report its status to Mesos slave over HTTPS, but doesnt have the appropriate certs/env setup inside the executor.

(Thanks to mslackbot/joseph for helping me figure this out on #mesos)

It turns out, the executor expects all SSL_* variables to be set inside `CommandInfo.environment` which gets picked up by the executor to successfully reports its status to the slave.

This part of __executor needing all the SSL_* variables to be set in its environment__ is missing in the Mesos SSL transitioning guide. I request you to please add this vital information to the doc.",Documentation,Major,pawanufl,2016-03-13T11:14:26.000+0000,5,Resolved,Complete,Document: Mesos Executor expects all SSL_* environment variables to be set,2016-03-13T11:14:26.000+0000,MESOS-4750,2.0,mesos,Mesosphere Sprint 30
jojy,2016-02-23T19:50:23.000+0000,jojy,Mesos now has support for fetching Appc images. Add tests that verifies the new component.,Task,Major,jojy,2016-03-03T02:07:06.000+0000,5,Resolved,Complete,Add Appc image fetcher tests.,2016-03-03T11:58:23.000+0000,MESOS-4748,3.0,mesos,Mesosphere Sprint 29
bbannier,2016-02-23T19:40:33.000+0000,bbannier,"Some cleanup of spawned processes is missing in {{ContainerLoggerTest.MesosContainerizerRecover}} so that when the test is run in isolation the global teardown might find lingering processes.
{code}
[==========] Running 1 test from 1 test case.
[----------] Global test environment set-up.
[----------] 1 test from ContainerLoggerTest
[ RUN      ] ContainerLoggerTest.MesosContainerizerRecover
[       OK ] ContainerLoggerTest.MesosContainerizerRecover (13 ms)
[----------] 1 test from ContainerLoggerTest (13 ms total)

[----------] Global test environment tear-down
../../src/tests/environment.cpp:728: Failure
Failed
Tests completed with child processes remaining:
-+- 7112 /SOME/PATH/src/mesos/build/src/.libs/mesos-tests --gtest_filter=ContainerLoggerTest.MesosContainerizerRecover
 \--- 7130 (sh)
[==========] 1 test from 1 test case ran. (23 ms total)
[  PASSED  ] 1 test.
[  FAILED  ] 0 tests, listed below:

 0 FAILED TESTS
{code}

Observered on OS X with clang-trunk and an unoptimized build.
",Bug,Major,bbannier,2016-02-24T08:02:39.000+0000,5,Resolved,Complete,ContainerLoggerTest.MesosContainerizerRecover cannot be executed in isolation,2016-02-29T09:37:38.000+0000,MESOS-4747,1.0,mesos,Mesosphere Sprint 29
kaysoky,2016-02-22T19:16:15.000+0000,kaysoky,"This test passes consistently on other OS's, but fails consistently on CentOS 6.

Verbose logs from test failure:
{code}
[ RUN      ] DockerContainerizerTest.ROOT_DOCKER_LaunchWithPersistentVolumes
I0222 18:16:12.327957 26681 leveldb.cpp:174] Opened db in 7.466102ms
I0222 18:16:12.330528 26681 leveldb.cpp:181] Compacted db in 2.540139ms
I0222 18:16:12.330580 26681 leveldb.cpp:196] Created db iterator in 16908ns
I0222 18:16:12.330592 26681 leveldb.cpp:202] Seeked to beginning of db in 1403ns
I0222 18:16:12.330600 26681 leveldb.cpp:271] Iterated through 0 keys in the db in 315ns
I0222 18:16:12.330634 26681 replica.cpp:779] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned
I0222 18:16:12.331082 26698 recover.cpp:447] Starting replica recovery
I0222 18:16:12.331289 26698 recover.cpp:473] Replica is in EMPTY status
I0222 18:16:12.332162 26703 replica.cpp:673] Replica in EMPTY status received a broadcasted recover request from (13761)@172.30.2.148:35274
I0222 18:16:12.332701 26701 recover.cpp:193] Received a recover response from a replica in EMPTY status
I0222 18:16:12.333230 26699 recover.cpp:564] Updating replica status to STARTING
I0222 18:16:12.334102 26698 master.cpp:376] Master 652149b4-3932-4d8b-ba6f-8c9d9045be70 (ip-172-30-2-148.mesosphere.io) started on 172.30.2.148:35274
I0222 18:16:12.334116 26698 master.cpp:378] Flags at startup: --acls="""" --allocation_interval=""1secs"" --allocator=""HierarchicalDRF"" --authenticate=""true"" --authenticate_http=""true"" --authenticate_slaves=""true"" --authenticators=""crammd5"" --authorizers=""local"" --credentials=""/tmp/QEhLBS/credentials"" --framework_sorter=""drf"" --help=""false"" --hostname_lookup=""true"" --http_authenticators=""basic"" --initialize_driver_logging=""true"" --log_auto_initialize=""true"" --logbufsecs=""0"" --logging_level=""INFO"" --max_completed_frameworks=""50"" --max_completed_tasks_per_framework=""1000"" --max_slave_ping_timeouts=""5"" --quiet=""false"" --recovery_slave_removal_limit=""100%"" --registry=""replicated_log"" --registry_fetch_timeout=""1mins"" --registry_store_timeout=""100secs"" --registry_strict=""true"" --root_submissions=""true"" --slave_ping_timeout=""15secs"" --slave_reregister_timeout=""10mins"" --user_sorter=""drf"" --version=""false"" --webui_dir=""/usr/local/share/mesos/webui"" --work_dir=""/tmp/QEhLBS/master"" --zk_session_timeout=""10secs""
I0222 18:16:12.334354 26698 master.cpp:423] Master only allowing authenticated frameworks to register
I0222 18:16:12.334363 26698 master.cpp:428] Master only allowing authenticated slaves to register
I0222 18:16:12.334369 26698 credentials.hpp:35] Loading credentials for authentication from '/tmp/QEhLBS/credentials'
I0222 18:16:12.335366 26698 master.cpp:468] Using default 'crammd5' authenticator
I0222 18:16:12.335492 26698 master.cpp:537] Using default 'basic' HTTP authenticator
I0222 18:16:12.335623 26698 master.cpp:571] Authorization enabled
I0222 18:16:12.335752 26703 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 2.314693ms
I0222 18:16:12.335769 26700 whitelist_watcher.cpp:77] No whitelist given
I0222 18:16:12.335778 26703 replica.cpp:320] Persisted replica status to STARTING
I0222 18:16:12.335821 26697 hierarchical.cpp:144] Initialized hierarchical allocator process
I0222 18:16:12.335965 26701 recover.cpp:473] Replica is in STARTING status
I0222 18:16:12.336771 26703 replica.cpp:673] Replica in STARTING status received a broadcasted recover request from (13763)@172.30.2.148:35274
I0222 18:16:12.337191 26696 recover.cpp:193] Received a recover response from a replica in STARTING status
I0222 18:16:12.337635 26700 recover.cpp:564] Updating replica status to VOTING
I0222 18:16:12.337671 26703 master.cpp:1712] The newly elected leader is master@172.30.2.148:35274 with id 652149b4-3932-4d8b-ba6f-8c9d9045be70
I0222 18:16:12.337698 26703 master.cpp:1725] Elected as the leading master!
I0222 18:16:12.337713 26703 master.cpp:1470] Recovering from registrar
I0222 18:16:12.337828 26696 registrar.cpp:307] Recovering registrar
I0222 18:16:12.339972 26702 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 2.06039ms
I0222 18:16:12.339994 26702 replica.cpp:320] Persisted replica status to VOTING
I0222 18:16:12.340082 26700 recover.cpp:578] Successfully joined the Paxos group
I0222 18:16:12.340267 26700 recover.cpp:462] Recover process terminated
I0222 18:16:12.340591 26699 log.cpp:659] Attempting to start the writer
I0222 18:16:12.341594 26698 replica.cpp:493] Replica received implicit promise request from (13764)@172.30.2.148:35274 with proposal 1
I0222 18:16:12.343598 26698 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 1.97941ms
I0222 18:16:12.343619 26698 replica.cpp:342] Persisted promised to 1
I0222 18:16:12.344182 26698 coordinator.cpp:238] Coordinator attempting to fill missing positions
I0222 18:16:12.345285 26702 replica.cpp:388] Replica received explicit promise request from (13765)@172.30.2.148:35274 for position 0 with proposal 2
I0222 18:16:12.347275 26702 leveldb.cpp:341] Persisting action (8 bytes) to leveldb took 1.960198ms
I0222 18:16:12.347296 26702 replica.cpp:712] Persisted action at 0
I0222 18:16:12.348201 26703 replica.cpp:537] Replica received write request for position 0 from (13766)@172.30.2.148:35274
I0222 18:16:12.348247 26703 leveldb.cpp:436] Reading position from leveldb took 21399ns
I0222 18:16:12.350667 26703 leveldb.cpp:341] Persisting action (14 bytes) to leveldb took 2.39166ms
I0222 18:16:12.350690 26703 replica.cpp:712] Persisted action at 0
I0222 18:16:12.351191 26696 replica.cpp:691] Replica received learned notice for position 0 from @0.0.0.0:0
I0222 18:16:12.353152 26696 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 1.935798ms
I0222 18:16:12.353173 26696 replica.cpp:712] Persisted action at 0
I0222 18:16:12.353188 26696 replica.cpp:697] Replica learned NOP action at position 0
I0222 18:16:12.353639 26696 log.cpp:675] Writer started with ending position 0
I0222 18:16:12.354508 26697 leveldb.cpp:436] Reading position from leveldb took 25625ns
I0222 18:16:12.355274 26696 registrar.cpp:340] Successfully fetched the registry (0B) in 17.406976ms
I0222 18:16:12.355357 26696 registrar.cpp:439] Applied 1 operations in 20977ns; attempting to update the 'registry'
I0222 18:16:12.355929 26697 log.cpp:683] Attempting to append 210 bytes to the log
I0222 18:16:12.356032 26703 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 1
I0222 18:16:12.356657 26698 replica.cpp:537] Replica received write request for position 1 from (13767)@172.30.2.148:35274
I0222 18:16:12.358566 26698 leveldb.cpp:341] Persisting action (229 bytes) to leveldb took 1.881945ms
I0222 18:16:12.358588 26698 replica.cpp:712] Persisted action at 1
I0222 18:16:12.359081 26697 replica.cpp:691] Replica received learned notice for position 1 from @0.0.0.0:0
I0222 18:16:12.361002 26697 leveldb.cpp:341] Persisting action (231 bytes) to leveldb took 1.894331ms
I0222 18:16:12.361023 26697 replica.cpp:712] Persisted action at 1
I0222 18:16:12.361038 26697 replica.cpp:697] Replica learned APPEND action at position 1
I0222 18:16:12.361883 26697 registrar.cpp:484] Successfully updated the 'registry' in 6.482944ms
I0222 18:16:12.361981 26697 registrar.cpp:370] Successfully recovered registrar
I0222 18:16:12.362052 26701 log.cpp:702] Attempting to truncate the log to 1
I0222 18:16:12.362167 26703 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 2
I0222 18:16:12.362421 26696 master.cpp:1522] Recovered 0 slaves from the Registry (171B) ; allowing 10mins for slaves to re-register
I0222 18:16:12.362447 26698 hierarchical.cpp:171] Skipping recovery of hierarchical allocator: nothing to recover
I0222 18:16:12.362911 26701 replica.cpp:537] Replica received write request for position 2 from (13768)@172.30.2.148:35274
I0222 18:16:12.364760 26701 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 1.819954ms
I0222 18:16:12.364783 26701 replica.cpp:712] Persisted action at 2
I0222 18:16:12.365384 26697 replica.cpp:691] Replica received learned notice for position 2 from @0.0.0.0:0
I0222 18:16:12.367961 26697 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 2.55143ms
I0222 18:16:12.368015 26697 leveldb.cpp:399] Deleting ~1 keys from leveldb took 28196ns
I0222 18:16:12.368028 26697 replica.cpp:712] Persisted action at 2
I0222 18:16:12.368044 26697 replica.cpp:697] Replica learned TRUNCATE action at position 2
I0222 18:16:12.376824 26703 slave.cpp:193] Slave started on 396)@172.30.2.148:35274
I0222 18:16:12.376838 26703 slave.cpp:194] Flags at startup: --appc_simple_discovery_uri_prefix=""http://"" --appc_store_dir=""/tmp/mesos/store/appc"" --authenticatee=""crammd5"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --credential=""/tmp/DockerContainerizerTest_ROOT_DOCKER_LaunchWithPersistentVolumes_U5vZX1/credential"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_auth_server=""https://auth.docker.io"" --docker_kill_orphans=""true"" --docker_puller_timeout=""60"" --docker_registry=""https://registry-1.docker.io"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/tmp/mesos/store/docker"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/tmp/DockerContainerizerTest_ROOT_DOCKER_LaunchWithPersistentVolumes_U5vZX1/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --hostname_lookup=""true"" --image_provisioner_backend=""copy"" --initialize_driver_logging=""true"" --isolation=""posix/cpu,posix/mem"" --launcher_dir=""/mnt/teamcity/work/4240ba9ddd0997c3/build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""10ms"" --resources=""cpu:2;mem:2048;disk(role1):2048"" --revocable_cpu_low_priority=""true"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --systemd_enable_support=""true"" --systemd_runtime_directory=""/run/systemd/system"" --version=""false"" --work_dir=""/tmp/DockerContainerizerTest_ROOT_DOCKER_LaunchWithPersistentVolumes_U5vZX1""
I0222 18:16:12.377109 26703 credentials.hpp:83] Loading credential for authentication from '/tmp/DockerContainerizerTest_ROOT_DOCKER_LaunchWithPersistentVolumes_U5vZX1/credential'
I0222 18:16:12.377300 26703 slave.cpp:324] Slave using credential for: test-principal
I0222 18:16:12.377439 26703 resources.cpp:576] Parsing resources as JSON failed: cpu:2;mem:2048;disk(role1):2048
Trying semicolon-delimited string format instead
I0222 18:16:12.377804 26703 slave.cpp:464] Slave resources: cpu(*):2; mem(*):2048; disk(role1):2048; cpus(*):8; ports(*):[31000-32000]
I0222 18:16:12.377881 26703 slave.cpp:472] Slave attributes: [  ]
I0222 18:16:12.377889 26703 slave.cpp:477] Slave hostname: ip-172-30-2-148.mesosphere.io
I0222 18:16:12.378779 26701 state.cpp:58] Recovering state from '/tmp/DockerContainerizerTest_ROOT_DOCKER_LaunchWithPersistentVolumes_U5vZX1/meta'
I0222 18:16:12.379092 26697 status_update_manager.cpp:200] Recovering status update manager
I0222 18:16:12.379156 26681 sched.cpp:222] Version: 0.28.0
I0222 18:16:12.379250 26697 docker.cpp:722] Recovering Docker containers
I0222 18:16:12.379421 26703 slave.cpp:4565] Finished recovery
I0222 18:16:12.379627 26700 sched.cpp:326] New master detected at master@172.30.2.148:35274
I0222 18:16:12.379735 26703 slave.cpp:4737] Querying resource estimator for oversubscribable resources
I0222 18:16:12.379765 26700 sched.cpp:382] Authenticating with master master@172.30.2.148:35274
I0222 18:16:12.379781 26700 sched.cpp:389] Using default CRAM-MD5 authenticatee
I0222 18:16:12.379964 26696 status_update_manager.cpp:174] Pausing sending status updates
I0222 18:16:12.379992 26702 authenticatee.cpp:121] Creating new client SASL connection
I0222 18:16:12.380030 26697 slave.cpp:796] New master detected at master@172.30.2.148:35274
I0222 18:16:12.380106 26697 slave.cpp:859] Authenticating with master master@172.30.2.148:35274
I0222 18:16:12.380127 26697 slave.cpp:864] Using default CRAM-MD5 authenticatee
I0222 18:16:12.380188 26699 master.cpp:5526] Authenticating scheduler-1850b1cd-3396-4479-b2f3-47ee6c3fa270@172.30.2.148:35274
I0222 18:16:12.380269 26700 authenticator.cpp:413] Starting authentication session for crammd5_authenticatee(832)@172.30.2.148:35274
I0222 18:16:12.380280 26698 authenticatee.cpp:121] Creating new client SASL connection
I0222 18:16:12.380307 26697 slave.cpp:832] Detecting new master
I0222 18:16:12.380450 26697 slave.cpp:4751] Received oversubscribable resources  from the resource estimator
I0222 18:16:12.380452 26699 master.cpp:5526] Authenticating slave(396)@172.30.2.148:35274
I0222 18:16:12.380506 26698 authenticator.cpp:98] Creating new server SASL connection
I0222 18:16:12.380540 26697 authenticator.cpp:413] Starting authentication session for crammd5_authenticatee(833)@172.30.2.148:35274
I0222 18:16:12.380635 26700 authenticatee.cpp:212] Received SASL authentication mechanisms: CRAM-MD5
I0222 18:16:12.380659 26700 authenticatee.cpp:238] Attempting to authenticate with mechanism 'CRAM-MD5'
I0222 18:16:12.380762 26700 authenticator.cpp:203] Received SASL authentication start
I0222 18:16:12.380765 26701 authenticator.cpp:98] Creating new server SASL connection
I0222 18:16:12.380843 26700 authenticator.cpp:325] Authentication requires more steps
I0222 18:16:12.380911 26698 authenticatee.cpp:212] Received SASL authentication mechanisms: CRAM-MD5
I0222 18:16:12.380931 26702 authenticatee.cpp:258] Received SASL authentication step
I0222 18:16:12.380936 26698 authenticatee.cpp:238] Attempting to authenticate with mechanism 'CRAM-MD5'
I0222 18:16:12.381036 26702 authenticator.cpp:231] Received SASL authentication step
I0222 18:16:12.381052 26698 authenticator.cpp:203] Received SASL authentication start
I0222 18:16:12.381062 26702 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: 'ip-172-30-2-148' server FQDN: 'ip-172-30-2-148' SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0222 18:16:12.381072 26702 auxprop.cpp:179] Looking up auxiliary property '*userPassword'
I0222 18:16:12.381104 26702 auxprop.cpp:179] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0222 18:16:12.381104 26698 authenticator.cpp:325] Authentication requires more steps
I0222 18:16:12.381134 26702 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: 'ip-172-30-2-148' server FQDN: 'ip-172-30-2-148' SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0222 18:16:12.381142 26702 auxprop.cpp:129] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0222 18:16:12.381147 26702 auxprop.cpp:129] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0222 18:16:12.381162 26702 authenticator.cpp:317] Authentication success
I0222 18:16:12.381184 26698 authenticatee.cpp:258] Received SASL authentication step
I0222 18:16:12.381247 26699 authenticatee.cpp:298] Authentication success
I0222 18:16:12.381283 26696 authenticator.cpp:231] Received SASL authentication step
I0222 18:16:12.381311 26696 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: 'ip-172-30-2-148' server FQDN: 'ip-172-30-2-148' SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0222 18:16:12.381325 26696 auxprop.cpp:179] Looking up auxiliary property '*userPassword'
I0222 18:16:12.381319 26701 master.cpp:5556] Successfully authenticated principal 'test-principal' at scheduler-1850b1cd-3396-4479-b2f3-47ee6c3fa270@172.30.2.148:35274
I0222 18:16:12.381345 26700 authenticator.cpp:431] Authentication session cleanup for crammd5_authenticatee(832)@172.30.2.148:35274
I0222 18:16:12.381361 26696 auxprop.cpp:179] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0222 18:16:12.381397 26696 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: 'ip-172-30-2-148' server FQDN: 'ip-172-30-2-148' SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0222 18:16:12.381413 26696 auxprop.cpp:129] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0222 18:16:12.381422 26696 auxprop.cpp:129] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0222 18:16:12.381441 26696 authenticator.cpp:317] Authentication success
I0222 18:16:12.381548 26698 sched.cpp:471] Successfully authenticated with master master@172.30.2.148:35274
I0222 18:16:12.381563 26698 sched.cpp:776] Sending SUBSCRIBE call to master@172.30.2.148:35274
I0222 18:16:12.381634 26700 authenticatee.cpp:298] Authentication success
I0222 18:16:12.381660 26698 sched.cpp:809] Will retry registration in 770.60771ms if necessary
I0222 18:16:12.381675 26697 master.cpp:5556] Successfully authenticated principal 'test-principal' at slave(396)@172.30.2.148:35274
I0222 18:16:12.381734 26702 authenticator.cpp:431] Authentication session cleanup for crammd5_authenticatee(833)@172.30.2.148:35274
I0222 18:16:12.381811 26697 master.cpp:2280] Received SUBSCRIBE call for framework 'default' at scheduler-1850b1cd-3396-4479-b2f3-47ee6c3fa270@172.30.2.148:35274
I0222 18:16:12.381882 26697 master.cpp:1751] Authorizing framework principal 'test-principal' to receive offers for role 'role1'
I0222 18:16:12.382004 26698 slave.cpp:927] Successfully authenticated with master master@172.30.2.148:35274
I0222 18:16:12.382123 26698 slave.cpp:1321] Will retry registration in 8.1941ms if necessary
I0222 18:16:12.382282 26701 master.cpp:4240] Registering slave at slave(396)@172.30.2.148:35274 (ip-172-30-2-148.mesosphere.io) with id 652149b4-3932-4d8b-ba6f-8c9d9045be70-S0
I0222 18:16:12.382482 26701 master.cpp:2351] Subscribing framework default with checkpointing disabled and capabilities [  ]
I0222 18:16:12.382612 26703 registrar.cpp:439] Applied 1 operations in 46327ns; attempting to update the 'registry'
I0222 18:16:12.382829 26699 hierarchical.cpp:265] Added framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000
I0222 18:16:12.382910 26699 hierarchical.cpp:1434] No resources available to allocate!
I0222 18:16:12.382915 26701 sched.cpp:703] Framework registered with 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000
I0222 18:16:12.382936 26699 hierarchical.cpp:1529] No inverse offers to send out!
I0222 18:16:12.382953 26699 hierarchical.cpp:1127] Performed allocation for 0 slaves in 89949ns
I0222 18:16:12.382982 26701 sched.cpp:717] Scheduler::registered took 46498ns
I0222 18:16:12.383536 26698 log.cpp:683] Attempting to append 423 bytes to the log
I0222 18:16:12.383628 26699 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 3
I0222 18:16:12.384196 26700 replica.cpp:537] Replica received write request for position 3 from (13775)@172.30.2.148:35274
I0222 18:16:12.386602 26700 leveldb.cpp:341] Persisting action (442 bytes) to leveldb took 2.377119ms
I0222 18:16:12.386625 26700 replica.cpp:712] Persisted action at 3
I0222 18:16:12.387104 26698 replica.cpp:691] Replica received learned notice for position 3 from @0.0.0.0:0
I0222 18:16:12.389159 26698 leveldb.cpp:341] Persisting action (444 bytes) to leveldb took 2.032301ms
I0222 18:16:12.389181 26698 replica.cpp:712] Persisted action at 3
I0222 18:16:12.389196 26698 replica.cpp:697] Replica learned APPEND action at position 3
I0222 18:16:12.390281 26698 registrar.cpp:484] Successfully updated the 'registry' in 7.619072ms
I0222 18:16:12.390444 26702 log.cpp:702] Attempting to truncate the log to 3
I0222 18:16:12.390569 26701 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 4
I0222 18:16:12.390904 26701 slave.cpp:3482] Received ping from slave-observer(364)@172.30.2.148:35274
I0222 18:16:12.391054 26700 master.cpp:4308] Registered slave 652149b4-3932-4d8b-ba6f-8c9d9045be70-S0 at slave(396)@172.30.2.148:35274 (ip-172-30-2-148.mesosphere.io) with cpu(*):2; mem(*):2048; disk(role1):2048; cpus(*):8; ports(*):[31000-32000]
I0222 18:16:12.391144 26703 slave.cpp:971] Registered with master master@172.30.2.148:35274; given slave ID 652149b4-3932-4d8b-ba6f-8c9d9045be70-S0
I0222 18:16:12.391168 26703 fetcher.cpp:81] Clearing fetcher cache
I0222 18:16:12.391238 26700 replica.cpp:537] Replica received write request for position 4 from (13776)@172.30.2.148:35274
I0222 18:16:12.391263 26701 status_update_manager.cpp:181] Resuming sending status updates
I0222 18:16:12.391304 26697 hierarchical.cpp:473] Added slave 652149b4-3932-4d8b-ba6f-8c9d9045be70-S0 (ip-172-30-2-148.mesosphere.io) with cpu(*):2; mem(*):2048; disk(role1):2048; cpus(*):8; ports(*):[31000-32000] (allocated: )
I0222 18:16:12.391388 26703 slave.cpp:994] Checkpointing SlaveInfo to '/tmp/DockerContainerizerTest_ROOT_DOCKER_LaunchWithPersistentVolumes_U5vZX1/meta/slaves/652149b4-3932-4d8b-ba6f-8c9d9045be70-S0/slave.info'
I0222 18:16:12.391636 26703 slave.cpp:1030] Forwarding total oversubscribed resources 
I0222 18:16:12.391772 26699 master.cpp:4649] Received update of slave 652149b4-3932-4d8b-ba6f-8c9d9045be70-S0 at slave(396)@172.30.2.148:35274 (ip-172-30-2-148.mesosphere.io) with total oversubscribed resources 
I0222 18:16:12.392011 26697 hierarchical.cpp:1529] No inverse offers to send out!
I0222 18:16:12.392053 26697 hierarchical.cpp:1147] Performed allocation for slave 652149b4-3932-4d8b-ba6f-8c9d9045be70-S0 in 708377ns
I0222 18:16:12.392307 26703 master.cpp:5355] Sending 1 offers to framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000 (default) at scheduler-1850b1cd-3396-4479-b2f3-47ee6c3fa270@172.30.2.148:35274
I0222 18:16:12.392374 26697 hierarchical.cpp:531] Slave 652149b4-3932-4d8b-ba6f-8c9d9045be70-S0 (ip-172-30-2-148.mesosphere.io) updated with oversubscribed resources  (total: cpu(*):2; mem(*):2048; disk(role1):2048; cpus(*):8; ports(*):[31000-32000], allocated: disk(role1):2048; cpu(*):2; mem(*):2048; cpus(*):8; ports(*):[31000-32000])
I0222 18:16:12.392500 26697 hierarchical.cpp:1434] No resources available to allocate!
I0222 18:16:12.392531 26697 hierarchical.cpp:1529] No inverse offers to send out!
I0222 18:16:12.392556 26697 hierarchical.cpp:1147] Performed allocation for slave 652149b4-3932-4d8b-ba6f-8c9d9045be70-S0 in 136779ns
I0222 18:16:12.392704 26701 sched.cpp:873] Scheduler::resourceOffers took 94330ns
I0222 18:16:12.393086 26681 resources.cpp:576] Parsing resources as JSON failed: cpus:1;mem:64;
Trying semicolon-delimited string format instead
I0222 18:16:12.393600 26700 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 2.326382ms
I0222 18:16:12.393625 26700 replica.cpp:712] Persisted action at 4
I0222 18:16:12.394162 26696 replica.cpp:691] Replica received learned notice for position 4 from @0.0.0.0:0
I0222 18:16:12.394533 26701 master.cpp:3138] Processing ACCEPT call for offers: [ 652149b4-3932-4d8b-ba6f-8c9d9045be70-O0 ] on slave 652149b4-3932-4d8b-ba6f-8c9d9045be70-S0 at slave(396)@172.30.2.148:35274 (ip-172-30-2-148.mesosphere.io) for framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000 (default) at scheduler-1850b1cd-3396-4479-b2f3-47ee6c3fa270@172.30.2.148:35274
I0222 18:16:12.394567 26701 master.cpp:2926] Authorizing principal 'test-principal' to create volumes
I0222 18:16:12.394628 26701 master.cpp:2825] Authorizing framework principal 'test-principal' to launch task 1 as user 'root'
I0222 18:16:12.395519 26701 master.cpp:3467] Applying CREATE operation for volumes disk(role1)[id1:path1]:64 from framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000 (default) at scheduler-1850b1cd-3396-4479-b2f3-47ee6c3fa270@172.30.2.148:35274 to slave 652149b4-3932-4d8b-ba6f-8c9d9045be70-S0 at slave(396)@172.30.2.148:35274 (ip-172-30-2-148.mesosphere.io)
I0222 18:16:12.395808 26701 master.cpp:6589] Sending checkpointed resources disk(role1)[id1:path1]:64 to slave 652149b4-3932-4d8b-ba6f-8c9d9045be70-S0 at slave(396)@172.30.2.148:35274 (ip-172-30-2-148.mesosphere.io)
I0222 18:16:12.396316 26696 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 2.130659ms
I0222 18:16:12.396317 26703 slave.cpp:2341] Updated checkpointed resources from  to disk(role1)[id1:path1]:64
I0222 18:16:12.396368 26696 leveldb.cpp:399] Deleting ~2 keys from leveldb took 30004ns
I0222 18:16:12.396381 26696 replica.cpp:712] Persisted action at 4
I0222 18:16:12.396397 26696 replica.cpp:697] Replica learned TRUNCATE action at position 4
I0222 18:16:12.396533 26701 master.hpp:176] Adding task 1 with resources cpus(*):1; mem(*):64; disk(role1)[id1:path1]:64 on slave 652149b4-3932-4d8b-ba6f-8c9d9045be70-S0 (ip-172-30-2-148.mesosphere.io)
I0222 18:16:12.396680 26701 master.cpp:3623] Launching task 1 of framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000 (default) at scheduler-1850b1cd-3396-4479-b2f3-47ee6c3fa270@172.30.2.148:35274 with resources cpus(*):1; mem(*):64; disk(role1)[id1:path1]:64 on slave 652149b4-3932-4d8b-ba6f-8c9d9045be70-S0 at slave(396)@172.30.2.148:35274 (ip-172-30-2-148.mesosphere.io)
I0222 18:16:12.397009 26696 slave.cpp:1361] Got assigned task 1 for framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000
I0222 18:16:12.397143 26696 resources.cpp:576] Parsing resources as JSON failed: cpus:0.1;mem:32
Trying semicolon-delimited string format instead
I0222 18:16:12.397306 26699 hierarchical.cpp:653] Updated allocation of framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000 on slave 652149b4-3932-4d8b-ba6f-8c9d9045be70-S0 from disk(role1):2048; cpu(*):2; mem(*):2048; cpus(*):8; ports(*):[31000-32000] to disk(role1):1984; cpu(*):2; mem(*):2048; cpus(*):8; ports(*):[31000-32000]; disk(role1)[id1:path1]:64
I0222 18:16:12.397625 26699 hierarchical.cpp:892] Recovered disk(role1):1984; cpu(*):2; mem(*):1984; cpus(*):7; ports(*):[31000-32000] (total: cpu(*):2; mem(*):2048; disk(role1):1984; cpus(*):8; ports(*):[31000-32000]; disk(role1)[id1:path1]:64, allocated: disk(role1)[id1:path1]:64; cpus(*):1; mem(*):64) on slave 652149b4-3932-4d8b-ba6f-8c9d9045be70-S0 from framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000
I0222 18:16:12.397857 26696 slave.cpp:1480] Launching task 1 for framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000
I0222 18:16:12.397943 26696 resources.cpp:576] Parsing resources as JSON failed: cpus:0.1;mem:32
Trying semicolon-delimited string format instead
I0222 18:16:12.398560 26696 paths.cpp:474] Trying to chown '/tmp/DockerContainerizerTest_ROOT_DOCKER_LaunchWithPersistentVolumes_U5vZX1/slaves/652149b4-3932-4d8b-ba6f-8c9d9045be70-S0/frameworks/652149b4-3932-4d8b-ba6f-8c9d9045be70-0000/executors/1/runs/207172a3-0ebd-4faa-946b-75a829fc75fc' to user 'root'
I0222 18:16:12.403491 26696 slave.cpp:5367] Launching executor 1 of framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000 with resources cpus(*):0.1; mem(*):32 in work directory '/tmp/DockerContainerizerTest_ROOT_DOCKER_LaunchWithPersistentVolumes_U5vZX1/slaves/652149b4-3932-4d8b-ba6f-8c9d9045be70-S0/frameworks/652149b4-3932-4d8b-ba6f-8c9d9045be70-0000/executors/1/runs/207172a3-0ebd-4faa-946b-75a829fc75fc'
I0222 18:16:12.404115 26696 slave.cpp:1698] Queuing task '1' for executor '1' of framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000
I0222 18:16:12.405709 26696 slave.cpp:749] Successfully attached file '/tmp/DockerContainerizerTest_ROOT_DOCKER_LaunchWithPersistentVolumes_U5vZX1/slaves/652149b4-3932-4d8b-ba6f-8c9d9045be70-S0/frameworks/652149b4-3932-4d8b-ba6f-8c9d9045be70-0000/executors/1/runs/207172a3-0ebd-4faa-946b-75a829fc75fc'
I0222 18:16:12.408308 26697 docker.cpp:1019] Starting container '207172a3-0ebd-4faa-946b-75a829fc75fc' for task '1' (and executor '1') of framework '652149b4-3932-4d8b-ba6f-8c9d9045be70-0000'
I0222 18:16:12.408592 26697 docker.cpp:1053] Running docker -H unix:///var/run/docker.sock inspect alpine:latest
I0222 18:16:12.520663 26702 docker.cpp:390] Docker pull alpine completed
I0222 18:16:12.520853 26702 docker.cpp:479] Changing the ownership of the persistent volume at '/tmp/DockerContainerizerTest_ROOT_DOCKER_LaunchWithPersistentVolumes_U5vZX1/volumes/roles/role1/id1' with uid 0 and gid 0
I0222 18:16:12.524782 26702 docker.cpp:500] Mounting '/tmp/DockerContainerizerTest_ROOT_DOCKER_LaunchWithPersistentVolumes_U5vZX1/volumes/roles/role1/id1' to '/tmp/DockerContainerizerTest_ROOT_DOCKER_LaunchWithPersistentVolumes_U5vZX1/slaves/652149b4-3932-4d8b-ba6f-8c9d9045be70-S0/frameworks/652149b4-3932-4d8b-ba6f-8c9d9045be70-0000/executors/1/runs/207172a3-0ebd-4faa-946b-75a829fc75fc/path1' for persistent volume disk(role1)[id1:path1]:64 of container 207172a3-0ebd-4faa-946b-75a829fc75fc
I0222 18:16:12.580834 26700 slave.cpp:2643] Got registration for executor '1' of framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000 from executor(1)@172.30.2.148:56026
I0222 18:16:12.581961 26699 docker.cpp:1299] Ignoring updating container '207172a3-0ebd-4faa-946b-75a829fc75fc' with resources passed to update is identical to existing resources
I0222 18:16:12.582307 26698 slave.cpp:1863] Sending queued task '1' to executor '1' of framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000 at executor(1)@172.30.2.148:56026
I0222 18:16:13.295573 26703 slave.cpp:3002] Handling status update TASK_RUNNING (UUID: 0f6cc8cc-cd72-4bda-ba53-2e573ea1e0a0) for task 1 of framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000 from executor(1)@172.30.2.148:56026
I0222 18:16:13.295940 26703 slave.cpp:3002] Handling status update TASK_FINISHED (UUID: ed5a5eb7-65cc-42fa-bb85-3aaf65d86e6b) for task 1 of framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000 from executor(1)@172.30.2.148:56026
I0222 18:16:13.296381 26701 status_update_manager.cpp:320] Received status update TASK_RUNNING (UUID: 0f6cc8cc-cd72-4bda-ba53-2e573ea1e0a0) for task 1 of framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000
I0222 18:16:13.296422 26701 status_update_manager.cpp:497] Creating StatusUpdate stream for task 1 of framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000
I0222 18:16:13.296422 26703 slave.cpp:5677] Terminating task 1
I0222 18:16:13.296839 26701 status_update_manager.cpp:374] Forwarding update TASK_RUNNING (UUID: 0f6cc8cc-cd72-4bda-ba53-2e573ea1e0a0) for task 1 of framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000 to the slave
I0222 18:16:13.296902 26702 docker.cpp:766] Running docker -H unix:///var/run/docker.sock inspect mesos-652149b4-3932-4d8b-ba6f-8c9d9045be70-S0.207172a3-0ebd-4faa-946b-75a829fc75fc
I0222 18:16:13.299427 26699 slave.cpp:3400] Forwarding the update TASK_RUNNING (UUID: 0f6cc8cc-cd72-4bda-ba53-2e573ea1e0a0) for task 1 of framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000 to master@172.30.2.148:35274
I0222 18:16:13.299921 26699 slave.cpp:3294] Status update manager successfully handled status update TASK_RUNNING (UUID: 0f6cc8cc-cd72-4bda-ba53-2e573ea1e0a0) for task 1 of framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000
I0222 18:16:13.299969 26699 slave.cpp:3310] Sending acknowledgement for status update TASK_RUNNING (UUID: 0f6cc8cc-cd72-4bda-ba53-2e573ea1e0a0) for task 1 of framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000 to executor(1)@172.30.2.148:56026
I0222 18:16:13.300130 26696 master.cpp:4794] Status update TASK_RUNNING (UUID: 0f6cc8cc-cd72-4bda-ba53-2e573ea1e0a0) for task 1 of framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000 from slave 652149b4-3932-4d8b-ba6f-8c9d9045be70-S0 at slave(396)@172.30.2.148:35274 (ip-172-30-2-148.mesosphere.io)
I0222 18:16:13.300176 26696 master.cpp:4842] Forwarding status update TASK_RUNNING (UUID: 0f6cc8cc-cd72-4bda-ba53-2e573ea1e0a0) for task 1 of framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000
I0222 18:16:13.300375 26696 master.cpp:6450] Updating the state of task 1 of framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000 (latest state: TASK_FINISHED, status update state: TASK_RUNNING)
I0222 18:16:13.300765 26703 sched.cpp:981] Scheduler::statusUpdate took 164263ns
I0222 18:16:13.300962 26700 hierarchical.cpp:892] Recovered cpus(*):1; mem(*):64; disk(role1)[id1:path1]:64 (total: cpu(*):2; mem(*):2048; disk(role1):1984; cpus(*):8; ports(*):[31000-32000]; disk(role1)[id1:path1]:64, allocated: ) on slave 652149b4-3932-4d8b-ba6f-8c9d9045be70-S0 from framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000
I0222 18:16:13.301178 26699 master.cpp:3952] Processing ACKNOWLEDGE call 0f6cc8cc-cd72-4bda-ba53-2e573ea1e0a0 for task 1 of framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000 (default) at scheduler-1850b1cd-3396-4479-b2f3-47ee6c3fa270@172.30.2.148:35274 on slave 652149b4-3932-4d8b-ba6f-8c9d9045be70-S0
I0222 18:16:13.301450 26699 status_update_manager.cpp:392] Received status update acknowledgement (UUID: 0f6cc8cc-cd72-4bda-ba53-2e573ea1e0a0) for task 1 of framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000
I0222 18:16:13.301697 26701 slave.cpp:2412] Status update manager successfully handled status update acknowledgement (UUID: 0f6cc8cc-cd72-4bda-ba53-2e573ea1e0a0) for task 1 of framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000
I0222 18:16:13.327133 26697 status_update_manager.cpp:320] Received status update TASK_FINISHED (UUID: ed5a5eb7-65cc-42fa-bb85-3aaf65d86e6b) for task 1 of framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000
I0222 18:16:13.327280 26697 status_update_manager.cpp:374] Forwarding update TASK_FINISHED (UUID: ed5a5eb7-65cc-42fa-bb85-3aaf65d86e6b) for task 1 of framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000 to the slave
I0222 18:16:13.327481 26696 slave.cpp:3400] Forwarding the update TASK_FINISHED (UUID: ed5a5eb7-65cc-42fa-bb85-3aaf65d86e6b) for task 1 of framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000 to master@172.30.2.148:35274
I0222 18:16:13.327621 26696 slave.cpp:3294] Status update manager successfully handled status update TASK_FINISHED (UUID: ed5a5eb7-65cc-42fa-bb85-3aaf65d86e6b) for task 1 of framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000
I0222 18:16:13.327679 26696 slave.cpp:3310] Sending acknowledgement for status update TASK_FINISHED (UUID: ed5a5eb7-65cc-42fa-bb85-3aaf65d86e6b) for task 1 of framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000 to executor(1)@172.30.2.148:56026
I0222 18:16:13.327800 26698 master.cpp:4794] Status update TASK_FINISHED (UUID: ed5a5eb7-65cc-42fa-bb85-3aaf65d86e6b) for task 1 of framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000 from slave 652149b4-3932-4d8b-ba6f-8c9d9045be70-S0 at slave(396)@172.30.2.148:35274 (ip-172-30-2-148.mesosphere.io)
I0222 18:16:13.327850 26698 master.cpp:4842] Forwarding status update TASK_FINISHED (UUID: ed5a5eb7-65cc-42fa-bb85-3aaf65d86e6b) for task 1 of framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000
I0222 18:16:13.327977 26698 master.cpp:6450] Updating the state of task 1 of framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000 (latest state: TASK_FINISHED, status update state: TASK_FINISHED)
I0222 18:16:13.328248 26699 sched.cpp:981] Scheduler::statusUpdate took 100279ns
I0222 18:16:13.328588 26700 master.cpp:3952] Processing ACKNOWLEDGE call ed5a5eb7-65cc-42fa-bb85-3aaf65d86e6b for task 1 of framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000 (default) at scheduler-1850b1cd-3396-4479-b2f3-47ee6c3fa270@172.30.2.148:35274 on slave 652149b4-3932-4d8b-ba6f-8c9d9045be70-S0
I0222 18:16:13.328662 26681 sched.cpp:1903] Asked to stop the driver
I0222 18:16:13.328630 26700 master.cpp:6516] Removing task 1 with resources cpus(*):1; mem(*):64; disk(role1)[id1:path1]:64 of framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000 on slave 652149b4-3932-4d8b-ba6f-8c9d9045be70-S0 at slave(396)@172.30.2.148:35274 (ip-172-30-2-148.mesosphere.io)
I0222 18:16:13.328747 26697 sched.cpp:1143] Stopping framework '652149b4-3932-4d8b-ba6f-8c9d9045be70-0000'
I0222 18:16:13.329064 26696 status_update_manager.cpp:392] Received status update acknowledgement (UUID: ed5a5eb7-65cc-42fa-bb85-3aaf65d86e6b) for task 1 of framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000
I0222 18:16:13.329069 26700 master.cpp:5926] Processing TEARDOWN call for framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000 (default) at scheduler-1850b1cd-3396-4479-b2f3-47ee6c3fa270@172.30.2.148:35274
I0222 18:16:13.329100 26700 master.cpp:5938] Removing framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000 (default) at scheduler-1850b1cd-3396-4479-b2f3-47ee6c3fa270@172.30.2.148:35274
I0222 18:16:13.329200 26696 status_update_manager.cpp:528] Cleaning up status update stream for task 1 of framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000
I0222 18:16:13.329218 26703 hierarchical.cpp:375] Deactivated framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000
I0222 18:16:13.329309 26697 slave.cpp:2079] Asked to shut down framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000 by master@172.30.2.148:35274
I0222 18:16:13.329346 26697 slave.cpp:2104] Shutting down framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000
I0222 18:16:13.329418 26697 slave.cpp:4198] Shutting down executor '1' of framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000 at executor(1)@172.30.2.148:56026
I0222 18:16:13.329578 26699 hierarchical.cpp:326] Removed framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000
I0222 18:16:13.329684 26697 slave.cpp:2412] Status update manager successfully handled status update acknowledgement (UUID: ed5a5eb7-65cc-42fa-bb85-3aaf65d86e6b) for task 1 of framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000
I0222 18:16:13.329733 26697 slave.cpp:5718] Completing task 1
I0222 18:16:13.337236 26703 hierarchical.cpp:1434] No resources available to allocate!
I0222 18:16:13.337266 26703 hierarchical.cpp:1127] Performed allocation for 1 slaves in 153077ns
I0222 18:16:14.297827 26702 slave.cpp:3528] executor(1)@172.30.2.148:56026 exited
I0222 18:16:14.332489 26697 docker.cpp:1915] Executor for container '207172a3-0ebd-4faa-946b-75a829fc75fc' has exited
I0222 18:16:14.332512 26697 docker.cpp:1679] Destroying container '207172a3-0ebd-4faa-946b-75a829fc75fc'
I0222 18:16:14.332600 26697 docker.cpp:1807] Running docker stop on container '207172a3-0ebd-4faa-946b-75a829fc75fc'
I0222 18:16:14.333111 26697 docker.cpp:908] Unmounting volume for container '207172a3-0ebd-4faa-946b-75a829fc75fc'
I0222 18:16:14.333288 26700 slave.cpp:3886] Executor '1' of framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000 exited with status 0
I0222 18:16:14.333340 26700 slave.cpp:3990] Cleaning up executor '1' of framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000 at executor(1)@172.30.2.148:56026
I0222 18:16:14.333603 26703 gc.cpp:54] Scheduling '/tmp/DockerContainerizerTest_ROOT_DOCKER_LaunchWithPersistentVolumes_U5vZX1/slaves/652149b4-3932-4d8b-ba6f-8c9d9045be70-S0/frameworks/652149b4-3932-4d8b-ba6f-8c9d9045be70-0000/executors/1/runs/207172a3-0ebd-4faa-946b-75a829fc75fc' for gc 6.99999614056593days in the future
I0222 18:16:14.333669 26681 docker.cpp:766] Running docker -H unix:///var/run/docker.sock inspect mesos-652149b4-3932-4d8b-ba6f-8c9d9045be70-S0.207172a3-0ebd-4faa-946b-75a829fc75fc
I0222 18:16:14.333704 26700 slave.cpp:4078] Cleaning up framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000
I0222 18:16:14.333726 26703 gc.cpp:54] Scheduling '/tmp/DockerContainerizerTest_ROOT_DOCKER_LaunchWithPersistentVolumes_U5vZX1/slaves/652149b4-3932-4d8b-ba6f-8c9d9045be70-S0/frameworks/652149b4-3932-4d8b-ba6f-8c9d9045be70-0000/executors/1' for gc 6.99999613825185days in the future
I0222 18:16:14.336545 26703 gc.cpp:54] Scheduling '/tmp/DockerContainerizerTest_ROOT_DOCKER_LaunchWithPersistentVolumes_U5vZX1/slaves/652149b4-3932-4d8b-ba6f-8c9d9045be70-S0/frameworks/652149b4-3932-4d8b-ba6f-8c9d9045be70-0000' for gc 6.9999961115763days in the future
I0222 18:16:14.336699 26701 status_update_manager.cpp:282] Closing status update streams for framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000
I0222 18:16:14.338240 26699 hierarchical.cpp:1434] No resources available to allocate!
I0222 18:16:14.338270 26699 hierarchical.cpp:1127] Performed allocation for 1 slaves in 191822ns
I0222 18:16:14.635416 26681 docker.cpp:766] Running docker -H unix:///var/run/docker.sock inspect mesos-652149b4-3932-4d8b-ba6f-8c9d9045be70-S0.207172a3-0ebd-4faa-946b-75a829fc75fc
I0222 18:16:14.940042 26681 docker.cpp:766] Running docker -H unix:///var/run/docker.sock inspect mesos-652149b4-3932-4d8b-ba6f-8c9d9045be70-S0.207172a3-0ebd-4faa-946b-75a829fc75fc
I0222 18:16:15.245256 26681 docker.cpp:766] Running docker -H unix:///var/run/docker.sock inspect mesos-652149b4-3932-4d8b-ba6f-8c9d9045be70-S0.207172a3-0ebd-4faa-946b-75a829fc75fc
I0222 18:16:15.339015 26697 hierarchical.cpp:1434] No resources available to allocate!
I0222 18:16:15.339053 26697 hierarchical.cpp:1127] Performed allocation for 1 slaves in 265093ns
I0222 18:16:15.549804 26681 docker.cpp:766] Running docker -H unix:///var/run/docker.sock inspect mesos-652149b4-3932-4d8b-ba6f-8c9d9045be70-S0.207172a3-0ebd-4faa-946b-75a829fc75fc
I0222 18:16:15.854646 26681 docker.cpp:766] Running docker -H unix:///var/run/docker.sock inspect mesos-652149b4-3932-4d8b-ba6f-8c9d9045be70-S0.207172a3-0ebd-4faa-946b-75a829fc75fc
I0222 18:16:16.159210 26681 docker.cpp:766] Running docker -H unix:///var/run/docker.sock inspect mesos-652149b4-3932-4d8b-ba6f-8c9d9045be70-S0.207172a3-0ebd-4faa-946b-75a829fc75fc
I0222 18:16:16.339910 26698 hierarchical.cpp:1434] No resources available to allocate!
I0222 18:16:16.339951 26698 hierarchical.cpp:1127] Performed allocation for 1 slaves in 255857ns
I0222 18:16:16.463809 26681 docker.cpp:766] Running docker -H unix:///var/run/docker.sock inspect mesos-652149b4-3932-4d8b-ba6f-8c9d9045be70-S0.207172a3-0ebd-4faa-946b-75a829fc75fc
I0222 18:16:16.768708 26681 docker.cpp:766] Running docker -H unix:///var/run/docker.sock inspect mesos-652149b4-3932-4d8b-ba6f-8c9d9045be70-S0.207172a3-0ebd-4faa-946b-75a829fc75fc
I0222 18:16:17.073479 26681 docker.cpp:766] Running docker -H unix:///var/run/docker.sock inspect mesos-652149b4-3932-4d8b-ba6f-8c9d9045be70-S0.207172a3-0ebd-4faa-946b-75a829fc75fc
I0222 18:16:17.340798 26696 hierarchical.cpp:1434] No resources available to allocate!
I0222 18:16:17.340864 26696 hierarchical.cpp:1127] Performed allocation for 1 slaves in 260467ns
I0222 18:16:17.377902 26681 docker.cpp:766] Running docker -H unix:///var/run/docker.sock inspect mesos-652149b4-3932-4d8b-ba6f-8c9d9045be70-S0.207172a3-0ebd-4faa-946b-75a829fc75fc
I0222 18:16:17.683398 26681 docker.cpp:766] Running docker -H unix:///var/run/docker.sock inspect mesos-652149b4-3932-4d8b-ba6f-8c9d9045be70-S0.207172a3-0ebd-4faa-946b-75a829fc75fc
I0222 18:16:17.988231 26681 docker.cpp:766] Running docker -H unix:///var/run/docker.sock inspect mesos-652149b4-3932-4d8b-ba6f-8c9d9045be70-S0.207172a3-0ebd-4faa-946b-75a829fc75fc
I0222 18:16:18.292505 26681 docker.cpp:766] Running docker -H unix:///var/run/docker.sock inspect mesos-652149b4-3932-4d8b-ba6f-8c9d9045be70-S0.207172a3-0ebd-4faa-946b-75a829fc75fc
I0222 18:16:18.330112 26700 slave.cpp:4231] Framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000 seems to have exited. Ignoring shutdown timeout for executor '1'
I0222 18:16:18.341600 26702 hierarchical.cpp:1434] No resources available to allocate!
I0222 18:16:18.341634 26702 hierarchical.cpp:1127] Performed allocation for 1 slaves in 252012ns
I0222 18:16:18.596279 26681 docker.cpp:766] Running docker -H unix:///var/run/docker.sock inspect mesos-652149b4-3932-4d8b-ba6f-8c9d9045be70-S0.207172a3-0ebd-4faa-946b-75a829fc75fc
I0222 18:16:18.901157 26681 docker.cpp:766] Running docker -H unix:///var/run/docker.sock inspect mesos-652149b4-3932-4d8b-ba6f-8c9d9045be70-S0.207172a3-0ebd-4faa-946b-75a829fc75fc
I0222 18:16:19.204834 26681 docker.cpp:766] Running docker -H unix:///var/run/docker.sock inspect mesos-652149b4-3932-4d8b-ba6f-8c9d9045be70-S0.207172a3-0ebd-4faa-946b-75a829fc75fc
I0222 18:16:19.342326 26699 hierarchical.cpp:1434] No resources available to allocate!
I0222 18:16:19.342358 26699 hierarchical.cpp:1127] Performed allocation for 1 slaves in 186829ns
I0222 18:16:19.508533 26681 docker.cpp:766] Running docker -H unix:///var/run/docker.sock inspect mesos-652149b4-3932-4d8b-ba6f-8c9d9045be70-S0.207172a3-0ebd-4faa-946b-75a829fc75fc
I0222 18:16:19.812255 26681 docker.cpp:766] Running docker -H unix:///var/run/docker.sock inspect mesos-652149b4-3932-4d8b-ba6f-8c9d9045be70-S0.207172a3-0ebd-4faa-946b-75a829fc75fc
I0222 18:16:20.116345 26681 docker.cpp:766] Running docker -H unix:///var/run/docker.sock inspect mesos-652149b4-3932-4d8b-ba6f-8c9d9045be70-S0.207172a3-0ebd-4faa-946b-75a829fc75fc
I0222 18:16:20.343556 26698 hierarchical.cpp:1434] No resources available to allocate!
I0222 18:16:20.343588 26698 hierarchical.cpp:1127] Performed allocation for 1 slaves in 194704ns
I0222 18:16:20.420814 26681 docker.cpp:766] Running docker -H unix:///var/run/docker.sock inspect mesos-652149b4-3932-4d8b-ba6f-8c9d9045be70-S0.207172a3-0ebd-4faa-946b-75a829fc75fc
I0222 18:16:20.724819 26681 docker.cpp:766] Running docker -H unix:///var/run/docker.sock inspect mesos-652149b4-3932-4d8b-ba6f-8c9d9045be70-S0.207172a3-0ebd-4faa-946b-75a829fc75fc
I0222 18:16:21.029549 26681 docker.cpp:766] Running docker -H unix:///var/run/docker.sock inspect mesos-652149b4-3932-4d8b-ba6f-8c9d9045be70-S0.207172a3-0ebd-4faa-946b-75a829fc75fc
I0222 18:16:21.334319 26681 docker.cpp:766] Running docker -H unix:///var/run/docker.sock inspect mesos-652149b4-3932-4d8b-ba6f-8c9d9045be70-S0.207172a3-0ebd-4faa-946b-75a829fc75fc
I0222 18:16:21.344859 26702 hierarchical.cpp:1434] No resources available to allocate!
I0222 18:16:21.344892 26702 hierarchical.cpp:1127] Performed allocation for 1 slaves in 241099ns
I0222 18:16:21.638164 26681 docker.cpp:766] Running docker -H unix:///var/run/docker.sock inspect mesos-652149b4-3932-4d8b-ba6f-8c9d9045be70-S0.207172a3-0ebd-4faa-946b-75a829fc75fc
../../src/tests/containerizer/docker_containerizer_tests.cpp:1434: Failure
os::read(path::join(volumePath, ""file"")): Failed to open file '/tmp/DockerContainerizerTest_ROOT_DOCKER_LaunchWithPersistentVolumes_U5vZX1/volumes/roles/role1/id1/file': No such file or directory
I0222 18:16:21.943008 26703 master.cpp:1027] Master terminating
I0222 18:16:21.943635 26696 hierarchical.cpp:505] Removed slave 652149b4-3932-4d8b-ba6f-8c9d9045be70-S0
I0222 18:16:21.943989 26702 slave.cpp:3528] master@172.30.2.148:35274 exited
W0222 18:16:21.944016 26702 slave.cpp:3531] Master disconnected! Waiting for a new master to be elected
I0222 18:16:21.948807 26699 slave.cpp:668] Slave terminating
I0222 18:16:21.951902 26681 docker.cpp:885] Running docker -H unix:///var/run/docker.sock ps -a
I0222 18:16:22.044273 26698 docker.cpp:766] Running docker -H unix:///var/run/docker.sock inspect mesos-652149b4-3932-4d8b-ba6f-8c9d9045be70-S0.207172a3-0ebd-4faa-946b-75a829fc75fc
I0222 18:16:22.148877 26681 docker.cpp:727] Running docker -H unix:///var/run/docker.sock rm -f -v 422bfef31d51d2d3d2aafcf49b3e502654354bd98a98b076f4089b9a8e274d05
[  FAILED  ] DockerContainerizerTest.ROOT_DOCKER_LaunchWithPersistentVolumes (10535 ms)
{code}",Bug,Major,kaysoky,,10020,Accepted,In Progress,DockerContainerizerTest.ROOT_DOCKER_LaunchWithPersistentVolumes fails on CentOS 6,2016-03-18T21:26:24.000+0000,MESOS-4736,2.0,mesos,
neilc,2016-02-22T01:32:13.000+0000,neilc,This should let us remove the duplicated code in {{http.cpp}} between {{model(Framework)}} and {{json(Full<Framework>)}}.,Improvement,Major,neilc,2016-02-28T03:21:36.000+0000,5,Resolved,Complete,Update /frameworks to use jsonify,2016-02-28T03:21:37.000+0000,MESOS-4731,3.0,mesos,Mesosphere Sprint 29
js84,2016-02-19T19:22:13.000+0000,js84,"The interface examples are slightly out of sync with scheduler.hpp, most notably missing the new acceptOffers call.",Documentation,Major,js84,2016-03-14T10:23:58.000+0000,5,Resolved,Complete,Document scheduler driver calls in framework development guide.,2016-03-14T12:13:44.000+0000,MESOS-4726,2.0,mesos,Mesosphere Sprint 30
bbannier,2016-02-19T12:06:22.000+0000,bbannier,We currently expose information on set quotas via dedicated quota endpoints. To diagnose allocator problems one additionally needs information about used quotas.,Improvement,Major,bbannier,2016-03-22T19:12:12.000+0000,5,Resolved,Complete,Add allocator metric for currently satisfied quotas,2016-03-22T19:12:12.000+0000,MESOS-4723,2.0,mesos,Mesosphere Sprint 29
bbannier,2016-02-19T12:05:32.000+0000,bbannier,To diagnose scenarios where frameworks unexpectedly do not receive offers information on currently active filters are needed.,Improvement,Major,bbannier,2016-03-26T00:13:29.000+0000,5,Resolved,Complete,Add allocator metric for number of active offer filters,2016-03-26T00:13:30.000+0000,MESOS-4722,1.0,mesos,Mesosphere Sprint 29
bbannier,2016-02-19T12:04:46.000+0000,bbannier,"The allocation algorithm has grown to become fairly expensive, gaining visibility into its latency enables monitoring and alerting.

Similar allocator timing-related information is already exposed in the log, but should also be exposed via an endpoint.",Improvement,Major,bbannier,2016-03-23T07:47:28.000+0000,5,Resolved,Complete,Expose allocation algorithm latency via a metric.,2016-03-23T07:47:28.000+0000,MESOS-4721,1.0,mesos,Mesosphere Sprint 29
bbannier,2016-02-19T12:03:48.000+0000,bbannier,"Exposing the current allocation breakdown as seen by the allocator will allow us to correlated the corresponding metrics in the master with what the allocator sees. We should expose at least allocated or available, and total.",Improvement,Major,bbannier,2016-03-23T01:02:33.000+0000,5,Resolved,Complete,Add allocator metrics for total vs offered/allocated resources.,2016-03-23T01:02:33.000+0000,MESOS-4720,2.0,mesos,Mesosphere Sprint 29
bbannier,2016-02-19T12:00:17.000+0000,bbannier,"A counter for the number of allocations to a framework can be used to monitor allocation progress, e.g., when agents are added to a cluster, and as other frameworks are added or removed.

Currently, an offer by the hierarchical allocator to a framework consists of a list of resources on possibly many agents. Resources might be offered in order to satisfy outstanding quota or for fairness. To capture allocations on fine granularity we should not count the number of offers, but instead the pieces making up that offer, as such a metric would better resolve the effect of changes (e.g., adding/removing a framework).
",Improvement,Major,bbannier,,10006,Reviewable,New,Add allocator metric for number of offers each framework received,2016-03-31T14:27:26.000+0000,MESOS-4719,2.0,mesos,Mesosphere Sprint 29
karya,2016-02-18T22:39:02.000+0000,karya,There is a missing '$(DESTDIR)' prefix in the install-data-hook that causes DESTDIR builds to be broken.,Bug,Major,karya,2016-02-18T22:54:41.000+0000,5,Resolved,Complete,"""make DESTDIR=<path> install"" broken",2016-02-18T22:54:41.000+0000,MESOS-4714,2.0,mesos,Mesosphere Sprint 29
vinodkone,2016-02-18T22:19:28.000+0000,vinodkone,"Instead of failing hard, ReviewBot should post an error to the review that a circular dependency is detected.",Task,Major,vinodkone,2016-02-19T23:04:37.000+0000,5,Resolved,Complete,ReviewBot should not fail hard if there are circular dependencies in a review chain,2016-02-19T23:04:37.000+0000,MESOS-4713,2.0,mesos,Mesosphere Sprint 29
vinodkone,2016-02-18T22:12:51.000+0000,vinodkone,"We/I introduced the `force` field in SUBSCRIBE call to deal with scheduler partition cases. Having thought a bit more and discussing with few other folks ([~anandmazumdar], [~greggomann]), I think we can get away from not having that field in the v1 API. The obvious advantage of removing the field is that framework devs don't have to think about how/when to set the field (the current semantics are a bit confusing).

The new workflow when a master receives a SUBSCRIBE call is that master always accepts this call and closes any existing connection (after sending ERROR event) from the same scheduler (identified by framework id).  

The expectation from schedulers is that they must close the old subscribe connection before resending a new SUBSCRIBE call.

Lets look at some tricky scenarios and see how this works and why it is safe.

1) Connection disconnection @ the scheduler but not @ the master
   
Scheduler sees the disconnection and sends a new SUBSCRIBE call. Master sends ERROR on the old connection (won't be received by the scheduler because the connection is already closed) and closes it.

2) Connection disconnection @ master but not @ scheduler

Scheduler realizes this from lack of HEARTBEAT events. It then closes its existing connection and sends a new SUBSCRIBE call. Master accepts the new SUBSCRIBE call. There is no old connection to close on the master as it is already closed.

3) Scheduler failover but no disconnection @ master

Newly elected scheduler sends a SUBSCRIBE call. Master sends ERROR event and closes the old connection (won't be received because the old scheduler failed over).

4) If Scheduler A got partitioned (but is alive and connected with master) and Scheduler B got elected as new leader.

When Scheduler B sends SUBSCRIBE, master sends ERROR and closes the connection from Scheduler A. Master accepts Scheduler B's connection. Typically Scheduler A aborts after receiving ERROR and gets restarted. After restart it won't become the leader because Scheduler B is already elected.

5) Scheduler sends SUBSCRIBE, times out, closes the SUBSCRIBE connection (A) and sends a new SUBSCRIBE (B). Master receives SUBSCRIBE (B) and then receives SUBSCRIBE (A) but doesn't see A's disconnection yet.

Master first accepts SUBSCRIBE (B). After it receives SUBSCRIBE (A), it sends ERROR to SUBSCRIBE (B) and closes that connection. When it accepts SUBSCRIBE (A) and tries to send SUBSCRIBED event the connection closure is detected. Scheduler retries the SUBSCRIBE connection after a backoff. I think this is a rare enough race for it to happen continuously in a loop.

",Task,Major,vinodkone,2016-03-03T02:11:08.000+0000,5,Resolved,Complete,Remove 'force' field from the Subscribe Call in v1 Scheduler API,2016-03-03T02:11:08.000+0000,MESOS-4712,5.0,mesos,Mesosphere Sprint 30
hausdorff,2016-02-18T03:50:35.000+0000,hausdorff,"Stout configuration is replicated in at least 3 configuration files -- stout itself, libprocess, and agent. More will follow in the future.

We should make a StoutConfigure.cmake that can be included by any package downstream.",Bug,Major,hausdorff,2016-03-01T18:43:19.000+0000,5,Resolved,Complete,"Make Stout configuration modular and consumable by downstream (e.g., libprocess and agent)",2016-03-01T18:43:19.000+0000,MESOS-4703,1.0,mesos,Mesosphere Sprint 30
neilc,2016-02-18T01:28:15.000+0000,neilc,"There isn't a default value (i.e., offers do not timeout by default), but we should clarify this in {{flags.cpp}} and {{configuration.md}}.",Documentation,Minor,neilc,2016-03-01T21:31:36.000+0000,5,Resolved,Complete,"Document default value of ""offer_timeout""",2016-03-01T21:31:36.000+0000,MESOS-4702,1.0,mesos,Mesosphere Sprint 30
,2016-02-17T20:01:43.000+0000,greggomann,"Currently, we require a framework or operator to specify `ReservationInfo.principal` when they reserve resources. This isn't necessary, however; we already know the principal and can fill in the field if it isn't set already.",Improvement,Major,greggomann,2016-03-02T20:54:33.000+0000,5,Resolved,Complete,Allow Reserve operations by a principal without `ReservationInfo.principal`,2016-03-02T20:54:33.000+0000,MESOS-4696,2.0,mesos,
,2016-02-17T19:55:29.000+0000,kaysoky,"{code}
[ RUN      ] SlaveTest.StateEndpoint
../../src/tests/slave_tests.cpp:1220: Failure
Value of: state.values[""start_time""].as<JSON::Number>().as<int>()
  Actual: 1458159086
Expected: static_cast<int>(Clock::now().secs())
  Which is: 1458159085
[  FAILED  ] SlaveTest.StateEndpoint (193 ms)
{code}

Even though this test does {{Clock::pause()}} before starting the agent, there's a possibility that a numified-stringified double to not equal itself, even after rounding to the nearest int.",Bug,Major,kaysoky,,1,Open,New,SlaveTest.StateEndpoint is flaky,2016-02-17T19:55:29.000+0000,MESOS-4695,1.0,mesos,
neilc,2016-02-17T08:20:49.000+0000,mcypark,"With {{Labels}} being part of the {{ReservationInfo}}, we should ensure that we don't observe a significant performance degradation in the allocator.",Task,Major,mcypark,2016-03-02T06:53:44.000+0000,5,Resolved,Complete,Add a HierarchicalAllocator benchmark with reservation labels.,2016-03-02T06:53:46.000+0000,MESOS-4691,3.0,mesos,Mesosphere Sprint 29
karya,2016-02-16T21:28:59.000+0000,karya,"This issues is currently being discussed in the dev mailing list:
http://www.mail-archive.com/dev@mesos.apache.org/msg34349.html",Epic,Major,karya,,3,In Progress,In Progress,Reorganize 3rdparty directory,2016-04-27T16:13:17.000+0000,MESOS-4690,5.0,mesos,Mesosphere Sprint 33
klueska,2016-02-16T18:37:53.000+0000,vinodkone,We need to design how the v1 operator API (all the HTTP endpoints exposed by master/agent that are not for scheduler/executor interactions) looks and works.,Documentation,Major,vinodkone,,3,In Progress,In Progress,Design doc for v1 Operator API,2016-04-27T16:13:16.000+0000,MESOS-4689,8.0,mesos,Mesosphere Sprint 29
neilc,2016-02-16T18:13:36.000+0000,neilc,Design doc: https://docs.google.com/document/d/14qLxjZsfIpfynbx0USLJR0GELSq8hdZJUWw6kaY_DXc/edit?usp=sharing,Improvement,Major,neilc,2016-02-27T20:58:22.000+0000,5,Resolved,Complete,Implement reliable floating point for scalar resources,2016-04-06T23:22:36.000+0000,MESOS-4687,5.0,mesos,Mesosphere Sprint 29
anandmazumdar,2016-02-16T18:13:36.000+0000,anandmazumdar,"Currently, the scheduler library creates its own {{MasterDetector}} object internally. We would need to create a standalone detector and create new tests for testing that callbacks are invoked correctly in the event of a master failover.",Task,Major,anandmazumdar,2016-02-28T17:57:17.000+0000,5,Resolved,Complete,Implement master failover tests for the scheduler library.,2016-02-28T17:57:17.000+0000,MESOS-4686,3.0,mesos,Mesosphere Sprint 29
gilbert,2016-02-16T07:15:15.000+0000,gilbert,"This should be widely used for unified containerizer testing. Should basically include:

*at least one layer.
*repositories.

For each layer:
*root file system as a layer tar ball.
*docker image json (manifest).
*docker version.",Bug,Major,gilbert,2016-03-14T17:21:29.000+0000,5,Resolved,Complete,Create base docker image for test suite.,2016-03-14T17:21:29.000+0000,MESOS-4684,3.0,mesos,Mesosphere Sprint 29
gilbert,2016-02-16T07:07:19.000+0000,gilbert,"Should include the following information:

*What features are currently supported in docker runtime isolator.
*How to use the docker runtime isolator (user manual).
*Compare the different semantics v.s. docker containerizer, and explain why.",Bug,Major,gilbert,2016-03-02T02:28:08.000+0000,5,Resolved,Complete,Document docker runtime isolator.,2016-03-02T02:28:08.000+0000,MESOS-4683,2.0,mesos,Mesosphere Sprint 29
chenzhiwei,2016-02-15T19:43:40.000+0000,neilc,"We currently vendor Protobuf 2.5.0. We should upgrade to Protobuf 2.6.1. This introduces various bugfixes, performance improvements, and at least one new feature we might want to eventually take advantage of ({{map}} data type). AFAIK there should be no backward compatibility concerns.",Improvement,Major,neilc,2016-03-31T18:07:46.000+0000,5,Resolved,Complete,Upgrade vendored Protobuf to 2.6.1,2016-05-04T15:02:30.000+0000,MESOS-4678,3.0,mesos,Mesosphere Sprint 31
kaysoky,2016-02-15T17:36:37.000+0000,bernd-mesos,"{noformat}
[18:06:25][Step 8/8] [ RUN      ] DockerContainerizerTest.ROOT_DOCKER_Logs
[18:06:25][Step 8/8] I0215 17:06:25.256103  1740 leveldb.cpp:174] Opened db in 6.548327ms
[18:06:25][Step 8/8] I0215 17:06:25.258002  1740 leveldb.cpp:181] Compacted db in 1.837816ms
[18:06:25][Step 8/8] I0215 17:06:25.258059  1740 leveldb.cpp:196] Created db iterator in 22044ns
[18:06:25][Step 8/8] I0215 17:06:25.258076  1740 leveldb.cpp:202] Seeked to beginning of db in 2347ns
[18:06:25][Step 8/8] I0215 17:06:25.258091  1740 leveldb.cpp:271] Iterated through 0 keys in the db in 571ns
[18:06:25][Step 8/8] I0215 17:06:25.258152  1740 replica.cpp:779] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned
[18:06:25][Step 8/8] I0215 17:06:25.258936  1758 recover.cpp:447] Starting replica recovery
[18:06:25][Step 8/8] I0215 17:06:25.259177  1758 recover.cpp:473] Replica is in EMPTY status
[18:06:25][Step 8/8] I0215 17:06:25.260327  1757 replica.cpp:673] Replica in EMPTY status received a broadcasted recover request from (13608)@172.30.2.239:39785
[18:06:25][Step 8/8] I0215 17:06:25.260545  1758 recover.cpp:193] Received a recover response from a replica in EMPTY status
[18:06:25][Step 8/8] I0215 17:06:25.261065  1757 master.cpp:376] Master 112363e2-c680-4946-8fee-d0626ed8b21e (ip-172-30-2-239.mesosphere.io) started on 172.30.2.239:39785
[18:06:25][Step 8/8] I0215 17:06:25.261209  1761 recover.cpp:564] Updating replica status to STARTING
[18:06:25][Step 8/8] I0215 17:06:25.261086  1757 master.cpp:378] Flags at startup: --acls="""" --allocation_interval=""1secs"" --allocator=""HierarchicalDRF"" --authenticate=""true"" --authenticate_http=""true"" --authenticate_slaves=""true"" --authenticators=""crammd5"" --authorizers=""local"" --credentials=""/tmp/HncLLj/credentials"" --framework_sorter=""drf"" --help=""false"" --hostname_lookup=""true"" --http_authenticators=""basic"" --initialize_driver_logging=""true"" --log_auto_initialize=""true"" --logbufsecs=""0"" --logging_level=""INFO"" --max_completed_frameworks=""50"" --max_completed_tasks_per_framework=""1000"" --max_slave_ping_timeouts=""5"" --quiet=""false"" --recovery_slave_removal_limit=""100%"" --registry=""replicated_log"" --registry_fetch_timeout=""1mins"" --registry_store_timeout=""100secs"" --registry_strict=""true"" --root_submissions=""true"" --slave_ping_timeout=""15secs"" --slave_reregister_timeout=""10mins"" --user_sorter=""drf"" --version=""false"" --webui_dir=""/usr/local/share/mesos/webui"" --work_dir=""/tmp/HncLLj/master"" --zk_session_timeout=""10secs""
[18:06:25][Step 8/8] I0215 17:06:25.261446  1757 master.cpp:423] Master only allowing authenticated frameworks to register
[18:06:25][Step 8/8] I0215 17:06:25.261456  1757 master.cpp:428] Master only allowing authenticated slaves to register
[18:06:25][Step 8/8] I0215 17:06:25.261462  1757 credentials.hpp:35] Loading credentials for authentication from '/tmp/HncLLj/credentials'
[18:06:25][Step 8/8] I0215 17:06:25.261723  1757 master.cpp:468] Using default 'crammd5' authenticator
[18:06:25][Step 8/8] I0215 17:06:25.261855  1757 master.cpp:537] Using default 'basic' HTTP authenticator
[18:06:25][Step 8/8] I0215 17:06:25.262022  1757 master.cpp:571] Authorization enabled
[18:06:25][Step 8/8] I0215 17:06:25.262177  1755 hierarchical.cpp:144] Initialized hierarchical allocator process
[18:06:25][Step 8/8] I0215 17:06:25.262177  1758 whitelist_watcher.cpp:77] No whitelist given
[18:06:25][Step 8/8] I0215 17:06:25.262899  1760 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 1.517992ms
[18:06:25][Step 8/8] I0215 17:06:25.262924  1760 replica.cpp:320] Persisted replica status to STARTING
[18:06:25][Step 8/8] I0215 17:06:25.263144  1754 recover.cpp:473] Replica is in STARTING status
[18:06:25][Step 8/8] I0215 17:06:25.264010  1757 master.cpp:1712] The newly elected leader is master@172.30.2.239:39785 with id 112363e2-c680-4946-8fee-d0626ed8b21e
[18:06:25][Step 8/8] I0215 17:06:25.264044  1757 master.cpp:1725] Elected as the leading master!
[18:06:25][Step 8/8] I0215 17:06:25.264061  1757 master.cpp:1470] Recovering from registrar
[18:06:25][Step 8/8] I0215 17:06:25.264117  1760 replica.cpp:673] Replica in STARTING status received a broadcasted recover request from (13610)@172.30.2.239:39785
[18:06:25][Step 8/8] I0215 17:06:25.264197  1758 registrar.cpp:307] Recovering registrar
[18:06:25][Step 8/8] I0215 17:06:25.264827  1756 recover.cpp:193] Received a recover response from a replica in STARTING status
[18:06:25][Step 8/8] I0215 17:06:25.265219  1757 recover.cpp:564] Updating replica status to VOTING
[18:06:25][Step 8/8] I0215 17:06:25.267302  1754 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 1.887739ms
[18:06:25][Step 8/8] I0215 17:06:25.267326  1754 replica.cpp:320] Persisted replica status to VOTING
[18:06:25][Step 8/8] I0215 17:06:25.267453  1759 recover.cpp:578] Successfully joined the Paxos group
[18:06:25][Step 8/8] I0215 17:06:25.267632  1759 recover.cpp:462] Recover process terminated
[18:06:25][Step 8/8] I0215 17:06:25.268007  1757 log.cpp:659] Attempting to start the writer
[18:06:25][Step 8/8] I0215 17:06:25.269055  1759 replica.cpp:493] Replica received implicit promise request from (13611)@172.30.2.239:39785 with proposal 1
[18:06:25][Step 8/8] I0215 17:06:25.270488  1759 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 1.406068ms
[18:06:25][Step 8/8] I0215 17:06:25.270511  1759 replica.cpp:342] Persisted promised to 1
[18:06:25][Step 8/8] I0215 17:06:25.271078  1761 coordinator.cpp:238] Coordinator attempting to fill missing positions
[18:06:25][Step 8/8] I0215 17:06:25.272146  1756 replica.cpp:388] Replica received explicit promise request from (13612)@172.30.2.239:39785 for position 0 with proposal 2
[18:06:25][Step 8/8] I0215 17:06:25.273478  1756 leveldb.cpp:341] Persisting action (8 bytes) to leveldb took 1.297217ms
[18:06:25][Step 8/8] I0215 17:06:25.273500  1756 replica.cpp:712] Persisted action at 0
[18:06:25][Step 8/8] I0215 17:06:25.274355  1757 replica.cpp:537] Replica received write request for position 0 from (13613)@172.30.2.239:39785
[18:06:25][Step 8/8] I0215 17:06:25.274405  1757 leveldb.cpp:436] Reading position from leveldb took 25294ns
[18:06:25][Step 8/8] I0215 17:06:25.275800  1757 leveldb.cpp:341] Persisting action (14 bytes) to leveldb took 1.362978ms
[18:06:25][Step 8/8] I0215 17:06:25.275823  1757 replica.cpp:712] Persisted action at 0
[18:06:25][Step 8/8] I0215 17:06:25.276348  1755 replica.cpp:691] Replica received learned notice for position 0 from @0.0.0.0:0
[18:06:25][Step 8/8] I0215 17:06:25.277765  1755 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 1.391531ms
[18:06:25][Step 8/8] I0215 17:06:25.277788  1755 replica.cpp:712] Persisted action at 0
[18:06:25][Step 8/8] I0215 17:06:25.277802  1755 replica.cpp:697] Replica learned NOP action at position 0
[18:06:25][Step 8/8] I0215 17:06:25.278336  1754 log.cpp:675] Writer started with ending position 0
[18:06:25][Step 8/8] I0215 17:06:25.279371  1755 leveldb.cpp:436] Reading position from leveldb took 29214ns
[18:06:25][Step 8/8] I0215 17:06:25.280272  1758 registrar.cpp:340] Successfully fetched the registry (0B) in 16.02688ms
[18:06:25][Step 8/8] I0215 17:06:25.280385  1758 registrar.cpp:439] Applied 1 operations in 31040ns; attempting to update the 'registry'
[18:06:25][Step 8/8] I0215 17:06:25.281054  1755 log.cpp:683] Attempting to append 210 bytes to the log
[18:06:25][Step 8/8] I0215 17:06:25.281165  1757 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 1
[18:06:25][Step 8/8] I0215 17:06:25.281780  1757 replica.cpp:537] Replica received write request for position 1 from (13614)@172.30.2.239:39785
[18:06:25][Step 8/8] I0215 17:06:25.283159  1757 leveldb.cpp:341] Persisting action (229 bytes) to leveldb took 1.348041ms
[18:06:25][Step 8/8] I0215 17:06:25.283184  1757 replica.cpp:712] Persisted action at 1
[18:06:25][Step 8/8] I0215 17:06:25.283695  1759 replica.cpp:691] Replica received learned notice for position 1 from @0.0.0.0:0
[18:06:25][Step 8/8] I0215 17:06:25.285059  1759 leveldb.cpp:341] Persisting action (231 bytes) to leveldb took 1.334577ms
[18:06:25][Step 8/8] I0215 17:06:25.285084  1759 replica.cpp:712] Persisted action at 1
[18:06:25][Step 8/8] I0215 17:06:25.285099  1759 replica.cpp:697] Replica learned APPEND action at position 1
[18:06:25][Step 8/8] I0215 17:06:25.285910  1758 registrar.cpp:484] Successfully updated the 'registry' in 5.46816ms
[18:06:25][Step 8/8] I0215 17:06:25.286043  1758 registrar.cpp:370] Successfully recovered registrar
[18:06:25][Step 8/8] I0215 17:06:25.286121  1755 log.cpp:702] Attempting to truncate the log to 1
[18:06:25][Step 8/8] I0215 17:06:25.286301  1756 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 2
[18:06:25][Step 8/8] I0215 17:06:25.286478  1759 hierarchical.cpp:171] Skipping recovery of hierarchical allocator: nothing to recover
[18:06:25][Step 8/8] I0215 17:06:25.286476  1754 master.cpp:1522] Recovered 0 slaves from the Registry (171B) ; allowing 10mins for slaves to re-register
[18:06:25][Step 8/8] I0215 17:06:25.287137  1755 replica.cpp:537] Replica received write request for position 2 from (13615)@172.30.2.239:39785
[18:06:25][Step 8/8] I0215 17:06:25.289104  1755 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 1.938609ms
[18:06:25][Step 8/8] I0215 17:06:25.289127  1755 replica.cpp:712] Persisted action at 2
[18:06:25][Step 8/8] I0215 17:06:25.289667  1759 replica.cpp:691] Replica received learned notice for position 2 from @0.0.0.0:0
[18:06:25][Step 8/8] I0215 17:06:25.290956  1759 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 1.256421ms
[18:06:25][Step 8/8] I0215 17:06:25.291007  1759 leveldb.cpp:399] Deleting ~1 keys from leveldb took 28064ns
[18:06:25][Step 8/8] I0215 17:06:25.291021  1759 replica.cpp:712] Persisted action at 2
[18:06:25][Step 8/8] I0215 17:06:25.291038  1759 replica.cpp:697] Replica learned TRUNCATE action at position 2
[18:06:25][Step 8/8] I0215 17:06:25.300550  1760 slave.cpp:193] Slave started on 393)@172.30.2.239:39785
[18:06:25][Step 8/8] I0215 17:06:25.300573  1760 slave.cpp:194] Flags at startup: --appc_store_dir=""/tmp/mesos/store/appc"" --authenticatee=""crammd5"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --credential=""/tmp/DockerContainerizerTest_ROOT_DOCKER_Logs_a4NS2N/credential"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_auth_server=""https://auth.docker.io"" --docker_kill_orphans=""true"" --docker_puller_timeout=""60"" --docker_registry=""https://registry-1.docker.io"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/tmp/mesos/store/docker"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/tmp/DockerContainerizerTest_ROOT_DOCKER_Logs_a4NS2N/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --hostname_lookup=""true"" --image_provisioner_backend=""copy"" --initialize_driver_logging=""true"" --isolation=""posix/cpu,posix/mem"" --launcher_dir=""/mnt/teamcity/work/4240ba9ddd0997c3/build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""10ms"" --resources=""cpus:2;mem:1024;disk:1024;ports:[31000-32000]"" --revocable_cpu_low_priority=""true"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --systemd_runtime_directory=""/run/systemd/system"" --version=""false"" --work_dir=""/tmp/DockerContainerizerTest_ROOT_DOCKER_Logs_a4NS2N""
[18:06:25][Step 8/8] I0215 17:06:25.300868  1760 credentials.hpp:83] Loading credential for authentication from '/tmp/DockerContainerizerTest_ROOT_DOCKER_Logs_a4NS2N/credential'
[18:06:25][Step 8/8] I0215 17:06:25.301030  1760 slave.cpp:324] Slave using credential for: test-principal
[18:06:25][Step 8/8] I0215 17:06:25.301180  1760 resources.cpp:576] Parsing resources as JSON failed: cpus:2;mem:1024;disk:1024;ports:[31000-32000]
[18:06:25][Step 8/8] Trying semicolon-delimited string format instead
[18:06:25][Step 8/8] I0215 17:06:25.301553  1760 slave.cpp:464] Slave resources: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
[18:06:25][Step 8/8] I0215 17:06:25.301609  1760 slave.cpp:472] Slave attributes: [  ]
[18:06:25][Step 8/8] I0215 17:06:25.301620  1760 slave.cpp:477] Slave hostname: ip-172-30-2-239.mesosphere.io
[18:06:25][Step 8/8] I0215 17:06:25.302417  1757 state.cpp:58] Recovering state from '/tmp/DockerContainerizerTest_ROOT_DOCKER_Logs_a4NS2N/meta'
[18:06:25][Step 8/8] I0215 17:06:25.302515  1740 sched.cpp:222] Version: 0.28.0
[18:06:25][Step 8/8] I0215 17:06:25.302772  1755 status_update_manager.cpp:200] Recovering status update manager
[18:06:25][Step 8/8] I0215 17:06:25.302956  1758 docker.cpp:559] Recovering Docker containers
[18:06:25][Step 8/8] I0215 17:06:25.303050  1761 sched.cpp:326] New master detected at master@172.30.2.239:39785
[18:06:25][Step 8/8] I0215 17:06:25.303133  1754 slave.cpp:4565] Finished recovery
[18:06:25][Step 8/8] I0215 17:06:25.303154  1761 sched.cpp:382] Authenticating with master master@172.30.2.239:39785
[18:06:25][Step 8/8] I0215 17:06:25.303169  1761 sched.cpp:389] Using default CRAM-MD5 authenticatee
[18:06:25][Step 8/8] I0215 17:06:25.303364  1759 authenticatee.cpp:121] Creating new client SASL connection
[18:06:25][Step 8/8] I0215 17:06:25.303467  1754 slave.cpp:4737] Querying resource estimator for oversubscribable resources
[18:06:25][Step 8/8] I0215 17:06:25.303668  1756 master.cpp:5523] Authenticating scheduler-806c70e3-1cf6-418f-aa30-6bb26db42d18@172.30.2.239:39785
[18:06:25][Step 8/8] I0215 17:06:25.303707  1760 status_update_manager.cpp:174] Pausing sending status updates
[18:06:25][Step 8/8] I0215 17:06:25.303707  1754 slave.cpp:796] New master detected at master@172.30.2.239:39785
[18:06:25][Step 8/8] I0215 17:06:25.303767  1755 authenticator.cpp:413] Starting authentication session for crammd5_authenticatee(829)@172.30.2.239:39785
[18:06:25][Step 8/8] I0215 17:06:25.303791  1754 slave.cpp:859] Authenticating with master master@172.30.2.239:39785
[18:06:25][Step 8/8] I0215 17:06:25.303805  1754 slave.cpp:864] Using default CRAM-MD5 authenticatee
[18:06:25][Step 8/8] I0215 17:06:25.303956  1754 slave.cpp:832] Detecting new master
[18:06:25][Step 8/8] I0215 17:06:25.303971  1761 authenticatee.cpp:121] Creating new client SASL connection
[18:06:25][Step 8/8] I0215 17:06:25.303984  1760 authenticator.cpp:98] Creating new server SASL connection
[18:06:25][Step 8/8] I0215 17:06:25.304131  1754 slave.cpp:4751] Received oversubscribable resources  from the resource estimator
[18:06:25][Step 8/8] I0215 17:06:25.304275  1757 master.cpp:5523] Authenticating slave(393)@172.30.2.239:39785
[18:06:25][Step 8/8] I0215 17:06:25.304344  1754 authenticatee.cpp:212] Received SASL authentication mechanisms: CRAM-MD5
[18:06:25][Step 8/8] I0215 17:06:25.304369  1754 authenticatee.cpp:238] Attempting to authenticate with mechanism 'CRAM-MD5'
[18:06:25][Step 8/8] I0215 17:06:25.304373  1761 authenticator.cpp:413] Starting authentication session for crammd5_authenticatee(830)@172.30.2.239:39785
[18:06:25][Step 8/8] I0215 17:06:25.304440  1757 authenticator.cpp:203] Received SASL authentication start
[18:06:25][Step 8/8] I0215 17:06:25.304491  1757 authenticator.cpp:325] Authentication requires more steps
[18:06:25][Step 8/8] I0215 17:06:25.304548  1754 authenticator.cpp:98] Creating new server SASL connection
[18:06:25][Step 8/8] I0215 17:06:25.304582  1761 authenticatee.cpp:258] Received SASL authentication step
[18:06:25][Step 8/8] I0215 17:06:25.304688  1761 authenticator.cpp:231] Received SASL authentication step
[18:06:25][Step 8/8] I0215 17:06:25.304714  1761 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: 'ip-172-30-2-239.mesosphere.io' server FQDN: 'ip-172-30-2-239.mesosphere.io' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
[18:06:25][Step 8/8] I0215 17:06:25.304723  1761 auxprop.cpp:179] Looking up auxiliary property '*userPassword'
[18:06:25][Step 8/8] I0215 17:06:25.304767  1761 auxprop.cpp:179] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
[18:06:25][Step 8/8] I0215 17:06:25.304805  1761 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: 'ip-172-30-2-239.mesosphere.io' server FQDN: 'ip-172-30-2-239.mesosphere.io' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
[18:06:25][Step 8/8] I0215 17:06:25.304817  1761 auxprop.cpp:129] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
[18:06:25][Step 8/8] I0215 17:06:25.304824  1761 auxprop.cpp:129] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
[18:06:25][Step 8/8] I0215 17:06:25.304836  1761 authenticator.cpp:317] Authentication success
[18:06:25][Step 8/8] I0215 17:06:25.304841  1758 authenticatee.cpp:212] Received SASL authentication mechanisms: CRAM-MD5
[18:06:25][Step 8/8] I0215 17:06:25.304870  1758 authenticatee.cpp:238] Attempting to authenticate with mechanism 'CRAM-MD5'
[18:06:25][Step 8/8] I0215 17:06:25.304909  1757 authenticatee.cpp:298] Authentication success
[18:06:25][Step 8/8] I0215 17:06:25.304983  1756 authenticator.cpp:203] Received SASL authentication start
[18:06:25][Step 8/8] I0215 17:06:25.305033  1756 authenticator.cpp:325] Authentication requires more steps
[18:06:25][Step 8/8] I0215 17:06:25.305042  1759 master.cpp:5553] Successfully authenticated principal 'test-principal' at scheduler-806c70e3-1cf6-418f-aa30-6bb26db42d18@172.30.2.239:39785
[18:06:25][Step 8/8] I0215 17:06:25.305071  1755 authenticator.cpp:431] Authentication session cleanup for crammd5_authenticatee(829)@172.30.2.239:39785
[18:06:25][Step 8/8] I0215 17:06:25.305124  1756 authenticatee.cpp:258] Received SASL authentication step
[18:06:25][Step 8/8] I0215 17:06:25.305222  1758 sched.cpp:471] Successfully authenticated with master master@172.30.2.239:39785
[18:06:25][Step 8/8] I0215 17:06:25.305246  1758 sched.cpp:776] Sending SUBSCRIBE call to master@172.30.2.239:39785
[18:06:25][Step 8/8] I0215 17:06:25.305286  1760 authenticator.cpp:231] Received SASL authentication step
[18:06:25][Step 8/8] I0215 17:06:25.305310  1760 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: 'ip-172-30-2-239.mesosphere.io' server FQDN: 'ip-172-30-2-239.mesosphere.io' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
[18:06:25][Step 8/8] I0215 17:06:25.305318  1760 auxprop.cpp:179] Looking up auxiliary property '*userPassword'
[18:06:25][Step 8/8] I0215 17:06:25.305344  1760 auxprop.cpp:179] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
[18:06:25][Step 8/8] I0215 17:06:25.305363  1758 sched.cpp:809] Will retry registration in 1.888777185secs if necessary
[18:06:25][Step 8/8] I0215 17:06:25.305379  1760 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: 'ip-172-30-2-239.mesosphere.io' server FQDN: 'ip-172-30-2-239.mesosphere.io' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
[18:06:25][Step 8/8] I0215 17:06:25.305397  1760 auxprop.cpp:129] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
[18:06:25][Step 8/8] I0215 17:06:25.305408  1760 auxprop.cpp:129] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
[18:06:25][Step 8/8] I0215 17:06:25.305426  1760 authenticator.cpp:317] Authentication success
[18:06:25][Step 8/8] I0215 17:06:25.305466  1761 master.cpp:2280] Received SUBSCRIBE call for framework 'default' at scheduler-806c70e3-1cf6-418f-aa30-6bb26db42d18@172.30.2.239:39785
[18:06:25][Step 8/8] I0215 17:06:25.305506  1756 authenticatee.cpp:298] Authentication success
[18:06:25][Step 8/8] I0215 17:06:25.305534  1761 master.cpp:1751] Authorizing framework principal 'test-principal' to receive offers for role '*'
[18:06:25][Step 8/8] I0215 17:06:25.305625  1755 authenticator.cpp:431] Authentication session cleanup for crammd5_authenticatee(830)@172.30.2.239:39785
[18:06:25][Step 8/8] I0215 17:06:25.305701  1761 master.cpp:5553] Successfully authenticated principal 'test-principal' at slave(393)@172.30.2.239:39785
[18:06:25][Step 8/8] I0215 17:06:25.305831  1758 slave.cpp:927] Successfully authenticated with master master@172.30.2.239:39785
[18:06:25][Step 8/8] I0215 17:06:25.305902  1757 master.cpp:2351] Subscribing framework default with checkpointing disabled and capabilities [  ]
[18:06:25][Step 8/8] I0215 17:06:25.305953  1758 slave.cpp:1321] Will retry registration in 1.941456ms if necessary
[18:06:25][Step 8/8] I0215 17:06:25.306280  1761 hierarchical.cpp:265] Added framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000
[18:06:25][Step 8/8] I0215 17:06:25.306352  1759 sched.cpp:703] Framework registered with 112363e2-c680-4946-8fee-d0626ed8b21e-0000
[18:06:25][Step 8/8] I0215 17:06:25.306363  1761 hierarchical.cpp:1403] No resources available to allocate!
[18:06:25][Step 8/8] I0215 17:06:25.306401  1761 hierarchical.cpp:1498] No inverse offers to send out!
[18:06:25][Step 8/8] I0215 17:06:25.306432  1761 hierarchical.cpp:1096] Performed allocation for 0 slaves in 126082ns
[18:06:25][Step 8/8] I0215 17:06:25.306447  1759 sched.cpp:717] Scheduler::registered took 67960ns
[18:06:25][Step 8/8] I0215 17:06:25.306437  1757 master.cpp:4237] Registering slave at slave(393)@172.30.2.239:39785 (ip-172-30-2-239.mesosphere.io) with id 112363e2-c680-4946-8fee-d0626ed8b21e-S0
[18:06:25][Step 8/8] I0215 17:06:25.306884  1759 registrar.cpp:439] Applied 1 operations in 63175ns; attempting to update the 'registry'
[18:06:25][Step 8/8] I0215 17:06:25.307592  1756 log.cpp:683] Attempting to append 396 bytes to the log
[18:06:25][Step 8/8] I0215 17:06:25.307724  1760 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 3
[18:06:25][Step 8/8] I0215 17:06:25.308398  1760 replica.cpp:537] Replica received write request for position 3 from (13622)@172.30.2.239:39785
[18:06:25][Step 8/8] I0215 17:06:25.308473  1755 slave.cpp:1321] Will retry registration in 37.671741ms if necessary
[18:06:25][Step 8/8] I0215 17:06:25.308627  1758 master.cpp:4225] Ignoring register slave message from slave(393)@172.30.2.239:39785 (ip-172-30-2-239.mesosphere.io) as admission is already in progress
[18:06:25][Step 8/8] I0215 17:06:25.310000  1760 leveldb.cpp:341] Persisting action (415 bytes) to leveldb took 1.556814ms
[18:06:25][Step 8/8] I0215 17:06:25.310025  1760 replica.cpp:712] Persisted action at 3
[18:06:25][Step 8/8] I0215 17:06:25.310541  1755 replica.cpp:691] Replica received learned notice for position 3 from @0.0.0.0:0
[18:06:25][Step 8/8] I0215 17:06:25.311928  1755 leveldb.cpp:341] Persisting action (417 bytes) to leveldb took 1.357404ms
[18:06:25][Step 8/8] I0215 17:06:25.311950  1755 replica.cpp:712] Persisted action at 3
[18:06:25][Step 8/8] I0215 17:06:25.311966  1755 replica.cpp:697] Replica learned APPEND action at position 3
[18:06:25][Step 8/8] I0215 17:06:25.313117  1755 registrar.cpp:484] Successfully updated the 'registry' in 6.16704ms
[18:06:25][Step 8/8] I0215 17:06:25.313297  1758 log.cpp:702] Attempting to truncate the log to 3
[18:06:25][Step 8/8] I0215 17:06:25.313391  1755 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 4
[18:06:25][Step 8/8] I0215 17:06:25.313807  1761 slave.cpp:3482] Received ping from slave-observer(360)@172.30.2.239:39785
[18:06:25][Step 8/8] I0215 17:06:25.313946  1754 master.cpp:4305] Registered slave 112363e2-c680-4946-8fee-d0626ed8b21e-S0 at slave(393)@172.30.2.239:39785 (ip-172-30-2-239.mesosphere.io) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
[18:06:25][Step 8/8] I0215 17:06:25.314067  1756 slave.cpp:971] Registered with master master@172.30.2.239:39785; given slave ID 112363e2-c680-4946-8fee-d0626ed8b21e-S0
[18:06:25][Step 8/8] I0215 17:06:25.314095  1756 fetcher.cpp:81] Clearing fetcher cache
[18:06:25][Step 8/8] I0215 17:06:25.314102  1760 replica.cpp:537] Replica received write request for position 4 from (13623)@172.30.2.239:39785
[18:06:25][Step 8/8] I0215 17:06:25.314164  1758 hierarchical.cpp:473] Added slave 112363e2-c680-4946-8fee-d0626ed8b21e-S0 (ip-172-30-2-239.mesosphere.io) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (allocated: )
[18:06:25][Step 8/8] I0215 17:06:25.314219  1761 status_update_manager.cpp:181] Resuming sending status updates
[18:06:25][Step 8/8] I0215 17:06:25.314370  1756 slave.cpp:994] Checkpointing SlaveInfo to '/tmp/DockerContainerizerTest_ROOT_DOCKER_Logs_a4NS2N/meta/slaves/112363e2-c680-4946-8fee-d0626ed8b21e-S0/slave.info'
[18:06:25][Step 8/8] I0215 17:06:25.314579  1756 slave.cpp:1030] Forwarding total oversubscribed resources 
[18:06:25][Step 8/8] I0215 17:06:25.314707  1756 master.cpp:4646] Received update of slave 112363e2-c680-4946-8fee-d0626ed8b21e-S0 at slave(393)@172.30.2.239:39785 (ip-172-30-2-239.mesosphere.io) with total oversubscribed resources 
[18:06:25][Step 8/8] I0215 17:06:25.314818  1758 hierarchical.cpp:1498] No inverse offers to send out!
[18:06:25][Step 8/8] I0215 17:06:25.314848  1758 hierarchical.cpp:1116] Performed allocation for slave 112363e2-c680-4946-8fee-d0626ed8b21e-S0 in 654176ns
[18:06:25][Step 8/8] I0215 17:06:25.315137  1758 hierarchical.cpp:531] Slave 112363e2-c680-4946-8fee-d0626ed8b21e-S0 (ip-172-30-2-239.mesosphere.io) updated with oversubscribed resources  (total: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000], allocated: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000])
[18:06:25][Step 8/8] I0215 17:06:25.315217  1756 master.cpp:5352] Sending 1 offers to framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000 (default) at scheduler-806c70e3-1cf6-418f-aa30-6bb26db42d18@172.30.2.239:39785
[18:06:25][Step 8/8] I0215 17:06:25.315238  1758 hierarchical.cpp:1403] No resources available to allocate!
[18:06:25][Step 8/8] I0215 17:06:25.315268  1758 hierarchical.cpp:1498] No inverse offers to send out!
[18:06:25][Step 8/8] I0215 17:06:25.315285  1758 hierarchical.cpp:1116] Performed allocation for slave 112363e2-c680-4946-8fee-d0626ed8b21e-S0 in 118646ns
[18:06:25][Step 8/8] I0215 17:06:25.315635  1755 sched.cpp:873] Scheduler::resourceOffers took 99802ns
[18:06:25][Step 8/8] I0215 17:06:25.317126  1755 master.cpp:3138] Processing ACCEPT call for offers: [ 112363e2-c680-4946-8fee-d0626ed8b21e-O0 ] on slave 112363e2-c680-4946-8fee-d0626ed8b21e-S0 at slave(393)@172.30.2.239:39785 (ip-172-30-2-239.mesosphere.io) for framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000 (default) at scheduler-806c70e3-1cf6-418f-aa30-6bb26db42d18@172.30.2.239:39785
[18:06:25][Step 8/8] I0215 17:06:25.317163  1755 master.cpp:2825] Authorizing framework principal 'test-principal' to launch task 1 as user 'root'
[18:06:25][Step 8/8] I0215 17:06:25.317229  1760 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 3.089068ms
[18:06:25][Step 8/8] I0215 17:06:25.317261  1760 replica.cpp:712] Persisted action at 4
[18:06:25][Step 8/8] I0215 17:06:25.317845  1759 replica.cpp:691] Replica received learned notice for position 4 from @0.0.0.0:0
[18:06:25][Step 8/8] I0215 17:06:25.318722  1755 master.hpp:176] Adding task 1 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave 112363e2-c680-4946-8fee-d0626ed8b21e-S0 (ip-172-30-2-239.mesosphere.io)
[18:06:25][Step 8/8] I0215 17:06:25.318886  1755 master.cpp:3623] Launching task 1 of framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000 (default) at scheduler-806c70e3-1cf6-418f-aa30-6bb26db42d18@172.30.2.239:39785 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave 112363e2-c680-4946-8fee-d0626ed8b21e-S0 at slave(393)@172.30.2.239:39785 (ip-172-30-2-239.mesosphere.io)
[18:06:25][Step 8/8] I0215 17:06:25.319195  1757 slave.cpp:1361] Got assigned task 1 for framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000
[18:06:25][Step 8/8] I0215 17:06:25.319305  1759 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 1.430044ms
[18:06:25][Step 8/8] I0215 17:06:25.319349  1757 resources.cpp:576] Parsing resources as JSON failed: cpus:0.1;mem:32
[18:06:25][Step 8/8] Trying semicolon-delimited string format instead
[18:06:25][Step 8/8] I0215 17:06:25.319363  1759 leveldb.cpp:399] Deleting ~2 keys from leveldb took 34738ns
[18:06:25][Step 8/8] I0215 17:06:25.319380  1759 replica.cpp:712] Persisted action at 4
[18:06:25][Step 8/8] I0215 17:06:25.319396  1759 replica.cpp:697] Replica learned TRUNCATE action at position 4
[18:06:25][Step 8/8] I0215 17:06:25.320034  1757 slave.cpp:1480] Launching task 1 for framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000
[18:06:25][Step 8/8] I0215 17:06:25.320127  1757 resources.cpp:576] Parsing resources as JSON failed: cpus:0.1;mem:32
[18:06:25][Step 8/8] Trying semicolon-delimited string format instead
[18:06:25][Step 8/8] I0215 17:06:25.320725  1757 paths.cpp:474] Trying to chown '/tmp/DockerContainerizerTest_ROOT_DOCKER_Logs_a4NS2N/slaves/112363e2-c680-4946-8fee-d0626ed8b21e-S0/frameworks/112363e2-c680-4946-8fee-d0626ed8b21e-0000/executors/1/runs/4f58cd28-7ac7-4960-b3e3-8d28918e6d08' to user 'root'
[18:06:25][Step 8/8] I0215 17:06:25.325739  1757 slave.cpp:5351] Launching executor 1 of framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000 with resources cpus(*):0.1; mem(*):32 in work directory '/tmp/DockerContainerizerTest_ROOT_DOCKER_Logs_a4NS2N/slaves/112363e2-c680-4946-8fee-d0626ed8b21e-S0/frameworks/112363e2-c680-4946-8fee-d0626ed8b21e-0000/executors/1/runs/4f58cd28-7ac7-4960-b3e3-8d28918e6d08'
[18:06:25][Step 8/8] I0215 17:06:25.326493  1757 slave.cpp:1698] Queuing task '1' for executor '1' of framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000
[18:06:25][Step 8/8] I0215 17:06:25.326633  1757 slave.cpp:749] Successfully attached file '/tmp/DockerContainerizerTest_ROOT_DOCKER_Logs_a4NS2N/slaves/112363e2-c680-4946-8fee-d0626ed8b21e-S0/frameworks/112363e2-c680-4946-8fee-d0626ed8b21e-0000/executors/1/runs/4f58cd28-7ac7-4960-b3e3-8d28918e6d08'
[18:06:25][Step 8/8] I0215 17:06:25.331328  1761 docker.cpp:803] Starting container '4f58cd28-7ac7-4960-b3e3-8d28918e6d08' for task '1' (and executor '1') of framework '112363e2-c680-4946-8fee-d0626ed8b21e-0000'
[18:06:25][Step 8/8] I0215 17:06:25.331699  1761 docker.cpp:1053] Running docker -H unix:///var/run/docker.sock inspect alpine:latest
[18:06:25][Step 8/8] I0215 17:06:25.449668  1758 docker.cpp:384] Docker pull alpine completed
[18:06:25][Step 8/8] I0215 17:06:25.511905  1760 slave.cpp:2643] Got registration for executor '1' of framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000 from executor(1)@172.30.2.239:38602
[18:06:25][Step 8/8] I0215 17:06:25.513098  1759 docker.cpp:1077] Ignoring updating container '4f58cd28-7ac7-4960-b3e3-8d28918e6d08' with resources passed to update is identical to existing resources
[18:06:25][Step 8/8] I0215 17:06:25.513494  1756 slave.cpp:1863] Sending queued task '1' to executor '1' of framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000 at executor(1)@172.30.2.239:38602
[18:06:26][Step 8/8] 2016-02-15 17:06:25,981:1740(0x7f870b7fe700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:36716] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
[18:06:26][Step 8/8] I0215 17:06:26.227973  1757 slave.cpp:3002] Handling status update TASK_RUNNING (UUID: b0f4bfb3-0f3c-4b03-843d-b5af82a902a3) for task 1 of framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000 from executor(1)@172.30.2.239:38602
[18:06:26][Step 8/8] I0215 17:06:26.228302  1757 slave.cpp:3002] Handling status update TASK_FAILED (UUID: 05810f46-10e7-4d50-a83d-d05bf79dd8e2) for task 1 of framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000 from executor(1)@172.30.2.239:38602
[18:06:26][Step 8/8] I0215 17:06:26.228734  1754 status_update_manager.cpp:320] Received status update TASK_RUNNING (UUID: b0f4bfb3-0f3c-4b03-843d-b5af82a902a3) for task 1 of framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000
[18:06:26][Step 8/8] I0215 17:06:26.228790  1754 status_update_manager.cpp:497] Creating StatusUpdate stream for task 1 of framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000
[18:06:26][Step 8/8] I0215 17:06:26.228837  1757 slave.cpp:5661] Terminating task 1
[18:06:26][Step 8/8] I0215 17:06:26.229243  1754 status_update_manager.cpp:374] Forwarding update TASK_RUNNING (UUID: b0f4bfb3-0f3c-4b03-843d-b5af82a902a3) for task 1 of framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000 to the slave
[18:06:26][Step 8/8] I0215 17:06:26.229346  1755 docker.cpp:766] Running docker -H unix:///var/run/docker.sock inspect mesos-112363e2-c680-4946-8fee-d0626ed8b21e-S0.4f58cd28-7ac7-4960-b3e3-8d28918e6d08
[18:06:26][Step 8/8] I0215 17:06:26.232147  1758 slave.cpp:3400] Forwarding the update TASK_RUNNING (UUID: b0f4bfb3-0f3c-4b03-843d-b5af82a902a3) for task 1 of framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000 to master@172.30.2.239:39785
[18:06:26][Step 8/8] I0215 17:06:26.232383  1758 slave.cpp:3294] Status update manager successfully handled status update TASK_RUNNING (UUID: b0f4bfb3-0f3c-4b03-843d-b5af82a902a3) for task 1 of framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000
[18:06:26][Step 8/8] I0215 17:06:26.232419  1758 slave.cpp:3310] Sending acknowledgement for status update TASK_RUNNING (UUID: b0f4bfb3-0f3c-4b03-843d-b5af82a902a3) for task 1 of framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000 to executor(1)@172.30.2.239:38602
[18:06:26][Step 8/8] I0215 17:06:26.232631  1759 master.cpp:4791] Status update TASK_RUNNING (UUID: b0f4bfb3-0f3c-4b03-843d-b5af82a902a3) for task 1 of framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000 from slave 112363e2-c680-4946-8fee-d0626ed8b21e-S0 at slave(393)@172.30.2.239:39785 (ip-172-30-2-239.mesosphere.io)
[18:06:26][Step 8/8] I0215 17:06:26.232681  1759 master.cpp:4839] Forwarding status update TASK_RUNNING (UUID: b0f4bfb3-0f3c-4b03-843d-b5af82a902a3) for task 1 of framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000
[18:06:26][Step 8/8] I0215 17:06:26.232911  1759 master.cpp:6447] Updating the state of task 1 of framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000 (latest state: TASK_FAILED, status update state: TASK_RUNNING)
[18:06:26][Step 8/8] I0215 17:06:26.233170  1756 sched.cpp:981] Scheduler::statusUpdate took 100304ns
[18:06:26][Step 8/8] I0215 17:06:26.233613  1754 hierarchical.cpp:892] Recovered cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (total: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000], allocated: ) on slave 112363e2-c680-4946-8fee-d0626ed8b21e-S0 from framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000
[18:06:26][Step 8/8] I0215 17:06:26.233642  1759 master.cpp:3949] Processing ACKNOWLEDGE call b0f4bfb3-0f3c-4b03-843d-b5af82a902a3 for task 1 of framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000 (default) at scheduler-806c70e3-1cf6-418f-aa30-6bb26db42d18@172.30.2.239:39785 on slave 112363e2-c680-4946-8fee-d0626ed8b21e-S0
[18:06:26][Step 8/8] I0215 17:06:26.233944  1759 status_update_manager.cpp:392] Received status update acknowledgement (UUID: b0f4bfb3-0f3c-4b03-843d-b5af82a902a3) for task 1 of framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000
[18:06:26][Step 8/8] I0215 17:06:26.234294  1761 slave.cpp:2412] Status update manager successfully handled status update acknowledgement (UUID: b0f4bfb3-0f3c-4b03-843d-b5af82a902a3) for task 1 of framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000
[18:06:26][Step 8/8] I0215 17:06:26.264482  1759 hierarchical.cpp:1498] No inverse offers to send out!
[18:06:26][Step 8/8] I0215 17:06:26.264554  1759 hierarchical.cpp:1096] Performed allocation for 1 slaves in 1.210209ms
[18:06:26][Step 8/8] I0215 17:06:26.264837  1757 master.cpp:5352] Sending 1 offers to framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000 (default) at scheduler-806c70e3-1cf6-418f-aa30-6bb26db42d18@172.30.2.239:39785
[18:06:26][Step 8/8] I0215 17:06:26.265275  1760 sched.cpp:873] Scheduler::resourceOffers took 26245ns
[18:06:26][Step 8/8] I0215 17:06:26.357859  1756 status_update_manager.cpp:320] Received status update TASK_FAILED (UUID: 05810f46-10e7-4d50-a83d-d05bf79dd8e2) for task 1 of framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000
[18:06:26][Step 8/8] I0215 17:06:26.358085  1756 status_update_manager.cpp:374] Forwarding update TASK_FAILED (UUID: 05810f46-10e7-4d50-a83d-d05bf79dd8e2) for task 1 of framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000 to the slave
[18:06:26][Step 8/8] I0215 17:06:26.358330  1758 slave.cpp:3400] Forwarding the update TASK_FAILED (UUID: 05810f46-10e7-4d50-a83d-d05bf79dd8e2) for task 1 of framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000 to master@172.30.2.239:39785
[18:06:26][Step 8/8] I0215 17:06:26.358554  1758 slave.cpp:3294] Status update manager successfully handled status update TASK_FAILED (UUID: 05810f46-10e7-4d50-a83d-d05bf79dd8e2) for task 1 of framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000
[18:06:26][Step 8/8] I0215 17:06:26.358594  1758 slave.cpp:3310] Sending acknowledgement for status update TASK_FAILED (UUID: 05810f46-10e7-4d50-a83d-d05bf79dd8e2) for task 1 of framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000 to executor(1)@172.30.2.239:38602
[18:06:26][Step 8/8] I0215 17:06:26.358687  1761 master.cpp:4791] Status update TASK_FAILED (UUID: 05810f46-10e7-4d50-a83d-d05bf79dd8e2) for task 1 of framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000 from slave 112363e2-c680-4946-8fee-d0626ed8b21e-S0 at slave(393)@172.30.2.239:39785 (ip-172-30-2-239.mesosphere.io)
[18:06:26][Step 8/8] I0215 17:06:26.358724  1761 master.cpp:4839] Forwarding status update TASK_FAILED (UUID: 05810f46-10e7-4d50-a83d-d05bf79dd8e2) for task 1 of framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000
[18:06:26][Step 8/8] I0215 17:06:26.358921  1761 master.cpp:6447] Updating the state of task 1 of framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000 (latest state: TASK_FAILED, status update state: TASK_FAILED)
[18:06:26][Step 8/8] ../../src/tests/containerizer/docker_containerizer_tests.cpp:1269: Failure
[18:06:26][Step 8/8] I0215 17:06:26.359128  1754 sched.cpp:981] Scheduler::statusUpdate took 108209ns
[18:06:26][Step 8/8] Value of: statusFinished.get().state()
[18:06:26][Step 8/8] I0215 17:06:26.359400  1759 master.cpp:3949] Processing ACKNOWLEDGE call 05810f46-10e7-4d50-a83d-d05bf79dd8e2 for task 1 of framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000 (default) at scheduler-806c70e3-1cf6-418f-aa30-6bb26db42d18@172.30.2.239:39785 on slave 112363e2-c680-4946-8fee-d0626ed8b21e-S0
[18:06:26][Step 8/8]   Actual: TASK_FAILED
[18:06:26][Step 8/8] Expected: TASK_FINISHED
[18:06:26][Step 8/8] ../../src/tests/containerizer/docker_containerizer_tests.cpp:1278: Failure
[18:06:26][Step 8/8] I0215 17:06:26.359470  1759 master.cpp:6513] Removing task 1 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] of framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000 on slave 112363e2-c680-4946-8fee-d0626ed8b21e-S0 at slave(393)@172.30.2.239:39785 (ip-172-30-2-239.mesosphere.io)
[18:06:26][Step 8/8] Value of: containsLine(lines, ""err"" + uuid)
[18:06:26][Step 8/8]   Actual: false
[18:06:26][Step 8/8] I0215 17:06:26.359928  1761 status_update_manager.cpp:392] Received status update acknowledgement (UUID: 05810f46-10e7-4d50-a83d-d05bf79dd8e2) for task 1 of framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000
[18:06:26][Step 8/8] Expected: true
[18:06:26][Step 8/8] I0215 17:06:26.359931  1740 sched.cpp:1903] Asked to stop the driver
[18:06:26][Step 8/8] ../../src/tests/containerizer/docker_containerizer_tests.cpp:1286: Failure
[18:06:26][Step 8/8] I0215 17:06:26.360031  1759 sched.cpp:1143] Stopping framework '112363e2-c680-4946-8fee-d0626ed8b21e-0000'
[18:06:26][Step 8/8] Value of: containsLine(lines, ""out"" + uuid)
[18:06:26][Step 8/8] I0215 17:06:26.360080  1761 status_update_manager.cpp:528] Cleaning up status update stream for task 1 of framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000
[18:06:26][Step 8/8]   Actual: false
[18:06:26][Step 8/8] Expected: true
[18:06:26][Step 8/8] I0215 17:06:26.360213  1760 master.cpp:5923] Processing TEARDOWN call for framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000 (default) at scheduler-806c70e3-1cf6-418f-aa30-6bb26db42d18@172.30.2.239:39785
[18:06:26][Step 8/8] I0215 17:06:26.360239  1760 master.cpp:5935] Removing framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000 (default) at scheduler-806c70e3-1cf6-418f-aa30-6bb26db42d18@172.30.2.239:39785
[18:06:26][Step 8/8] I0215 17:06:26.360481  1755 slave.cpp:2412] Status update manager successfully handled status update acknowledgement (UUID: 05810f46-10e7-4d50-a83d-d05bf79dd8e2) for task 1 of framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000
[18:06:26][Step 8/8] I0215 17:06:26.360522  1755 slave.cpp:5702] Completing task 1
[18:06:26][Step 8/8] I0215 17:06:26.360539  1756 hierarchical.cpp:375] Deactivated framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000
[18:06:26][Step 8/8] I0215 17:06:26.360586  1755 slave.cpp:2079] Asked to shut down framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000 by master@172.30.2.239:39785
[18:06:26][Step 8/8] I0215 17:06:26.360610  1755 slave.cpp:2104] Shutting down framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000
[18:06:26][Step 8/8] I0215 17:06:26.360666  1755 slave.cpp:4198] Shutting down executor '1' of framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000 at executor(1)@172.30.2.239:38602
[18:06:26][Step 8/8] I0215 17:06:26.361110  1761 hierarchical.cpp:892] Recovered cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (total: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000], allocated: ) on slave 112363e2-c680-4946-8fee-d0626ed8b21e-S0 from framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000
[18:06:26][Step 8/8] I0215 17:06:26.361395  1756 hierarchical.cpp:326] Removed framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000
[18:06:26][Step 8/8] I0215 17:06:26.361397  1760 master.cpp:1027] Master terminating
[18:06:26][Step 8/8] I0215 17:06:26.361800  1755 hierarchical.cpp:505] Removed slave 112363e2-c680-4946-8fee-d0626ed8b21e-S0
[18:06:26][Step 8/8] I0215 17:06:26.362174  1756 slave.cpp:3528] master@172.30.2.239:39785 exited
[18:06:26][Step 8/8] W0215 17:06:26.362200  1756 slave.cpp:3531] Master disconnected! Waiting for a new master to be elected
[18:06:26][Step 8/8] I0215 17:06:26.367146  1758 docker.cpp:1455] Destroying container '4f58cd28-7ac7-4960-b3e3-8d28918e6d08'
[18:06:26][Step 8/8] I0215 17:06:26.367168  1758 docker.cpp:1515] Sending SIGTERM to executor with pid: 10194
[18:06:26][Step 8/8] I0215 17:06:26.383013  1758 docker.cpp:1557] Running docker stop on container '4f58cd28-7ac7-4960-b3e3-8d28918e6d08'
[18:06:26][Step 8/8] I0215 17:06:26.384843  1757 slave.cpp:3528] executor(1)@172.30.2.239:38602 exited
[18:06:26][Step 8/8] I0215 17:06:26.458639  1761 docker.cpp:1654] Executor for container '4f58cd28-7ac7-4960-b3e3-8d28918e6d08' has exited
[18:06:26][Step 8/8] I0215 17:06:26.458806  1757 slave.cpp:3886] Executor '1' of framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000 terminated with signal Terminated
[18:06:26][Step 8/8] I0215 17:06:26.458852  1757 slave.cpp:3990] Cleaning up executor '1' of framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000 at executor(1)@172.30.2.239:38602
[18:06:26][Step 8/8] I0215 17:06:26.459166  1754 gc.cpp:54] Scheduling '/tmp/DockerContainerizerTest_ROOT_DOCKER_Logs_a4NS2N/slaves/112363e2-c680-4946-8fee-d0626ed8b21e-S0/frameworks/112363e2-c680-4946-8fee-d0626ed8b21e-0000/executors/1/runs/4f58cd28-7ac7-4960-b3e3-8d28918e6d08' for gc 6.99999468745481days in the future
[18:06:26][Step 8/8] I0215 17:06:26.459262  1757 slave.cpp:4078] Cleaning up framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000
[18:06:26][Step 8/8] I0215 17:06:26.459336  1754 gc.cpp:54] Scheduling '/tmp/DockerContainerizerTest_ROOT_DOCKER_Logs_a4NS2N/slaves/112363e2-c680-4946-8fee-d0626ed8b21e-S0/frameworks/112363e2-c680-4946-8fee-d0626ed8b21e-0000/executors/1' for gc 6.99999468505778days in the future
[18:06:26][Step 8/8] I0215 17:06:26.459372  1761 status_update_manager.cpp:282] Closing status update streams for framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000
[18:06:26][Step 8/8] I0215 17:06:26.459481  1754 gc.cpp:54] Scheduling '/tmp/DockerContainerizerTest_ROOT_DOCKER_Logs_a4NS2N/slaves/112363e2-c680-4946-8fee-d0626ed8b21e-S0/frameworks/112363e2-c680-4946-8fee-d0626ed8b21e-0000' for gc 6.99999468310222days in the future
[18:06:26][Step 8/8] I0215 17:06:26.459915  1759 slave.cpp:668] Slave terminating
[18:06:26][Step 8/8] I0215 17:06:26.463074  1740 docker.cpp:885] Running docker -H unix:///var/run/docker.sock ps -a
[18:06:26][Step 8/8] I0215 17:06:26.560154  1760 docker.cpp:766] Running docker -H unix:///var/run/docker.sock inspect mesos-112363e2-c680-4946-8fee-d0626ed8b21e-S0.4f58cd28-7ac7-4960-b3e3-8d28918e6d08
[18:06:26][Step 8/8] I0215 17:06:26.666368  1740 docker.cpp:727] Running docker -H unix:///var/run/docker.sock rm -f -v c7f89c245f256c03222551178f6a0c26413bb91dfb3c843bff837b365d7c7432
[18:06:26][Step 8/8] [  FAILED  ] DockerContainerizerTest.ROOT_DOCKER_Logs (1519 ms)
{noformat}",Bug,Major,bernd-mesos,2016-02-25T20:40:10.000+0000,5,Resolved,Complete,ROOT_DOCKER_Logs is flaky.,2016-03-03T22:51:29.000+0000,MESOS-4676,2.0,mesos,Mesosphere Sprint 29
jvanremoortere,2016-02-15T17:35:03.000+0000,jvanremoortere,"On certain platforms the systemd init system is available, but not used.
Not being able to disable the mesos systemd integration on these platforms makes it hard to operate using a different init / monit system.",Bug,Major,jvanremoortere,2016-02-15T18:57:26.000+0000,5,Resolved,Complete,Cannot disable systemd support,2016-02-27T00:51:00.000+0000,MESOS-4675,1.0,mesos,Mesosphere Sprint 28
kaysoky,2016-02-15T16:37:06.000+0000,bernd-mesos,"LinuxFilesystemIsolatorTest.ROOT_ImageInVolumeWithRootFilesystem sometimes fails on CentOS 7 with this kind of output:
{noformat}
../../src/tests/containerizer/filesystem_isolator_tests.cpp:1054: Failure
Failed to wait 2mins for launch
{noformat}

LinuxFilesystemIsolatorTest.ROOT_MultipleContainers often has this output:
{noformat}
../../src/tests/containerizer/filesystem_isolator_tests.cpp:1138: Failure
Failed to wait 1mins for launch1
{noformat}

Whether SSL is configured makes no difference.

This test may also fail on other platforms, but more rarely.

",Bug,Major,bernd-mesos,2016-02-20T06:52:57.000+0000,5,Resolved,Complete,Linux filesystem isolator tests are flaky.,2016-03-03T22:51:29.000+0000,MESOS-4674,3.0,mesos,Mesosphere Sprint 29
avinash@mesosphere.io,2016-02-13T01:59:47.000+0000,anandmazumdar,"Previously, all status update messages from the executor were forwarded by the agent to the master in the order that they had been received. 

However, that seems to be no longer valid due to a recently introduced change in the agent:

{code}
// Before sending update, we need to retrieve the container status.
  containerizer->status(executor->containerId)
    .onAny(defer(self(),
                 &Slave::_statusUpdate,
                 update,
                 pid,
                 executor->id,
                 lambda::_1));
{code}

This can sometimes lead to status updates being sent out of order depending on the order the {{Future}} is fulfilled from the call to {{status(...)}}.",Bug,Blocker,anandmazumdar,2016-02-17T23:07:03.000+0000,5,Resolved,Complete,Status updates from executor can be forwarded out of order by the Agent.,2016-02-18T22:17:45.000+0000,MESOS-4671,1.0,mesos,Mesosphere Sprint 29
avinash@mesosphere.io,2016-02-12T23:15:34.000+0000,avinash@mesosphere.io,"The ComposingContainerizer currently does not have a `status` method. This results in no `ContainerStatus` being updated in the agent, when uses `ComposingContainerizer` to launch containers. This would specifically happen when the agent is launched with `--containerizer=docker,mesos`",Bug,Major,avinash@mesosphere.io,2016-02-18T14:35:02.000+0000,5,Resolved,Complete,`cgroup_info` not being exposed in state.json when ComposingContainerizer is used.,2016-02-18T14:35:03.000+0000,MESOS-4670,1.0,mesos,Mesosphere Sprint 29
jojy,2016-02-12T22:10:23.000+0000,jojy,We need GZIP uncompress utility for Appc image fetching functionality. The images are tar + gzip'ed and they needs to be first uncompressed so that we can compute sha 512 checksum on it.,Bug,Major,jojy,2016-02-25T16:54:39.000+0000,5,Resolved,Complete,Add common compression utility,2016-02-25T16:54:39.000+0000,MESOS-4669,2.0,mesos,Mesosphere Sprint 29
neilc,2016-02-12T19:26:51.000+0000,neilc,The per-slave {{reserved_resources}} information returned by {{/state}} does not seem to include information about persistent volumes. This makes it hard for operators to use the {{/destroy-volumes}} endpoint.,Improvement,Major,neilc,2016-02-29T07:28:39.000+0000,5,Resolved,Complete,Expose persistent volume information in HTTP endpoints,2016-02-29T07:28:39.000+0000,MESOS-4667,3.0,mesos,Mesosphere Sprint 29
avinash@mesosphere.io,2016-02-11T19:50:04.000+0000,jieyu,We need to add a section in the doc to describe how to use cgroups/net_cls isolator.,Task,Major,jieyu,2016-02-26T22:09:22.000+0000,5,Resolved,Complete,Document net_cls isolator in docs/mesos-containerizer.md.,2016-02-26T22:09:22.000+0000,MESOS-4660,1.0,mesos,Mesosphere Sprint 29
avinash@mesosphere.io,2016-02-11T15:32:51.000+0000,avinash@mesosphere.io,We need to add LOG(INFO) during the prepare phase of `cgroups/net_cls` for debugging management of `net_cls` handles within the isolator. ,Improvement,Minor,avinash@mesosphere.io,2016-02-11T23:29:05.000+0000,5,Resolved,Complete,Add LOG(INFO) in `cgroups/net_cls` for debugging allocation of net_cls handles.,2016-02-11T23:29:05.000+0000,MESOS-4657,1.0,mesos,Mesosphere Sprint 28
kaysoky,2016-02-09T23:04:08.000+0000,kaysoky,"Tests that use the {{StartMaster}} test helper are generally fragile when the test fails an assert/expect in the middle of the test.  This is because the {{StartMaster}} helper takes raw pointer arguments, which may be stack-allocated.

In case of an assert failure, the test immediately exits (destroying stack allocated objects) and proceeds onto test cleanup.  The test cleanup may dereference some of these destroyed objects, leading to a test crash like:
{code}
[18:27:36][Step 8/8] F0204 18:27:35.981302 23085 logging.cpp:64] RAW: Pure virtual method called
[18:27:36][Step 8/8]     @     0x7f7077055e1c  google::LogMessage::Fail()
[18:27:36][Step 8/8]     @     0x7f707705ba6f  google::RawLog__()
[18:27:36][Step 8/8]     @     0x7f70760f76c9  __cxa_pure_virtual
[18:27:36][Step 8/8]     @           0xa9423c  mesos::internal::tests::Cluster::Slaves::shutdown()
[18:27:36][Step 8/8]     @          0x1074e45  mesos::internal::tests::MesosTest::ShutdownSlaves()
[18:27:36][Step 8/8]     @          0x1074de4  mesos::internal::tests::MesosTest::Shutdown()
[18:27:36][Step 8/8]     @          0x1070ec7  mesos::internal::tests::MesosTest::TearDown()
{code}

The {{StartMaster}} helper should take {{shared_ptr}} arguments instead.
This also means that we can remove the {{Shutdown}} helper from most of these tests.",Bug,Major,kaysoky,2016-03-16T13:23:43.000+0000,5,Resolved,Complete,Tests will dereference stack allocated master objects upon assertion/expectation failure.,2016-03-22T12:49:15.000+0000,MESOS-4634,5.0,mesos,Mesosphere Sprint 28
kaysoky,2016-02-09T22:28:20.000+0000,kaysoky,"Tests that use the {{StartSlave}} test helper are generally fragile when the test fails an assert/expect in the middle of the test.  This is because the {{StartSlave}} helper takes raw pointer arguments, which may be stack-allocated.

In case of an assert failure, the test immediately exits (destroying stack allocated objects) and proceeds onto test cleanup.  The test cleanup may dereference some of these destroyed objects, leading to a test crash like:
{code}
[18:27:36][Step 8/8] F0204 18:27:35.981302 23085 logging.cpp:64] RAW: Pure virtual method called
[18:27:36][Step 8/8]     @     0x7f7077055e1c  google::LogMessage::Fail()
[18:27:36][Step 8/8]     @     0x7f707705ba6f  google::RawLog__()
[18:27:36][Step 8/8]     @     0x7f70760f76c9  __cxa_pure_virtual
[18:27:36][Step 8/8]     @           0xa9423c  mesos::internal::tests::Cluster::Slaves::shutdown()
[18:27:36][Step 8/8]     @          0x1074e45  mesos::internal::tests::MesosTest::ShutdownSlaves()
[18:27:36][Step 8/8]     @          0x1074de4  mesos::internal::tests::MesosTest::Shutdown()
[18:27:36][Step 8/8]     @          0x1070ec7  mesos::internal::tests::MesosTest::TearDown()
{code}

The {{StartSlave}} helper should take {{shared_ptr}} arguments instead.
This also means that we can remove the {{Shutdown}} helper from most of these tests.",Bug,Major,kaysoky,2016-03-16T13:23:16.000+0000,5,Resolved,Complete,Tests will dereference stack allocated agent objects upon assertion/expectation failure.,2016-03-22T12:49:15.000+0000,MESOS-4633,5.0,mesos,Mesosphere Sprint 28
anandmazumdar,2016-02-09T19:06:33.000+0000,anandmazumdar,"Currently, the HTTP V1 API does not have partition tests similar to the one in src/tests/partition_tests.cpp.

For more information see MESOS-3355.",Task,Major,anandmazumdar,,10020,Accepted,In Progress,Implement partition tests for the HTTP Scheduler API.,2016-03-28T03:50:57.000+0000,MESOS-4630,5.0,mesos,
anandmazumdar,2016-02-09T19:03:41.000+0000,anandmazumdar,"Currently, the HTTP V1 API does not have fault tolerance tests similar to the one in {{src/tests/fault_tolerance_tests.cpp}}. 

For more information see MESOS-3355.",Task,Major,anandmazumdar,2016-03-16T00:04:29.000+0000,5,Resolved,Complete,Implement fault tolerance tests for the HTTP Scheduler API.,2016-03-16T00:04:30.000+0000,MESOS-4629,5.0,mesos,Mesosphere Sprint 31
,2016-02-09T09:59:44.000+0000,bmahler,"When filesystem isolation is enabled, containers that use Nvidia GPU resources need access to GPU libraries residing on the host.

We'll need to provide a means for operators to inject the necessary volumes into *all* containers that use ""gpus"" resources.

See the nvidia-docker project for more details:
[nvidia-docker/tools/src/nvidia/volumes.go|https://github.com/NVIDIA/nvidia-docker/blob/fda10b2d27bf5578cc5337c23877f827e4d1ed77/tools/src/nvidia/volumes.go#L50-L103]",Task,Major,bmahler,,1,Open,New,Support Nvidia GPUs with filesystem isolation enabled.,2016-02-09T10:00:28.000+0000,MESOS-4626,13.0,mesos,
klueska,2016-02-09T09:52:27.000+0000,bmahler,"The Nvidia GPU isolator will need to use the device cgroup to restrict access to GPU resources, and will need to recover this information after agent failover. For now this will require that the operator specifies the GPU devices via a flag.

To handle filesystem isolation requires that we provide mechanisms for operators to inject volumes with the necessary libraries into all containers using GPU resources, we'll tackle this in a separate ticket.",Task,Major,bmahler,2016-03-31T19:46:03.000+0000,5,Resolved,Complete,Implement Nvidia GPU isolation w/o filesystem isolation enabled.,2016-03-31T19:46:03.000+0000,MESOS-4625,5.0,mesos,Mesosphere Sprint 31
klueska,2016-02-09T09:47:55.000+0000,bmahler,"Allocation metrics are currently hard-coded to include only {{\[""cpus"", ""mem"", ""disk""\]}} resources. We'll need to add ""gpus"" to the list to start, possibly following up on the TODO to remove the hard-coding.

See:
https://github.com/apache/mesos/blob/0.27.0/src/master/metrics.cpp#L266-L269
https://github.com/apache/mesos/blob/0.27.0/src/slave/metrics.cpp#L123-L126
",Task,Major,bmahler,2016-04-08T21:21:19.000+0000,5,Resolved,Complete,"Add allocation metrics for ""gpus"" resources.",2016-04-08T21:21:19.000+0000,MESOS-4624,1.0,mesos,Mesosphere Sprint 32
klueska,2016-02-09T09:43:13.000+0000,bmahler,"We'll first wire up a skeleton Nvidia GPU isolator, which needs to be guarded by a configure flag due to the dependency on NVML.",Task,Major,bmahler,2016-03-29T23:26:42.000+0000,5,Resolved,Complete,Add a stub Nvidia GPU isolator.,2016-03-29T23:26:42.000+0000,MESOS-4623,3.0,mesos,Mesosphere Sprint 31
avinash@mesosphere.io,2016-02-08T22:39:29.000+0000,avinash@mesosphere.io,"As part of the net_cls epic, we introduce an agent flag called `--cgroup_net_cls_primary_handle` . We need to update configuration.md with the corresponding help string. ",Documentation,Major,avinash@mesosphere.io,2016-02-10T17:13:42.000+0000,5,Resolved,Complete,Update configuration.md with `--cgroups_net_cls_primary_handle` agent flag.,2016-03-11T19:01:57.000+0000,MESOS-4622,1.0,mesos,
karya,2016-02-08T00:39:36.000+0000,karya,The doxygen html pages corresponding to doc/* markdown files are redundant and have broken links. They don't serve any reasonable purpose in doxygen site.,Bug,Major,karya,2016-02-20T16:53:26.000+0000,5,Resolved,Complete,Remove markdown files from doxygen pages,2016-02-20T16:57:55.000+0000,MESOS-4619,1.0,mesos,Mesosphere Sprint 28
kaysoky,2016-02-06T01:43:12.000+0000,greggomann,"Just saw this failure on the ASF CI:

{code}
[ RUN      ] ContainerLoggerTest.DefaultToSandbox
I0206 01:25:03.766458  2824 leveldb.cpp:174] Opened db in 72.979786ms
I0206 01:25:03.811712  2824 leveldb.cpp:181] Compacted db in 45.162067ms
I0206 01:25:03.811810  2824 leveldb.cpp:196] Created db iterator in 26090ns
I0206 01:25:03.811828  2824 leveldb.cpp:202] Seeked to beginning of db in 3173ns
I0206 01:25:03.811839  2824 leveldb.cpp:271] Iterated through 0 keys in the db in 497ns
I0206 01:25:03.811900  2824 replica.cpp:779] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned
I0206 01:25:03.812785  2849 recover.cpp:447] Starting replica recovery
I0206 01:25:03.813043  2849 recover.cpp:473] Replica is in EMPTY status
I0206 01:25:03.814668  2854 replica.cpp:673] Replica in EMPTY status received a broadcasted recover request from (371)@172.17.0.8:37843
I0206 01:25:03.815210  2849 recover.cpp:193] Received a recover response from a replica in EMPTY status
I0206 01:25:03.815732  2854 recover.cpp:564] Updating replica status to STARTING
I0206 01:25:03.819664  2857 master.cpp:376] Master 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de (74ef606c4063) started on 172.17.0.8:37843
I0206 01:25:03.819703  2857 master.cpp:378] Flags at startup: --acls="""" --allocation_interval=""1secs"" --allocator=""HierarchicalDRF"" --authenticate=""true"" --authenticate_http=""true"" --authenticate_slaves=""true"" --authenticators=""crammd5"" --authorizers=""local"" --credentials=""/tmp/h5vu5I/credentials"" --framework_sorter=""drf"" --help=""false"" --hostname_lookup=""true"" --http_authenticators=""basic"" --initialize_driver_logging=""true"" --log_auto_initialize=""true"" --logbufsecs=""0"" --logging_level=""INFO"" --max_completed_frameworks=""50"" --max_completed_tasks_per_framework=""1000"" --max_slave_ping_timeouts=""5"" --quiet=""false"" --recovery_slave_removal_limit=""100%"" --registry=""replicated_log"" --registry_fetch_timeout=""1mins"" --registry_store_timeout=""100secs"" --registry_strict=""true"" --root_submissions=""true"" --slave_ping_timeout=""15secs"" --slave_reregister_timeout=""10mins"" --user_sorter=""drf"" --version=""false"" --webui_dir=""/mesos/mesos-0.28.0/_inst/share/mesos/webui"" --work_dir=""/tmp/h5vu5I/master"" --zk_session_timeout=""10secs""
I0206 01:25:03.820241  2857 master.cpp:423] Master only allowing authenticated frameworks to register
I0206 01:25:03.820257  2857 master.cpp:428] Master only allowing authenticated slaves to register
I0206 01:25:03.820269  2857 credentials.hpp:35] Loading credentials for authentication from '/tmp/h5vu5I/credentials'
I0206 01:25:03.821110  2857 master.cpp:468] Using default 'crammd5' authenticator
I0206 01:25:03.821311  2857 master.cpp:537] Using default 'basic' HTTP authenticator
I0206 01:25:03.821636  2857 master.cpp:571] Authorization enabled
I0206 01:25:03.821979  2846 hierarchical.cpp:144] Initialized hierarchical allocator process
I0206 01:25:03.822057  2846 whitelist_watcher.cpp:77] No whitelist given
I0206 01:25:03.825460  2847 master.cpp:1712] The newly elected leader is master@172.17.0.8:37843 with id 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de
I0206 01:25:03.825512  2847 master.cpp:1725] Elected as the leading master!
I0206 01:25:03.825533  2847 master.cpp:1470] Recovering from registrar
I0206 01:25:03.825835  2847 registrar.cpp:307] Recovering registrar
I0206 01:25:03.848212  2854 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 32.226093ms
I0206 01:25:03.848299  2854 replica.cpp:320] Persisted replica status to STARTING
I0206 01:25:03.848702  2854 recover.cpp:473] Replica is in STARTING status
I0206 01:25:03.850728  2858 replica.cpp:673] Replica in STARTING status received a broadcasted recover request from (373)@172.17.0.8:37843
I0206 01:25:03.851230  2854 recover.cpp:193] Received a recover response from a replica in STARTING status
I0206 01:25:03.852018  2854 recover.cpp:564] Updating replica status to VOTING
I0206 01:25:03.881681  2854 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 29.184163ms
I0206 01:25:03.881772  2854 replica.cpp:320] Persisted replica status to VOTING
I0206 01:25:03.882058  2854 recover.cpp:578] Successfully joined the Paxos group
I0206 01:25:03.882258  2854 recover.cpp:462] Recover process terminated
I0206 01:25:03.883076  2854 log.cpp:659] Attempting to start the writer
I0206 01:25:03.885040  2854 replica.cpp:493] Replica received implicit promise request from (374)@172.17.0.8:37843 with proposal 1
I0206 01:25:03.915132  2854 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 29.980589ms
I0206 01:25:03.915215  2854 replica.cpp:342] Persisted promised to 1
I0206 01:25:03.916038  2856 coordinator.cpp:238] Coordinator attempting to fill missing positions
I0206 01:25:03.917659  2856 replica.cpp:388] Replica received explicit promise request from (375)@172.17.0.8:37843 for position 0 with proposal 2
I0206 01:25:03.948698  2856 leveldb.cpp:341] Persisting action (8 bytes) to leveldb took 30.974607ms
I0206 01:25:03.948786  2856 replica.cpp:712] Persisted action at 0
I0206 01:25:03.950920  2849 replica.cpp:537] Replica received write request for position 0 from (376)@172.17.0.8:37843
I0206 01:25:03.951011  2849 leveldb.cpp:436] Reading position from leveldb took 44263ns
I0206 01:25:03.982026  2849 leveldb.cpp:341] Persisting action (14 bytes) to leveldb took 30.947321ms
I0206 01:25:03.982225  2849 replica.cpp:712] Persisted action at 0
I0206 01:25:03.983867  2849 replica.cpp:691] Replica received learned notice for position 0 from @0.0.0.0:0
I0206 01:25:04.015499  2849 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 30.957888ms
I0206 01:25:04.015591  2849 replica.cpp:712] Persisted action at 0
I0206 01:25:04.015682  2849 replica.cpp:697] Replica learned NOP action at position 0
I0206 01:25:04.016666  2849 log.cpp:675] Writer started with ending position 0
I0206 01:25:04.017881  2855 leveldb.cpp:436] Reading position from leveldb took 56779ns
I0206 01:25:04.018934  2852 registrar.cpp:340] Successfully fetched the registry (0B) in 193.048064ms
I0206 01:25:04.019076  2852 registrar.cpp:439] Applied 1 operations in 38180ns; attempting to update the 'registry'
I0206 01:25:04.020100  2844 log.cpp:683] Attempting to append 170 bytes to the log
I0206 01:25:04.020288  2855 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 1
I0206 01:25:04.021323  2844 replica.cpp:537] Replica received write request for position 1 from (377)@172.17.0.8:37843
I0206 01:25:04.054726  2844 leveldb.cpp:341] Persisting action (189 bytes) to leveldb took 33.309419ms
I0206 01:25:04.054818  2844 replica.cpp:712] Persisted action at 1
I0206 01:25:04.055933  2844 replica.cpp:691] Replica received learned notice for position 1 from @0.0.0.0:0
I0206 01:25:04.088142  2844 leveldb.cpp:341] Persisting action (191 bytes) to leveldb took 32.116643ms
I0206 01:25:04.088230  2844 replica.cpp:712] Persisted action at 1
I0206 01:25:04.088265  2844 replica.cpp:697] Replica learned APPEND action at position 1
I0206 01:25:04.090070  2856 registrar.cpp:484] Successfully updated the 'registry' in 70.90816ms
I0206 01:25:04.090338  2851 log.cpp:702] Attempting to truncate the log to 1
I0206 01:25:04.090358  2856 registrar.cpp:370] Successfully recovered registrar
I0206 01:25:04.090507  2847 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 2
I0206 01:25:04.090867  2858 master.cpp:1522] Recovered 0 slaves from the Registry (131B) ; allowing 10mins for slaves to re-register
I0206 01:25:04.091449  2858 hierarchical.cpp:171] Skipping recovery of hierarchical allocator: nothing to recover
I0206 01:25:04.092280  2857 replica.cpp:537] Replica received write request for position 2 from (378)@172.17.0.8:37843
I0206 01:25:04.125702  2857 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 33.192265ms
I0206 01:25:04.125804  2857 replica.cpp:712] Persisted action at 2
I0206 01:25:04.127400  2857 replica.cpp:691] Replica received learned notice for position 2 from @0.0.0.0:0
I0206 01:25:04.157727  2857 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 30.268594ms
I0206 01:25:04.157905  2857 leveldb.cpp:399] Deleting ~1 keys from leveldb took 88436ns
I0206 01:25:04.157941  2857 replica.cpp:712] Persisted action at 2
I0206 01:25:04.157984  2857 replica.cpp:697] Replica learned TRUNCATE action at position 2
I0206 01:25:04.166174  2824 containerizer.cpp:149] Using isolation: posix/cpu,posix/mem,filesystem/posix
W0206 01:25:04.166954  2824 backend.cpp:48] Failed to create 'bind' backend: BindBackend requires root privileges
I0206 01:25:04.172008  2844 slave.cpp:193] Slave started on 9)@172.17.0.8:37843
I0206 01:25:04.172046  2844 slave.cpp:194] Flags at startup: --appc_store_dir=""/tmp/mesos/store/appc"" --authenticatee=""crammd5"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --credential=""/tmp/ContainerLoggerTest_DefaultToSandbox_FMaKSw/credential"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_auth_server=""https://auth.docker.io"" --docker_kill_orphans=""true"" --docker_puller_timeout=""60"" --docker_registry=""https://registry-1.docker.io"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/tmp/mesos/store/docker"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/tmp/ContainerLoggerTest_DefaultToSandbox_FMaKSw/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --hostname_lookup=""true"" --image_provisioner_backend=""copy"" --initialize_driver_logging=""true"" --isolation=""posix/cpu,posix/mem"" --launcher_dir=""/mesos/mesos-0.28.0/_build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""10ms"" --resources=""cpus:2;mem:1024;disk:1024;ports:[31000-32000]"" --revocable_cpu_low_priority=""true"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --systemd_runtime_directory=""/run/systemd/system"" --version=""false"" --work_dir=""/tmp/ContainerLoggerTest_DefaultToSandbox_FMaKSw""
I0206 01:25:04.172569  2844 credentials.hpp:83] Loading credential for authentication from '/tmp/ContainerLoggerTest_DefaultToSandbox_FMaKSw/credential'
I0206 01:25:04.172886  2844 slave.cpp:324] Slave using credential for: test-principal
I0206 01:25:04.173141  2844 resources.cpp:564] Parsing resources as JSON failed: cpus:2;mem:1024;disk:1024;ports:[31000-32000]
Trying semicolon-delimited string format instead
I0206 01:25:04.173620  2844 slave.cpp:464] Slave resources: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I0206 01:25:04.173686  2844 slave.cpp:472] Slave attributes: [  ]
I0206 01:25:04.173702  2844 slave.cpp:477] Slave hostname: 74ef606c4063
I0206 01:25:04.174816  2847 state.cpp:58] Recovering state from '/tmp/ContainerLoggerTest_DefaultToSandbox_FMaKSw/meta'
I0206 01:25:04.175441  2847 status_update_manager.cpp:200] Recovering status update manager
I0206 01:25:04.175678  2858 containerizer.cpp:397] Recovering containerizer
I0206 01:25:04.177573  2858 provisioner.cpp:245] Provisioner recovery complete
I0206 01:25:04.178231  2847 slave.cpp:4496] Finished recovery
I0206 01:25:04.178834  2847 slave.cpp:4668] Querying resource estimator for oversubscribable resources
I0206 01:25:04.179405  2847 slave.cpp:796] New master detected at master@172.17.0.8:37843
I0206 01:25:04.179500  2847 slave.cpp:859] Authenticating with master master@172.17.0.8:37843
I0206 01:25:04.179525  2847 slave.cpp:864] Using default CRAM-MD5 authenticatee
I0206 01:25:04.179656  2858 status_update_manager.cpp:174] Pausing sending status updates
I0206 01:25:04.179798  2847 slave.cpp:832] Detecting new master
I0206 01:25:04.179891  2852 authenticatee.cpp:121] Creating new client SASL connection
I0206 01:25:04.179916  2847 slave.cpp:4682] Received oversubscribable resources  from the resource estimator
I0206 01:25:04.180286  2847 master.cpp:5523] Authenticating slave(9)@172.17.0.8:37843
I0206 01:25:04.180569  2847 authenticator.cpp:413] Starting authentication session for crammd5_authenticatee(32)@172.17.0.8:37843
I0206 01:25:04.181000  2847 authenticator.cpp:98] Creating new server SASL connection
I0206 01:25:04.181315  2847 authenticatee.cpp:212] Received SASL authentication mechanisms: CRAM-MD5
I0206 01:25:04.181387  2847 authenticatee.cpp:238] Attempting to authenticate with mechanism 'CRAM-MD5'
I0206 01:25:04.181562  2847 authenticator.cpp:203] Received SASL authentication start
I0206 01:25:04.181648  2847 authenticator.cpp:325] Authentication requires more steps
I0206 01:25:04.181843  2847 authenticatee.cpp:258] Received SASL authentication step
I0206 01:25:04.182034  2853 authenticator.cpp:231] Received SASL authentication step
I0206 01:25:04.182071  2853 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: '74ef606c4063' server FQDN: '74ef606c4063' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0206 01:25:04.182093  2853 auxprop.cpp:179] Looking up auxiliary property '*userPassword'
I0206 01:25:04.182145  2853 auxprop.cpp:179] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0206 01:25:04.182173  2853 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: '74ef606c4063' server FQDN: '74ef606c4063' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0206 01:25:04.182185  2853 auxprop.cpp:129] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0206 01:25:04.182193  2853 auxprop.cpp:129] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0206 01:25:04.182211  2853 authenticator.cpp:317] Authentication success
I0206 01:25:04.182333  2849 authenticatee.cpp:298] Authentication success
I0206 01:25:04.182422  2853 master.cpp:5553] Successfully authenticated principal 'test-principal' at slave(9)@172.17.0.8:37843
I0206 01:25:04.182510  2853 authenticator.cpp:431] Authentication session cleanup for crammd5_authenticatee(32)@172.17.0.8:37843
I0206 01:25:04.182945  2849 slave.cpp:927] Successfully authenticated with master master@172.17.0.8:37843
I0206 01:25:04.183178  2849 slave.cpp:1321] Will retry registration in 9.87937ms if necessary
I0206 01:25:04.183466  2852 master.cpp:4237] Registering slave at slave(9)@172.17.0.8:37843 (74ef606c4063) with id 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-S0
I0206 01:25:04.184039  2845 registrar.cpp:439] Applied 1 operations in 89453ns; attempting to update the 'registry'
I0206 01:25:04.185288  2856 log.cpp:683] Attempting to append 339 bytes to the log
I0206 01:25:04.185672  2850 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 3
I0206 01:25:04.186674  2846 replica.cpp:537] Replica received write request for position 3 from (392)@172.17.0.8:37843
I0206 01:25:04.195863  2856 slave.cpp:1321] Will retry registration in 11.038094ms if necessary
I0206 01:25:04.196233  2856 master.cpp:4225] Ignoring register slave message from slave(9)@172.17.0.8:37843 (74ef606c4063) as admission is already in progress
I0206 01:25:04.208094  2856 slave.cpp:1321] Will retry registration in 27.881223ms if necessary
I0206 01:25:04.208472  2856 master.cpp:4225] Ignoring register slave message from slave(9)@172.17.0.8:37843 (74ef606c4063) as admission is already in progress
I0206 01:25:04.216698  2846 leveldb.cpp:341] Persisting action (358 bytes) to leveldb took 29.961291ms
I0206 01:25:04.216789  2846 replica.cpp:712] Persisted action at 3
I0206 01:25:04.218246  2845 replica.cpp:691] Replica received learned notice for position 3 from @0.0.0.0:0
I0206 01:25:04.237861  2846 slave.cpp:1321] Will retry registration in 1.006941ms if necessary
I0206 01:25:04.238221  2846 master.cpp:4225] Ignoring register slave message from slave(9)@172.17.0.8:37843 (74ef606c4063) as admission is already in progress
I0206 01:25:04.239858  2856 slave.cpp:1321] Will retry registration in 167.305686ms if necessary
I0206 01:25:04.240044  2856 master.cpp:4225] Ignoring register slave message from slave(9)@172.17.0.8:37843 (74ef606c4063) as admission is already in progress
I0206 01:25:04.241482  2845 leveldb.cpp:341] Persisting action (360 bytes) to leveldb took 23.193162ms
I0206 01:25:04.241524  2845 replica.cpp:712] Persisted action at 3
I0206 01:25:04.241557  2845 replica.cpp:697] Replica learned APPEND action at position 3
I0206 01:25:04.243746  2844 registrar.cpp:484] Successfully updated the 'registry' in 59.587072ms
I0206 01:25:04.244210  2857 log.cpp:702] Attempting to truncate the log to 3
I0206 01:25:04.244344  2845 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 4
I0206 01:25:04.244597  2856 master.cpp:4305] Registered slave 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-S0 at slave(9)@172.17.0.8:37843 (74ef606c4063) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I0206 01:25:04.244746  2843 slave.cpp:3436] Received ping from slave-observer(8)@172.17.0.8:37843
I0206 01:25:04.244976  2845 hierarchical.cpp:473] Added slave 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-S0 (74ef606c4063) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (allocated: )
I0206 01:25:04.245072  2843 slave.cpp:971] Registered with master master@172.17.0.8:37843; given slave ID 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-S0
I0206 01:25:04.245121  2843 fetcher.cpp:81] Clearing fetcher cache
I0206 01:25:04.245146  2845 hierarchical.cpp:1403] No resources available to allocate!
I0206 01:25:04.245178  2845 hierarchical.cpp:1116] Performed allocation for slave 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-S0 in 159744ns
I0206 01:25:04.245465  2846 status_update_manager.cpp:181] Resuming sending status updates
I0206 01:25:04.245776  2843 slave.cpp:994] Checkpointing SlaveInfo to '/tmp/ContainerLoggerTest_DefaultToSandbox_FMaKSw/meta/slaves/914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-S0/slave.info'
I0206 01:25:04.245745  2846 replica.cpp:537] Replica received write request for position 4 from (393)@172.17.0.8:37843
I0206 01:25:04.246273  2843 slave.cpp:1030] Forwarding total oversubscribed resources 
I0206 01:25:04.246507  2850 master.cpp:4646] Received update of slave 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-S0 at slave(9)@172.17.0.8:37843 (74ef606c4063) with total oversubscribed resources 
I0206 01:25:04.247180  2824 sched.cpp:222] Version: 0.28.0
I0206 01:25:04.247155  2850 hierarchical.cpp:531] Slave 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-S0 (74ef606c4063) updated with oversubscribed resources  (total: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000], allocated: )
I0206 01:25:04.247357  2850 hierarchical.cpp:1403] No resources available to allocate!
I0206 01:25:04.247406  2850 hierarchical.cpp:1116] Performed allocation for slave 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-S0 in 183250ns
I0206 01:25:04.247938  2854 sched.cpp:326] New master detected at master@172.17.0.8:37843
I0206 01:25:04.248157  2854 sched.cpp:382] Authenticating with master master@172.17.0.8:37843
I0206 01:25:04.248265  2854 sched.cpp:389] Using default CRAM-MD5 authenticatee
I0206 01:25:04.248769  2854 authenticatee.cpp:121] Creating new client SASL connection
I0206 01:25:04.249311  2854 master.cpp:5523] Authenticating scheduler-f50aad75-78d0-4d9f-b1a4-488d5ab932d6@172.17.0.8:37843
I0206 01:25:04.249646  2854 authenticator.cpp:413] Starting authentication session for crammd5_authenticatee(33)@172.17.0.8:37843
I0206 01:25:04.250114  2854 authenticator.cpp:98] Creating new server SASL connection
I0206 01:25:04.250453  2854 authenticatee.cpp:212] Received SASL authentication mechanisms: CRAM-MD5
I0206 01:25:04.250525  2854 authenticatee.cpp:238] Attempting to authenticate with mechanism 'CRAM-MD5'
I0206 01:25:04.250814  2853 authenticator.cpp:203] Received SASL authentication start
I0206 01:25:04.250881  2853 authenticator.cpp:325] Authentication requires more steps
I0206 01:25:04.250982  2853 authenticatee.cpp:258] Received SASL authentication step
I0206 01:25:04.251092  2853 authenticator.cpp:231] Received SASL authentication step
I0206 01:25:04.251128  2853 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: '74ef606c4063' server FQDN: '74ef606c4063' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0206 01:25:04.251144  2853 auxprop.cpp:179] Looking up auxiliary property '*userPassword'
I0206 01:25:04.251200  2853 auxprop.cpp:179] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0206 01:25:04.251242  2853 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: '74ef606c4063' server FQDN: '74ef606c4063' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0206 01:25:04.251260  2853 auxprop.cpp:129] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0206 01:25:04.251269  2853 auxprop.cpp:129] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0206 01:25:04.251288  2853 authenticator.cpp:317] Authentication success
I0206 01:25:04.251471  2853 authenticatee.cpp:298] Authentication success
I0206 01:25:04.251574  2853 master.cpp:5553] Successfully authenticated principal 'test-principal' at scheduler-f50aad75-78d0-4d9f-b1a4-488d5ab932d6@172.17.0.8:37843
I0206 01:25:04.251669  2851 authenticator.cpp:431] Authentication session cleanup for crammd5_authenticatee(33)@172.17.0.8:37843
I0206 01:25:04.252162  2854 sched.cpp:471] Successfully authenticated with master master@172.17.0.8:37843
I0206 01:25:04.252188  2854 sched.cpp:776] Sending SUBSCRIBE call to master@172.17.0.8:37843
I0206 01:25:04.252286  2854 sched.cpp:809] Will retry registration in 1.575999657secs if necessary
I0206 01:25:04.252583  2853 master.cpp:2280] Received SUBSCRIBE call for framework 'default' at scheduler-f50aad75-78d0-4d9f-b1a4-488d5ab932d6@172.17.0.8:37843
I0206 01:25:04.252694  2853 master.cpp:1751] Authorizing framework principal 'test-principal' to receive offers for role '*'
I0206 01:25:04.253110  2853 master.cpp:2351] Subscribing framework default with checkpointing disabled and capabilities [  ]
I0206 01:25:04.253703  2843 hierarchical.cpp:265] Added framework 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000
I0206 01:25:04.255300  2843 hierarchical.cpp:1498] No inverse offers to send out!
I0206 01:25:04.255367  2843 hierarchical.cpp:1096] Performed allocation for 1 slaves in 1.621522ms
I0206 01:25:04.255820  2844 sched.cpp:703] Framework registered with 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000
I0206 01:25:04.256006  2844 sched.cpp:717] Scheduler::registered took 105156ns
I0206 01:25:04.256572  2853 master.cpp:5352] Sending 1 offers to framework 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000 (default) at scheduler-f50aad75-78d0-4d9f-b1a4-488d5ab932d6@172.17.0.8:37843
I0206 01:25:04.257524  2853 sched.cpp:873] Scheduler::resourceOffers took 173470ns
I0206 01:25:04.260818  2855 master.cpp:3138] Processing ACCEPT call for offers: [ 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-O0 ] on slave 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-S0 at slave(9)@172.17.0.8:37843 (74ef606c4063) for framework 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000 (default) at scheduler-f50aad75-78d0-4d9f-b1a4-488d5ab932d6@172.17.0.8:37843
I0206 01:25:04.260968  2855 master.cpp:2825] Authorizing framework principal 'test-principal' to launch task 0e7267ed-c5ed-4914-9042-5970b2aaec1c as user 'mesos'
I0206 01:25:04.264458  2844 master.hpp:176] Adding task 0e7267ed-c5ed-4914-9042-5970b2aaec1c with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-S0 (74ef606c4063)
I0206 01:25:04.264796  2844 master.cpp:3623] Launching task 0e7267ed-c5ed-4914-9042-5970b2aaec1c of framework 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000 (default) at scheduler-f50aad75-78d0-4d9f-b1a4-488d5ab932d6@172.17.0.8:37843 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-S0 at slave(9)@172.17.0.8:37843 (74ef606c4063)
I0206 01:25:04.265341  2855 slave.cpp:1361] Got assigned task 0e7267ed-c5ed-4914-9042-5970b2aaec1c for framework 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000
I0206 01:25:04.265941  2855 resources.cpp:564] Parsing resources as JSON failed: cpus:0.1;mem:32
Trying semicolon-delimited string format instead
I0206 01:25:04.267323  2855 slave.cpp:1480] Launching task 0e7267ed-c5ed-4914-9042-5970b2aaec1c for framework 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000
I0206 01:25:04.267627  2855 resources.cpp:564] Parsing resources as JSON failed: cpus:0.1;mem:32
Trying semicolon-delimited string format instead
I0206 01:25:04.268705  2855 paths.cpp:474] Trying to chown '/tmp/ContainerLoggerTest_DefaultToSandbox_FMaKSw/slaves/914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-S0/frameworks/914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000/executors/0e7267ed-c5ed-4914-9042-5970b2aaec1c/runs/5c952202-44cf-427a-8452-0f501140a4b7' to user 'mesos'
I0206 01:25:04.274116  2855 slave.cpp:5282] Launching executor 0e7267ed-c5ed-4914-9042-5970b2aaec1c of framework 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000 with resources cpus(*):0.1; mem(*):32 in work directory '/tmp/ContainerLoggerTest_DefaultToSandbox_FMaKSw/slaves/914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-S0/frameworks/914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000/executors/0e7267ed-c5ed-4914-9042-5970b2aaec1c/runs/5c952202-44cf-427a-8452-0f501140a4b7'
I0206 01:25:04.275185  2844 containerizer.cpp:656] Starting container '5c952202-44cf-427a-8452-0f501140a4b7' for executor '0e7267ed-c5ed-4914-9042-5970b2aaec1c' of framework '914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000'
I0206 01:25:04.275311  2846 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 29.403837ms
I0206 01:25:04.275390  2846 replica.cpp:712] Persisted action at 4
I0206 01:25:04.275511  2855 slave.cpp:1698] Queuing task '0e7267ed-c5ed-4914-9042-5970b2aaec1c' for executor '0e7267ed-c5ed-4914-9042-5970b2aaec1c' of framework 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000
I0206 01:25:04.275832  2855 slave.cpp:749] Successfully attached file '/tmp/ContainerLoggerTest_DefaultToSandbox_FMaKSw/slaves/914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-S0/frameworks/914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000/executors/0e7267ed-c5ed-4914-9042-5970b2aaec1c/runs/5c952202-44cf-427a-8452-0f501140a4b7'
I0206 01:25:04.276707  2855 replica.cpp:691] Replica received learned notice for position 4 from @0.0.0.0:0
I0206 01:25:04.284708  2844 launcher.cpp:132] Forked child with pid '2872' for container '5c952202-44cf-427a-8452-0f501140a4b7'
I0206 01:25:04.301365  2855 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 24.497489ms
I0206 01:25:04.301528  2855 leveldb.cpp:399] Deleting ~2 keys from leveldb took 92156ns
I0206 01:25:04.301563  2855 replica.cpp:712] Persisted action at 4
I0206 01:25:04.301640  2855 replica.cpp:697] Replica learned TRUNCATE action at position 4
I0206 01:25:04.823314  2854 hierarchical.cpp:1403] No resources available to allocate!
I0206 01:25:04.823387  2854 hierarchical.cpp:1498] No inverse offers to send out!
I0206 01:25:04.823420  2854 hierarchical.cpp:1096] Performed allocation for 1 slaves in 327509ns
I0206 01:25:05.825943  2850 hierarchical.cpp:1403] No resources available to allocate!
I0206 01:25:05.826027  2850 hierarchical.cpp:1498] No inverse offers to send out!
I0206 01:25:05.826066  2850 hierarchical.cpp:1096] Performed allocation for 1 slaves in 362856ns
I0206 01:25:06.827154  2857 hierarchical.cpp:1403] No resources available to allocate!
I0206 01:25:06.827235  2857 hierarchical.cpp:1498] No inverse offers to send out!
I0206 01:25:06.827275  2857 hierarchical.cpp:1096] Performed allocation for 1 slaves in 328221ns
I0206 01:25:07.828547  2843 hierarchical.cpp:1403] No resources available to allocate!
I0206 01:25:07.828753  2843 hierarchical.cpp:1498] No inverse offers to send out!
I0206 01:25:07.828907  2843 hierarchical.cpp:1096] Performed allocation for 1 slaves in 624979ns
I0206 01:25:08.829737  2855 hierarchical.cpp:1403] No resources available to allocate!
I0206 01:25:08.829918  2855 hierarchical.cpp:1498] No inverse offers to send out!
I0206 01:25:08.830070  2855 hierarchical.cpp:1096] Performed allocation for 1 slaves in 596793ns
I0206 01:25:09.831233  2856 hierarchical.cpp:1403] No resources available to allocate!
I0206 01:25:09.831316  2856 hierarchical.cpp:1498] No inverse offers to send out!
I0206 01:25:09.831352  2856 hierarchical.cpp:1096] Performed allocation for 1 slaves in 353864ns
I0206 01:25:10.832953  2849 hierarchical.cpp:1403] No resources available to allocate!
I0206 01:25:10.833307  2849 hierarchical.cpp:1498] No inverse offers to send out!
I0206 01:25:10.833411  2849 hierarchical.cpp:1096] Performed allocation for 1 slaves in 731864ns
I0206 01:25:11.834967  2847 hierarchical.cpp:1403] No resources available to allocate!
I0206 01:25:11.835149  2847 hierarchical.cpp:1498] No inverse offers to send out!
I0206 01:25:11.835294  2847 hierarchical.cpp:1096] Performed allocation for 1 slaves in 586988ns
I0206 01:25:12.174247  2853 slave.cpp:2643] Got registration for executor '0e7267ed-c5ed-4914-9042-5970b2aaec1c' of framework 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000 from executor(1)@172.17.0.8:43659
I0206 01:25:12.179061  2844 slave.cpp:1863] Sending queued task '0e7267ed-c5ed-4914-9042-5970b2aaec1c' to executor '0e7267ed-c5ed-4914-9042-5970b2aaec1c' of framework 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000 at executor(1)@172.17.0.8:43659
I0206 01:25:12.194753  2858 slave.cpp:3002] Handling status update TASK_RUNNING (UUID: 9d924a5b-76ab-4886-8091-7af3428ff179) for task 0e7267ed-c5ed-4914-9042-5970b2aaec1c of framework 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000 from executor(1)@172.17.0.8:43659
I0206 01:25:12.195852  2858 status_update_manager.cpp:320] Received status update TASK_RUNNING (UUID: 9d924a5b-76ab-4886-8091-7af3428ff179) for task 0e7267ed-c5ed-4914-9042-5970b2aaec1c of framework 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000
I0206 01:25:12.196094  2858 status_update_manager.cpp:497] Creating StatusUpdate stream for task 0e7267ed-c5ed-4914-9042-5970b2aaec1c of framework 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000
I0206 01:25:12.197000  2858 status_update_manager.cpp:374] Forwarding update TASK_RUNNING (UUID: 9d924a5b-76ab-4886-8091-7af3428ff179) for task 0e7267ed-c5ed-4914-9042-5970b2aaec1c of framework 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000 to the slave
I0206 01:25:12.197739  2855 slave.cpp:3354] Forwarding the update TASK_RUNNING (UUID: 9d924a5b-76ab-4886-8091-7af3428ff179) for task 0e7267ed-c5ed-4914-9042-5970b2aaec1c of framework 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000 to master@172.17.0.8:37843
I0206 01:25:12.198442  2855 master.cpp:4791] Status update TASK_RUNNING (UUID: 9d924a5b-76ab-4886-8091-7af3428ff179) for task 0e7267ed-c5ed-4914-9042-5970b2aaec1c of framework 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000 from slave 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-S0 at slave(9)@172.17.0.8:37843 (74ef606c4063)
I0206 01:25:12.198673  2855 master.cpp:4839] Forwarding status update TASK_RUNNING (UUID: 9d924a5b-76ab-4886-8091-7af3428ff179) for task 0e7267ed-c5ed-4914-9042-5970b2aaec1c of framework 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000
I0206 01:25:12.199038  2855 master.cpp:6447] Updating the state of task 0e7267ed-c5ed-4914-9042-5970b2aaec1c of framework 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000 (latest state: TASK_RUNNING, status update state: TASK_RUNNING)
I0206 01:25:12.199581  2854 sched.cpp:981] Scheduler::statusUpdate took 159022ns
I0206 01:25:12.200568  2854 master.cpp:3949] Processing ACKNOWLEDGE call 9d924a5b-76ab-4886-8091-7af3428ff179 for task 0e7267ed-c5ed-4914-9042-5970b2aaec1c of framework 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000 (default) at scheduler-f50aad75-78d0-4d9f-b1a4-488d5ab932d6@172.17.0.8:37843 on slave 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-S0
I0206 01:25:12.201513  2858 status_update_manager.cpp:392] Received status update acknowledgement (UUID: 9d924a5b-76ab-4886-8091-7af3428ff179) for task 0e7267ed-c5ed-4914-9042-5970b2aaec1c of framework 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000
../../src/tests/container_logger_tests.cpp:350: Failure
Value of: strings::contains(stdout.get(), ""Hello World!"")
  Actual: false
Expected: true
I0206 01:25:12.201702  2824 sched.cpp:1903] Asked to stop the driver
I0206 01:25:12.202831  2848 sched.cpp:1143] Stopping framework '914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000'
I0206 01:25:12.203284  2848 master.cpp:5923] Processing TEARDOWN call for framework 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000 (default) at scheduler-f50aad75-78d0-4d9f-b1a4-488d5ab932d6@172.17.0.8:37843
I0206 01:25:12.203321  2848 master.cpp:5935] Removing framework 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000 (default) at scheduler-f50aad75-78d0-4d9f-b1a4-488d5ab932d6@172.17.0.8:37843
I0206 01:25:12.201762  2854 slave.cpp:3248] Status update manager successfully handled status update TASK_RUNNING (UUID: 9d924a5b-76ab-4886-8091-7af3428ff179) for task 0e7267ed-c5ed-4914-9042-5970b2aaec1c of framework 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000
I0206 01:25:12.203384  2854 slave.cpp:3264] Sending acknowledgement for status update TASK_RUNNING (UUID: 9d924a5b-76ab-4886-8091-7af3428ff179) for task 0e7267ed-c5ed-4914-9042-5970b2aaec1c of framework 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000 to executor(1)@172.17.0.8:43659
I0206 01:25:12.204712  2843 hierarchical.cpp:375] Deactivated framework 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000
I0206 01:25:12.204953  2848 master.cpp:6447] Updating the state of task 0e7267ed-c5ed-4914-9042-5970b2aaec1c of framework 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000 (latest state: TASK_KILLED, status update state: TASK_KILLED)
I0206 01:25:12.205885  2854 slave.cpp:2412] Status update manager successfully handled status update acknowledgement (UUID: 9d924a5b-76ab-4886-8091-7af3428ff179) for task 0e7267ed-c5ed-4914-9042-5970b2aaec1c of framework 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000
I0206 01:25:12.206082  2854 slave.cpp:2079] Asked to shut down framework 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000 by master@172.17.0.8:37843
I0206 01:25:12.206125  2854 slave.cpp:2104] Shutting down framework 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000
I0206 01:25:12.206331  2854 slave.cpp:4129] Shutting down executor '0e7267ed-c5ed-4914-9042-5970b2aaec1c' of framework 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000 at executor(1)@172.17.0.8:43659
I0206 01:25:12.206408  2843 hierarchical.cpp:892] Recovered cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (total: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000], allocated: ) on slave 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-S0 from framework 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000
I0206 01:25:12.207352  2848 master.cpp:6513] Removing task 0e7267ed-c5ed-4914-9042-5970b2aaec1c with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] of framework 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000 on slave 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-S0 at slave(9)@172.17.0.8:37843 (74ef606c4063)
I0206 01:25:12.208258  2848 master.cpp:1027] Master terminating
I0206 01:25:12.208703  2857 hierarchical.cpp:326] Removed framework 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000
I0206 01:25:12.209658  2857 hierarchical.cpp:505] Removed slave 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-S0
I0206 01:25:12.212208  2848 slave.cpp:3482] master@172.17.0.8:37843 exited
W0206 01:25:12.212261  2848 slave.cpp:3485] Master disconnected! Waiting for a new master to be elected
I0206 01:25:12.224596  2854 containerizer.cpp:1318] Destroying container '5c952202-44cf-427a-8452-0f501140a4b7'
I0206 01:25:12.241466  2852 slave.cpp:3482] executor(1)@172.17.0.8:43659 exited
I0206 01:25:12.250931  2856 containerizer.cpp:1534] Executor for container '5c952202-44cf-427a-8452-0f501140a4b7' has exited
I0206 01:25:12.253350  2850 provisioner.cpp:306] Ignoring destroy request for unknown container 5c952202-44cf-427a-8452-0f501140a4b7
I0206 01:25:12.253885  2850 slave.cpp:3817] Executor '0e7267ed-c5ed-4914-9042-5970b2aaec1c' of framework 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000 terminated with signal Killed
I0206 01:25:12.254125  2850 slave.cpp:3921] Cleaning up executor '0e7267ed-c5ed-4914-9042-5970b2aaec1c' of framework 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000 at executor(1)@172.17.0.8:43659
I0206 01:25:12.254545  2847 gc.cpp:54] Scheduling '/tmp/ContainerLoggerTest_DefaultToSandbox_FMaKSw/slaves/914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-S0/frameworks/914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000/executors/0e7267ed-c5ed-4914-9042-5970b2aaec1c/runs/5c952202-44cf-427a-8452-0f501140a4b7' for gc 6.99999705530074days in the future
I0206 01:25:12.254803  2850 slave.cpp:4009] Cleaning up framework 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000
I0206 01:25:12.254822  2847 gc.cpp:54] Scheduling '/tmp/ContainerLoggerTest_DefaultToSandbox_FMaKSw/slaves/914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-S0/frameworks/914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000/executors/0e7267ed-c5ed-4914-9042-5970b2aaec1c' for gc 6.99999705202667days in the future
I0206 01:25:12.255084  2857 status_update_manager.cpp:282] Closing status update streams for framework 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000
I0206 01:25:12.255143  2856 gc.cpp:54] Scheduling '/tmp/ContainerLoggerTest_DefaultToSandbox_FMaKSw/slaves/914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-S0/frameworks/914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000' for gc 6.99999704808days in the future
I0206 01:25:12.255190  2857 status_update_manager.cpp:528] Cleaning up status update stream for task 0e7267ed-c5ed-4914-9042-5970b2aaec1c of framework 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000
I0206 01:25:12.255192  2850 slave.cpp:668] Slave terminating
[  FAILED  ] ContainerLoggerTest.DefaultToSandbox (8566 ms)
{code}",Bug,Major,greggomann,2016-02-17T15:33:19.000+0000,5,Resolved,Complete,ContainerLoggerTest.DefaultToSandbox is flaky,2016-02-17T15:33:19.000+0000,MESOS-4615,1.0,mesos,Mesosphere Sprint 28
anandmazumdar,2016-02-06T00:56:24.000+0000,greggomann,"Just saw this failure on the ASF CI:

{code}
[ RUN      ] SlaveRecoveryTest/0.CleanupHTTPExecutor
I0206 00:22:44.791671  2824 leveldb.cpp:174] Opened db in 2.539372ms
I0206 00:22:44.792459  2824 leveldb.cpp:181] Compacted db in 740473ns
I0206 00:22:44.792510  2824 leveldb.cpp:196] Created db iterator in 24164ns
I0206 00:22:44.792532  2824 leveldb.cpp:202] Seeked to beginning of db in 1831ns
I0206 00:22:44.792548  2824 leveldb.cpp:271] Iterated through 0 keys in the db in 342ns
I0206 00:22:44.792605  2824 replica.cpp:779] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned
I0206 00:22:44.793256  2847 recover.cpp:447] Starting replica recovery
I0206 00:22:44.793480  2847 recover.cpp:473] Replica is in EMPTY status
I0206 00:22:44.794538  2847 replica.cpp:673] Replica in EMPTY status received a broadcasted recover request from (9472)@172.17.0.2:43484
I0206 00:22:44.795040  2848 recover.cpp:193] Received a recover response from a replica in EMPTY status
I0206 00:22:44.795644  2848 recover.cpp:564] Updating replica status to STARTING
I0206 00:22:44.796519  2850 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 752810ns
I0206 00:22:44.796545  2850 replica.cpp:320] Persisted replica status to STARTING
I0206 00:22:44.796725  2848 recover.cpp:473] Replica is in STARTING status
I0206 00:22:44.797828  2857 replica.cpp:673] Replica in STARTING status received a broadcasted recover request from (9473)@172.17.0.2:43484
I0206 00:22:44.798355  2850 recover.cpp:193] Received a recover response from a replica in STARTING status
I0206 00:22:44.799193  2850 recover.cpp:564] Updating replica status to VOTING
I0206 00:22:44.799583  2855 master.cpp:376] Master 0b206a40-a9c3-4d44-a5bd-8032d60a32ca (6632562f1ade) started on 172.17.0.2:43484
I0206 00:22:44.799609  2855 master.cpp:378] Flags at startup: --acls="""" --allocation_interval=""1secs"" --allocator=""HierarchicalDRF"" --authenticate=""true"" --authenticate_http=""true"" --authenticate_slaves=""true"" --authenticators=""crammd5"" --authorizers=""local"" --credentials=""/tmp/n2FxQV/credentials"" --framework_sorter=""drf"" --help=""false"" --hostname_lookup=""true"" --http_authenticators=""basic"" --initialize_driver_logging=""true"" --log_auto_initialize=""true"" --logbufsecs=""0"" --logging_level=""INFO"" --max_completed_frameworks=""50"" --max_completed_tasks_per_framework=""1000"" --max_slave_ping_timeouts=""5"" --quiet=""false"" --recovery_slave_removal_limit=""100%"" --registry=""replicated_log"" --registry_fetch_timeout=""1mins"" --registry_store_timeout=""100secs"" --registry_strict=""true"" --root_submissions=""true"" --slave_ping_timeout=""15secs"" --slave_reregister_timeout=""10mins"" --user_sorter=""drf"" --version=""false"" --webui_dir=""/mesos/mesos-0.28.0/_inst/share/mesos/webui"" --work_dir=""/tmp/n2FxQV/master"" --zk_session_timeout=""10secs""
I0206 00:22:44.799991  2855 master.cpp:423] Master only allowing authenticated frameworks to register
I0206 00:22:44.800009  2855 master.cpp:428] Master only allowing authenticated slaves to register
I0206 00:22:44.800020  2855 credentials.hpp:35] Loading credentials for authentication from '/tmp/n2FxQV/credentials'
I0206 00:22:44.800245  2850 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 679345ns
I0206 00:22:44.800370  2850 replica.cpp:320] Persisted replica status to VOTING
I0206 00:22:44.800397  2855 master.cpp:468] Using default 'crammd5' authenticator
I0206 00:22:44.800693  2855 master.cpp:537] Using default 'basic' HTTP authenticator
I0206 00:22:44.800815  2855 master.cpp:571] Authorization enabled
I0206 00:22:44.801216  2850 recover.cpp:578] Successfully joined the Paxos group
I0206 00:22:44.801604  2850 recover.cpp:462] Recover process terminated
I0206 00:22:44.801759  2856 whitelist_watcher.cpp:77] No whitelist given
I0206 00:22:44.801725  2847 hierarchical.cpp:144] Initialized hierarchical allocator process
I0206 00:22:44.803982  2855 master.cpp:1712] The newly elected leader is master@172.17.0.2:43484 with id 0b206a40-a9c3-4d44-a5bd-8032d60a32ca
I0206 00:22:44.804026  2855 master.cpp:1725] Elected as the leading master!
I0206 00:22:44.804059  2855 master.cpp:1470] Recovering from registrar
I0206 00:22:44.804424  2855 registrar.cpp:307] Recovering registrar
I0206 00:22:44.805202  2855 log.cpp:659] Attempting to start the writer
I0206 00:22:44.806782  2856 replica.cpp:493] Replica received implicit promise request from (9475)@172.17.0.2:43484 with proposal 1
I0206 00:22:44.807368  2856 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 547939ns
I0206 00:22:44.807395  2856 replica.cpp:342] Persisted promised to 1
I0206 00:22:44.808375  2856 coordinator.cpp:238] Coordinator attempting to fill missing positions
I0206 00:22:44.809460  2848 replica.cpp:388] Replica received explicit promise request from (9476)@172.17.0.2:43484 for position 0 with proposal 2
I0206 00:22:44.809929  2848 leveldb.cpp:341] Persisting action (8 bytes) to leveldb took 427561ns
I0206 00:22:44.809967  2848 replica.cpp:712] Persisted action at 0
I0206 00:22:44.811035  2850 replica.cpp:537] Replica received write request for position 0 from (9477)@172.17.0.2:43484
I0206 00:22:44.811149  2850 leveldb.cpp:436] Reading position from leveldb took 36452ns
I0206 00:22:44.811532  2850 leveldb.cpp:341] Persisting action (14 bytes) to leveldb took 318924ns
I0206 00:22:44.811615  2850 replica.cpp:712] Persisted action at 0
I0206 00:22:44.812532  2850 replica.cpp:691] Replica received learned notice for position 0 from @0.0.0.0:0
I0206 00:22:44.813117  2850 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 476530ns
I0206 00:22:44.813143  2850 replica.cpp:712] Persisted action at 0
I0206 00:22:44.813166  2850 replica.cpp:697] Replica learned NOP action at position 0
I0206 00:22:44.813984  2848 log.cpp:675] Writer started with ending position 0
I0206 00:22:44.815549  2848 leveldb.cpp:436] Reading position from leveldb took 31800ns
I0206 00:22:44.817061  2848 registrar.cpp:340] Successfully fetched the registry (0B) in 12.591104ms
I0206 00:22:44.817319  2848 registrar.cpp:439] Applied 1 operations in 63480ns; attempting to update the 'registry'
I0206 00:22:44.818780  2845 log.cpp:683] Attempting to append 170 bytes to the log
I0206 00:22:44.818981  2845 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 1
I0206 00:22:44.819941  2845 replica.cpp:537] Replica received write request for position 1 from (9478)@172.17.0.2:43484
I0206 00:22:44.820582  2845 leveldb.cpp:341] Persisting action (189 bytes) to leveldb took 600949ns
I0206 00:22:44.820608  2845 replica.cpp:712] Persisted action at 1
I0206 00:22:44.821552  2845 replica.cpp:691] Replica received learned notice for position 1 from @0.0.0.0:0
I0206 00:22:44.821934  2845 leveldb.cpp:341] Persisting action (191 bytes) to leveldb took 352813ns
I0206 00:22:44.821960  2845 replica.cpp:712] Persisted action at 1
I0206 00:22:44.821979  2845 replica.cpp:697] Replica learned APPEND action at position 1
I0206 00:22:44.823447  2845 registrar.cpp:484] Successfully updated the 'registry' in 5.987072ms
I0206 00:22:44.823580  2845 registrar.cpp:370] Successfully recovered registrar
I0206 00:22:44.823833  2845 log.cpp:702] Attempting to truncate the log to 1
I0206 00:22:44.824203  2845 master.cpp:1522] Recovered 0 slaves from the Registry (131B) ; allowing 10mins for slaves to re-register
I0206 00:22:44.824291  2845 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 2
I0206 00:22:44.824645  2845 hierarchical.cpp:171] Skipping recovery of hierarchical allocator: nothing to recover
I0206 00:22:44.825222  2850 replica.cpp:537] Replica received write request for position 2 from (9479)@172.17.0.2:43484
I0206 00:22:44.825742  2850 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 481617ns
I0206 00:22:44.825772  2850 replica.cpp:712] Persisted action at 2
I0206 00:22:44.826748  2852 replica.cpp:691] Replica received learned notice for position 2 from @0.0.0.0:0
I0206 00:22:44.827368  2852 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 588591ns
I0206 00:22:44.827432  2852 leveldb.cpp:399] Deleting ~1 keys from leveldb took 33059ns
I0206 00:22:44.827450  2852 replica.cpp:712] Persisted action at 2
I0206 00:22:44.827468  2852 replica.cpp:697] Replica learned TRUNCATE action at position 2
I0206 00:22:44.838011  2824 containerizer.cpp:149] Using isolation: posix/cpu,posix/mem,filesystem/posix
W0206 00:22:44.838873  2824 backend.cpp:48] Failed to create 'bind' backend: BindBackend requires root privileges
I0206 00:22:44.843785  2857 slave.cpp:193] Slave started on 172.17.0.2:43484
I0206 00:22:44.843819  2857 slave.cpp:194] Flags at startup: --appc_store_dir=""/tmp/mesos/store/appc"" --authenticatee=""crammd5"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --credential=""/tmp/SlaveRecoveryTest_0_CleanupHTTPExecutor_kAXwvw/credential"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_auth_server=""https://auth.docker.io"" --docker_kill_orphans=""true"" --docker_puller_timeout=""60"" --docker_registry=""https://registry-1.docker.io"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/tmp/mesos/store/docker"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/tmp/SlaveRecoveryTest_0_CleanupHTTPExecutor_kAXwvw/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --hostname_lookup=""true"" --image_provisioner_backend=""copy"" --initialize_driver_logging=""true"" --isolation=""posix/cpu,posix/mem"" --launcher_dir=""/mesos/mesos-0.28.0/_build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""10ms"" --resources=""cpus:2;mem:1024;disk:1024;ports:[31000-32000]"" --revocable_cpu_low_priority=""true"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --systemd_runtime_directory=""/run/systemd/system"" --version=""false"" --work_dir=""/tmp/SlaveRecoveryTest_0_CleanupHTTPExecutor_kAXwvw""
I0206 00:22:44.844292  2857 credentials.hpp:83] Loading credential for authentication from '/tmp/SlaveRecoveryTest_0_CleanupHTTPExecutor_kAXwvw/credential'
I0206 00:22:44.844518  2857 slave.cpp:324] Slave using credential for: test-principal
I0206 00:22:44.844696  2857 resources.cpp:564] Parsing resources as JSON failed: cpus:2;mem:1024;disk:1024;ports:[31000-32000]
Trying semicolon-delimited string format instead
I0206 00:22:44.845243  2857 slave.cpp:464] Slave resources: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I0206 00:22:44.845326  2857 slave.cpp:472] Slave attributes: [  ]
I0206 00:22:44.845342  2857 slave.cpp:477] Slave hostname: 6632562f1ade
I0206 00:22:44.845953  2824 sched.cpp:222] Version: 0.28.0
I0206 00:22:44.846853  2848 sched.cpp:326] New master detected at master@172.17.0.2:43484
I0206 00:22:44.846936  2848 sched.cpp:382] Authenticating with master master@172.17.0.2:43484
I0206 00:22:44.846958  2848 sched.cpp:389] Using default CRAM-MD5 authenticatee
I0206 00:22:44.847692  2858 state.cpp:58] Recovering state from '/tmp/SlaveRecoveryTest_0_CleanupHTTPExecutor_kAXwvw/meta'
I0206 00:22:44.848108  2850 status_update_manager.cpp:200] Recovering status update manager
I0206 00:22:44.848325  2852 containerizer.cpp:397] Recovering containerizer
I0206 00:22:44.848603  2845 authenticatee.cpp:121] Creating new client SASL connection
I0206 00:22:44.849719  2845 master.cpp:5523] Authenticating scheduler-63899759-d7fc-42b2-8371-57484f352895@172.17.0.2:43484
I0206 00:22:44.850052  2852 authenticator.cpp:413] Starting authentication session for crammd5_authenticatee(662)@172.17.0.2:43484
I0206 00:22:44.850227  2854 provisioner.cpp:245] Provisioner recovery complete
I0206 00:22:44.850410  2852 authenticator.cpp:98] Creating new server SASL connection
I0206 00:22:44.850692  2852 authenticatee.cpp:212] Received SASL authentication mechanisms: CRAM-MD5
I0206 00:22:44.850720  2852 authenticatee.cpp:238] Attempting to authenticate with mechanism 'CRAM-MD5'
I0206 00:22:44.850805  2852 authenticator.cpp:203] Received SASL authentication start
I0206 00:22:44.850862  2852 authenticator.cpp:325] Authentication requires more steps
I0206 00:22:44.850939  2852 authenticatee.cpp:258] Received SASL authentication step
I0206 00:22:44.851027  2852 authenticator.cpp:231] Received SASL authentication step
I0206 00:22:44.851052  2852 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: '6632562f1ade' server FQDN: '6632562f1ade' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0206 00:22:44.851063  2852 auxprop.cpp:179] Looking up auxiliary property '*userPassword'
I0206 00:22:44.851102  2852 auxprop.cpp:179] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0206 00:22:44.851121  2852 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: '6632562f1ade' server FQDN: '6632562f1ade' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0206 00:22:44.851130  2852 auxprop.cpp:129] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0206 00:22:44.851136  2852 auxprop.cpp:129] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0206 00:22:44.851150  2852 authenticator.cpp:317] Authentication success
I0206 00:22:44.851219  2850 authenticatee.cpp:298] Authentication success
I0206 00:22:44.851310  2850 master.cpp:5553] Successfully authenticated principal 'test-principal' at scheduler-63899759-d7fc-42b2-8371-57484f352895@172.17.0.2:43484
I0206 00:22:44.851485  2849 slave.cpp:4496] Finished recovery
I0206 00:22:44.852154  2843 sched.cpp:471] Successfully authenticated with master master@172.17.0.2:43484
I0206 00:22:44.852175  2843 sched.cpp:776] Sending SUBSCRIBE call to master@172.17.0.2:43484
I0206 00:22:44.852262  2843 sched.cpp:809] Will retry registration in 939.183679ms if necessary
I0206 00:22:44.852375  2844 master.cpp:2280] Received SUBSCRIBE call for framework 'default' at scheduler-63899759-d7fc-42b2-8371-57484f352895@172.17.0.2:43484
I0206 00:22:44.852448  2844 master.cpp:1751] Authorizing framework principal 'test-principal' to receive offers for role '*'
I0206 00:22:44.852699  2852 authenticator.cpp:431] Authentication session cleanup for crammd5_authenticatee(662)@172.17.0.2:43484
I0206 00:22:44.852782  2844 master.cpp:2351] Subscribing framework default with checkpointing enabled and capabilities [  ]
I0206 00:22:44.853056  2849 slave.cpp:4668] Querying resource estimator for oversubscribable resources
I0206 00:22:44.853421  2856 hierarchical.cpp:265] Added framework 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000
I0206 00:22:44.853513  2856 hierarchical.cpp:1403] No resources available to allocate!
I0206 00:22:44.853582  2844 sched.cpp:703] Framework registered with 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000
I0206 00:22:44.853613  2852 slave.cpp:4682] Received oversubscribable resources  from the resource estimator
I0206 00:22:44.853663  2844 sched.cpp:717] Scheduler::registered took 53762ns
I0206 00:22:44.853899  2843 slave.cpp:796] New master detected at master@172.17.0.2:43484
I0206 00:22:44.853955  2854 status_update_manager.cpp:174] Pausing sending status updates
I0206 00:22:44.853997  2856 hierarchical.cpp:1498] No inverse offers to send out!
I0206 00:22:44.853960  2843 slave.cpp:859] Authenticating with master master@172.17.0.2:43484
I0206 00:22:44.854035  2843 slave.cpp:864] Using default CRAM-MD5 authenticatee
I0206 00:22:44.854030  2856 hierarchical.cpp:1096] Performed allocation for 0 slaves in 581355ns
I0206 00:22:44.854182  2843 slave.cpp:832] Detecting new master
I0206 00:22:44.854277  2854 authenticatee.cpp:121] Creating new client SASL connection
I0206 00:22:44.854517  2843 master.cpp:5523] Authenticating slave@172.17.0.2:43484
I0206 00:22:44.854603  2854 authenticator.cpp:413] Starting authentication session for crammd5_authenticatee(663)@172.17.0.2:43484
I0206 00:22:44.854836  2855 authenticator.cpp:98] Creating new server SASL connection
I0206 00:22:44.855013  2852 authenticatee.cpp:212] Received SASL authentication mechanisms: CRAM-MD5
I0206 00:22:44.855044  2852 authenticatee.cpp:238] Attempting to authenticate with mechanism 'CRAM-MD5'
I0206 00:22:44.855139  2855 authenticator.cpp:203] Received SASL authentication start
I0206 00:22:44.855186  2855 authenticator.cpp:325] Authentication requires more steps
I0206 00:22:44.855263  2855 authenticatee.cpp:258] Received SASL authentication step
I0206 00:22:44.855352  2855 authenticator.cpp:231] Received SASL authentication step
I0206 00:22:44.855381  2855 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: '6632562f1ade' server FQDN: '6632562f1ade' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0206 00:22:44.855389  2855 auxprop.cpp:179] Looking up auxiliary property '*userPassword'
I0206 00:22:44.855419  2855 auxprop.cpp:179] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0206 00:22:44.855438  2855 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: '6632562f1ade' server FQDN: '6632562f1ade' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0206 00:22:44.855448  2855 auxprop.cpp:129] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0206 00:22:44.855453  2855 auxprop.cpp:129] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0206 00:22:44.855464  2855 authenticator.cpp:317] Authentication success
I0206 00:22:44.855540  2851 authenticatee.cpp:298] Authentication success
I0206 00:22:44.855721  2851 authenticator.cpp:431] Authentication session cleanup for crammd5_authenticatee(663)@172.17.0.2:43484
I0206 00:22:44.855832  2852 slave.cpp:927] Successfully authenticated with master master@172.17.0.2:43484
I0206 00:22:44.855615  2855 master.cpp:5553] Successfully authenticated principal 'test-principal' at slave@172.17.0.2:43484
I0206 00:22:44.855973  2852 slave.cpp:1321] Will retry registration in 9.327708ms if necessary
I0206 00:22:44.856145  2854 master.cpp:4237] Registering slave at slave@172.17.0.2:43484 (6632562f1ade) with id 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-S0
I0206 00:22:44.856598  2851 registrar.cpp:439] Applied 1 operations in 59112ns; attempting to update the 'registry'
I0206 00:22:44.857403  2851 log.cpp:683] Attempting to append 339 bytes to the log
I0206 00:22:44.857525  2855 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 3
I0206 00:22:44.858482  2844 replica.cpp:537] Replica received write request for position 3 from (9493)@172.17.0.2:43484
I0206 00:22:44.858755  2844 leveldb.cpp:341] Persisting action (358 bytes) to leveldb took 228484ns
I0206 00:22:44.858855  2844 replica.cpp:712] Persisted action at 3
I0206 00:22:44.859751  2852 replica.cpp:691] Replica received learned notice for position 3 from @0.0.0.0:0
I0206 00:22:44.860332  2852 leveldb.cpp:341] Persisting action (360 bytes) to leveldb took 549638ns
I0206 00:22:44.860358  2852 replica.cpp:712] Persisted action at 3
I0206 00:22:44.860411  2852 replica.cpp:697] Replica learned APPEND action at position 3
I0206 00:22:44.862709  2856 registrar.cpp:484] Successfully updated the 'registry' in 6.020864ms
I0206 00:22:44.863106  2850 log.cpp:702] Attempting to truncate the log to 3
I0206 00:22:44.863358  2850 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 4
I0206 00:22:44.864321  2850 slave.cpp:3436] Received ping from slave-observer(288)@172.17.0.2:43484
I0206 00:22:44.864706  2849 hierarchical.cpp:473] Added slave 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-S0 (6632562f1ade) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (allocated: )
I0206 00:22:44.864716  2843 replica.cpp:537] Replica received write request for position 4 from (9494)@172.17.0.2:43484
I0206 00:22:44.865309  2843 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 410199ns
I0206 00:22:44.865337  2843 replica.cpp:712] Persisted action at 4
I0206 00:22:44.866092  2849 hierarchical.cpp:1498] No inverse offers to send out!
I0206 00:22:44.866132  2848 replica.cpp:691] Replica received learned notice for position 4 from @0.0.0.0:0
I0206 00:22:44.866137  2849 hierarchical.cpp:1116] Performed allocation for slave 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-S0 in 1.30657ms
I0206 00:22:44.866497  2856 master.cpp:4305] Registered slave 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-S0 at slave@172.17.0.2:43484 (6632562f1ade) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I0206 00:22:44.866564  2843 slave.cpp:1321] Will retry registration in 32.803438ms if necessary
I0206 00:22:44.866690  2843 slave.cpp:971] Registered with master master@172.17.0.2:43484; given slave ID 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-S0
I0206 00:22:44.866716  2843 fetcher.cpp:81] Clearing fetcher cache
I0206 00:22:44.867066  2856 master.cpp:5352] Sending 1 offers to framework 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000 (default) at scheduler-63899759-d7fc-42b2-8371-57484f352895@172.17.0.2:43484
I0206 00:22:44.867105  2843 slave.cpp:994] Checkpointing SlaveInfo to '/tmp/SlaveRecoveryTest_0_CleanupHTTPExecutor_kAXwvw/meta/slaves/0b206a40-a9c3-4d44-a5bd-8032d60a32ca-S0/slave.info'
I0206 00:22:44.867347  2856 master.cpp:4207] Slave 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-S0 at slave@172.17.0.2:43484 (6632562f1ade) already registered, resending acknowledgement
I0206 00:22:44.867441  2856 status_update_manager.cpp:181] Resuming sending status updates
I0206 00:22:44.867465  2843 slave.cpp:1030] Forwarding total oversubscribed resources 
W0206 00:22:44.867547  2843 slave.cpp:1016] Already registered with master master@172.17.0.2:43484
I0206 00:22:44.867574  2843 slave.cpp:1030] Forwarding total oversubscribed resources 
I0206 00:22:44.867710  2843 master.cpp:4646] Received update of slave 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-S0 at slave@172.17.0.2:43484 (6632562f1ade) with total oversubscribed resources 
I0206 00:22:44.867951  2856 sched.cpp:873] Scheduler::resourceOffers took 133371ns
I0206 00:22:44.867961  2843 master.cpp:4646] Received update of slave 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-S0 at slave@172.17.0.2:43484 (6632562f1ade) with total oversubscribed resources 
I0206 00:22:44.868484  2856 hierarchical.cpp:531] Slave 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-S0 (6632562f1ade) updated with oversubscribed resources  (total: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000], allocated: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000])
I0206 00:22:44.868599  2848 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 2.418545ms
I0206 00:22:44.868700  2848 leveldb.cpp:399] Deleting ~2 keys from leveldb took 54053ns
I0206 00:22:44.868751  2848 replica.cpp:712] Persisted action at 4
I0206 00:22:44.868811  2848 replica.cpp:697] Replica learned TRUNCATE action at position 4
I0206 00:22:44.869241  2856 hierarchical.cpp:1403] No resources available to allocate!
I0206 00:22:44.869287  2856 hierarchical.cpp:1498] No inverse offers to send out!
I0206 00:22:44.869321  2856 hierarchical.cpp:1116] Performed allocation for slave 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-S0 in 782848ns
I0206 00:22:44.869840  2856 hierarchical.cpp:531] Slave 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-S0 (6632562f1ade) updated with oversubscribed resources  (total: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000], allocated: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000])
I0206 00:22:44.869985  2856 hierarchical.cpp:1403] No resources available to allocate!
I0206 00:22:44.870028  2856 hierarchical.cpp:1498] No inverse offers to send out!
I0206 00:22:44.870053  2856 hierarchical.cpp:1116] Performed allocation for slave 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-S0 in 160104ns
I0206 00:22:44.871824  2853 master.cpp:3138] Processing ACCEPT call for offers: [ 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-O0 ] on slave 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-S0 at slave@172.17.0.2:43484 (6632562f1ade) for framework 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000 (default) at scheduler-63899759-d7fc-42b2-8371-57484f352895@172.17.0.2:43484
I0206 00:22:44.871868  2853 master.cpp:2825] Authorizing framework principal 'test-principal' to launch task 1 as user 'mesos'
W0206 00:22:44.873613  2843 validation.cpp:404] Executor http for task 1 uses less CPUs (None) than the minimum required (0.01). Please update your executor, as this will be mandatory in future releases.
W0206 00:22:44.873667  2843 validation.cpp:416] Executor http for task 1 uses less memory (None) than the minimum required (32MB). Please update your executor, as this will be mandatory in future releases.
I0206 00:22:44.874035  2843 master.hpp:176] Adding task 1 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-S0 (6632562f1ade)
I0206 00:22:44.874223  2843 master.cpp:3623] Launching task 1 of framework 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000 (default) at scheduler-63899759-d7fc-42b2-8371-57484f352895@172.17.0.2:43484 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-S0 at slave@172.17.0.2:43484 (6632562f1ade)
I0206 00:22:44.874802  2843 slave.cpp:1361] Got assigned task 1 for framework 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000
I0206 00:22:44.874966  2843 slave.cpp:5202] Checkpointing FrameworkInfo to '/tmp/SlaveRecoveryTest_0_CleanupHTTPExecutor_kAXwvw/meta/slaves/0b206a40-a9c3-4d44-a5bd-8032d60a32ca-S0/frameworks/0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000/framework.info'
I0206 00:22:44.875440  2843 slave.cpp:5213] Checkpointing framework pid 'scheduler-63899759-d7fc-42b2-8371-57484f352895@172.17.0.2:43484' to '/tmp/SlaveRecoveryTest_0_CleanupHTTPExecutor_kAXwvw/meta/slaves/0b206a40-a9c3-4d44-a5bd-8032d60a32ca-S0/frameworks/0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000/framework.pid'
I0206 00:22:44.876106  2843 slave.cpp:1480] Launching task 1 for framework 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000
I0206 00:22:44.876644  2843 paths.cpp:474] Trying to chown '/tmp/SlaveRecoveryTest_0_CleanupHTTPExecutor_kAXwvw/slaves/0b206a40-a9c3-4d44-a5bd-8032d60a32ca-S0/frameworks/0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000/executors/http/runs/fd4649a4-1c82-4eda-b663-b568b6110d17' to user 'mesos'
I0206 00:22:44.884089  2843 slave.cpp:5654] Checkpointing ExecutorInfo to '/tmp/SlaveRecoveryTest_0_CleanupHTTPExecutor_kAXwvw/meta/slaves/0b206a40-a9c3-4d44-a5bd-8032d60a32ca-S0/frameworks/0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000/executors/http/executor.info'
I0206 00:22:44.900928  2843 slave.cpp:5282] Launching executor http of framework 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000 with resources  in work directory '/tmp/SlaveRecoveryTest_0_CleanupHTTPExecutor_kAXwvw/slaves/0b206a40-a9c3-4d44-a5bd-8032d60a32ca-S0/frameworks/0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000/executors/http/runs/fd4649a4-1c82-4eda-b663-b568b6110d17'
I0206 00:22:44.901449  2853 containerizer.cpp:656] Starting container 'fd4649a4-1c82-4eda-b663-b568b6110d17' for executor 'http' of framework '0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000'
I0206 00:22:44.901561  2843 slave.cpp:5677] Checkpointing TaskInfo to '/tmp/SlaveRecoveryTest_0_CleanupHTTPExecutor_kAXwvw/meta/slaves/0b206a40-a9c3-4d44-a5bd-8032d60a32ca-S0/frameworks/0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000/executors/http/runs/fd4649a4-1c82-4eda-b663-b568b6110d17/tasks/1/task.info'
I0206 00:22:44.902060  2843 slave.cpp:1698] Queuing task '1' for executor 'http' of framework 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000
I0206 00:22:44.902207  2843 slave.cpp:749] Successfully attached file '/tmp/SlaveRecoveryTest_0_CleanupHTTPExecutor_kAXwvw/slaves/0b206a40-a9c3-4d44-a5bd-8032d60a32ca-S0/frameworks/0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000/executors/http/runs/fd4649a4-1c82-4eda-b663-b568b6110d17'
I0206 00:22:44.907027  2850 launcher.cpp:132] Forked child with pid '8875' for container 'fd4649a4-1c82-4eda-b663-b568b6110d17'
I0206 00:22:44.907229  2850 containerizer.cpp:1094] Checkpointing executor's forked pid 8875 to '/tmp/SlaveRecoveryTest_0_CleanupHTTPExecutor_kAXwvw/meta/slaves/0b206a40-a9c3-4d44-a5bd-8032d60a32ca-S0/frameworks/0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000/executors/http/runs/fd4649a4-1c82-4eda-b663-b568b6110d17/pids/forked.pid'
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0206 00:22:45.080060  8875 process.cpp:991] libprocess is initialized on 172.17.0.2:49724 for 16 cpus
I0206 00:22:45.082499  8875 logging.cpp:193] Logging to STDERR
I0206 00:22:45.082862  8875 executor.cpp:172] Version: 0.28.0
I0206 00:22:45.087201  8903 executor.cpp:316] Connected with the agent
I0206 00:22:45.802878  2858 hierarchical.cpp:1403] No resources available to allocate!
I0206 00:22:45.802969  2858 hierarchical.cpp:1498] No inverse offers to send out!
I0206 00:22:45.803014  2858 hierarchical.cpp:1096] Performed allocation for 1 slaves in 424120ns
2016-02-06 00:22:45,982:2824(0x7fd8c5ffb700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:40712] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
W0206 00:22:46.588022  2854 group.cpp:503] Timed out waiting to connect to ZooKeeper. Forcing ZooKeeper session (sessionId=0) expiration
I0206 00:22:46.588969  2854 group.cpp:519] ZooKeeper session expired
2016-02-06 00:22:46,589:2824(0x7fd9fefd1700):ZOO_INFO@zookeeper_close@2522: Freeing zookeeper resources for sessionId=0

2016-02-06 00:22:46,589:2824(0x7fda03fdb700):ZOO_INFO@log_env@712: Client environment:zookeeper.version=zookeeper C client 3.4.5
2016-02-06 00:22:46,589:2824(0x7fda03fdb700):ZOO_INFO@log_env@716: Client environment:host.name=6632562f1ade
2016-02-06 00:22:46,589:2824(0x7fda03fdb700):ZOO_INFO@log_env@723: Client environment:os.name=Linux
2016-02-06 00:22:46,589:2824(0x7fda03fdb700):ZOO_INFO@log_env@724: Client environment:os.arch=3.13.0-36-lowlatency
2016-02-06 00:22:46,589:2824(0x7fda03fdb700):ZOO_INFO@log_env@725: Client environment:os.version=#63-Ubuntu SMP PREEMPT Wed Sep 3 21:56:12 UTC 2014
2016-02-06 00:22:46,589:2824(0x7fda03fdb700):ZOO_INFO@log_env@733: Client environment:user.name=(null)
2016-02-06 00:22:46,589:2824(0x7fda03fdb700):ZOO_INFO@log_env@741: Client environment:user.home=/home/mesos
2016-02-06 00:22:46,589:2824(0x7fda03fdb700):ZOO_INFO@log_env@753: Client environment:user.dir=/tmp/n2FxQV
2016-02-06 00:22:46,590:2824(0x7fda03fdb700):ZOO_INFO@zookeeper_init@786: Initiating client connection, host=127.0.0.1:40712 sessionTimeout=10000 watcher=0x7fda10e9e520 sessionId=0 sessionPasswd=<null> context=0x7fd9d401bc10 flags=0
2016-02-06 00:22:46,590:2824(0x7fd8c67fc700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:40712] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
I0206 00:22:46.804400  2844 hierarchical.cpp:1403] No resources available to allocate!
I0206 00:22:46.804481  2844 hierarchical.cpp:1498] No inverse offers to send out!
I0206 00:22:46.804514  2844 hierarchical.cpp:1096] Performed allocation for 1 slaves in 347954ns
I0206 00:22:47.805842  2847 hierarchical.cpp:1403] No resources available to allocate!
I0206 00:22:47.805934  2847 hierarchical.cpp:1498] No inverse offers to send out!
I0206 00:22:47.805980  2847 hierarchical.cpp:1096] Performed allocation for 1 slaves in 415449ns
I0206 00:22:48.807723  2851 hierarchical.cpp:1403] No resources available to allocate!
I0206 00:22:48.807814  2851 hierarchical.cpp:1498] No inverse offers to send out!
I0206 00:22:48.807857  2851 hierarchical.cpp:1096] Performed allocation for 1 slaves in 442104ns
I0206 00:22:49.808733  2848 hierarchical.cpp:1403] No resources available to allocate!
I0206 00:22:49.808816  2848 hierarchical.cpp:1498] No inverse offers to send out!
I0206 00:22:49.808856  2848 hierarchical.cpp:1096] Performed allocation for 1 slaves in 384959ns
2016-02-06 00:22:49,926:2824(0x7fd8c67fc700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:40712] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
I0206 00:22:50.810307  2847 hierarchical.cpp:1403] No resources available to allocate!
I0206 00:22:50.810400  2847 hierarchical.cpp:1498] No inverse offers to send out!
I0206 00:22:50.810443  2847 hierarchical.cpp:1096] Performed allocation for 1 slaves in 389572ns
I0206 00:22:51.811586  2849 hierarchical.cpp:1403] No resources available to allocate!
I0206 00:22:51.811681  2849 hierarchical.cpp:1498] No inverse offers to send out!
I0206 00:22:51.811722  2849 hierarchical.cpp:1096] Performed allocation for 1 slaves in 404450ns
I0206 00:22:52.812860  2851 hierarchical.cpp:1403] No resources available to allocate!
I0206 00:22:52.812944  2851 hierarchical.cpp:1498] No inverse offers to send out!
I0206 00:22:52.812981  2851 hierarchical.cpp:1096] Performed allocation for 1 slaves in 359671ns
2016-02-06 00:22:53,263:2824(0x7fd8c67fc700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:40712] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
I0206 00:22:53.814512  2847 hierarchical.cpp:1403] No resources available to allocate!
I0206 00:22:53.814599  2847 hierarchical.cpp:1498] No inverse offers to send out!
I0206 00:22:53.814651  2847 hierarchical.cpp:1096] Performed allocation for 1 slaves in 386669ns
I0206 00:22:54.815238  2852 hierarchical.cpp:1403] No resources available to allocate!
I0206 00:22:54.815321  2852 hierarchical.cpp:1498] No inverse offers to send out!
I0206 00:22:54.815356  2852 hierarchical.cpp:1096] Performed allocation for 1 slaves in 376235ns
I0206 00:22:55.816453  2846 hierarchical.cpp:1403] No resources available to allocate!
I0206 00:22:55.816550  2846 hierarchical.cpp:1498] No inverse offers to send out!
I0206 00:22:55.816596  2846 hierarchical.cpp:1096] Performed allocation for 1 slaves in 416350ns
W0206 00:22:56.592408  2849 group.cpp:503] Timed out waiting to connect to ZooKeeper. Forcing ZooKeeper session (sessionId=0) expiration
I0206 00:22:56.593480  2849 group.cpp:519] ZooKeeper session expired
2016-02-06 00:22:56,593:2824(0x7fda017d6700):ZOO_INFO@zookeeper_close@2522: Freeing zookeeper resources for sessionId=0

2016-02-06 00:22:56,594:2824(0x7fda007d4700):ZOO_INFO@log_env@712: Client environment:zookeeper.version=zookeeper C client 3.4.5
2016-02-06 00:22:56,594:2824(0x7fda007d4700):ZOO_INFO@log_env@716: Client environment:host.name=6632562f1ade
2016-02-06 00:22:56,594:2824(0x7fda007d4700):ZOO_INFO@log_env@723: Client environment:os.name=Linux
2016-02-06 00:22:56,594:2824(0x7fda007d4700):ZOO_INFO@log_env@724: Client environment:os.arch=3.13.0-36-lowlatency
2016-02-06 00:22:56,594:2824(0x7fda007d4700):ZOO_INFO@log_env@725: Client environment:os.version=#63-Ubuntu SMP PREEMPT Wed Sep 3 21:56:12 UTC 2014
2016-02-06 00:22:56,594:2824(0x7fda007d4700):ZOO_INFO@log_env@733: Client environment:user.name=(null)
2016-02-06 00:22:56,594:2824(0x7fda007d4700):ZOO_INFO@log_env@741: Client environment:user.home=/home/mesos
2016-02-06 00:22:56,594:2824(0x7fda007d4700):ZOO_INFO@log_env@753: Client environment:user.dir=/tmp/n2FxQV
2016-02-06 00:22:56,594:2824(0x7fda007d4700):ZOO_INFO@zookeeper_init@786: Initiating client connection, host=127.0.0.1:40712 sessionTimeout=10000 watcher=0x7fda10e9e520 sessionId=0 sessionPasswd=<null> context=0x7fd9e401f350 flags=0
2016-02-06 00:22:56,595:2824(0x7fd8c5ffb700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:40712] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
I0206 00:22:56.817683  2848 hierarchical.cpp:1403] No resources available to allocate!
I0206 00:22:56.817766  2848 hierarchical.cpp:1498] No inverse offers to send out!
I0206 00:22:56.817803  2848 hierarchical.cpp:1096] Performed allocation for 1 slaves in 374115ns
I0206 00:22:57.818447  2844 hierarchical.cpp:1403] No resources available to allocate!
I0206 00:22:57.818526  2844 hierarchical.cpp:1498] No inverse offers to send out!
I0206 00:22:57.818562  2844 hierarchical.cpp:1096] Performed allocation for 1 slaves in 344545ns
I0206 00:22:58.819828  2851 hierarchical.cpp:1403] No resources available to allocate!
I0206 00:22:58.819914  2851 hierarchical.cpp:1498] No inverse offers to send out!
I0206 00:22:58.819957  2851 hierarchical.cpp:1096] Performed allocation for 1 slaves in 376948ns
I0206 00:22:59.820874  2848 hierarchical.cpp:1403] No resources available to allocate!
I0206 00:22:59.820957  2848 hierarchical.cpp:1498] No inverse offers to send out!
I0206 00:22:59.820991  2848 hierarchical.cpp:1096] Performed allocation for 1 slaves in 344192ns
I0206 00:22:59.854698  2845 slave.cpp:4668] Querying resource estimator for oversubscribable resources
I0206 00:22:59.854991  2845 slave.cpp:4682] Received oversubscribable resources  from the resource estimator
I0206 00:22:59.864612  2857 slave.cpp:3436] Received ping from slave-observer(288)@172.17.0.2:43484
../../src/tests/slave_recovery_tests.cpp:1105: Failure
Failed to wait 15secs for updateCall1
I0206 00:22:59.876358  2852 master.cpp:1213] Framework 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000 (default) at scheduler-63899759-d7fc-42b2-8371-57484f352895@172.17.0.2:43484 disconnected
I0206 00:22:59.876410  2852 master.cpp:2576] Disconnecting framework 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000 (default) at scheduler-63899759-d7fc-42b2-8371-57484f352895@172.17.0.2:43484
I0206 00:22:59.876456  2852 master.cpp:2600] Deactivating framework 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000 (default) at scheduler-63899759-d7fc-42b2-8371-57484f352895@172.17.0.2:43484
I0206 00:22:59.876569  2852 master.cpp:1237] Giving framework 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000 (default) at scheduler-63899759-d7fc-42b2-8371-57484f352895@172.17.0.2:43484 0ns to failover
I0206 00:22:59.876981  2844 hierarchical.cpp:375] Deactivated framework 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000
I0206 00:22:59.877049  2844 master.cpp:5204] Framework failover timeout, removing framework 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000 (default) at scheduler-63899759-d7fc-42b2-8371-57484f352895@172.17.0.2:43484
I0206 00:22:59.877075  2844 master.cpp:5935] Removing framework 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000 (default) at scheduler-63899759-d7fc-42b2-8371-57484f352895@172.17.0.2:43484
I0206 00:22:59.877276  2844 master.cpp:6447] Updating the state of task 1 of framework 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000 (latest state: TASK_KILLED, status update state: TASK_KILLED)
I0206 00:22:59.878051  2844 master.cpp:6513] Removing task 1 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] of framework 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000 on slave 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-S0 at slave@172.17.0.2:43484 (6632562f1ade)
I0206 00:22:59.878433  2844 master.cpp:6542] Removing executor 'http' with resources  of framework 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000 on slave 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-S0 at slave@172.17.0.2:43484 (6632562f1ade)
I0206 00:22:59.878667  2852 slave.cpp:2079] Asked to shut down framework 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000 by master@172.17.0.2:43484
I0206 00:22:59.878733  2852 slave.cpp:2104] Shutting down framework 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000
I0206 00:22:59.878806  2852 slave.cpp:4129] Shutting down executor 'http' of framework 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000
W0206 00:22:59.878834  2852 slave.hpp:655] Unable to send event to executor 'http' of framework 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000: unknown connection type
I0206 00:22:59.879550  2844 master.cpp:1027] Master terminating
I0206 00:22:59.879703  2854 hierarchical.cpp:892] Recovered cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (total: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000], allocated: ) on slave 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-S0 from framework 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000
I0206 00:22:59.879947  2854 hierarchical.cpp:326] Removed framework 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000
I0206 00:22:59.880306  2854 hierarchical.cpp:505] Removed slave 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-S0
I0206 00:22:59.880666  2852 slave.cpp:3482] master@172.17.0.2:43484 exited
W0206 00:22:59.880695  2852 slave.cpp:3485] Master disconnected! Waiting for a new master to be elected
I0206 00:22:59.885498  2857 containerizer.cpp:1318] Destroying container 'fd4649a4-1c82-4eda-b663-b568b6110d17'
I0206 00:22:59.904532  2858 containerizer.cpp:1534] Executor for container 'fd4649a4-1c82-4eda-b663-b568b6110d17' has exited
I0206 00:22:59.907024  2858 provisioner.cpp:306] Ignoring destroy request for unknown container fd4649a4-1c82-4eda-b663-b568b6110d17
I0206 00:22:59.907428  2858 slave.cpp:3817] Executor 'http' of framework 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000 terminated with signal Killed
I0206 00:22:59.907538  2858 slave.cpp:3921] Cleaning up executor 'http' of framework 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000
I0206 00:22:59.908213  2858 slave.cpp:4009] Cleaning up framework 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000
I0206 00:22:59.908555  2858 gc.cpp:54] Scheduling '/tmp/SlaveRecoveryTest_0_CleanupHTTPExecutor_kAXwvw/slaves/0b206a40-a9c3-4d44-a5bd-8032d60a32ca-S0/frameworks/0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000/executors/http/runs/fd4649a4-1c82-4eda-b663-b568b6110d17' for gc 6.99998949252444days in the future
I0206 00:22:59.908720  2858 gc.cpp:54] Scheduling '/tmp/SlaveRecoveryTest_0_CleanupHTTPExecutor_kAXwvw/slaves/0b206a40-a9c3-4d44-a5bd-8032d60a32ca-S0/frameworks/0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000/executors/http' for gc 6.99998949082074days in the future
I0206 00:22:59.908807  2858 gc.cpp:54] Scheduling '/tmp/SlaveRecoveryTest_0_CleanupHTTPExecutor_kAXwvw/meta/slaves/0b206a40-a9c3-4d44-a5bd-8032d60a32ca-S0/frameworks/0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000/executors/http/runs/fd4649a4-1c82-4eda-b663-b568b6110d17' for gc 6.99998948980444days in the future
I0206 00:22:59.908927  2858 gc.cpp:54] Scheduling '/tmp/SlaveRecoveryTest_0_CleanupHTTPExecutor_kAXwvw/meta/slaves/0b206a40-a9c3-4d44-a5bd-8032d60a32ca-S0/frameworks/0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000/executors/http' for gc 6.99998948890074days in the future
I0206 00:22:59.909009  2858 gc.cpp:54] Scheduling '/tmp/SlaveRecoveryTest_0_CleanupHTTPExecutor_kAXwvw/slaves/0b206a40-a9c3-4d44-a5bd-8032d60a32ca-S0/frameworks/0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000' for gc 6.99998948710518days in the future
I0206 00:22:59.909121  2858 gc.cpp:54] Scheduling '/tmp/SlaveRecoveryTest_0_CleanupHTTPExecutor_kAXwvw/meta/slaves/0b206a40-a9c3-4d44-a5bd-8032d60a32ca-S0/frameworks/0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000' for gc 6.99998948630815days in the future
I0206 00:22:59.909211  2858 status_update_manager.cpp:282] Closing status update streams for framework 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000
I0206 00:22:59.910423  2853 slave.cpp:668] Slave terminating
../../3rdparty/libprocess/include/process/gmock.hpp:425: Failure
Actual function call count doesn't match EXPECT_CALL(filter->mock, filter(testing::A<const HttpEvent&>()))...
    Expected args: union http matcher (72-byte object <D0-11 44-12 DA-7F 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 01-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00>, UPDATE, 1-byte object <1B>, 16-byte object <1B-F4 34-01 00-00 00-00 00-00 00-00 DA-7F 00-00>)
         Expected: to be called once
           Actual: never called - unsatisfied and active
../../3rdparty/libprocess/include/process/gmock.hpp:425: Failure
Actual function call count doesn't match EXPECT_CALL(filter->mock, filter(testing::A<const HttpEvent&>()))...
    Expected args: union http matcher (72-byte object <D0-11 44-12 DA-7F 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 01-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00>, UPDATE, 1-byte object <1B>, 16-byte object <1B-F4 34-01 00-00 00-00 00-00 00-00 DA-7F 00-00>)
         Expected: to be called once
           Actual: never called - unsatisfied and active
[  FAILED  ] SlaveRecoveryTest/0.CleanupHTTPExecutor, where TypeParam = mesos::internal::slave::MesosContainerizer (15126 ms)
{code}",Bug,Major,greggomann,2016-02-08T23:35:10.000+0000,5,Resolved,Complete,SlaveRecoveryTest/0.CleanupHTTPExecutor is flaky,2016-02-27T00:05:06.000+0000,MESOS-4614,3.0,mesos,Mesosphere Sprint 28
chenzhiwei,2016-02-05T22:59:00.000+0000,cmaloney,"See: http://zookeeper.apache.org/doc/r3.4.8/releasenotes.html for improvements / bug fixes

Added a new patch that solved [ZOOKEEPER-1643](https://issues.apache.org/jira/browse/ZOOKEEPER-1643)

The original patch: <https://github.com/apache/zookeeper/commit/46b565e6abd8423c43f1bb8da782d76bac7c392c>",Improvement,Major,cmaloney,2016-04-04T20:53:37.000+0000,5,Resolved,Complete,Update vendored ZooKeeper to 3.4.8,2016-04-06T22:00:49.000+0000,MESOS-4612,3.0,mesos,Mesosphere Sprint 32
haosdent@gmail.com,2016-02-05T20:55:37.000+0000,klueska,"The following idiom does not currently compile:

{code}
  Future<Nothing> initialized = dispatch(pid, [] () -> Nothing {
    return Nothing();
  });
{code}

This seems non-intuitive because the following template exists for dispatch:

{code}
template <typename R>
Future<R> dispatch(const UPID& pid, const std::function<R()>& f)
{
  std::shared_ptr<Promise<R>> promise(new Promise<R>()); 
 
  std::shared_ptr<std::function<void(ProcessBase*)>> f_(
      new std::function<void(ProcessBase*)>(
          [=](ProcessBase*) {
            promise->set(f());
          }));

  internal::dispatch(pid, f_);
  
  return promise->future();
}     
{code}

However, lambdas cannot be implicitly cast to a corresponding std::function<R()> type.
To make this work, you have to explicitly type the lambda before passing it to dispatch.

{code}
  std::function<Nothing()> f = []() { return Nothing(); };
  Future<Nothing> initialized = dispatch(pid, f);
{code}

We should add template support to allow lambdas to be passed to dispatch() without explicit typing. 
",Bug,Major,klueska,,10006,Reviewable,New,Passing a lambda to dispatch() always matches the template returning void,2016-04-26T09:22:05.000+0000,MESOS-4611,5.0,mesos,
kaysoky,2016-02-05T18:21:47.000+0000,kaysoky,"Mostly copied from [this comment|https://issues.apache.org/jira/browse/MESOS-4598?focusedCommentId=15133497&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-15133497]

A subprocess inheriting the environment variables {{LIBPROCESS_*}} may run into some accidental fatalities:

| || Subprocess uses libprocess || Subprocess is something else ||
|| Subprocess sets/inherits the same {{PORT}} by accident | Bind failure -> exit | Nothing happens (?) |
|| Subprocess sets a different {{PORT}} on purpose | Bind success (?) | Nothing happens (?) |

(?) = means this is usually the case, but not 100%.

A complete fix would look something like:
* If the {{subprocess}} call gets {{environment = None()}}, we should automatically remove {{LIBPROCESS_PORT}} from the inherited environment.  
* The parts of [{{executorEnvironment}}|https://github.com/apache/mesos/blame/master/src/slave/containerizer/containerizer.cpp#L265] dealing with libprocess & libmesos should be refactored into libprocess as a helper.  We would use this helper for the Containerizer, Fetcher, and ContainerLogger module.
* If the {{subprocess}} call is given {{LIBPROCESS_PORT == os::getenv(""LIBPROCESS_PORT"")}}, we can LOG(WARN) and unset the env var locally.",Bug,Major,kaysoky,,10006,Reviewable,New,Subprocess should be more intelligent about setting/inheriting libprocess environment variables ,2016-02-29T18:58:33.000+0000,MESOS-4609,2.0,mesos,Mesosphere Sprint 28
kaysoky,2016-02-05T09:24:18.000+0000,nfnt,"Log from Teamcity that is running {{sudo ./bin/mesos-tests.sh}} on AWS EC2 instances:
{noformat}
[18:27:14][Step 8/8] [----------] 8 tests from HealthCheckTest
[18:27:14][Step 8/8] [ RUN      ] HealthCheckTest.HealthyTask
[18:27:17][Step 8/8] [       OK ] HealthCheckTest.HealthyTask (2222 ms)
[18:27:17][Step 8/8] [ RUN      ] HealthCheckTest.ROOT_DOCKER_DockerHealthyTask
[18:27:36][Step 8/8] ../../src/tests/health_check_tests.cpp:388: Failure
[18:27:36][Step 8/8] Failed to wait 15secs for termination
[18:27:36][Step 8/8] F0204 18:27:35.981302 23085 logging.cpp:64] RAW: Pure virtual method called
[18:27:36][Step 8/8]     @     0x7f7077055e1c  google::LogMessage::Fail()
[18:27:36][Step 8/8]     @     0x7f707705ba6f  google::RawLog__()
[18:27:36][Step 8/8]     @     0x7f70760f76c9  __cxa_pure_virtual
[18:27:36][Step 8/8]     @           0xa9423c  mesos::internal::tests::Cluster::Slaves::shutdown()
[18:27:36][Step 8/8]     @          0x1074e45  mesos::internal::tests::MesosTest::ShutdownSlaves()
[18:27:36][Step 8/8]     @          0x1074de4  mesos::internal::tests::MesosTest::Shutdown()
[18:27:36][Step 8/8]     @          0x1070ec7  mesos::internal::tests::MesosTest::TearDown()
[18:27:36][Step 8/8]     @          0x16eb7b2  testing::internal::HandleSehExceptionsInMethodIfSupported<>()
[18:27:36][Step 8/8]     @          0x16e61a9  testing::internal::HandleExceptionsInMethodIfSupported<>()
[18:27:36][Step 8/8]     @          0x16c56aa  testing::Test::Run()
[18:27:36][Step 8/8]     @          0x16c5e89  testing::TestInfo::Run()
[18:27:36][Step 8/8]     @          0x16c650a  testing::TestCase::Run()
[18:27:36][Step 8/8]     @          0x16cd1f6  testing::internal::UnitTestImpl::RunAllTests()
[18:27:36][Step 8/8]     @          0x16ec513  testing::internal::HandleSehExceptionsInMethodIfSupported<>()
[18:27:36][Step 8/8]     @          0x16e6df1  testing::internal::HandleExceptionsInMethodIfSupported<>()
[18:27:36][Step 8/8]     @          0x16cbe26  testing::UnitTest::Run()
[18:27:36][Step 8/8]     @           0xe54c84  RUN_ALL_TESTS()
[18:27:36][Step 8/8]     @           0xe54867  main
[18:27:36][Step 8/8]     @     0x7f7071560a40  (unknown)
[18:27:36][Step 8/8]     @           0x9b52d9  _start
[18:27:36][Step 8/8] Aborted (core dumped)
[18:27:36][Step 8/8] Process exited with code 134
{noformat}
Happens with Ubuntu 15.04, CentOS 6, CentOS 7 _quite_ often. ",Bug,Major,nfnt,,10020,Accepted,In Progress,ROOT_DOCKER_DockerHealthyTask is flaky.,2016-04-13T03:36:57.000+0000,MESOS-4604,2.0,mesos,
,2016-02-04T20:03:00.000+0000,ijimenez,"We'd like to have a consistent format for error strings through the code base.
 As per this comment:  [MESOS-3772|https://issues.apache.org/jira/browse/MESOS-3772?focusedCommentId=14965652&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-14965652]

We can then overload the stream operator to make sur strings are quoted as needed.

Note: We need to first require compilers that support C++14. For now we have to wait for MSVC to be part of that list.",Improvement,Trivial,ijimenez,,1,Open,New,Use `std::quoted` for strings in error messages,2016-02-04T20:03:37.000+0000,MESOS-4600,3.0,mesos,
kaysoky,2016-02-04T19:41:40.000+0000,kaysoky,"The {{LogrotateContainerLogger}} starts libprocess-using subprocesses.  Libprocess initialization will attempt to resolve the IP from the hostname.  If a DNS service is not available, this step will fail, which terminates the logger subprocess prematurely.

Since the logger subprocesses live on the agent, they should use the same {{LIBPROCESS_IP}} supplied to the agent.",Bug,Major,kaysoky,2016-02-05T18:26:59.000+0000,5,Resolved,Complete,Logrotate ContainerLogger should not remove IP from environment.,2016-02-15T20:15:39.000+0000,MESOS-4598,1.0,mesos,Mesosphere Sprint 28
jojy,2016-02-04T18:32:45.000+0000,jojy," Add common utility functions such as :
      - validating image information against actual data in the image directory.
      - getting list of dependencies at depth 1 for an image.
      - getting image path simple image discovery.
",Task,Major,jojy,2016-02-11T23:27:09.000+0000,5,Resolved,Complete,Add common Appc spec utilities.,2016-02-11T23:27:09.000+0000,MESOS-4596,2.0,mesos,Mesosphere Sprint 28
greggomann,2016-02-03T21:18:25.000+0000,greggomann,"When frameworks reserve resources, the validation of the operation ensures that the {{role}} of the reservation matches the {{role}} of the framework. For the case of the {{/reserve}} operator endpoint, however, the operator has no role to validate, so this check isn't performed.

This means that if an ACL exists which authorizes a framework's principal to reserve resources, that same principal can be used to reserve resources for _any_ role through the operator endpoint.

We should restrict reservations made through the operator endpoint to specified roles. A few possibilities:
* The {{object}} of the {{reserve_resources}} ACL could be changed from {{resources}} to {{roles}}
* A second ACL could be added for authorization of {{reserve}} operations, with an {{object}} of {{role}}
* Our conception of the {{resources}} object in the {{reserve_resources}} ACL could be expanded to include role information, i.e., {{disk(role1);mem(role1)}}",Bug,Major,greggomann,2016-02-26T19:44:07.000+0000,5,Resolved,Complete,`/reserve` and `/create-volumes` endpoints allow operations for any role,2016-02-26T19:44:07.000+0000,MESOS-4591,3.0,mesos,Mesosphere Sprint 29
neilc,2016-02-03T20:10:17.000+0000,neilc,We don't have a test case that covers $SUBJECT; we probably should.,Task,Major,neilc,2016-02-10T01:38:16.000+0000,5,Resolved,Complete,"Add test case for reservations with same role, different principals",2016-02-10T01:38:17.000+0000,MESOS-4590,2.0,mesos,Mesosphere Sprint 28
karya,2016-02-03T00:12:50.000+0000,karya,The stuff in site/ directory needs some updates to make it easier to generate updates for mesos.apache.org site.,Bug,Major,karya,2016-02-20T16:51:46.000+0000,5,Resolved,Complete,Update Rakefile for mesos site generation,2016-02-20T16:57:01.000+0000,MESOS-4584,2.0,mesos,Mesosphere Sprint 28
yongtang,2016-02-02T22:41:55.000+0000,anandmazumdar,We already have {{examples/test_framework.cpp}} for testing {{PID}} based frameworks. We would ideally want to rename {{event_call_framework}} to correctly reflect that it's an example for HTTP based framework.,Bug,Major,anandmazumdar,2016-03-03T22:48:42.000+0000,5,Resolved,Complete,Rename `examples/event_call_framework.cpp` to `examples/test_http_framework.cpp`,2016-03-03T22:48:42.000+0000,MESOS-4583,1.0,mesos,
mcypark,2016-02-02T21:05:03.000+0000,mgummelt,"state.json is serving duplicate ""active"" fields in frameworks.  See the framework ""47df96c2-3f85-4bc5-b781-709b2c30c752-0000"" In the attached file",Bug,Blocker,mgummelt,2016-02-16T23:27:17.000+0000,5,Resolved,Complete,"state.json serving duplicate ""active"" fields",2016-02-27T00:25:44.000+0000,MESOS-4582,1.0,mesos,Mesosphere Sprint 28
gyliu,2016-02-01T21:12:13.000+0000,kaysoky,"We may want to add a helper to {{stout/os.hpp}} that will natively emulate the functionality of the Linux utility {{which}}.  i.e.
{code}
Option<string> which(const string& command)
{
  Option<string> path = os::getenv(""PATH"");

  // Loop through path and return the first one which os::exists(...).

  return None();
}
{code}

This helper may be useful:
* for test filters in {{src/tests/environment.cpp}}
* a few tests in {{src/tests/containerizer/port_mapping_tests.cpp}}
* the {{sha512}} utility in {{src/common/command_utils.cpp}}
* as runtime checks in the {{LogrotateContainerLogger}}
* etc.",Improvement,Major,kaysoky,2016-04-19T17:24:48.000+0000,5,Resolved,Complete,"Introduce a stout helper for ""which""",2016-04-19T17:24:48.000+0000,MESOS-4576,2.0,mesos,Mesosphere Sprint 29
jojy,2016-02-01T19:51:06.000+0000,jojy,"As Appc image fetcher is being developed, Image cache needs to be shared between store and the image fetcher.",Improvement,Major,jojy,2016-02-11T23:26:34.000+0000,5,Resolved,Complete,Fix Appc image caching to share with image fetcher,2016-02-11T23:26:34.000+0000,MESOS-4575,3.0,mesos,Mesosphere Sprint 28
greggomann,2016-02-01T16:28:19.000+0000,greggomann,"This ticket is for the design of HTTP stream IDs, for use with HTTP schedulers. These IDs allow Mesos to distinguish between different instances of HTTP framework schedulers.",Bug,Major,greggomann,2016-02-26T23:39:26.000+0000,5,Resolved,Complete,Design doc for scheduler HTTP Stream IDs,2016-02-26T23:39:26.000+0000,MESOS-4573,5.0,mesos,Mesosphere Sprint 28
gilbert,2016-01-31T02:39:15.000+0000,tillt,"{noformat}
../configure --enable-ssl --enable-libevent && make check
{noformat}

{noformat}
--gtest_repeat=-1 --gtest_break_on_failure  --gtest_filter=DockerFetcherPluginTest.INTERNET_CURL_FetchImage 
{noformat}

Failed at the 22nd run. 

{noformat}
[ RUN      ] DockerFetcherPluginTest.INTERNET_CURL_FetchImage
../../src/tests/uri_fetcher_tests.cpp:276: Failure
Failed to wait 15secs for fetcher.get()->fetch(uri, dir)
*** Aborted at 1454207653 (unix time) try ""date -d @1454207653"" if you are using GNU date ***
PC: @          0x167023a testing::UnitTest::AddTestPartResult()
*** SIGSEGV (@0x0) received by PID 19868 (TID 0x7f500fc877c0) from PID 0; stack trace: ***
    @     0x7f5008f368d0 (unknown)
    @          0x167023a testing::UnitTest::AddTestPartResult()
    @          0x1664c73 testing::internal::AssertHelper::operator=()
    @          0x146ac6f mesos::internal::tests::DockerFetcherPluginTest_INTERNET_CURL_FetchImage_Test::TestBody()
    @          0x168dc70 testing::internal::HandleSehExceptionsInMethodIfSupported<>()
    @          0x1688cc8 testing::internal::HandleExceptionsInMethodIfSupported<>()
    @          0x166a013 testing::Test::Run()
    @          0x166a7a1 testing::TestInfo::Run()
    @          0x166addc testing::TestCase::Run()
    @          0x167172b testing::internal::UnitTestImpl::RunAllTests()
    @          0x168e8ff testing::internal::HandleSehExceptionsInMethodIfSupported<>()
    @          0x168981e testing::internal::HandleExceptionsInMethodIfSupported<>()
    @          0x167045b testing::UnitTest::Run()
    @           0xe2d476 RUN_ALL_TESTS()
    @           0xe2d08c main
    @     0x7f5008b9fb45 (unknown)
    @           0x9c6bf9 (unknown)
{noformat}",Bug,Major,tillt,2016-02-08T21:40:09.000+0000,5,Resolved,Complete,DockerFetcherPluginTest.INTERNET_CURL_FetchImage seems flaky.,2016-02-08T21:40:09.000+0000,MESOS-4570,1.0,mesos,Mesosphere Sprint 28
,2016-01-31T00:12:00.000+0000,karya,"We currently have the following task stages:

* TASK_STAGING -> set by slave
* TASK_STARTING -> set by the executor (?)
* TASK_RUNNING -> set by the executor when the task is running 
* TASK_XXX -> task termination statuses

The confusion here is about TASK_STARTING. This is the state between TASK_STAGING and TASK_RUNNING and is somewhat non-intuitive for the reader. Further, looks like no where in the source code, we are setting the TASK_STARTING state.

Why shouldn't we just deprecate/remove it?",Task,Major,karya,2016-01-31T19:26:22.000+0000,5,Resolved,Complete,Deprecate TASK_STARTING state,2016-01-31T19:26:22.000+0000,MESOS-4567,2.0,mesos,
mcypark,2016-01-31T00:05:32.000+0000,mcypark,"A few of the critical code paths in {{jsonify}} involve unnecessary temporary string construction and copies (inherited from the {{JSON::*}}). For example, {{strings::trim}} is used to remove trailing 0s from printing {{double}}s. We print {{double}}s a lot, and therefore constructing a temporary {{std::string}} on printing of every double is extremely costly. This ticket captures the work involved in avoiding them.",Improvement,Major,mcypark,2016-02-01T05:23:13.000+0000,5,Resolved,Complete,Avoid unnecessary temporary `std::string` constructions and copies in `jsonify`.,2016-02-27T00:24:19.000+0000,MESOS-4566,1.0,mesos,Mesosphere Sprint 27
jojy,2016-01-30T02:11:11.000+0000,jojy,It would be cleaner to keep the Appc protobuf messages separate from other mesos messages.,Improvement,Major,jojy,2016-02-08T23:38:59.000+0000,5,Resolved,Complete,Separate Appc protobuf messages to its own file.,2016-02-08T23:38:59.000+0000,MESOS-4564,2.0,mesos,Mesosphere Sprint 27
karya,2016-01-29T23:20:21.000+0000,karya,"The task started field shows the number of tasks in state ""TASKS_STARTING"" as opposed to those in ""TASK_RUNNING"" state.",Bug,Critical,karya,2016-01-31T22:23:10.000+0000,5,Resolved,Complete,"Mesos UI shows wrong count for ""started"" tasks",2016-01-31T22:23:10.000+0000,MESOS-4562,2.0,mesos,Mesosphere Sprint 27
vinodkone,2016-01-29T22:21:14.000+0000,vinodkone,The build job is already created on ASF CI (https://builds.apache.org/job/Mesos-Benchmarks/) but is currently disabled due to MESOS-4558.,Task,Major,vinodkone,,1,Open,New,Run benchmark tests in ASF CI,2016-01-29T22:21:29.000+0000,MESOS-4559,2.0,mesos,
klaus1982,2016-01-29T22:19:49.000+0000,vinodkone,"Currently benchmark tests take a long time (>5 hours). It would be nice to reduce the total time taken by the benchmark tests to enable us to run them on ASF CI.

Command to run only benchmark tests
{code}
MESOS_BENCHMARK=1 GTEST_FILTER=""*BENCHMARK*"" make check
{code}",Task,Major,vinodkone,,1,Open,New,Reduce the running time of benchmark tests.,2016-04-22T18:39:03.000+0000,MESOS-4558,2.0,mesos,
,2016-01-29T20:57:33.000+0000,greggomann,"To ensure that the command-line flag documentation in {{configuration.md}} stays in sync with the help strings in the various {{flags.cpp}} files, it could be beneficial to automate the generation of those docs. Such a script could be run as part of the build process, ensuring that changes to the help strings would show up in the documentation as well.

In addition to parsing and formatting the help strings for display as HTML, this could also involve specifying collections of flags to be grouped together in order to provide logical structure to the {{configuration.md}} documentation.",Improvement,Major,greggomann,,1,Open,New,Automatically generate command-line flag documentation,2016-01-29T21:02:25.000+0000,MESOS-4557,3.0,mesos,
,2016-01-29T17:48:25.000+0000,anandmazumdar,"Showed up on ASF CI:
https://builds.apache.org/job/Mesos/COMPILER=clang,CONFIGURATION=--verbose%20--enable-libevent%20--enable-ssl,OS=ubuntu:14.04,label_exp=docker%7C%7CHadoop/1579/console

The test crashed with the following logs:
{code}
[ RUN      ] ContentType/ExecutorHttpApiTest.DefaultAccept/1
I0129 02:00:35.137161 31926 leveldb.cpp:174] Opened db in 118.902333ms
I0129 02:00:35.187021 31926 leveldb.cpp:181] Compacted db in 49.836241ms
I0129 02:00:35.187088 31926 leveldb.cpp:196] Created db iterator in 33825ns
I0129 02:00:35.187109 31926 leveldb.cpp:202] Seeked to beginning of db in 7965ns
I0129 02:00:35.187121 31926 leveldb.cpp:271] Iterated through 0 keys in the db in 6350ns
I0129 02:00:35.187165 31926 replica.cpp:779] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned
I0129 02:00:35.188433 31950 recover.cpp:447] Starting replica recovery
I0129 02:00:35.188796 31950 recover.cpp:473] Replica is in EMPTY status
I0129 02:00:35.190021 31949 replica.cpp:673] Replica in EMPTY status received a broadcasted recover request from (11817)@172.17.0.3:60904
I0129 02:00:35.190569 31958 recover.cpp:193] Received a recover response from a replica in EMPTY status
I0129 02:00:35.190994 31959 recover.cpp:564] Updating replica status to STARTING
I0129 02:00:35.191522 31953 master.cpp:374] Master 823f2212-bf28-4dd6-959d-796029d32afb (90665f991b70) started on 172.17.0.3:60904
I0129 02:00:35.191640 31953 master.cpp:376] Flags at startup: --acls="""" --allocation_interval=""1secs"" --allocator=""HierarchicalDRF"" --authenticate=""true"" --authenticate_http=""true"" --authenticate_slaves=""true"" --authenticators=""crammd5"" --authorizers=""local"" --credentials=""/tmp/B9O6zq/credentials"" --framework_sorter=""drf"" --help=""false"" --hostname_lookup=""true"" --http_authenticators=""basic"" --initialize_driver_logging=""true"" --log_auto_initialize=""true"" --logbufsecs=""0"" --logging_level=""INFO"" --max_completed_frameworks=""50"" --max_completed_tasks_per_framework=""1000"" --max_slave_ping_timeouts=""5"" --quiet=""false"" --recovery_slave_removal_limit=""100%"" --registry=""replicated_log"" --registry_fetch_timeout=""1mins"" --registry_store_timeout=""25secs"" --registry_strict=""true"" --root_submissions=""true"" --slave_ping_timeout=""15secs"" --slave_reregister_timeout=""10mins"" --user_sorter=""drf"" --version=""false"" --webui_dir=""/mesos/mesos-0.28.0/_inst/share/mesos/webui"" --work_dir=""/tmp/B9O6zq/master"" --zk_session_timeout=""10secs""
I0129 02:00:35.191926 31953 master.cpp:421] Master only allowing authenticated frameworks to register
I0129 02:00:35.191936 31953 master.cpp:426] Master only allowing authenticated slaves to register
I0129 02:00:35.191943 31953 credentials.hpp:35] Loading credentials for authentication from '/tmp/B9O6zq/credentials'
I0129 02:00:35.192229 31953 master.cpp:466] Using default 'crammd5' authenticator
I0129 02:00:35.192366 31953 master.cpp:535] Using default 'basic' HTTP authenticator
I0129 02:00:35.192530 31953 master.cpp:569] Authorization enabled
I0129 02:00:35.192719 31950 whitelist_watcher.cpp:77] No whitelist given
I0129 02:00:35.192756 31957 hierarchical.cpp:144] Initialized hierarchical allocator process
I0129 02:00:35.194291 31955 master.cpp:1710] The newly elected leader is master@172.17.0.3:60904 with id 823f2212-bf28-4dd6-959d-796029d32afb
I0129 02:00:35.194335 31955 master.cpp:1723] Elected as the leading master!
I0129 02:00:35.194350 31955 master.cpp:1468] Recovering from registrar
I0129 02:00:35.194545 31958 registrar.cpp:307] Recovering registrar
I0129 02:00:35.220226 31948 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 29.150097ms
I0129 02:00:35.220262 31948 replica.cpp:320] Persisted replica status to STARTING
I0129 02:00:35.220484 31959 recover.cpp:473] Replica is in STARTING status
I0129 02:00:35.221220 31954 replica.cpp:673] Replica in STARTING status received a broadcasted recover request from (11819)@172.17.0.3:60904
I0129 02:00:35.221539 31959 recover.cpp:193] Received a recover response from a replica in STARTING status
I0129 02:00:35.221871 31954 recover.cpp:564] Updating replica status to VOTING
I0129 02:00:35.245329 31949 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 23.326002ms
I0129 02:00:35.245367 31949 replica.cpp:320] Persisted replica status to VOTING
I0129 02:00:35.245522 31955 recover.cpp:578] Successfully joined the Paxos group
I0129 02:00:35.245800 31955 recover.cpp:462] Recover process terminated
I0129 02:00:35.246181 31951 log.cpp:659] Attempting to start the writer
I0129 02:00:35.247228 31953 replica.cpp:493] Replica received implicit promise request from (11820)@172.17.0.3:60904 with proposal 1
I0129 02:00:35.270472 31953 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 23.225846ms
I0129 02:00:35.270510 31953 replica.cpp:342] Persisted promised to 1
I0129 02:00:35.271306 31957 coordinator.cpp:238] Coordinator attempting to fill missing positions
I0129 02:00:35.272373 31949 replica.cpp:388] Replica received explicit promise request from (11821)@172.17.0.3:60904 for position 0 with proposal 2
I0129 02:00:35.295600 31949 leveldb.cpp:341] Persisting action (8 bytes) to leveldb took 23.181008ms
I0129 02:00:35.295639 31949 replica.cpp:712] Persisted action at 0
I0129 02:00:35.296815 31950 replica.cpp:537] Replica received write request for position 0 from (11822)@172.17.0.3:60904
I0129 02:00:35.296879 31950 leveldb.cpp:436] Reading position from leveldb took 43203ns
I0129 02:00:35.320659 31950 leveldb.cpp:341] Persisting action (14 bytes) to leveldb took 23.753935ms
I0129 02:00:35.320699 31950 replica.cpp:712] Persisted action at 0
I0129 02:00:35.321394 31950 replica.cpp:691] Replica received learned notice for position 0 from @0.0.0.0:0
I0129 02:00:35.345837 31950 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 24.358655ms
I0129 02:00:35.345877 31950 replica.cpp:712] Persisted action at 0
I0129 02:00:35.345898 31950 replica.cpp:697] Replica learned NOP action at position 0
I0129 02:00:35.346683 31950 log.cpp:675] Writer started with ending position 0
I0129 02:00:35.347913 31957 leveldb.cpp:436] Reading position from leveldb took 55621ns
I0129 02:00:35.349047 31947 registrar.cpp:340] Successfully fetched the registry (0B) in 154.395904ms
I0129 02:00:35.349185 31947 registrar.cpp:439] Applied 1 operations in 46347ns; attempting to update the 'registry'
I0129 02:00:35.350008 31952 log.cpp:683] Attempting to append 170 bytes to the log
I0129 02:00:35.350132 31957 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 1
I0129 02:00:35.351042 31953 replica.cpp:537] Replica received write request for position 1 from (11823)@172.17.0.3:60904
I0129 02:00:35.370906 31953 leveldb.cpp:341] Persisting action (189 bytes) to leveldb took 19.829257ms
I0129 02:00:35.370946 31953 replica.cpp:712] Persisted action at 1
I0129 02:00:35.371840 31952 replica.cpp:691] Replica received learned notice for position 1 from @0.0.0.0:0
I0129 02:00:35.396082 31952 leveldb.cpp:341] Persisting action (191 bytes) to leveldb took 24.218894ms
I0129 02:00:35.396122 31952 replica.cpp:712] Persisted action at 1
I0129 02:00:35.396144 31952 replica.cpp:697] Replica learned APPEND action at position 1
I0129 02:00:35.397250 31954 registrar.cpp:484] Successfully updated the 'registry' in 47.99104ms
I0129 02:00:35.397452 31954 registrar.cpp:370] Successfully recovered registrar
I0129 02:00:35.397678 31946 log.cpp:702] Attempting to truncate the log to 1
I0129 02:00:35.397881 31956 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 2
I0129 02:00:35.398066 31951 master.cpp:1520] Recovered 0 slaves from the Registry (131B) ; allowing 10mins for slaves to re-register
I0129 02:00:35.398111 31957 hierarchical.cpp:171] Skipping recovery of hierarchical allocator: nothing to recover
I0129 02:00:35.398982 31955 replica.cpp:537] Replica received write request for position 2 from (11824)@172.17.0.3:60904
I0129 02:00:35.421293 31955 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 22.286476ms
I0129 02:00:35.421339 31955 replica.cpp:712] Persisted action at 2
I0129 02:00:35.422046 31944 replica.cpp:691] Replica received learned notice for position 2 from @0.0.0.0:0
I0129 02:00:35.446316 31944 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 24.246177ms
I0129 02:00:35.446406 31944 leveldb.cpp:399] Deleting ~1 keys from leveldb took 84415ns
I0129 02:00:35.446466 31944 replica.cpp:712] Persisted action at 2
I0129 02:00:35.446491 31944 replica.cpp:697] Replica learned TRUNCATE action at position 2
I0129 02:00:35.452579 31957 slave.cpp:192] Slave started on 372)@172.17.0.3:60904
I0129 02:00:35.452620 31957 slave.cpp:193] Flags at startup: --appc_store_dir=""/tmp/mesos/store/appc"" --authenticatee=""crammd5"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --credential=""/tmp/ContentType_ExecutorHttpApiTest_DefaultAccept_1_r4GUhM/credential"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_auth_server=""https://auth.docker.io"" --docker_kill_orphans=""true"" --docker_puller_timeout=""60"" --docker_registry=""https://registry-1.docker.io"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/tmp/mesos/store/docker"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/tmp/ContentType_ExecutorHttpApiTest_DefaultAccept_1_r4GUhM/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --hostname_lookup=""true"" --image_provisioner_backend=""copy"" --initialize_driver_logging=""true"" --isolation=""posix/cpu,posix/mem"" --launcher_dir=""/mesos/mesos-0.28.0/_build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""10ms"" --resources=""cpus:2;mem:1024;disk:1024;ports:[31000-32000]"" --revocable_cpu_low_priority=""true"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --systemd_runtime_directory=""/run/systemd/system"" --version=""false"" --work_dir=""/tmp/ContentType_ExecutorHttpApiTest_DefaultAccept_1_r4GUhM""
I0129 02:00:35.453012 31957 credentials.hpp:83] Loading credential for authentication from '/tmp/ContentType_ExecutorHttpApiTest_DefaultAccept_1_r4GUhM/credential'
I0129 02:00:35.453191 31957 slave.cpp:323] Slave using credential for: test-principal
I0129 02:00:35.453368 31957 resources.cpp:564] Parsing resources as JSON failed: cpus:2;mem:1024;disk:1024;ports:[31000-32000]
Trying semicolon-delimited string format instead
I0129 02:00:35.453853 31957 slave.cpp:463] Slave resources: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I0129 02:00:35.453938 31957 slave.cpp:471] Slave attributes: [  ]
I0129 02:00:35.453953 31957 slave.cpp:476] Slave hostname: 90665f991b70
I0129 02:00:35.454794 31950 state.cpp:58] Recovering state from '/tmp/ContentType_ExecutorHttpApiTest_DefaultAccept_1_r4GUhM/meta'
I0129 02:00:35.455080 31948 status_update_manager.cpp:200] Recovering status update manager
I0129 02:00:35.455225 31926 sched.cpp:222] Version: 0.28.0
I0129 02:00:35.455535 31956 slave.cpp:4495] Finished recovery
I0129 02:00:35.455798 31945 sched.cpp:326] New master detected at master@172.17.0.3:60904
I0129 02:00:35.455879 31945 sched.cpp:382] Authenticating with master master@172.17.0.3:60904
I0129 02:00:35.455904 31945 sched.cpp:389] Using default CRAM-MD5 authenticatee
I0129 02:00:35.455943 31956 slave.cpp:4667] Querying resource estimator for oversubscribable resources
I0129 02:00:35.456167 31950 authenticatee.cpp:121] Creating new client SASL connection
I0129 02:00:35.456218 31953 status_update_manager.cpp:174] Pausing sending status updates
I0129 02:00:35.456219 31956 slave.cpp:795] New master detected at master@172.17.0.3:60904
I0129 02:00:35.456298 31956 slave.cpp:858] Authenticating with master master@172.17.0.3:60904
I0129 02:00:35.456323 31956 slave.cpp:863] Using default CRAM-MD5 authenticatee
I0129 02:00:35.456490 31948 authenticatee.cpp:121] Creating new client SASL connection
I0129 02:00:35.456492 31956 slave.cpp:831] Detecting new master
I0129 02:00:35.456588 31946 master.cpp:5521] Authenticating scheduler-93e745f0-0e48-4a8f-b227-93569976c5e8@172.17.0.3:60904
I0129 02:00:35.456686 31956 slave.cpp:4681] Received oversubscribable resources  from the resource estimator
I0129 02:00:35.456805 31953 authenticator.cpp:413] Starting authentication session for crammd5_authenticatee(804)@172.17.0.3:60904
I0129 02:00:35.456878 31946 master.cpp:5521] Authenticating slave(372)@172.17.0.3:60904
I0129 02:00:35.457124 31953 authenticator.cpp:413] Starting authentication session for crammd5_authenticatee(805)@172.17.0.3:60904
I0129 02:00:35.457157 31948 authenticator.cpp:98] Creating new server SASL connection
I0129 02:00:35.457373 31946 authenticatee.cpp:212] Received SASL authentication mechanisms: CRAM-MD5
I0129 02:00:35.457381 31951 authenticator.cpp:98] Creating new server SASL connection
I0129 02:00:35.457491 31946 authenticatee.cpp:238] Attempting to authenticate with mechanism 'CRAM-MD5'
I0129 02:00:35.457598 31946 authenticatee.cpp:212] Received SASL authentication mechanisms: CRAM-MD5
I0129 02:00:35.457612 31951 authenticator.cpp:203] Received SASL authentication start
I0129 02:00:35.457635 31946 authenticatee.cpp:238] Attempting to authenticate with mechanism 'CRAM-MD5'
I0129 02:00:35.457680 31951 authenticator.cpp:325] Authentication requires more steps
I0129 02:00:35.457767 31954 authenticator.cpp:203] Received SASL authentication start
I0129 02:00:35.457768 31948 authenticatee.cpp:258] Received SASL authentication step
I0129 02:00:35.457830 31954 authenticator.cpp:325] Authentication requires more steps
I0129 02:00:35.457885 31948 authenticator.cpp:231] Received SASL authentication step
I0129 02:00:35.457918 31948 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: '90665f991b70' server FQDN: '90665f991b70' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0129 02:00:35.457933 31948 auxprop.cpp:179] Looking up auxiliary property '*userPassword'
I0129 02:00:35.457954 31959 authenticatee.cpp:258] Received SASL authentication step
I0129 02:00:35.457993 31948 auxprop.cpp:179] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0129 02:00:35.458031 31948 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: '90665f991b70' server FQDN: '90665f991b70' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0129 02:00:35.458050 31948 auxprop.cpp:129] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0129 02:00:35.458065 31948 auxprop.cpp:129] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0129 02:00:35.458096 31948 authenticator.cpp:317] Authentication success
I0129 02:00:35.458112 31944 authenticator.cpp:231] Received SASL authentication step
I0129 02:00:35.458142 31944 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: '90665f991b70' server FQDN: '90665f991b70' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0129 02:00:35.458173 31944 auxprop.cpp:179] Looking up auxiliary property '*userPassword'
I0129 02:00:35.458206 31954 authenticatee.cpp:298] Authentication success
I0129 02:00:35.458256 31957 master.cpp:5551] Successfully authenticated principal 'test-principal' at scheduler-93e745f0-0e48-4a8f-b227-93569976c5e8@172.17.0.3:60904
I0129 02:00:35.458206 31944 auxprop.cpp:179] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0129 02:00:35.458360 31944 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: '90665f991b70' server FQDN: '90665f991b70' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0129 02:00:35.458382 31944 auxprop.cpp:129] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0129 02:00:35.458397 31944 auxprop.cpp:129] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0129 02:00:35.458489 31944 authenticator.cpp:317] Authentication success
I0129 02:00:35.458623 31953 sched.cpp:471] Successfully authenticated with master master@172.17.0.3:60904
I0129 02:00:35.458649 31953 sched.cpp:780] Sending SUBSCRIBE call to master@172.17.0.3:60904
I0129 02:00:35.458653 31956 authenticator.cpp:431] Authentication session cleanup for crammd5_authenticatee(804)@172.17.0.3:60904
I0129 02:00:35.458673 31951 authenticatee.cpp:298] Authentication success
I0129 02:00:35.458709 31952 master.cpp:5551] Successfully authenticated principal 'test-principal' at slave(372)@172.17.0.3:60904
I0129 02:00:35.458906 31955 slave.cpp:926] Successfully authenticated with master master@172.17.0.3:60904
I0129 02:00:35.458983 31956 authenticator.cpp:431] Authentication session cleanup for crammd5_authenticatee(805)@172.17.0.3:60904
I0129 02:00:35.459033 31955 slave.cpp:1320] Will retry registration in 7.075135ms if necessary
I0129 02:00:35.459128 31953 sched.cpp:813] Will retry registration in 86.579738ms if necessary
I0129 02:00:35.459193 31950 master.cpp:4235] Registering slave at slave(372)@172.17.0.3:60904 (90665f991b70) with id 823f2212-bf28-4dd6-959d-796029d32afb-S0
I0129 02:00:35.459489 31950 master.cpp:2278] Received SUBSCRIBE call for framework 'default' at scheduler-93e745f0-0e48-4a8f-b227-93569976c5e8@172.17.0.3:60904
I0129 02:00:35.459513 31950 master.cpp:1749] Authorizing framework principal 'test-principal' to receive offers for role '*'
I0129 02:00:35.459516 31959 registrar.cpp:439] Applied 1 operations in 62499ns; attempting to update the 'registry'
I0129 02:00:35.459766 31956 master.cpp:2349] Subscribing framework default with checkpointing disabled and capabilities [  ]
I0129 02:00:35.460095 31955 log.cpp:683] Attempting to append 339 bytes to the log
I0129 02:00:35.460192 31948 hierarchical.cpp:265] Added framework 823f2212-bf28-4dd6-959d-796029d32afb-0000
I0129 02:00:35.460247 31956 sched.cpp:707] Framework registered with 823f2212-bf28-4dd6-959d-796029d32afb-0000
I0129 02:00:35.460314 31958 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 3
I0129 02:00:35.460388 31948 hierarchical.cpp:1403] No resources available to allocate!
I0129 02:00:35.460449 31948 hierarchical.cpp:1498] No inverse offers to send out!
I0129 02:00:35.460402 31956 sched.cpp:721] Scheduler::registered took 136519ns
I0129 02:00:35.460482 31948 hierarchical.cpp:1096] Performed allocation for 0 slaves in 158218ns
I0129 02:00:35.461187 31944 replica.cpp:537] Replica received write request for position 3 from (11829)@172.17.0.3:60904
I0129 02:00:35.467929 31954 slave.cpp:1320] Will retry registration in 14.701381ms if necessary
I0129 02:00:35.468183 31952 master.cpp:4223] Ignoring register slave message from slave(372)@172.17.0.3:60904 (90665f991b70) as admission is already in progress
I0129 02:00:35.483300 31959 slave.cpp:1320] Will retry registration in 8.003223ms if necessary
I0129 02:00:35.483500 31946 master.cpp:4223] Ignoring register slave message from slave(372)@172.17.0.3:60904 (90665f991b70) as admission is already in progress
I0129 02:00:35.491843 31945 slave.cpp:1320] Will retry registration in 52.952447ms if necessary
I0129 02:00:35.491962 31948 master.cpp:4223] Ignoring register slave message from slave(372)@172.17.0.3:60904 (90665f991b70) as admission is already in progress
I0129 02:00:35.503868 31944 leveldb.cpp:341] Persisting action (358 bytes) to leveldb took 42.66008ms
I0129 02:00:35.503917 31944 replica.cpp:712] Persisted action at 3
I0129 02:00:35.504838 31953 replica.cpp:691] Replica received learned notice for position 3 from @0.0.0.0:0
I0129 02:00:35.545286 31955 slave.cpp:1320] Will retry registration in 298.440134ms if necessary
I0129 02:00:35.545500 31957 master.cpp:4223] Ignoring register slave message from slave(372)@172.17.0.3:60904 (90665f991b70) as admission is already in progress
I0129 02:00:35.545524 31953 leveldb.cpp:341] Persisting action (360 bytes) to leveldb took 40.663886ms
I0129 02:00:35.545560 31953 replica.cpp:712] Persisted action at 3
I0129 02:00:35.545584 31953 replica.cpp:697] Replica learned APPEND action at position 3
I0129 02:00:35.547586 31945 registrar.cpp:484] Successfully updated the 'registry' in 87.995136ms
I0129 02:00:35.547767 31949 log.cpp:702] Attempting to truncate the log to 3
I0129 02:00:35.547906 31954 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 4
I0129 02:00:35.548713 31945 slave.cpp:3435] Received ping from slave-observer(343)@172.17.0.3:60904
I0129 02:00:35.549018 31957 replica.cpp:537] Replica received write request for position 4 from (11830)@172.17.0.3:60904
I0129 02:00:35.549124 31956 slave.cpp:970] Registered with master master@172.17.0.3:60904; given slave ID 823f2212-bf28-4dd6-959d-796029d32afb-S0
I0129 02:00:35.549049 31953 master.cpp:4303] Registered slave 823f2212-bf28-4dd6-959d-796029d32afb-S0 at slave(372)@172.17.0.3:60904 (90665f991b70) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I0129 02:00:35.549175 31956 fetcher.cpp:81] Clearing fetcher cache
I0129 02:00:35.549362 31954 status_update_manager.cpp:181] Resuming sending status updates
I0129 02:00:35.549350 31959 hierarchical.cpp:473] Added slave 823f2212-bf28-4dd6-959d-796029d32afb-S0 (90665f991b70) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (allocated: )
I0129 02:00:35.549720 31956 slave.cpp:993] Checkpointing SlaveInfo to '/tmp/ContentType_ExecutorHttpApiTest_DefaultAccept_1_r4GUhM/meta/slaves/823f2212-bf28-4dd6-959d-796029d32afb-S0/slave.info'
I0129 02:00:35.550135 31956 slave.cpp:1029] Forwarding total oversubscribed resources 
I0129 02:00:35.550341 31949 master.cpp:4644] Received update of slave 823f2212-bf28-4dd6-959d-796029d32afb-S0 at slave(372)@172.17.0.3:60904 (90665f991b70) with total oversubscribed resources 
I0129 02:00:35.550400 31959 hierarchical.cpp:1498] No inverse offers to send out!
I0129 02:00:35.550475 31959 hierarchical.cpp:1116] Performed allocation for slave 823f2212-bf28-4dd6-959d-796029d32afb-S0 in 1.046149ms
I0129 02:00:35.550946 31956 hierarchical.cpp:531] Slave 823f2212-bf28-4dd6-959d-796029d32afb-S0 (90665f991b70) updated with oversubscribed resources  (total: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000], allocated: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000])
I0129 02:00:35.550976 31949 master.cpp:5350] Sending 1 offers to framework 823f2212-bf28-4dd6-959d-796029d32afb-0000 (default) at scheduler-93e745f0-0e48-4a8f-b227-93569976c5e8@172.17.0.3:60904
I0129 02:00:35.551132 31956 hierarchical.cpp:1403] No resources available to allocate!
I0129 02:00:35.551187 31956 hierarchical.cpp:1498] No inverse offers to send out!
I0129 02:00:35.551225 31956 hierarchical.cpp:1116] Performed allocation for slave 823f2212-bf28-4dd6-959d-796029d32afb-S0 in 229801ns
I0129 02:00:35.551635 31951 sched.cpp:877] Scheduler::resourceOffers took 155532ns
I0129 02:00:35.553310 31944 master.cpp:3136] Processing ACCEPT call for offers: [ 823f2212-bf28-4dd6-959d-796029d32afb-O0 ] on slave 823f2212-bf28-4dd6-959d-796029d32afb-S0 at slave(372)@172.17.0.3:60904 (90665f991b70) for framework 823f2212-bf28-4dd6-959d-796029d32afb-0000 (default) at scheduler-93e745f0-0e48-4a8f-b227-93569976c5e8@172.17.0.3:60904
I0129 02:00:35.553364 31944 master.cpp:2823] Authorizing framework principal 'test-principal' to launch task 558bdc51-38dc-48e3-9b81-ad42b942050c as user 'mesos'
W0129 02:00:35.554951 31944 validation.cpp:404] Executor default for task 558bdc51-38dc-48e3-9b81-ad42b942050c uses less CPUs (None) than the minimum required (0.01). Please update your executor, as this will be mandatory in future releases.
W0129 02:00:35.555004 31944 validation.cpp:416] Executor default for task 558bdc51-38dc-48e3-9b81-ad42b942050c uses less memory (None) than the minimum required (32MB). Please update your executor, as this will be mandatory in future releases.
I0129 02:00:35.555403 31944 master.hpp:176] Adding task 558bdc51-38dc-48e3-9b81-ad42b942050c with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave 823f2212-bf28-4dd6-959d-796029d32afb-S0 (90665f991b70)
I0129 02:00:35.555660 31944 master.cpp:3621] Launching task 558bdc51-38dc-48e3-9b81-ad42b942050c of framework 823f2212-bf28-4dd6-959d-796029d32afb-0000 (default) at scheduler-93e745f0-0e48-4a8f-b227-93569976c5e8@172.17.0.3:60904 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave 823f2212-bf28-4dd6-959d-796029d32afb-S0 at slave(372)@172.17.0.3:60904 (90665f991b70)
I0129 02:00:35.556067 31948 slave.cpp:1360] Got assigned task 558bdc51-38dc-48e3-9b81-ad42b942050c for framework 823f2212-bf28-4dd6-959d-796029d32afb-0000
I0129 02:00:35.556691 31948 slave.cpp:1479] Launching task 558bdc51-38dc-48e3-9b81-ad42b942050c for framework 823f2212-bf28-4dd6-959d-796029d32afb-0000
I0129 02:00:35.557307 31948 paths.cpp:472] Trying to chown '/tmp/ContentType_ExecutorHttpApiTest_DefaultAccept_1_r4GUhM/slaves/823f2212-bf28-4dd6-959d-796029d32afb-S0/frameworks/823f2212-bf28-4dd6-959d-796029d32afb-0000/executors/default/runs/92af8a30-2bb0-48fc-874a-e854ff82225b' to user 'mesos'
I0129 02:00:35.580426 31948 slave.cpp:5281] Launching executor default of framework 823f2212-bf28-4dd6-959d-796029d32afb-0000 with resources  in work directory '/tmp/ContentType_ExecutorHttpApiTest_DefaultAccept_1_r4GUhM/slaves/823f2212-bf28-4dd6-959d-796029d32afb-S0/frameworks/823f2212-bf28-4dd6-959d-796029d32afb-0000/executors/default/runs/92af8a30-2bb0-48fc-874a-e854ff82225b'
*** Aborted at 1454032835 (unix time) try ""date -d @1454032835"" if you are using GNU date ***
I0129 02:00:35.582674 31948 exec.cpp:134] Version: 0.28.0
PC: @     0x2b342648a8dd (unknown)
I0129 02:00:35.582969 31958 exec.cpp:184] Executor started at: executor(123)@172.17.0.3:60904 with pid 31926
I0129 02:00:35.583271 31948 slave.cpp:1697] Queuing task '558bdc51-38dc-48e3-9b81-ad42b942050c' for executor 'default' of framework 823f2212-bf28-4dd6-959d-796029d32afb-0000
I0129 02:00:35.583444 31948 slave.cpp:748] Successfully attached file '/tmp/ContentType_ExecutorHttpApiTest_DefaultAccept_1_r4GUhM/slaves/823f2212-bf28-4dd6-959d-796029d32afb-S0/frameworks/823f2212-bf28-4dd6-959d-796029d32afb-0000/executors/default/runs/92af8a30-2bb0-48fc-874a-e854ff82225b'
I0129 02:00:35.583636 31948 slave.cpp:2642] Got registration for executor 'default' of framework 823f2212-bf28-4dd6-959d-796029d32afb-0000 from executor(123)@172.17.0.3:60904
I0129 02:00:35.584103 31950 exec.cpp:208] Executor registered on slave 823f2212-bf28-4dd6-959d-796029d32afb-S0
I0129 02:00:35.584170 31950 exec.cpp:220] Executor::registered took 39070ns
I0129 02:00:35.584476 31948 slave.cpp:1862] Sending queued task '558bdc51-38dc-48e3-9b81-ad42b942050c' to executor 'default' of framework 823f2212-bf28-4dd6-959d-796029d32afb-0000 at executor(123)@172.17.0.3:60904
I0129 02:00:35.584918 31944 exec.cpp:295] Executor asked to run task '558bdc51-38dc-48e3-9b81-ad42b942050c'
I0129 02:00:35.585036 31944 exec.cpp:304] Executor::launchTask took 93979ns
I0129 02:00:35.585160 31944 exec.cpp:517] Executor sending status update TASK_RUNNING (UUID: a9be0e7b-c011-4099-aba9-c914c911d7a9) for task 558bdc51-38dc-48e3-9b81-ad42b942050c of framework 823f2212-bf28-4dd6-959d-796029d32afb-0000
I0129 02:00:35.585564 31956 slave.cpp:3001] Handling status update TASK_RUNNING (UUID: a9be0e7b-c011-4099-aba9-c914c911d7a9) for task 558bdc51-38dc-48e3-9b81-ad42b942050c of framework 823f2212-bf28-4dd6-959d-796029d32afb-0000 from executor(123)@172.17.0.3:60904
I0129 02:00:35.585914 31944 status_update_manager.cpp:320] Received status update TASK_RUNNING (UUID: a9be0e7b-c011-4099-aba9-c914c911d7a9) for task 558bdc51-38dc-48e3-9b81-ad42b942050c of framework 823f2212-bf28-4dd6-959d-796029d32afb-0000
I0129 02:00:35.585973 31944 status_update_manager.cpp:497] Creating StatusUpdate stream for task 558bdc51-38dc-48e3-9b81-ad42b942050c of framework 823f2212-bf28-4dd6-959d-796029d32afb-0000
I0129 02:00:35.586472 31944 status_update_manager.cpp:374] Forwarding update TASK_RUNNING (UUID: a9be0e7b-c011-4099-aba9-c914c911d7a9) for task 558bdc51-38dc-48e3-9b81-ad42b942050c of framework 823f2212-bf28-4dd6-959d-796029d32afb-0000 to the slave
I0129 02:00:35.586774 31953 slave.cpp:3353] Forwarding the update TASK_RUNNING (UUID: a9be0e7b-c011-4099-aba9-c914c911d7a9) for task 558bdc51-38dc-48e3-9b81-ad42b942050c of framework 823f2212-bf28-4dd6-959d-796029d32afb-0000 to master@172.17.0.3:60904
I0129 02:00:35.587050 31953 slave.cpp:3247] Status update manager successfully handled status update TASK_RUNNING (UUID: a9be0e7b-c011-4099-aba9-c914c911d7a9) for task 558bdc51-38dc-48e3-9b81-ad42b942050c of framework 823f2212-bf28-4dd6-959d-796029d32afb-0000
I0129 02:00:35.587118 31953 slave.cpp:3263] Sending acknowledgement for status update TASK_RUNNING (UUID: a9be0e7b-c011-4099-aba9-c914c911d7a9) for task 558bdc51-38dc-48e3-9b81-ad42b942050c of framework 823f2212-bf28-4dd6-959d-796029d32afb-0000 to executor(123)@172.17.0.3:60904
I0129 02:00:35.587172 31948 master.cpp:4789] Status update TASK_RUNNING (UUID: a9be0e7b-c011-4099-aba9-c914c911d7a9) for task 558bdc51-38dc-48e3-9b81-ad42b942050c of framework 823f2212-bf28-4dd6-959d-796029d32afb-0000 from slave 823f2212-bf28-4dd6-959d-796029d32afb-S0 at slave(372)@172.17.0.3:60904 (90665f991b70)
I0129 02:00:35.587226 31948 master.cpp:4837] Forwarding status update TASK_RUNNING (UUID: a9be0e7b-c011-4099-aba9-c914c911d7a9) for task 558bdc51-38dc-48e3-9b81-ad42b942050c of framework 823f2212-bf28-4dd6-959d-796029d32afb-0000
I0129 02:00:35.587316 31953 exec.cpp:341] Executor received status update acknowledgement a9be0e7b-c011-4099-aba9-c914c911d7a9 for task 558bdc51-38dc-48e3-9b81-ad42b942050c of framework 823f2212-bf28-4dd6-959d-796029d32afb-0000
I0129 02:00:35.587404 31948 master.cpp:6445] Updating the state of task 558bdc51-38dc-48e3-9b81-ad42b942050c of framework 823f2212-bf28-4dd6-959d-796029d32afb-0000 (latest state: TASK_RUNNING, status update state: TASK_RUNNING)
I0129 02:00:35.587743 31953 sched.cpp:985] Scheduler::statusUpdate took 84229ns
I0129 02:00:35.588039 31957 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 38.985866ms
I0129 02:00:35.588095 31957 replica.cpp:712] Persisted action at 4
I0129 02:00:35.588568 31948 master.cpp:3947] Processing ACKNOWLEDGE call a9be0e7b-c011-4099-aba9-c914c911d7a9 for task 558bdc51-38dc-48e3-9b81-ad42b942050c of framework 823f2212-bf28-4dd6-959d-796029d32afb-0000 (default) at scheduler-93e745f0-0e48-4a8f-b227-93569976c5e8@172.17.0.3:60904 on slave 823f2212-bf28-4dd6-959d-796029d32afb-S0
I0129 02:00:35.588979 31950 replica.cpp:691] Replica received learned notice for position 4 from @0.0.0.0:0
I0129 02:00:35.589004 31957 status_update_manager.cpp:392] Received status update acknowledgement (UUID: a9be0e7b-c011-4099-aba9-c914c911d7a9) for task 558bdc51-38dc-48e3-9b81-ad42b942050c of framework 823f2212-bf28-4dd6-959d-796029d32afb-0000
I0129 02:00:35.589700 31954 slave.cpp:2411] Status update manager successfully handled status update acknowledgement (UUID: a9be0e7b-c011-4099-aba9-c914c911d7a9) for task 558bdc51-38dc-48e3-9b81-ad42b942050c of framework 823f2212-bf28-4dd6-959d-796029d32afb-0000
I0129 02:00:35.590204 31948 process.cpp:3141] Handling HTTP event for process 'slave(372)' with path: '/slave(372)/api/v1/executor'
I0129 02:00:35.590828 31951 http.cpp:190] HTTP POST for /slave(372)/api/v1/executor from 172.17.0.3:52186
I0129 02:00:35.591156 31951 slave.cpp:2475] Received Subscribe request for HTTP executor 'default' of framework 823f2212-bf28-4dd6-959d-796029d32afb-0000 at executor(123)@172.17.0.3:60904
I0129 02:00:35.593617 31948 master.cpp:1025] Master terminating
W0129 02:00:35.593758 31948 master.cpp:6497] Removing task 558bdc51-38dc-48e3-9b81-ad42b942050c with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] of framework 823f2212-bf28-4dd6-959d-796029d32afb-0000 on slave 823f2212-bf28-4dd6-959d-796029d32afb-S0 at slave(372)@172.17.0.3:60904 (90665f991b70) in non-terminal state TASK_RUNNING
I0129 02:00:35.594292 31958 hierarchical.cpp:505] Removed slave 823f2212-bf28-4dd6-959d-796029d32afb-S0
I0129 02:00:35.594655 31948 master.cpp:6540] Removing executor 'default' with resources  of framework 823f2212-bf28-4dd6-959d-796029d32afb-0000 on slave 823f2212-bf28-4dd6-959d-796029d32afb-S0 at slave(372)@172.17.0.3:60904 (90665f991b70)
I0129 02:00:35.595233 31955 hierarchical.cpp:326] Removed framework 823f2212-bf28-4dd6-959d-796029d32afb-0000
I0129 02:00:35.595634 31944 slave.cpp:3481] master@172.17.0.3:60904 exited
W0129 02:00:35.595664 31944 slave.cpp:3484] Master disconnected! Waiting for a new master to be elected
*** SIGSEGV (@0xd0) received by PID 31926 (TID 0x2b34f7a20700) from PID 208; stack trace: ***
    @     0x2b34330d90b7 os::Linux::chained_handler()
    @     0x2b34330dd219 JVM_handle_linux_signal
    @     0x2b3426241340 (unknown)
    @     0x2b342648a8dd (unknown)
    @     0x2b3426480681 (unknown)
    @     0x2b34264d798e (unknown)
    @     0x2b34264d78cf (unknown)
    @     0x2b342402a520 handle_socket_error_msg
I0129 02:00:35.646546 31950 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 57.450403ms
I0129 02:00:35.646659 31950 leveldb.cpp:399] Deleting ~2 keys from leveldb took 64897ns
I0129 02:00:35.646680 31950 replica.cpp:712] Persisted action at 4
    @     0x2b342402b3bb zookeeper_process
I0129 02:00:35.646709 31950 replica.cpp:697] Replica learned TRUNCATE action at position 4
I0129 02:00:35.648186 31948 slave.cpp:3481] executor(123)@172.17.0.3:60904 exited
I0129 02:00:35.648372 31952 slave.cpp:3816] Executor 'default' of framework 823f2212-bf28-4dd6-959d-796029d32afb-0000 exited with status 0
I0129 02:00:35.648558 31952 slave.cpp:3001] Handling status update TASK_FAILED (UUID: 7584c149-3dfe-47d7-9f91-ddfa2c3b4e38) for task 558bdc51-38dc-48e3-9b81-ad42b942050c of framework 823f2212-bf28-4dd6-959d-796029d32afb-0000 from @0.0.0.0:0
I0129 02:00:35.648720 31952 slave.cpp:5591] Terminating task 558bdc51-38dc-48e3-9b81-ad42b942050c
I0129 02:00:35.649889 31948 status_update_manager.cpp:320] Received status update TASK_FAILED (UUID: 7584c149-3dfe-47d7-9f91-ddfa2c3b4e38) for task 558bdc51-38dc-48e3-9b81-ad42b942050c of framework 823f2212-bf28-4dd6-959d-796029d32afb-0000
I0129 02:00:35.649999 31952 slave.cpp:667] Slave terminating
I0129 02:00:35.650074 31952 slave.cpp:2078] Asked to shut down framework 823f2212-bf28-4dd6-959d-796029d32afb-0000 by @0.0.0.0:0
I0129 02:00:35.650120 31948 status_update_manager.cpp:374] Forwarding update TASK_FAILED (UUID: 7584c149-3dfe-47d7-9f91-ddfa2c3b4e38) for task 558bdc51-38dc-48e3-9b81-ad42b942050c of framework 823f2212-bf28-4dd6-959d-796029d32afb-0000 to the slave
I0129 02:00:35.650193 31952 slave.cpp:2103] Shutting down framework 823f2212-bf28-4dd6-959d-796029d32afb-0000
I0129 02:00:35.650257 31952 slave.cpp:3920] Cleaning up executor 'default' of framework 823f2212-bf28-4dd6-959d-796029d32afb-0000 (via HTTP)
I0129 02:00:35.650461 31957 gc.cpp:54] Scheduling '/tmp/ContentType_ExecutorHttpApiTest_DefaultAccept_1_r4GUhM/slaves/823f2212-bf28-4dd6-959d-796029d32afb-S0/frameworks/823f2212-bf28-4dd6-959d-796029d32afb-0000/executors/default/runs/92af8a30-2bb0-48fc-874a-e854ff82225b' for gc 6.9999924726163days in the future
I0129 02:00:35.650661 31952 slave.cpp:4008] Cleaning up framework 823f2212-bf28-4dd6-959d-796029d32afb-0000
I0129 02:00:35.650661 31955 gc.cpp:54] Scheduling '/tmp/ContentType_ExecutorHttpApiTest_DefaultAccept_1_r4GUhM/slaves/823f2212-bf28-4dd6-959d-796029d32afb-S0/frameworks/823f2212-bf28-4dd6-959d-796029d32afb-0000/executors/default' for gc 6.99999247009185days in the future
I0129 02:00:35.650902 31955 status_update_manager.cpp:282] Closing status update streams for framework 823f2212-bf28-4dd6-959d-796029d32afb-0000
I0129 02:00:35.650960 31955 status_update_manager.cpp:528] Cleaning up status update stream for task 558bdc51-38dc-48e3-9b81-ad42b942050c of framework 823f2212-bf28-4dd6-959d-796029d32afb-0000
I0129 02:00:35.651008 31948 gc.cpp:54] Scheduling '/tmp/ContentType_ExecutorHttpApiTest_DefaultAccept_1_r4GUhM/slaves/823f2212-bf28-4dd6-959d-796029d32afb-S0/frameworks/823f2212-bf28-4dd6-959d-796029d32afb-0000' for gc 6.99999246598519days in the future
    @     0x2b34240319cb do_io
    @     0x2b3426239182 start_thread
[       OK ] ContentType/ExecutorHttpApiTest.DefaultAccept/1 (637 ms)
    @     0x2b342654947d (unknown)
{code}

There was another test {{MasterMaintenanceTest.InverseOffersFilters}} which failed right after completion with a similar stack trace:

{code}
*** Aborted at 1454095687 (unix time) try ""date -d @1454095687"" if you are using GNU date ***
PC: @     0x2b2faec908dd (unknown)
*** SIGSEGV (@0x51) received by PID 30503 (TID 0x2b307fe5f700) from PID 81; stack trace: ***
    @     0x2b2fbb1e50b7 os::Linux::chained_handler()
    @     0x2b2fbb1e9219 JVM_handle_linux_signal
    @     0x2b2faea47340 (unknown)
    @     0x2b2faec908dd (unknown)
    @     0x2b2faec86681 (unknown)
    @     0x2b2faecdd98e (unknown)
    @     0x2b2faecdd8cf (unknown)
    @     0x2b2facc1b0d0 handle_socket_error_msg
    @     0x2b2facc1bf6b zookeeper_process
    @     0x2b2facc2209b do_io
    @     0x2b2faea3f182 start_thread
    @     0x2b2faed4f47d (unknown)
make[4]: *** [check-local] Segmentation fault
make[4]: Leaving directory `/mesos/mesos-0.28.0/_build/src'
make[3]: Leaving directory `/mesos/mesos-0.28.0/_build/src'
make[3]: *** [check-am] Error 2
make[2]: Leaving directory `/mesos/mesos-0.28.0/_build/src'
make[2]: *** [check] Error 2
make[1]: *** [check-recursive] Error 1
make[1]: Leaving directory `/mesos/mesos-0.28.0/_build'
make: *** [distcheck] Error 1
{code}
",Bug,Major,anandmazumdar,,10020,Accepted,In Progress,Investigate test suite crashes after ZK socket disconnections.,2016-02-01T19:39:16.000+0000,MESOS-4554,3.0,mesos,
neilc,2016-01-28T21:24:55.000+0000,cmaloney,"Sample Mesos Agent log: https://gist.github.com/brndnmtthws/fb846fa988487250a809

Note, zookeeper has a function to change the list of servers at runtime: https://github.com/apache/zookeeper/blob/735ea78909e67c648a4978c8d31d63964986af73/src/c/src/zookeeper.c#L1207-L1232

This comes up when using an AWS AutoScalingGroup for managing the set of masters. 

The agent when it comes up the first time, resolves the zk:// string. Once all the hosts that were in the original string fail (Each fails, is replaced by a new machine, which has the same DNS name), the agent just keeps spinning in an internal loop, never re-resolving the DNS names.

Two solutions I see are 
1. Update the list of servers / re-resolve
2. Have the agent detect it hasn't connected recently, and kill itself (Which will force a re-resolution when the agent starts back up)",Bug,Blocker,cmaloney,2016-01-31T01:45:14.000+0000,5,Resolved,Complete,Mesos Agents needs to re-resolve hosts in zk string on leader change / failure to connect,2016-02-15T20:45:06.000+0000,MESOS-4546,3.0,mesos,Mesosphere Sprint 27
alexr,2016-01-28T11:13:59.000+0000,alexr,"Can be reproduced by running {{GLOG_v=1 GTEST_FILTER=""MasterQuotaTest.AvailableResourcesAfterRescinding"" ./bin/mesos-tests.sh --gtest_shuffle --gtest_break_on_failure --gtest_repeat=1000 --verbose}}.

h5. Verbose log from a bad run:
{code}
[ RUN      ] MasterQuotaTest.AvailableResourcesAfterRescinding
I0128 12:20:27.568657 2080858880 resources.cpp:564] Parsing resources as JSON failed: cpus:2;mem:1024;disk:1024;ports:[31000-32000]
Trying semicolon-delimited string format instead
I0128 12:20:27.570142 2080858880 resources.cpp:564] Parsing resources as JSON failed: cpus:2;mem:1024;disk:1024;ports:[31000-32000]
Trying semicolon-delimited string format instead
I0128 12:20:27.583225 2080858880 leveldb.cpp:174] Opened db in 6241us
I0128 12:20:27.584353 2080858880 leveldb.cpp:181] Compacted db in 1026us
I0128 12:20:27.584429 2080858880 leveldb.cpp:196] Created db iterator in 12us
I0128 12:20:27.584442 2080858880 leveldb.cpp:202] Seeked to beginning of db in 7us
I0128 12:20:27.584453 2080858880 leveldb.cpp:271] Iterated through 0 keys in the db in 6us
I0128 12:20:27.584475 2080858880 replica.cpp:779] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned
I0128 12:20:27.584918 300445696 recover.cpp:447] Starting replica recovery
I0128 12:20:27.585113 300445696 recover.cpp:473] Replica is in EMPTY status
I0128 12:20:27.585916 297226240 replica.cpp:673] Replica in EMPTY status received a broadcasted recover request from (18274)@192.168.178.24:51278
I0128 12:20:27.586086 297762816 recover.cpp:193] Received a recover response from a replica in EMPTY status
I0128 12:20:27.586449 297226240 recover.cpp:564] Updating replica status to STARTING
I0128 12:20:27.587204 300445696 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 624us
I0128 12:20:27.587242 300445696 replica.cpp:320] Persisted replica status to STARTING
I0128 12:20:27.587376 299372544 recover.cpp:473] Replica is in STARTING status
I0128 12:20:27.588050 300982272 replica.cpp:673] Replica in STARTING status received a broadcasted recover request from (18275)@192.168.178.24:51278
I0128 12:20:27.588235 300445696 recover.cpp:193] Received a recover response from a replica in STARTING status
I0128 12:20:27.588572 297762816 recover.cpp:564] Updating replica status to VOTING
I0128 12:20:27.588850 297226240 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 140us
I0128 12:20:27.588879 297226240 replica.cpp:320] Persisted replica status to VOTING
I0128 12:20:27.588975 299909120 recover.cpp:578] Successfully joined the Paxos group
I0128 12:20:27.589154 299909120 recover.cpp:462] Recover process terminated
I0128 12:20:27.599486 298835968 master.cpp:374] Master 531344bd-56f4-4e4f-8f6f-a6a9d36058c7 (alexr.fritz.box) started on 192.168.178.24:51278
I0128 12:20:27.599520 298835968 master.cpp:376] Flags at startup: --acls="""" --allocation_interval=""50ms"" --allocator=""HierarchicalDRF"" --authenticate=""true"" --authenticate_http=""true"" --authenticate_slaves=""true"" --authenticators=""crammd5"" --authorizers=""local"" --credentials=""/private/tmp/NlzPSo/credentials"" --framework_sorter=""drf"" --help=""false"" --hostname_lookup=""true"" --http_authenticators=""basic"" --initialize_driver_logging=""true"" --log_auto_initialize=""true"" --logbufsecs=""0"" --logging_level=""INFO"" --max_completed_frameworks=""50"" --max_completed_tasks_per_framework=""1000"" --max_slave_ping_timeouts=""5"" --quiet=""false"" --recovery_slave_removal_limit=""100%"" --registry=""replicated_log"" --registry_fetch_timeout=""1mins"" --registry_store_timeout=""25secs"" --registry_strict=""true"" --roles=""role1,role2"" --root_submissions=""true"" --slave_ping_timeout=""15secs"" --slave_reregister_timeout=""10mins"" --user_sorter=""drf"" --version=""false"" --webui_dir=""/usr/local/share/mesos/webui"" --work_dir=""/private/tmp/NlzPSo/master"" --zk_session_timeout=""10secs""
I0128 12:20:27.599753 298835968 master.cpp:421] Master only allowing authenticated frameworks to register
I0128 12:20:27.599769 298835968 master.cpp:426] Master only allowing authenticated slaves to register
I0128 12:20:27.599781 298835968 credentials.hpp:35] Loading credentials for authentication from '/private/tmp/NlzPSo/credentials'
I0128 12:20:27.600082 298835968 master.cpp:466] Using default 'crammd5' authenticator
I0128 12:20:27.600163 298835968 master.cpp:535] Using default 'basic' HTTP authenticator
I0128 12:20:27.600327 298835968 master.cpp:569] Authorization enabled
W0128 12:20:27.600345 298835968 master.cpp:629] The '--roles' flag is deprecated. This flag will be removed in the future. See the Mesos 0.27 upgrade notes for more information
I0128 12:20:27.600497 297762816 whitelist_watcher.cpp:77] No whitelist given
I0128 12:20:27.600503 297226240 hierarchical.cpp:144] Initialized hierarchical allocator process
I0128 12:20:27.601965 297226240 master.cpp:1710] The newly elected leader is master@192.168.178.24:51278 with id 531344bd-56f4-4e4f-8f6f-a6a9d36058c7
I0128 12:20:27.601995 297226240 master.cpp:1723] Elected as the leading master!
I0128 12:20:27.602007 297226240 master.cpp:1468] Recovering from registrar
I0128 12:20:27.602083 300445696 registrar.cpp:307] Recovering registrar
I0128 12:20:27.602460 297226240 log.cpp:659] Attempting to start the writer
I0128 12:20:27.603514 299909120 replica.cpp:493] Replica received implicit promise request from (18277)@192.168.178.24:51278 with proposal 1
I0128 12:20:27.603734 299909120 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 205us
I0128 12:20:27.603768 299909120 replica.cpp:342] Persisted promised to 1
I0128 12:20:27.604194 299909120 coordinator.cpp:238] Coordinator attempting to fill missing positions
I0128 12:20:27.605311 299372544 replica.cpp:388] Replica received explicit promise request from (18278)@192.168.178.24:51278 for position 0 with proposal 2
I0128 12:20:27.605468 299372544 leveldb.cpp:341] Persisting action (8 bytes) to leveldb took 133us
I0128 12:20:27.605494 299372544 replica.cpp:712] Persisted action at 0
I0128 12:20:27.606441 298835968 replica.cpp:537] Replica received write request for position 0 from (18279)@192.168.178.24:51278
I0128 12:20:27.606492 298835968 leveldb.cpp:436] Reading position from leveldb took 29us
I0128 12:20:27.606665 298835968 leveldb.cpp:341] Persisting action (14 bytes) to leveldb took 151us
I0128 12:20:27.606688 298835968 replica.cpp:712] Persisted action at 0
I0128 12:20:27.607244 297226240 replica.cpp:691] Replica received learned notice for position 0 from @0.0.0.0:0
I0128 12:20:27.607409 297226240 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 152us
I0128 12:20:27.607441 297226240 replica.cpp:712] Persisted action at 0
I0128 12:20:27.607457 297226240 replica.cpp:697] Replica learned NOP action at position 0
I0128 12:20:27.607853 297226240 log.cpp:675] Writer started with ending position 0
I0128 12:20:27.608649 299372544 leveldb.cpp:436] Reading position from leveldb took 158us
I0128 12:20:27.609539 298835968 registrar.cpp:340] Successfully fetched the registry (0B) in 7.426816ms
I0128 12:20:27.609763 298835968 registrar.cpp:439] Applied 1 operations in 54us; attempting to update the 'registry'
I0128 12:20:27.610216 300982272 log.cpp:683] Attempting to append 186 bytes to the log
I0128 12:20:27.610297 298835968 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 1
I0128 12:20:27.611016 299909120 replica.cpp:537] Replica received write request for position 1 from (18280)@192.168.178.24:51278
I0128 12:20:27.611188 299909120 leveldb.cpp:341] Persisting action (205 bytes) to leveldb took 153us
I0128 12:20:27.611222 299909120 replica.cpp:712] Persisted action at 1
I0128 12:20:27.611843 299909120 replica.cpp:691] Replica received learned notice for position 1 from @0.0.0.0:0
I0128 12:20:27.612004 299909120 leveldb.cpp:341] Persisting action (207 bytes) to leveldb took 147us
I0128 12:20:27.612035 299909120 replica.cpp:712] Persisted action at 1
I0128 12:20:27.612052 299909120 replica.cpp:697] Replica learned APPEND action at position 1
I0128 12:20:27.612742 300982272 registrar.cpp:484] Successfully updated the 'registry' in 2.924032ms
I0128 12:20:27.612846 300982272 registrar.cpp:370] Successfully recovered registrar
I0128 12:20:27.612936 298835968 log.cpp:702] Attempting to truncate the log to 1
I0128 12:20:27.613005 297762816 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 2
I0128 12:20:27.613323 298299392 master.cpp:1520] Recovered 0 slaves from the Registry (147B) ; allowing 10mins for slaves to re-register
I0128 12:20:27.613364 298835968 hierarchical.cpp:171] Skipping recovery of hierarchical allocator: nothing to recover
I0128 12:20:27.613966 300445696 replica.cpp:537] Replica received write request for position 2 from (18281)@192.168.178.24:51278
I0128 12:20:27.614131 300445696 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 151us
I0128 12:20:27.614166 300445696 replica.cpp:712] Persisted action at 2
I0128 12:20:27.614660 299372544 replica.cpp:691] Replica received learned notice for position 2 from @0.0.0.0:0
I0128 12:20:27.614828 299372544 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 158us
I0128 12:20:27.614876 299372544 leveldb.cpp:399] Deleting ~1 keys from leveldb took 28us
I0128 12:20:27.614898 299372544 replica.cpp:712] Persisted action at 2
I0128 12:20:27.614915 299372544 replica.cpp:697] Replica learned TRUNCATE action at position 2
I0128 12:20:27.625591 2080858880 containerizer.cpp:143] Using isolation: posix/cpu,posix/mem,filesystem/posix
I0128 12:20:27.629758 298299392 slave.cpp:192] Slave started on 871)@192.168.178.24:51278
I0128 12:20:27.629791 298299392 slave.cpp:193] Flags at startup: --appc_store_dir=""/tmp/mesos/store/appc"" --authenticatee=""crammd5"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --credential=""/tmp/MasterQuotaTest_AvailableResourcesAfterRescinding_gS9Qcf/credential"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_auth_server=""https://auth.docker.io"" --docker_kill_orphans=""true"" --docker_puller_timeout=""60"" --docker_registry=""https://registry-1.docker.io"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/tmp/mesos/store/docker"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/tmp/MasterQuotaTest_AvailableResourcesAfterRescinding_gS9Qcf/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --hostname_lookup=""true"" --image_provisioner_backend=""copy"" --initialize_driver_logging=""true"" --isolation=""posix/cpu,posix/mem"" --launcher_dir=""/Users/alex/Projects/mesos/build/default/src"" --logbufsecs=""0"" --logging_level=""INFO"" --oversubscribed_resources_interval=""15secs"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""10ms"" --resources=""cpus:2;mem:1024;disk:1024;ports:[31000-32000]"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --version=""false"" --work_dir=""/tmp/MasterQuotaTest_AvailableResourcesAfterRescinding_gS9Qcf""
I0128 12:20:27.630067 298299392 credentials.hpp:83] Loading credential for authentication from '/tmp/MasterQuotaTest_AvailableResourcesAfterRescinding_gS9Qcf/credential'
I0128 12:20:27.630223 298299392 slave.cpp:323] Slave using credential for: test-principal
I0128 12:20:27.630360 298299392 resources.cpp:564] Parsing resources as JSON failed: cpus:2;mem:1024;disk:1024;ports:[31000-32000]
Trying semicolon-delimited string format instead
I0128 12:20:27.630818 298299392 slave.cpp:463] Slave resources: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I0128 12:20:27.630869 298299392 slave.cpp:471] Slave attributes: [  ]
I0128 12:20:27.630882 298299392 slave.cpp:476] Slave hostname: alexr.fritz.box
I0128 12:20:27.631352 300982272 state.cpp:58] Recovering state from '/tmp/MasterQuotaTest_AvailableResourcesAfterRescinding_gS9Qcf/meta'
I0128 12:20:27.631515 299909120 status_update_manager.cpp:200] Recovering status update manager
I0128 12:20:27.631702 298835968 containerizer.cpp:390] Recovering containerizer
I0128 12:20:27.632589 297226240 provisioner.cpp:245] Provisioner recovery complete
I0128 12:20:27.632807 298835968 slave.cpp:4495] Finished recovery
I0128 12:20:27.633539 298835968 slave.cpp:4667] Querying resource estimator for oversubscribable resources
I0128 12:20:27.633752 300445696 status_update_manager.cpp:174] Pausing sending status updates
I0128 12:20:27.633754 298835968 slave.cpp:795] New master detected at master@192.168.178.24:51278
I0128 12:20:27.633806 298835968 slave.cpp:858] Authenticating with master master@192.168.178.24:51278
I0128 12:20:27.633824 298835968 slave.cpp:863] Using default CRAM-MD5 authenticatee
I0128 12:20:27.633903 298835968 slave.cpp:831] Detecting new master
I0128 12:20:27.633913 299372544 authenticatee.cpp:121] Creating new client SASL connection
I0128 12:20:27.634016 298835968 slave.cpp:4681] Received oversubscribable resources  from the resource estimator
I0128 12:20:27.634076 297226240 master.cpp:5521] Authenticating slave(871)@192.168.178.24:51278
I0128 12:20:27.634130 299372544 authenticator.cpp:413] Starting authentication session for crammd5_authenticatee(1741)@192.168.178.24:51278
I0128 12:20:27.634255 297226240 authenticator.cpp:98] Creating new server SASL connection
I0128 12:20:27.634348 300982272 authenticatee.cpp:212] Received SASL authentication mechanisms: CRAM-MD5
I0128 12:20:27.634367 300982272 authenticatee.cpp:238] Attempting to authenticate with mechanism 'CRAM-MD5'
I0128 12:20:27.634454 298835968 authenticator.cpp:203] Received SASL authentication start
I0128 12:20:27.634515 298835968 authenticator.cpp:325] Authentication requires more steps
I0128 12:20:27.634572 298835968 authenticatee.cpp:258] Received SASL authentication step
I0128 12:20:27.634706 297226240 authenticator.cpp:231] Received SASL authentication step
I0128 12:20:27.634757 297226240 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: 'alexr.fritz.box' server FQDN: 'alexr.fritz.box' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0128 12:20:27.634771 297226240 auxprop.cpp:179] Looking up auxiliary property '*userPassword'
I0128 12:20:27.634793 297226240 auxprop.cpp:179] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0128 12:20:27.634809 297226240 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: 'alexr.fritz.box' server FQDN: 'alexr.fritz.box' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0128 12:20:27.634819 297226240 auxprop.cpp:129] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0128 12:20:27.634827 297226240 auxprop.cpp:129] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0128 12:20:27.634893 297226240 authenticator.cpp:317] Authentication success
I0128 12:20:27.634958 298835968 authenticatee.cpp:298] Authentication success
I0128 12:20:27.635030 298299392 master.cpp:5551] Successfully authenticated principal 'test-principal' at slave(871)@192.168.178.24:51278
I0128 12:20:27.635079 300445696 authenticator.cpp:431] Authentication session cleanup for crammd5_authenticatee(1741)@192.168.178.24:51278
I0128 12:20:27.635195 299372544 slave.cpp:926] Successfully authenticated with master master@192.168.178.24:51278
I0128 12:20:27.635273 299372544 slave.cpp:1320] Will retry registration in 5.823453ms if necessary
I0128 12:20:27.635365 299909120 master.cpp:4235] Registering slave at slave(871)@192.168.178.24:51278 (alexr.fritz.box) with id 531344bd-56f4-4e4f-8f6f-a6a9d36058c7-S0
I0128 12:20:27.635542 297762816 registrar.cpp:439] Applied 1 operations in 41us; attempting to update the 'registry'
I0128 12:20:27.635889 299372544 log.cpp:683] Attempting to append 358 bytes to the log
I0128 12:20:27.636011 298299392 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 3
I0128 12:20:27.636693 300982272 replica.cpp:537] Replica received write request for position 3 from (18295)@192.168.178.24:51278
I0128 12:20:27.636860 300982272 leveldb.cpp:341] Persisting action (377 bytes) to leveldb took 139us
I0128 12:20:27.636885 300982272 replica.cpp:712] Persisted action at 3
I0128 12:20:27.637380 299909120 replica.cpp:691] Replica received learned notice for position 3 from @0.0.0.0:0
I0128 12:20:27.637547 299909120 leveldb.cpp:341] Persisting action (379 bytes) to leveldb took 132us
I0128 12:20:27.637573 299909120 replica.cpp:712] Persisted action at 3
I0128 12:20:27.637589 299909120 replica.cpp:697] Replica learned APPEND action at position 3
I0128 12:20:27.638362 298835968 registrar.cpp:484] Successfully updated the 'registry' in 2.77504ms
I0128 12:20:27.638589 300445696 log.cpp:702] Attempting to truncate the log to 3
I0128 12:20:27.638684 298299392 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 4
I0128 12:20:27.638825 300445696 slave.cpp:3435] Received ping from slave-observer(871)@192.168.178.24:51278
I0128 12:20:27.639081 300982272 hierarchical.cpp:473] Added slave 531344bd-56f4-4e4f-8f6f-a6a9d36058c7-S0 (alexr.fritz.box) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (allocated: )
I0128 12:20:27.639117 299909120 master.cpp:4303] Registered slave 531344bd-56f4-4e4f-8f6f-a6a9d36058c7-S0 at slave(871)@192.168.178.24:51278 (alexr.fritz.box) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I0128 12:20:27.639165 300982272 hierarchical.cpp:1403] No resources available to allocate!
I0128 12:20:27.639168 297226240 slave.cpp:970] Registered with master master@192.168.178.24:51278; given slave ID 531344bd-56f4-4e4f-8f6f-a6a9d36058c7-S0
I0128 12:20:27.639189 297226240 fetcher.cpp:81] Clearing fetcher cache
I0128 12:20:27.639183 300982272 hierarchical.cpp:1116] Performed allocation for slave 531344bd-56f4-4e4f-8f6f-a6a9d36058c7-S0 in 77us
I0128 12:20:27.639348 297762816 status_update_manager.cpp:181] Resuming sending status updates
I0128 12:20:27.639519 298835968 replica.cpp:537] Replica received write request for position 4 from (18296)@192.168.178.24:51278
I0128 12:20:27.639678 298835968 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 142us
I0128 12:20:27.639708 298835968 replica.cpp:712] Persisted action at 4
I0128 12:20:27.640115 300982272 replica.cpp:691] Replica received learned notice for position 4 from @0.0.0.0:0
I0128 12:20:27.640276 300982272 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 137us
I0128 12:20:27.640312 300982272 leveldb.cpp:399] Deleting ~2 keys from leveldb took 21us
I0128 12:20:27.640326 300982272 replica.cpp:712] Persisted action at 4
I0128 12:20:27.640336 300982272 replica.cpp:697] Replica learned TRUNCATE action at position 4
I0128 12:20:27.642145 297226240 slave.cpp:993] Checkpointing SlaveInfo to '/tmp/MasterQuotaTest_AvailableResourcesAfterRescinding_gS9Qcf/meta/slaves/531344bd-56f4-4e4f-8f6f-a6a9d36058c7-S0/slave.info'
I0128 12:20:27.643354 297226240 slave.cpp:1029] Forwarding total oversubscribed resources 
I0128 12:20:27.643458 300445696 master.cpp:4644] Received update of slave 531344bd-56f4-4e4f-8f6f-a6a9d36058c7-S0 at slave(871)@192.168.178.24:51278 (alexr.fritz.box) with total oversubscribed resources 
I0128 12:20:27.643710 298299392 hierarchical.cpp:531] Slave 531344bd-56f4-4e4f-8f6f-a6a9d36058c7-S0 (alexr.fritz.box) updated with oversubscribed resources  (total: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000], allocated: )
I0128 12:20:27.643769 298299392 hierarchical.cpp:1403] No resources available to allocate!
I0128 12:20:27.643805 298299392 hierarchical.cpp:1116] Performed allocation for slave 531344bd-56f4-4e4f-8f6f-a6a9d36058c7-S0 in 78us
I0128 12:20:27.644645 2080858880 containerizer.cpp:143] Using isolation: posix/cpu,posix/mem,filesystem/posix
I0128 12:20:27.649093 297226240 slave.cpp:192] Slave started on 872)@192.168.178.24:51278
I0128 12:20:27.649138 297226240 slave.cpp:193] Flags at startup: --appc_store_dir=""/tmp/mesos/store/appc"" --authenticatee=""crammd5"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --credential=""/tmp/MasterQuotaTest_AvailableResourcesAfterRescinding_6ycfWv/credential"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_auth_server=""https://auth.docker.io"" --docker_kill_orphans=""true"" --docker_puller_timeout=""60"" --docker_registry=""https://registry-1.docker.io"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/tmp/mesos/store/docker"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/tmp/MasterQuotaTest_AvailableResourcesAfterRescinding_6ycfWv/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --hostname_lookup=""true"" --image_provisioner_backend=""copy"" --initialize_driver_logging=""true"" --isolation=""posix/cpu,posix/mem"" --launcher_dir=""/Users/alex/Projects/mesos/build/default/src"" --logbufsecs=""0"" --logging_level=""INFO"" --oversubscribed_resources_interval=""15secs"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""10ms"" --resources=""cpus:2;mem:1024;disk:1024;ports:[31000-32000]"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --version=""false"" --work_dir=""/tmp/MasterQuotaTest_AvailableResourcesAfterRescinding_6ycfWv""
I0128 12:20:27.649353 297226240 credentials.hpp:83] Loading credential for authentication from '/tmp/MasterQuotaTest_AvailableResourcesAfterRescinding_6ycfWv/credential'
I0128 12:20:27.649451 297226240 slave.cpp:323] Slave using credential for: test-principal
I0128 12:20:27.649569 297226240 resources.cpp:564] Parsing resources as JSON failed: cpus:2;mem:1024;disk:1024;ports:[31000-32000]
Trying semicolon-delimited string format instead
I0128 12:20:27.650039 297226240 slave.cpp:463] Slave resources: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I0128 12:20:27.650085 297226240 slave.cpp:471] Slave attributes: [  ]
I0128 12:20:27.650096 297226240 slave.cpp:476] Slave hostname: alexr.fritz.box
I0128 12:20:27.650509 299909120 state.cpp:58] Recovering state from '/tmp/MasterQuotaTest_AvailableResourcesAfterRescinding_6ycfWv/meta'
I0128 12:20:27.650699 298299392 status_update_manager.cpp:200] Recovering status update manager
I0128 12:20:27.650701 300445696 hierarchical.cpp:1403] No resources available to allocate!
I0128 12:20:27.650738 300445696 hierarchical.cpp:1096] Performed allocation for 1 slaves in 101us
I0128 12:20:27.650887 297226240 containerizer.cpp:390] Recovering containerizer
I0128 12:20:27.651747 299909120 provisioner.cpp:245] Provisioner recovery complete
I0128 12:20:27.651974 300982272 slave.cpp:4495] Finished recovery
I0128 12:20:27.653733 300982272 slave.cpp:4667] Querying resource estimator for oversubscribable resources
I0128 12:20:27.653928 300982272 slave.cpp:795] New master detected at master@192.168.178.24:51278
I0128 12:20:27.653928 299372544 status_update_manager.cpp:174] Pausing sending status updates
I0128 12:20:27.653975 300982272 slave.cpp:858] Authenticating with master master@192.168.178.24:51278
I0128 12:20:27.653991 300982272 slave.cpp:863] Using default CRAM-MD5 authenticatee
I0128 12:20:27.654091 300982272 slave.cpp:831] Detecting new master
I0128 12:20:27.654098 297226240 authenticatee.cpp:121] Creating new client SASL connection
I0128 12:20:27.654216 300982272 slave.cpp:4681] Received oversubscribable resources  from the resource estimator
I0128 12:20:27.654276 297762816 master.cpp:5521] Authenticating slave(872)@192.168.178.24:51278
I0128 12:20:27.654350 299909120 authenticator.cpp:413] Starting authentication session for crammd5_authenticatee(1742)@192.168.178.24:51278
I0128 12:20:27.654498 298299392 authenticator.cpp:98] Creating new server SASL connection
I0128 12:20:27.654602 300982272 authenticatee.cpp:212] Received SASL authentication mechanisms: CRAM-MD5
I0128 12:20:27.654625 300982272 authenticatee.cpp:238] Attempting to authenticate with mechanism 'CRAM-MD5'
I0128 12:20:27.654700 299909120 authenticator.cpp:203] Received SASL authentication start
I0128 12:20:27.654752 299909120 authenticator.cpp:325] Authentication requires more steps
I0128 12:20:27.654819 299909120 authenticatee.cpp:258] Received SASL authentication step
I0128 12:20:27.654940 299372544 authenticator.cpp:231] Received SASL authentication step
I0128 12:20:27.654965 299372544 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: 'alexr.fritz.box' server FQDN: 'alexr.fritz.box' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0128 12:20:27.654978 299372544 auxprop.cpp:179] Looking up auxiliary property '*userPassword'
I0128 12:20:27.654997 299372544 auxprop.cpp:179] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0128 12:20:27.655012 299372544 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: 'alexr.fritz.box' server FQDN: 'alexr.fritz.box' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0128 12:20:27.655024 299372544 auxprop.cpp:129] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0128 12:20:27.655031 299372544 auxprop.cpp:129] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0128 12:20:27.655047 299372544 authenticator.cpp:317] Authentication success
I0128 12:20:27.655143 299909120 authenticatee.cpp:298] Authentication success
I0128 12:20:27.655120 297762816 master.cpp:5551] Successfully authenticated principal 'test-principal' at slave(872)@192.168.178.24:51278
I0128 12:20:27.655163 299372544 authenticator.cpp:431] Authentication session cleanup for crammd5_authenticatee(1742)@192.168.178.24:51278
I0128 12:20:27.655326 300445696 slave.cpp:926] Successfully authenticated with master master@192.168.178.24:51278
I0128 12:20:27.655465 300445696 slave.cpp:1320] Will retry registration in 13.985296ms if necessary
I0128 12:20:27.655565 299909120 master.cpp:4235] Registering slave at slave(872)@192.168.178.24:51278 (alexr.fritz.box) with id 531344bd-56f4-4e4f-8f6f-a6a9d36058c7-S1
I0128 12:20:27.655823 300982272 registrar.cpp:439] Applied 1 operations in 64us; attempting to update the 'registry'
I0128 12:20:27.656354 297226240 log.cpp:683] Attempting to append 527 bytes to the log
I0128 12:20:27.656429 300445696 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 5
I0128 12:20:27.657187 300445696 replica.cpp:537] Replica received write request for position 5 from (18310)@192.168.178.24:51278
I0128 12:20:27.657429 300445696 leveldb.cpp:341] Persisting action (546 bytes) to leveldb took 224us
I0128 12:20:27.657464 300445696 replica.cpp:712] Persisted action at 5
I0128 12:20:27.658007 300445696 replica.cpp:691] Replica received learned notice for position 5 from @0.0.0.0:0
I0128 12:20:27.658190 300445696 leveldb.cpp:341] Persisting action (548 bytes) to leveldb took 170us
I0128 12:20:27.658223 300445696 replica.cpp:712] Persisted action at 5
I0128 12:20:27.658239 300445696 replica.cpp:697] Replica learned APPEND action at position 5
I0128 12:20:27.659104 300982272 registrar.cpp:484] Successfully updated the 'registry' in 3.227904ms
I0128 12:20:27.659373 298835968 log.cpp:702] Attempting to truncate the log to 5
I0128 12:20:27.659446 298299392 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 6
I0128 12:20:27.659636 300982272 slave.cpp:3435] Received ping from slave-observer(872)@192.168.178.24:51278
I0128 12:20:27.659855 297226240 hierarchical.cpp:473] Added slave 531344bd-56f4-4e4f-8f6f-a6a9d36058c7-S1 (alexr.fritz.box) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (allocated: )
I0128 12:20:27.659960 297226240 hierarchical.cpp:1403] No resources available to allocate!
I0128 12:20:27.659936 297762816 master.cpp:4303] Registered slave 531344bd-56f4-4e4f-8f6f-a6a9d36058c7-S1 at slave(872)@192.168.178.24:51278 (alexr.fritz.box) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I0128 12:20:27.659981 297226240 hierarchical.cpp:1116] Performed allocation for slave 531344bd-56f4-4e4f-8f6f-a6a9d36058c7-S1 in 80us
I0128 12:20:27.659986 299909120 slave.cpp:970] Registered with master master@192.168.178.24:51278; given slave ID 531344bd-56f4-4e4f-8f6f-a6a9d36058c7-S1
I0128 12:20:27.660013 299909120 fetcher.cpp:81] Clearing fetcher cache
I0128 12:20:27.660092 297226240 replica.cpp:537] Replica received write request for position 6 from (18311)@192.168.178.24:51278
I0128 12:20:27.660246 297226240 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 131us
I0128 12:20:27.660270 297226240 replica.cpp:712] Persisted action at 6
I0128 12:20:27.660454 300445696 status_update_manager.cpp:181] Resuming sending status updates
I0128 12:20:27.660742 299372544 replica.cpp:691] Replica received learned notice for position 6 from @0.0.0.0:0
I0128 12:20:27.660924 299372544 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 209us
I0128 12:20:27.661015 299372544 leveldb.cpp:399] Deleting ~2 keys from leveldb took 37us
I0128 12:20:27.661039 299372544 replica.cpp:712] Persisted action at 6
I0128 12:20:27.661061 299372544 replica.cpp:697] Replica learned TRUNCATE action at position 6
I0128 12:20:27.661752 299909120 slave.cpp:993] Checkpointing SlaveInfo to '/tmp/MasterQuotaTest_AvailableResourcesAfterRescinding_6ycfWv/meta/slaves/531344bd-56f4-4e4f-8f6f-a6a9d36058c7-S1/slave.info'
I0128 12:20:27.662113 299909120 slave.cpp:1029] Forwarding total oversubscribed resources 
I0128 12:20:27.662199 297762816 master.cpp:4644] Received update of slave 531344bd-56f4-4e4f-8f6f-a6a9d36058c7-S1 at slave(872)@192.168.178.24:51278 (alexr.fritz.box) with total oversubscribed resources 
I0128 12:20:27.662508 297762816 hierarchical.cpp:531] Slave 531344bd-56f4-4e4f-8f6f-a6a9d36058c7-S1 (alexr.fritz.box) updated with oversubscribed resources  (total: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000], allocated: )
I0128 12:20:27.662577 297762816 hierarchical.cpp:1403] No resources available to allocate!
I0128 12:20:27.662590 297762816 hierarchical.cpp:1116] Performed allocation for slave 531344bd-56f4-4e4f-8f6f-a6a9d36058c7-S1 in 51us
I0128 12:20:27.663261 2080858880 containerizer.cpp:143] Using isolation: posix/cpu,posix/mem,filesystem/posix
I0128 12:20:27.669075 299372544 slave.cpp:192] Slave started on 873)@192.168.178.24:51278
I0128 12:20:27.669107 299372544 slave.cpp:193] Flags at startup: --appc_store_dir=""/tmp/mesos/store/appc"" --authenticatee=""crammd5"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --credential=""/tmp/MasterQuotaTest_AvailableResourcesAfterRescinding_eAr35P/credential"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_auth_server=""https://auth.docker.io"" --docker_kill_orphans=""true"" --docker_puller_timeout=""60"" --docker_registry=""https://registry-1.docker.io"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/tmp/mesos/store/docker"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/tmp/MasterQuotaTest_AvailableResourcesAfterRescinding_eAr35P/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --hostname_lookup=""true"" --image_provisioner_backend=""copy"" --initialize_driver_logging=""true"" --isolation=""posix/cpu,posix/mem"" --launcher_dir=""/Users/alex/Projects/mesos/build/default/src"" --logbufsecs=""0"" --logging_level=""INFO"" --oversubscribed_resources_interval=""15secs"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""10ms"" --resources=""cpus:2;mem:1024;disk:1024;ports:[31000-32000]"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --version=""false"" --work_dir=""/tmp/MasterQuotaTest_AvailableResourcesAfterRescinding_eAr35P""
I0128 12:20:27.669395 299372544 credentials.hpp:83] Loading credential for authentication from '/tmp/MasterQuotaTest_AvailableResourcesAfterRescinding_eAr35P/credential'
I0128 12:20:27.669497 299372544 slave.cpp:323] Slave using credential for: test-principal
I0128 12:20:27.669631 299372544 resources.cpp:564] Parsing resources as JSON failed: cpus:2;mem:1024;disk:1024;ports:[31000-32000]
Trying semicolon-delimited string format instead
I0128 12:20:27.670105 299372544 slave.cpp:463] Slave resources: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I0128 12:20:27.670146 299372544 slave.cpp:471] Slave attributes: [  ]
I0128 12:20:27.670155 299372544 slave.cpp:476] Slave hostname: alexr.fritz.box
I0128 12:20:27.670567 298835968 state.cpp:58] Recovering state from '/tmp/MasterQuotaTest_AvailableResourcesAfterRescinding_eAr35P/meta'
I0128 12:20:27.670752 300445696 status_update_manager.cpp:200] Recovering status update manager
I0128 12:20:27.670913 300445696 containerizer.cpp:390] Recovering containerizer
I0128 12:20:27.671908 297226240 provisioner.cpp:245] Provisioner recovery complete
I0128 12:20:27.672124 298835968 slave.cpp:4495] Finished recovery
I0128 12:20:27.673331 298835968 slave.cpp:4667] Querying resource estimator for oversubscribable resources
I0128 12:20:27.673528 297762816 status_update_manager.cpp:174] Pausing sending status updates
I0128 12:20:27.673527 298835968 slave.cpp:795] New master detected at master@192.168.178.24:51278
I0128 12:20:27.673573 298835968 slave.cpp:858] Authenticating with master master@192.168.178.24:51278
I0128 12:20:27.673599 298835968 slave.cpp:863] Using default CRAM-MD5 authenticatee
I0128 12:20:27.673671 298835968 slave.cpp:831] Detecting new master
I0128 12:20:27.673686 298299392 authenticatee.cpp:121] Creating new client SASL connection
I0128 12:20:27.673797 298835968 slave.cpp:4681] Received oversubscribable resources  from the resource estimator
I0128 12:20:27.673849 300982272 master.cpp:5521] Authenticating slave(873)@192.168.178.24:51278
I0128 12:20:27.673909 300445696 authenticator.cpp:413] Starting authentication session for crammd5_authenticatee(1743)@192.168.178.24:51278
I0128 12:20:27.674026 297226240 authenticator.cpp:98] Creating new server SASL connection
I0128 12:20:27.674127 299909120 authenticatee.cpp:212] Received SASL authentication mechanisms: CRAM-MD5
I0128 12:20:27.674154 299909120 authenticatee.cpp:238] Attempting to authenticate with mechanism 'CRAM-MD5'
I0128 12:20:27.674247 297762816 authenticator.cpp:203] Received SASL authentication start
I0128 12:20:27.674293 297762816 authenticator.cpp:325] Authentication requires more steps
I0128 12:20:27.674357 299909120 authenticatee.cpp:258] Received SASL authentication step
I0128 12:20:27.674450 299909120 authenticator.cpp:231] Received SASL authentication step
I0128 12:20:27.674471 299909120 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: 'alexr.fritz.box' server FQDN: 'alexr.fritz.box' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0128 12:20:27.674484 299909120 auxprop.cpp:179] Looking up auxiliary property '*userPassword'
I0128 12:20:27.674505 299909120 auxprop.cpp:179] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0128 12:20:27.674522 299909120 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: 'alexr.fritz.box' server FQDN: 'alexr.fritz.box' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0128 12:20:27.674535 299909120 auxprop.cpp:129] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0128 12:20:27.674545 299909120 auxprop.cpp:129] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0128 12:20:27.674558 299909120 authenticator.cpp:317] Authentication success
I0128 12:20:27.674621 297762816 authenticatee.cpp:298] Authentication success
I0128 12:20:27.674667 299372544 master.cpp:5551] Successfully authenticated principal 'test-principal' at slave(873)@192.168.178.24:51278
I0128 12:20:27.674734 298835968 authenticator.cpp:431] Authentication session cleanup for crammd5_authenticatee(1743)@192.168.178.24:51278
I0128 12:20:27.674832 298299392 slave.cpp:926] Successfully authenticated with master master@192.168.178.24:51278
I0128 12:20:27.674908 298299392 slave.cpp:1320] Will retry registration in 13.363505ms if necessary
I0128 12:20:27.674993 297226240 master.cpp:4235] Registering slave at slave(873)@192.168.178.24:51278 (alexr.fritz.box) with id 531344bd-56f4-4e4f-8f6f-a6a9d36058c7-S2
I0128 12:20:27.675194 299372544 registrar.cpp:439] Applied 1 operations in 52us; attempting to update the 'registry'
I0128 12:20:27.675604 300445696 log.cpp:683] Attempting to append 696 bytes to the log
I0128 12:20:27.675693 297762816 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 7
I0128 12:20:27.676316 297226240 replica.cpp:537] Replica received write request for position 7 from (18325)@192.168.178.24:51278
I0128 12:20:27.676472 297226240 leveldb.cpp:341] Persisting action (715 bytes) to leveldb took 146us
I0128 12:20:27.676506 297226240 replica.cpp:712] Persisted action at 7
I0128 12:20:27.677014 300982272 replica.cpp:691] Replica received learned notice for position 7 from @0.0.0.0:0
I0128 12:20:27.677176 300982272 leveldb.cpp:341] Persisting action (717 bytes) to leveldb took 138us
I0128 12:20:27.677201 300982272 replica.cpp:712] Persisted action at 7
I0128 12:20:27.677211 300982272 replica.cpp:697] Replica learned APPEND action at position 7
I0128 12:20:27.678407 299909120 registrar.cpp:484] Successfully updated the 'registry' in 3.181056ms
I0128 12:20:27.678652 300982272 log.cpp:702] Attempting to truncate the log to 7
I0128 12:20:27.678741 297762816 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 8
I0128 12:20:27.678907 300445696 slave.cpp:3435] Received ping from slave-observer(873)@192.168.178.24:51278
I0128 12:20:27.679098 297762816 hierarchical.cpp:473] Added slave 531344bd-56f4-4e4f-8f6f-a6a9d36058c7-S2 (alexr.fritz.box) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (allocated: )
I0128 12:20:27.679177 297762816 hierarchical.cpp:1403] No resources available to allocate!
I0128 12:20:27.679195 297762816 hierarchical.cpp:1116] Performed allocation for slave 531344bd-56f4-4e4f-8f6f-a6a9d36058c7-S2 in 73us
I0128 12:20:27.679214 299372544 slave.cpp:970] Registered with master master@192.168.178.24:51278; given slave ID 531344bd-56f4-4e4f-8f6f-a6a9d36058c7-S2
I0128 12:20:27.679186 298299392 master.cpp:4303] Registered slave 531344bd-56f4-4e4f-8f6f-a6a9d36058c7-S2 at slave(873)@192.168.178.24:51278 (alexr.fritz.box) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I0128 12:20:27.679239 299372544 fetcher.cpp:81] Clearing fetcher cache
I0128 12:20:27.679404 298835968 status_update_manager.cpp:181] Resuming sending status updates
I0128 12:20:27.679461 300445696 replica.cpp:537] Replica received write request for position 8 from (18326)@192.168.178.24:51278
I0128 12:20:27.679652 300445696 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 182us
I0128 12:20:27.679687 300445696 replica.cpp:712] Persisted action at 8
I0128 12:20:27.679913 299372544 slave.cpp:993] Checkpointing SlaveInfo to '/tmp/MasterQuotaTest_AvailableResourcesAfterRescinding_eAr35P/meta/slaves/531344bd-56f4-4e4f-8f6f-a6a9d36058c7-S2/slave.info'
I0128 12:20:27.680150 298835968 replica.cpp:691] Replica received learned notice for position 8 from @0.0.0.0:0
I0128 12:20:27.680279 299372544 slave.cpp:1029] Forwarding total oversubscribed resources 
I0128 12:20:27.680302 298835968 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 140us
I0128 12:20:27.680351 298835968 leveldb.cpp:399] Deleting ~2 keys from leveldb took 28us
I0128 12:20:27.680371 298835968 replica.cpp:712] Persisted action at 8
I0128 12:20:27.680377 299372544 master.cpp:4644] Received update of slave 531344bd-56f4-4e4f-8f6f-a6a9d36058c7-S2 at slave(873)@192.168.178.24:51278 (alexr.fritz.box) with total oversubscribed resources 
I0128 12:20:27.680387 298835968 replica.cpp:697] Replica learned TRUNCATE action at position 8
I0128 12:20:27.680680 299909120 hierarchical.cpp:531] Slave 531344bd-56f4-4e4f-8f6f-a6a9d36058c7-S2 (alexr.fritz.box) updated with oversubscribed resources  (total: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000], allocated: )
I0128 12:20:27.680749 299909120 hierarchical.cpp:1403] No resources available to allocate!
I0128 12:20:27.680768 299909120 hierarchical.cpp:1116] Performed allocation for slave 531344bd-56f4-4e4f-8f6f-a6a9d36058c7-S2 in 60us
I0128 12:20:27.682180 2080858880 sched.cpp:222] Version: 0.28.0
I0128 12:20:27.682505 298299392 sched.cpp:326] New master detected at master@192.168.178.24:51278
I0128 12:20:27.682551 298299392 sched.cpp:382] Authenticating with master master@192.168.178.24:51278
I0128 12:20:27.682566 298299392 sched.cpp:389] Using default CRAM-MD5 authenticatee
I0128 12:20:27.682714 300982272 authenticatee.cpp:121] Creating new client SASL connection
I0128 12:20:27.682870 300445696 master.cpp:5521] Authenticating scheduler-05b9ac89-54ac-4d24-84e7-bb9cedfa77c4@192.168.178.24:51278
I0128 12:20:27.682965 298835968 authenticator.cpp:413] Starting authentication session for crammd5_authenticatee(1744)@192.168.178.24:51278
I0128 12:20:27.683362 297762816 authenticator.cpp:98] Creating new server SASL connection
I0128 12:20:27.683498 297226240 authenticatee.cpp:212] Received SASL authentication mechanisms: CRAM-MD5
I0128 12:20:27.683526 297226240 authenticatee.cpp:238] Attempting to authenticate with mechanism 'CRAM-MD5'
I0128 12:20:27.683636 298299392 authenticator.cpp:203] Received SASL authentication start
I0128 12:20:27.683687 298299392 authenticator.cpp:325] Authentication requires more steps
I0128 12:20:27.683758 297226240 authenticatee.cpp:258] Received SASL authentication step
I0128 12:20:27.683857 297226240 authenticator.cpp:231] Received SASL authentication step
I0128 12:20:27.683877 297226240 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: 'alexr.fritz.box' server FQDN: 'alexr.fritz.box' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0128 12:20:27.683897 297226240 auxprop.cpp:179] Looking up auxiliary property '*userPassword'
I0128 12:20:27.683917 297226240 auxprop.cpp:179] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0128 12:20:27.683930 297226240 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: 'alexr.fritz.box' server FQDN: 'alexr.fritz.box' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0128 12:20:27.683940 297226240 auxprop.cpp:129] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0128 12:20:27.683948 297226240 auxprop.cpp:129] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0128 12:20:27.683962 297226240 authenticator.cpp:317] Authentication success
I0128 12:20:27.684010 299909120 authenticatee.cpp:298] Authentication success
I0128 12:20:27.684079 300445696 master.cpp:5551] Successfully authenticated principal 'test-principal' at scheduler-05b9ac89-54ac-4d24-84e7-bb9cedfa77c4@192.168.178.24:51278
I0128 12:20:27.684172 300982272 authenticator.cpp:431] Authentication session cleanup for crammd5_authenticatee(1744)@192.168.178.24:51278
I0128 12:20:27.684288 298835968 sched.cpp:471] Successfully authenticated with master master@192.168.178.24:51278
I0128 12:20:27.684306 298835968 sched.cpp:780] Sending SUBSCRIBE call to master@192.168.178.24:51278
I0128 12:20:27.684378 298835968 sched.cpp:813] Will retry registration in 1.868624245secs if necessary
I0128 12:20:27.684437 299909120 master.cpp:2278] Received SUBSCRIBE call for framework 'framework(18327)' at scheduler-05b9ac89-54ac-4d24-84e7-bb9cedfa77c4@192.168.178.24:51278
W0128 12:20:27.684456 299909120 master.cpp:2285] Framework at scheduler-05b9ac89-54ac-4d24-84e7-bb9cedfa77c4@192.168.178.24:51278 (authenticated as 'test-principal') does not set 'principal' in FrameworkInfo
I0128 12:20:27.684471 299909120 master.cpp:1749] Authorizing framework principal '' to receive offers for role 'role1'
I0128 12:20:27.684675 300445696 master.cpp:2349] Subscribing framework framework(18327) with checkpointing disabled and capabilities [  ]
I0128 12:20:27.684921 297226240 hierarchical.cpp:265] Added framework framework(18327)
I0128 12:20:27.685066 299909120 sched.cpp:707] Framework registered with framework(18327)
I0128 12:20:27.685122 299909120 sched.cpp:721] Scheduler::registered took 48us
W0128 12:20:27.685184 299372544 slave.cpp:2236] Ignoring updating pid for framework framework(18327) because it does not exist
W0128 12:20:27.685223 299909120 slave.cpp:2236] Ignoring updating pid for framework framework(18327) because it does not exist
W0128 12:20:27.685281 297762816 slave.cpp:2236] Ignoring updating pid for framework framework(18327) because it does not exist
I0128 12:20:27.685915 297226240 hierarchical.cpp:1498] No inverse offers to send out!
I0128 12:20:27.685945 297226240 hierarchical.cpp:1096] Performed allocation for 3 slaves in 1015us
I0128 12:20:27.686295 299372544 master.cpp:5350] Sending 3 offers to framework framework(18327) (framework(18327)) at scheduler-05b9ac89-54ac-4d24-84e7-bb9cedfa77c4@192.168.178.24:51278
I0128 12:20:27.686750 298299392 sched.cpp:877] Scheduler::resourceOffers took 161us
I0128 12:20:27.688932 2080858880 sched.cpp:222] Version: 0.28.0
I0128 12:20:27.689265 298299392 sched.cpp:326] New master detected at master@192.168.178.24:51278
I0128 12:20:27.689319 298299392 sched.cpp:382] Authenticating with master master@192.168.178.24:51278
I0128 12:20:27.689333 298299392 sched.cpp:389] Using default CRAM-MD5 authenticatee
I0128 12:20:27.689450 300445696 authenticatee.cpp:121] Creating new client SASL connection
I0128 12:20:27.689577 300982272 master.cpp:5521] Authenticating scheduler-7953b576-d913-4dce-993d-375e2fb34aba@192.168.178.24:51278
I0128 12:20:27.689654 298835968 authenticator.cpp:413] Starting authentication session for crammd5_authenticatee(1745)@192.168.178.24:51278
I0128 12:20:27.689810 299909120 authenticator.cpp:98] Creating new server SASL connection
I0128 12:20:27.689904 300982272 authenticatee.cpp:212] Received SASL authentication mechanisms: CRAM-MD5
I0128 12:20:27.689931 300982272 authenticatee.cpp:238] Attempting to authenticate with mechanism 'CRAM-MD5'
I0128 12:20:27.690032 297762816 authenticator.cpp:203] Received SASL authentication start
I0128 12:20:27.690073 297762816 authenticator.cpp:325] Authentication requires more steps
I0128 12:20:27.690166 298299392 authenticatee.cpp:258] Received SASL authentication step
I0128 12:20:27.690271 298835968 authenticator.cpp:231] Received SASL authentication step
I0128 12:20:27.690291 298835968 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: 'alexr.fritz.box' server FQDN: 'alexr.fritz.box' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0128 12:20:27.690304 298835968 auxprop.cpp:179] Looking up auxiliary property '*userPassword'
I0128 12:20:27.690325 298835968 auxprop.cpp:179] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0128 12:20:27.690340 298835968 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: 'alexr.fritz.box' server FQDN: 'alexr.fritz.box' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0128 12:20:27.690351 298835968 auxprop.cpp:129] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0128 12:20:27.690382 298835968 auxprop.cpp:129] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0128 12:20:27.690393 298835968 authenticator.cpp:317] Authentication success
I0128 12:20:27.690402 2080858880 sched.cpp:222] Version: 0.28.0
I0128 12:20:27.690454 299909120 authenticatee.cpp:298] Authentication success
I0128 12:20:27.690512 299372544 master.cpp:5551] Successfully authenticated principal 'test-principal' at scheduler-7953b576-d913-4dce-993d-375e2fb34aba@192.168.178.24:51278
I0128 12:20:27.690639 300982272 authenticator.cpp:431] Authentication session cleanup for crammd5_authenticatee(1745)@192.168.178.24:51278
I0128 12:20:27.690727 299909120 sched.cpp:471] Successfully authenticated with master master@192.168.178.24:51278
I0128 12:20:27.690744 299909120 sched.cpp:780] Sending SUBSCRIBE call to master@192.168.178.24:51278
I0128 12:20:27.690832 300445696 sched.cpp:326] New master detected at master@192.168.178.24:51278
I0128 12:20:27.690878 299909120 sched.cpp:813] Will retry registration in 370.645806ms if necessary
I0128 12:20:27.690896 300445696 sched.cpp:382] Authenticating with master master@192.168.178.24:51278
I0128 12:20:27.690909 300445696 sched.cpp:389] Using default CRAM-MD5 authenticatee
I0128 12:20:27.690932 300982272 master.cpp:2278] Received SUBSCRIBE call for framework 'framework(18328)' at scheduler-7953b576-d913-4dce-993d-375e2fb34aba@192.168.178.24:51278
W0128 12:20:27.690953 300982272 master.cpp:2285] Framework at scheduler-7953b576-d913-4dce-993d-375e2fb34aba@192.168.178.24:51278 (authenticated as 'test-principal') does not set 'principal' in FrameworkInfo
I0128 12:20:27.690979 300982272 master.cpp:1749] Authorizing framework principal '' to receive offers for role 'role2'
I0128 12:20:27.691057 299372544 authenticatee.cpp:121] Creating new client SASL connection
I0128 12:20:27.691160 300982272 master.cpp:5521] Authenticating scheduler-1ed5257a-cda6-4cef-aaf1-87934636893e@192.168.178.24:51278
I0128 12:20:27.691257 298299392 authenticator.cpp:413] Starting authentication session for crammd5_authenticatee(1746)@192.168.178.24:51278
I0128 12:20:27.691284 300982272 master.cpp:2349] Subscribing framework framework(18328) with checkpointing disabled and capabilities [  ]
I0128 12:20:27.691396 297762816 authenticator.cpp:98] Creating new server SASL connection
I0128 12:20:27.691516 300445696 hierarchical.cpp:265] Added framework framework(18328)
I0128 12:20:27.691515 299372544 authenticatee.cpp:212] Received SASL authentication mechanisms: CRAM-MD5
I0128 12:20:27.691553 299372544 authenticatee.cpp:238] Attempting to authenticate with mechanism 'CRAM-MD5'
I0128 12:20:27.691562 298299392 sched.cpp:707] Framework registered with framework(18328)
I0128 12:20:27.691599 300445696 hierarchical.cpp:1403] No resources available to allocate!
W0128 12:20:27.691620 297226240 slave.cpp:2236] Ignoring updating pid for framework framework(18328) because it does not exist
I0128 12:20:27.691627 300445696 hierarchical.cpp:1498] No inverse offers to send out!
I0128 12:20:27.691629 298835968 authenticator.cpp:203] Received SASL authentication start
W0128 12:20:27.691661 299909120 slave.cpp:2236] Ignoring updating pid for framework framework(18328) because it does not exist
I0128 12:20:27.691656 300445696 hierarchical.cpp:1096] Performed allocation for 3 slaves in 128us
I0128 12:20:27.691653 298299392 sched.cpp:721] Scheduler::registered took 86us
I0128 12:20:27.691680 298835968 authenticator.cpp:325] Authentication requires more steps
W0128 12:20:27.691718 297762816 slave.cpp:2236] Ignoring updating pid for framework framework(18328) because it does not exist
I0128 12:20:27.691797 298299392 authenticatee.cpp:258] Received SASL authentication step
I0128 12:20:27.691896 299372544 authenticator.cpp:231] Received SASL authentication step
I0128 12:20:27.691917 299372544 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: 'alexr.fritz.box' server FQDN: 'alexr.fritz.box' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0128 12:20:27.691929 299372544 auxprop.cpp:179] Looking up auxiliary property '*userPassword'
I0128 12:20:27.691947 299372544 auxprop.cpp:179] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0128 12:20:27.691962 299372544 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: 'alexr.fritz.box' server FQDN: 'alexr.fritz.box' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0128 12:20:27.691973 299372544 auxprop.cpp:129] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0128 12:20:27.691982 299372544 auxprop.cpp:129] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0128 12:20:27.691993 299372544 authenticator.cpp:317] Authentication success
I0128 12:20:27.692075 299909120 authenticatee.cpp:298] Authentication success
I0128 12:20:27.692142 298299392 master.cpp:5551] Successfully authenticated principal 'test-principal' at scheduler-1ed5257a-cda6-4cef-aaf1-87934636893e@192.168.178.24:51278
I0128 12:20:27.692203 298835968 authenticator.cpp:431] Authentication session cleanup for crammd5_authenticatee(1746)@192.168.178.24:51278
I0128 12:20:27.692313 297762816 sched.cpp:471] Successfully authenticated with master master@192.168.178.24:51278
I0128 12:20:27.692330 297762816 sched.cpp:780] Sending SUBSCRIBE call to master@192.168.178.24:51278
I0128 12:20:27.692392 297762816 sched.cpp:813] Will retry registration in 284.450047ms if necessary
I0128 12:20:27.692456 299909120 master.cpp:2278] Received SUBSCRIBE call for framework 'framework(18329)' at scheduler-1ed5257a-cda6-4cef-aaf1-87934636893e@192.168.178.24:51278
W0128 12:20:27.692473 299909120 master.cpp:2285] Framework at scheduler-1ed5257a-cda6-4cef-aaf1-87934636893e@192.168.178.24:51278 (authenticated as 'test-principal') does not set 'principal' in FrameworkInfo
I0128 12:20:27.692486 299909120 master.cpp:1749] Authorizing framework principal '' to receive offers for role 'role2'
I0128 12:20:27.692632 297762816 master.cpp:2349] Subscribing framework framework(18329) with checkpointing disabled and capabilities [  ]
I0128 12:20:27.692858 298299392 hierarchical.cpp:265] Added framework framework(18329)
I0128 12:20:27.692924 299372544 sched.cpp:707] Framework registered with framework(18329)
W0128 12:20:27.692945 297226240 slave.cpp:2236] Ignoring updating pid for framework framework(18329) because it does not exist
I0128 12:20:27.692952 298299392 hierarchical.cpp:1403] No resources available to allocate!
W0128 12:20:27.692972 297762816 slave.cpp:2236] Ignoring updating pid for framework framework(18329) because it does not exist
I0128 12:20:27.692982 298299392 hierarchical.cpp:1498] No inverse offers to send out!
I0128 12:20:27.692986 299372544 sched.cpp:721] Scheduler::registered took 52us
W0128 12:20:27.693003 297226240 slave.cpp:2236] Ignoring updating pid for framework framework(18329) because it does not exist
I0128 12:20:27.693001 298299392 hierarchical.cpp:1096] Performed allocation for 3 slaves in 124us
I0128 12:20:27.693220 2080858880 resources.cpp:564] Parsing resources as JSON failed: cpus:1;mem:512
Trying semicolon-delimited string format instead
I0128 12:20:27.694814 297762816 process.cpp:3141] Handling HTTP event for process 'master' with path: '/master/quota'
I0128 12:20:27.695245 299372544 http.cpp:503] HTTP POST for /master/quota from 192.168.178.24:51641
I0128 12:20:27.695274 299372544 quota_handler.cpp:211] Setting quota from request: '{""force"":false,""guarantee"":[{""name"":""cpus"",""role"":""*"",""scalar"":{""value"":1.0},""type"":""SCALAR""},{""name"":""mem"",""role"":""*"",""scalar"":{""value"":512.0},""type"":""SCALAR""}],""role"":""role2""}'
I0128 12:20:27.695569 299372544 quota_handler.cpp:446] Authorizing principal 'test-principal' to request quota for role 'role2'
I0128 12:20:27.695719 299372544 quota_handler.cpp:70] Performing capacity heuristic check for a set quota request
I0128 12:20:27.695996 297226240 registrar.cpp:439] Applied 1 operations in 77us; attempting to update the 'registry'
I0128 12:20:27.696362 300445696 log.cpp:683] Attempting to append 770 bytes to the log
I0128 12:20:27.696458 299372544 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 9
I0128 12:20:27.697157 297226240 replica.cpp:537] Replica received write request for position 9 from (18332)@192.168.178.24:51278
I0128 12:20:27.697350 297226240 leveldb.cpp:341] Persisting action (789 bytes) to leveldb took 182us
I0128 12:20:27.697382 297226240 replica.cpp:712] Persisted action at 9
I0128 12:20:27.697803 297762816 replica.cpp:691] Replica received learned notice for position 9 from @0.0.0.0:0
I0128 12:20:27.697944 297762816 leveldb.cpp:341] Persisting action (791 bytes) to leveldb took 127us
I0128 12:20:27.697968 297762816 replica.cpp:712] Persisted action at 9
I0128 12:20:27.697978 297762816 replica.cpp:697] Replica learned APPEND action at position 9
I0128 12:20:27.699586 297762816 registrar.cpp:484] Successfully updated the 'registry' in 3.547904ms
I0128 12:20:27.699905 300982272 log.cpp:702] Attempting to truncate the log to 9
I0128 12:20:27.700006 298835968 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 10
I0128 12:20:27.699996 297762816 hierarchical.cpp:1024] Set quota cpus(*):1; mem(*):512 for role 'role2'
I0128 12:20:27.700466 300982272 sched.cpp:903] Rescinded offer 531344bd-56f4-4e4f-8f6f-a6a9d36058c7-O0
I0128 12:20:27.700542 300982272 sched.cpp:914] Scheduler::offerRescinded took 59us
I0128 12:20:27.700582 297762816 hierarchical.cpp:1403] No resources available to allocate!
I0128 12:20:27.700615 297762816 hierarchical.cpp:1498] No inverse offers to send out!
I0128 12:20:27.700635 297762816 hierarchical.cpp:1096] Performed allocation for 3 slaves in 607us
I0128 12:20:27.700886 297762816 hierarchical.cpp:892] Recovered cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (total: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000], allocated: ) on slave 531344bd-56f4-4e4f-8f6f-a6a9d36058c7-S2 from framework framework(18327)
I0128 12:20:27.701004 299372544 sched.cpp:903] Rescinded offer 531344bd-56f4-4e4f-8f6f-a6a9d36058c7-O1
I0128 12:20:27.701046 299372544 sched.cpp:914] Scheduler::offerRescinded took 29us
I0128 12:20:27.701098 298299392 replica.cpp:537] Replica received write request for position 10 from (18333)@192.168.178.24:51278
I0128 12:20:27.701093 297762816 hierarchical.cpp:892] Recovered cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (total: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000], allocated: ) on slave 531344bd-56f4-4e4f-8f6f-a6a9d36058c7-S1 from framework framework(18327)
I0128 12:20:27.701261 298299392 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 237us
I0128 12:20:27.701382 298299392 replica.cpp:712] Persisted action at 10
I0128 12:20:27.702061 299909120 replica.cpp:691] Replica received learned notice for position 10 from @0.0.0.0:0
I0128 12:20:27.702244 299909120 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 171us
I0128 12:20:27.702301 299909120 leveldb.cpp:399] Deleting ~2 keys from leveldb took 34us
I0128 12:20:27.702322 299909120 replica.cpp:712] Persisted action at 10
I0128 12:20:27.702339 299909120 replica.cpp:697] Replica learned TRUNCATE action at position 10
I0128 12:20:27.702792 297762816 hierarchical.cpp:1498] No inverse offers to send out!
I0128 12:20:27.702831 297762816 hierarchical.cpp:1096] Performed allocation for 3 slaves in 1644us
I0128 12:20:27.703033 299909120 master.cpp:5350] Sending 1 offers to framework framework(18327) (framework(18327)) at scheduler-05b9ac89-54ac-4d24-84e7-bb9cedfa77c4@192.168.178.24:51278
I0128 12:20:27.703428 299909120 master.cpp:5350] Sending 1 offers to framework framework(18328) (framework(18328)) at scheduler-7953b576-d913-4dce-993d-375e2fb34aba@192.168.178.24:51278

GMOCK WARNING:
Uninteresting mock function call - returning directly.
    Function call: resourceOffers(0x7fff5bdd4c88, @0x111e86250 { 144-byte object <90-06 1B-0B 01-00 00-00 00-00 00-00 00-00 00-00 00-A9 61-FB FD-7F 00-00 70-A9 61-FB FD-7F 00-00 B0-A9 61-FB FD-7F 00-00 F0-B0 61-FB FD-7F 00-00 10-B1 61-FB FD-7F 00-00../../../src/tests/master_quota_tests.cpp:899: Failure
Mock function called more times than expected - returning directly.
    Function call: resourceOffers(0x7fff5bdd4dd0, @0x111c7a250 { 144-byte object <90-06 1B-0B 01-00 00-00 00-00 00-00 00-00 00-00 30-C1 C4-FC FD-7F 00-00 80-C1 4A-FB FD-7F 00-00 F0-BD 48-FB FD-7F 00-00 E0-64 4A-FB FD-7F 00-00 D0-56 C0-FC FD-7F 00-00 C0-94 C8-FC FD-7F 00-00 ... 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 FD-7F 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 1F-00 00-00> })
         Expected: to be called once
           Actual: called twice - over-saturated and active
I0128 12:20:27.704130 297762816 master.cpp:1025] Master terminating
 7*** Aborted at 1453980027 (unix time) try ""date -d @1453980027"" if you are using GNU date ***
0-D6 60-FB FD-7F 00-00 ... 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 FD-7F 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 1F-00 00-00> })
Stack trace:
I0128 12:20:27.704699 299372544 hierarchical.cpp:505] Removed slave 531344bd-56f4-4e4f-8f6f-a6a9d36058c7-S2
I0128 12:20:27.705009 297226240 hierarchical.cpp:505] Removed slave 531344bd-56f4-4e4f-8f6f-a6a9d36058c7-S1
I0128 12:20:27.705271 300445696 sched.cpp:877] Scheduler::resourceOffers took 1623us
I0128 12:20:27.705306 299909120 hierarchical.cpp:505] Removed slave 531344bd-56f4-4e4f-8f6f-a6a9d36058c7-S0
I0128 12:20:27.705575 298835968 hierarchical.cpp:326] Removed framework framework(18329)
I0128 12:20:27.705803 298835968 hierarchical.cpp:326] Removed framework framework(18328)
I0128 12:20:27.705878 300445696 slave.cpp:3481] master@192.168.178.24:51278 exited
W0128 12:20:27.705901 300445696 slave.cpp:3484] Master disconnected! Waiting for a new master to be elected
I0128 12:20:27.705904 300982272 slave.cpp:3481] master@192.168.178.24:51278 exited
W0128 12:20:27.705921 300982272 slave.cpp:3484] Master disconnected! Waiting for a new master to be elected
I0128 12:20:27.705955 300445696 slave.cpp:3481] master@192.168.178.24:51278 exited
W0128 12:20:27.705971 300445696 slave.cpp:3484] Master disconnected! Waiting for a new master to be elected
I0128 12:20:27.706085 298835968 hierarchical.cpp:326] Removed framework framework(18327)
I0128 12:20:27.709558 299372544 slave.cpp:667] Slave terminating
I0128 12:20:27.712924 299372544 slave.cpp:667] Slave terminating
I0128 12:20:27.716169 299909120 slave.cpp:667] Slave terminating
PC: @        0x1054638f5 testing::UnitTest::AddTestPartResult()
*** SIGSEGV (@0x0) received by PID 98630 (TID 0x111c7b000) stack trace: ***
    @     0x7fff91018f1a _sigtramp
    @     0x7fff93077187 malloc
    @        0x1054630a7 testing::internal::AssertHelper::operator=()
    @        0x1054cebcb testing::internal::GoogleTestFailureReporter::ReportFailure()
    @        0x103f20058 testing::internal::Expect()
    @        0x1054a2d86 testing::internal::UntypedFunctionMockerBase::UntypedInvokeWith()
    @        0x104b3ec18 testing::internal::FunctionMockerBase<>::InvokeWith()
    @        0x104b3ebdb testing::internal::FunctionMocker<>::Invoke()
    @        0x104b068dd mesos::internal::tests::MockScheduler::resourceOffers()
I0128 12:20:27.756392 299909120 hierarchical.cpp:1403] No resources available to allocate!
I0128 12:20:27.756412 299909120 hierarchical.cpp:1096] Performed allocation for 0 slaves in 81us
I0128 12:20:27.808101 297226240 hierarchical.cpp:1403] No resources available to allocate!
I0128 12:20:27.808130 297226240 hierarchical.cpp:1096] Performed allocation for 0 slaves in 99us
    @        0x108d500fb mesos::internal::SchedulerProcess::resourceOffers()
    @        0x108d78d0f ProtobufProcess<>::handler2<>()
    @        0x108d7ba6b _ZNSt3__110__function6__funcINS_6__bindIPFvPN5mesos8internal16SchedulerProcessEMS5_FvRKN7process4UPIDERKNS_6vectorINS3_5OfferENS_9allocatorISC_EEEERKNSB_INS_12basic_stringIcNS_11char_traitsIcEENSD_IcEEEENSD_ISM_EEEEEMNS4_21ResourceOffersMessageEKFRKN6google8protobuf16RepeatedPtrFieldISC_EEvEMST_KFRKNSW_ISM_EEvESA_RKSM_EJRS6_RSS_RS11_RS16_RNS_12placeholders4__phILi1EEERNS1G_ILi2EEEEEENSD_IS1L_EEFvSA_S18_EEclESA_S18_
    @        0x1080af494 std::__1::function<>::operator()()
    @        0x108d4932e ProtobufProcess<>::visit()
I0128 12:20:27.858326 300445696 hierarchical.cpp:1403] No resources available to allocate!
I0128 12:20:27.858350 300445696 hierarchical.cpp:1096] Performed allocation for 0 slaves in 99us
    @        0x108d49477 ProtobufProcess<>::visit()
    @        0x108b099be process::MessageEvent::visit()
    @        0x108097df1 process::ProcessBase::serve()
    @        0x10a86d4d0 process::ProcessManager::resume()
    @        0x10a87d8af process::ProcessManager::init_threads()::$_1::operator()()
    @        0x10a87d522 _ZNSt3__114__thread_proxyINS_5tupleIJNS_6__bindIZN7process14ProcessManager12init_threadsEvE3$_1JNS_17reference_wrapperIKNS_6atomicIbEEEEEEEEEEEEPvSD_
    @     0x7fff8f415268 _pthread_body
    @     0x7fff8f4151e5 _pthread_start
    @     0x7fff8f41341d thread_start
zsh: segmentation fault  GLOG_v=1 GTEST_FILTER=""MasterQuotaTest.AvailableResourcesAfterRescinding""
{code}

h5. Verbose log from a good run:
{code}
[ RUN      ] MasterQuotaTest.AvailableResourcesAfterRescinding
I0128 12:20:27.320648 2080858880 resources.cpp:564] Parsing resources as JSON failed: cpus:2;mem:1024;disk:1024;ports:[31000-32000]
Trying semicolon-delimited string format instead
I0128 12:20:27.321002 2080858880 resources.cpp:564] Parsing resources as JSON failed: cpus:2;mem:1024;disk:1024;ports:[31000-32000]
Trying semicolon-delimited string format instead
I0128 12:20:27.327103 2080858880 leveldb.cpp:174] Opened db in 3111us
I0128 12:20:27.327848 2080858880 leveldb.cpp:181] Compacted db in 700us
I0128 12:20:27.327899 2080858880 leveldb.cpp:196] Created db iterator in 17us
I0128 12:20:27.327922 2080858880 leveldb.cpp:202] Seeked to beginning of db in 11us
I0128 12:20:27.327940 2080858880 leveldb.cpp:271] Iterated through 0 keys in the db in 11us
I0128 12:20:27.327973 2080858880 replica.cpp:779] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned
I0128 12:20:27.328737 298299392 recover.cpp:447] Starting replica recovery
I0128 12:20:27.329010 298299392 recover.cpp:473] Replica is in EMPTY status
I0128 12:20:27.330369 299909120 replica.cpp:673] Replica in EMPTY status received a broadcasted recover request from (18211)@192.168.178.24:51278
I0128 12:20:27.330749 297226240 recover.cpp:193] Received a recover response from a replica in EMPTY status
I0128 12:20:27.331166 297762816 recover.cpp:564] Updating replica status to STARTING
I0128 12:20:27.331825 300445696 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 437us
I0128 12:20:27.331897 300445696 replica.cpp:320] Persisted replica status to STARTING
I0128 12:20:27.332077 300445696 recover.cpp:473] Replica is in STARTING status
I0128 12:20:27.332474 298299392 master.cpp:374] Master bd6f3479-18eb-478d-8a08-a41364ecbd05 (alexr.fritz.box) started on 192.168.178.24:51278
I0128 12:20:27.332520 298299392 master.cpp:376] Flags at startup: --acls="""" --allocation_interval=""50ms"" --allocator=""HierarchicalDRF"" --authenticate=""true"" --authenticate_http=""true"" --authenticate_slaves=""true"" --authenticators=""crammd5"" --authorizers=""local"" --credentials=""/private/tmp/rkmzt7/credentials"" --framework_sorter=""drf"" --help=""false"" --hostname_lookup=""true"" --http_authenticators=""basic"" --initialize_driver_logging=""true"" --log_auto_initialize=""true"" --logbufsecs=""0"" --logging_level=""INFO"" --max_completed_frameworks=""50"" --max_completed_tasks_per_framework=""1000"" --max_slave_ping_timeouts=""5"" --quiet=""false"" --recovery_slave_removal_limit=""100%"" --registry=""replicated_log"" --registry_fetch_timeout=""1mins"" --registry_store_timeout=""25secs"" --registry_strict=""true"" --roles=""role1,role2"" --root_submissions=""true"" --slave_ping_timeout=""15secs"" --slave_reregister_timeout=""10mins"" --user_sorter=""drf"" --version=""false"" --webui_dir=""/usr/local/share/mesos/webui"" --work_dir=""/private/tmp/rkmzt7/master"" --zk_session_timeout=""10secs""
I0128 12:20:27.332871 298299392 master.cpp:421] Master only allowing authenticated frameworks to register
I0128 12:20:27.332898 298299392 master.cpp:426] Master only allowing authenticated slaves to register
I0128 12:20:27.332957 298299392 credentials.hpp:35] Loading credentials for authentication from '/private/tmp/rkmzt7/credentials'
I0128 12:20:27.333255 300982272 replica.cpp:673] Replica in STARTING status received a broadcasted recover request from (18212)@192.168.178.24:51278
I0128 12:20:27.333359 298299392 master.cpp:466] Using default 'crammd5' authenticator
I0128 12:20:27.333549 298299392 master.cpp:535] Using default 'basic' HTTP authenticator
I0128 12:20:27.333550 299909120 recover.cpp:193] Received a recover response from a replica in STARTING status
I0128 12:20:27.333890 298299392 master.cpp:569] Authorization enabled
W0128 12:20:27.333930 298299392 master.cpp:629] The '--roles' flag is deprecated. This flag will be removed in the future. See the Mesos 0.27 upgrade notes for more information
I0128 12:20:27.334220 298835968 recover.cpp:564] Updating replica status to VOTING
I0128 12:20:27.334226 300445696 whitelist_watcher.cpp:77] No whitelist given
I0128 12:20:27.334276 300982272 hierarchical.cpp:144] Initialized hierarchical allocator process
I0128 12:20:27.334724 300982272 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 242us
I0128 12:20:27.334772 300982272 replica.cpp:320] Persisted replica status to VOTING
I0128 12:20:27.334895 298835968 recover.cpp:578] Successfully joined the Paxos group
I0128 12:20:27.335198 298835968 recover.cpp:462] Recover process terminated
I0128 12:20:27.336282 298299392 master.cpp:1710] The newly elected leader is master@192.168.178.24:51278 with id bd6f3479-18eb-478d-8a08-a41364ecbd05
I0128 12:20:27.336321 298299392 master.cpp:1723] Elected as the leading master!
I0128 12:20:27.336340 298299392 master.cpp:1468] Recovering from registrar
I0128 12:20:27.336454 300982272 registrar.cpp:307] Recovering registrar
I0128 12:20:27.336956 299909120 log.cpp:659] Attempting to start the writer
I0128 12:20:27.338927 298299392 replica.cpp:493] Replica received implicit promise request from (18214)@192.168.178.24:51278 with proposal 1
I0128 12:20:27.339247 298299392 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 291us
I0128 12:20:27.339287 298299392 replica.cpp:342] Persisted promised to 1
I0128 12:20:27.340066 297226240 coordinator.cpp:238] Coordinator attempting to fill missing positions
I0128 12:20:27.341961 297226240 replica.cpp:388] Replica received explicit promise request from (18215)@192.168.178.24:51278 for position 0 with proposal 2
I0128 12:20:27.342239 297226240 leveldb.cpp:341] Persisting action (8 bytes) to leveldb took 247us
I0128 12:20:27.342278 297226240 replica.cpp:712] Persisted action at 0
I0128 12:20:27.343778 299372544 replica.cpp:537] Replica received write request for position 0 from (18216)@192.168.178.24:51278
I0128 12:20:27.343852 299372544 leveldb.cpp:436] Reading position from leveldb took 42us
I0128 12:20:27.344118 299372544 leveldb.cpp:341] Persisting action (14 bytes) to leveldb took 238us
I0128 12:20:27.344156 299372544 replica.cpp:712] Persisted action at 0
I0128 12:20:27.344981 297762816 replica.cpp:691] Replica received learned notice for position 0 from @0.0.0.0:0
I0128 12:20:27.345227 297762816 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 224us
I0128 12:20:27.345264 297762816 replica.cpp:712] Persisted action at 0
I0128 12:20:27.345284 297762816 replica.cpp:697] Replica learned NOP action at position 0
I0128 12:20:27.346266 298835968 log.cpp:675] Writer started with ending position 0
I0128 12:20:27.347331 298835968 leveldb.cpp:436] Reading position from leveldb took 63us
I0128 12:20:27.348314 300982272 registrar.cpp:340] Successfully fetched the registry (0B) in 11.819264ms
I0128 12:20:27.348456 300982272 registrar.cpp:439] Applied 1 operations in 71us; attempting to update the 'registry'
I0128 12:20:27.349019 297226240 log.cpp:683] Attempting to append 186 bytes to the log
I0128 12:20:27.349143 300982272 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 1
I0128 12:20:27.350441 298835968 replica.cpp:537] Replica received write request for position 1 from (18217)@192.168.178.24:51278
I0128 12:20:27.350811 298835968 leveldb.cpp:341] Persisting action (205 bytes) to leveldb took 337us
I0128 12:20:27.350846 298835968 replica.cpp:712] Persisted action at 1
I0128 12:20:27.351572 298835968 replica.cpp:691] Replica received learned notice for position 1 from @0.0.0.0:0
I0128 12:20:27.351801 298835968 leveldb.cpp:341] Persisting action (207 bytes) to leveldb took 212us
I0128 12:20:27.351836 298835968 replica.cpp:712] Persisted action at 1
I0128 12:20:27.351851 298835968 replica.cpp:697] Replica learned APPEND action at position 1
I0128 12:20:27.352608 299372544 registrar.cpp:484] Successfully updated the 'registry' in 4.096768ms
I0128 12:20:27.352735 299372544 registrar.cpp:370] Successfully recovered registrar
I0128 12:20:27.352877 298299392 log.cpp:702] Attempting to truncate the log to 1
I0128 12:20:27.353091 300982272 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 2
I0128 12:20:27.353314 300445696 master.cpp:1520] Recovered 0 slaves from the Registry (147B) ; allowing 10mins for slaves to re-register
I0128 12:20:27.353341 298835968 hierarchical.cpp:171] Skipping recovery of hierarchical allocator: nothing to recover
I0128 12:20:27.354064 297762816 replica.cpp:537] Replica received write request for position 2 from (18218)@192.168.178.24:51278
I0128 12:20:27.354333 297762816 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 242us
I0128 12:20:27.354372 297762816 replica.cpp:712] Persisted action at 2
I0128 12:20:27.355080 299909120 replica.cpp:691] Replica received learned notice for position 2 from @0.0.0.0:0
I0128 12:20:27.355311 299909120 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 214us
I0128 12:20:27.355362 299909120 leveldb.cpp:399] Deleting ~1 keys from leveldb took 27us
I0128 12:20:27.355382 299909120 replica.cpp:712] Persisted action at 2
I0128 12:20:27.355398 299909120 replica.cpp:697] Replica learned TRUNCATE action at position 2
I0128 12:20:27.366089 2080858880 containerizer.cpp:143] Using isolation: posix/cpu,posix/mem,filesystem/posix
I0128 12:20:27.369942 300445696 slave.cpp:192] Slave started on 868)@192.168.178.24:51278
I0128 12:20:27.369989 300445696 slave.cpp:193] Flags at startup: --appc_store_dir=""/tmp/mesos/store/appc"" --authenticatee=""crammd5"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --credential=""/tmp/MasterQuotaTest_AvailableResourcesAfterRescinding_UwHS2p/credential"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_auth_server=""https://auth.docker.io"" --docker_kill_orphans=""true"" --docker_puller_timeout=""60"" --docker_registry=""https://registry-1.docker.io"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/tmp/mesos/store/docker"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/tmp/MasterQuotaTest_AvailableResourcesAfterRescinding_UwHS2p/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --hostname_lookup=""true"" --image_provisioner_backend=""copy"" --initialize_driver_logging=""true"" --isolation=""posix/cpu,posix/mem"" --launcher_dir=""/Users/alex/Projects/mesos/build/default/src"" --logbufsecs=""0"" --logging_level=""INFO"" --oversubscribed_resources_interval=""15secs"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""10ms"" --resources=""cpus:2;mem:1024;disk:1024;ports:[31000-32000]"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --version=""false"" --work_dir=""/tmp/MasterQuotaTest_AvailableResourcesAfterRescinding_UwHS2p""
I0128 12:20:27.370420 300445696 credentials.hpp:83] Loading credential for authentication from '/tmp/MasterQuotaTest_AvailableResourcesAfterRescinding_UwHS2p/credential'
I0128 12:20:27.370616 300445696 slave.cpp:323] Slave using credential for: test-principal
I0128 12:20:27.370738 300445696 resources.cpp:564] Parsing resources as JSON failed: cpus:2;mem:1024;disk:1024;ports:[31000-32000]
Trying semicolon-delimited string format instead
I0128 12:20:27.371325 300445696 slave.cpp:463] Slave resources: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I0128 12:20:27.371383 300445696 slave.cpp:471] Slave attributes: [  ]
I0128 12:20:27.371399 300445696 slave.cpp:476] Slave hostname: alexr.fritz.box
I0128 12:20:27.371879 298835968 state.cpp:58] Recovering state from '/tmp/MasterQuotaTest_AvailableResourcesAfterRescinding_UwHS2p/meta'
I0128 12:20:27.372112 299909120 status_update_manager.cpp:200] Recovering status update manager
I0128 12:20:27.372323 297226240 containerizer.cpp:390] Recovering containerizer
I0128 12:20:27.373388 298835968 provisioner.cpp:245] Provisioner recovery complete
I0128 12:20:27.373706 297226240 slave.cpp:4495] Finished recovery
I0128 12:20:27.374156 297226240 slave.cpp:4667] Querying resource estimator for oversubscribable resources
I0128 12:20:27.374402 298299392 status_update_manager.cpp:174] Pausing sending status updates
I0128 12:20:27.374408 297226240 slave.cpp:795] New master detected at master@192.168.178.24:51278
I0128 12:20:27.374454 297226240 slave.cpp:858] Authenticating with master master@192.168.178.24:51278
I0128 12:20:27.374469 297226240 slave.cpp:863] Using default CRAM-MD5 authenticatee
I0128 12:20:27.374605 297226240 slave.cpp:831] Detecting new master
I0128 12:20:27.374637 300982272 authenticatee.cpp:121] Creating new client SASL connection
I0128 12:20:27.374800 297226240 slave.cpp:4681] Received oversubscribable resources  from the resource estimator
I0128 12:20:27.374876 297762816 master.cpp:5521] Authenticating slave(868)@192.168.178.24:51278
I0128 12:20:27.374999 297226240 authenticator.cpp:413] Starting authentication session for crammd5_authenticatee(1735)@192.168.178.24:51278
I0128 12:20:27.375211 298835968 authenticator.cpp:98] Creating new server SASL connection
I0128 12:20:27.375344 297762816 authenticatee.cpp:212] Received SASL authentication mechanisms: CRAM-MD5
I0128 12:20:27.375380 297762816 authenticatee.cpp:238] Attempting to authenticate with mechanism 'CRAM-MD5'
I0128 12:20:27.375537 298835968 authenticator.cpp:203] Received SASL authentication start
I0128 12:20:27.375601 298835968 authenticator.cpp:325] Authentication requires more steps
I0128 12:20:27.375694 297762816 authenticatee.cpp:258] Received SASL authentication step
I0128 12:20:27.375838 299909120 authenticator.cpp:231] Received SASL authentication step
I0128 12:20:27.375860 299909120 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: 'alexr.fritz.box' server FQDN: 'alexr.fritz.box' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0128 12:20:27.375871 299909120 auxprop.cpp:179] Looking up auxiliary property '*userPassword'
I0128 12:20:27.375890 299909120 auxprop.cpp:179] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0128 12:20:27.375902 299909120 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: 'alexr.fritz.box' server FQDN: 'alexr.fritz.box' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0128 12:20:27.375911 299909120 auxprop.cpp:129] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0128 12:20:27.375918 299909120 auxprop.cpp:129] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0128 12:20:27.375929 299909120 authenticator.cpp:317] Authentication success
I0128 12:20:27.376003 297762816 authenticatee.cpp:298] Authentication success
I0128 12:20:27.376076 300445696 master.cpp:5551] Successfully authenticated principal 'test-principal' at slave(868)@192.168.178.24:51278
I0128 12:20:27.376180 298299392 authenticator.cpp:431] Authentication session cleanup for crammd5_authenticatee(1735)@192.168.178.24:51278
I0128 12:20:27.376325 300982272 slave.cpp:926] Successfully authenticated with master master@192.168.178.24:51278
I0128 12:20:27.376410 300982272 slave.cpp:1320] Will retry registration in 18.436231ms if necessary
I0128 12:20:27.376557 297762816 master.cpp:4235] Registering slave at slave(868)@192.168.178.24:51278 (alexr.fritz.box) with id bd6f3479-18eb-478d-8a08-a41364ecbd05-S0
I0128 12:20:27.376785 298299392 registrar.cpp:439] Applied 1 operations in 59us; attempting to update the 'registry'
I0128 12:20:27.377285 300445696 log.cpp:683] Attempting to append 358 bytes to the log
I0128 12:20:27.377408 297762816 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 3
I0128 12:20:27.378199 300445696 replica.cpp:537] Replica received write request for position 3 from (18232)@192.168.178.24:51278
I0128 12:20:27.378413 300445696 leveldb.cpp:341] Persisting action (377 bytes) to leveldb took 192us
I0128 12:20:27.378446 300445696 replica.cpp:712] Persisted action at 3
I0128 12:20:27.379106 298299392 replica.cpp:691] Replica received learned notice for position 3 from @0.0.0.0:0
I0128 12:20:27.379331 298299392 leveldb.cpp:341] Persisting action (379 bytes) to leveldb took 220us
I0128 12:20:27.379406 298299392 replica.cpp:712] Persisted action at 3
I0128 12:20:27.379429 298299392 replica.cpp:697] Replica learned APPEND action at position 3
I0128 12:20:27.380581 299372544 registrar.cpp:484] Successfully updated the 'registry' in 3.747328ms
I0128 12:20:27.381187 300445696 log.cpp:702] Attempting to truncate the log to 3
I0128 12:20:27.381304 299909120 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 4
I0128 12:20:27.381430 300445696 slave.cpp:3435] Received ping from slave-observer(868)@192.168.178.24:51278
I0128 12:20:27.381759 297762816 hierarchical.cpp:473] Added slave bd6f3479-18eb-478d-8a08-a41364ecbd05-S0 (alexr.fritz.box) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (allocated: )
I0128 12:20:27.381801 297226240 slave.cpp:970] Registered with master master@192.168.178.24:51278; given slave ID bd6f3479-18eb-478d-8a08-a41364ecbd05-S0
I0128 12:20:27.381765 298835968 master.cpp:4303] Registered slave bd6f3479-18eb-478d-8a08-a41364ecbd05-S0 at slave(868)@192.168.178.24:51278 (alexr.fritz.box) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I0128 12:20:27.381829 297226240 fetcher.cpp:81] Clearing fetcher cache
I0128 12:20:27.381908 297762816 hierarchical.cpp:1403] No resources available to allocate!
I0128 12:20:27.381950 297762816 hierarchical.cpp:1116] Performed allocation for slave bd6f3479-18eb-478d-8a08-a41364ecbd05-S0 in 156us
I0128 12:20:27.382125 299372544 status_update_manager.cpp:181] Resuming sending status updates
I0128 12:20:27.382279 300982272 replica.cpp:537] Replica received write request for position 4 from (18233)@192.168.178.24:51278
I0128 12:20:27.382540 300982272 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 240us
I0128 12:20:27.382585 300982272 replica.cpp:712] Persisted action at 4
I0128 12:20:27.382804 297226240 slave.cpp:993] Checkpointing SlaveInfo to '/tmp/MasterQuotaTest_AvailableResourcesAfterRescinding_UwHS2p/meta/slaves/bd6f3479-18eb-478d-8a08-a41364ecbd05-S0/slave.info'
I0128 12:20:27.384191 298299392 replica.cpp:691] Replica received learned notice for position 4 from @0.0.0.0:0
I0128 12:20:27.384424 298299392 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 235us
I0128 12:20:27.384517 298299392 leveldb.cpp:399] Deleting ~2 keys from leveldb took 52us
I0128 12:20:27.384557 298299392 replica.cpp:712] Persisted action at 4
I0128 12:20:27.384578 298299392 replica.cpp:697] Replica learned TRUNCATE action at position 4
I0128 12:20:27.384601 299372544 hierarchical.cpp:1403] No resources available to allocate!
I0128 12:20:27.384624 299372544 hierarchical.cpp:1096] Performed allocation for 1 slaves in 124us
I0128 12:20:27.385365 297226240 slave.cpp:1029] Forwarding total oversubscribed resources 
I0128 12:20:27.385526 297226240 master.cpp:4644] Received update of slave bd6f3479-18eb-478d-8a08-a41364ecbd05-S0 at slave(868)@192.168.178.24:51278 (alexr.fritz.box) with total oversubscribed resources 
I0128 12:20:27.386008 297226240 hierarchical.cpp:531] Slave bd6f3479-18eb-478d-8a08-a41364ecbd05-S0 (alexr.fritz.box) updated with oversubscribed resources  (total: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000], allocated: )
I0128 12:20:27.386137 297226240 hierarchical.cpp:1403] No resources available to allocate!
I0128 12:20:27.386155 297226240 hierarchical.cpp:1116] Performed allocation for slave bd6f3479-18eb-478d-8a08-a41364ecbd05-S0 in 91us
I0128 12:20:27.386893 2080858880 containerizer.cpp:143] Using isolation: posix/cpu,posix/mem,filesystem/posix
I0128 12:20:27.413722 299909120 slave.cpp:192] Slave started on 869)@192.168.178.24:51278
I0128 12:20:27.413766 299909120 slave.cpp:193] Flags at startup: --appc_store_dir=""/tmp/mesos/store/appc"" --authenticatee=""crammd5"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --credential=""/tmp/MasterQuotaTest_AvailableResourcesAfterRescinding_TNjwCO/credential"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_auth_server=""https://auth.docker.io"" --docker_kill_orphans=""true"" --docker_puller_timeout=""60"" --docker_registry=""https://registry-1.docker.io"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/tmp/mesos/store/docker"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/tmp/MasterQuotaTest_AvailableResourcesAfterRescinding_TNjwCO/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --hostname_lookup=""true"" --image_provisioner_backend=""copy"" --initialize_driver_logging=""true"" --isolation=""posix/cpu,posix/mem"" --launcher_dir=""/Users/alex/Projects/mesos/build/default/src"" --logbufsecs=""0"" --logging_level=""INFO"" --oversubscribed_resources_interval=""15secs"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""10ms"" --resources=""cpus:2;mem:1024;disk:1024;ports:[31000-32000]"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --version=""false"" --work_dir=""/tmp/MasterQuotaTest_AvailableResourcesAfterRescinding_TNjwCO""
I0128 12:20:27.414484 299909120 credentials.hpp:83] Loading credential for authentication from '/tmp/MasterQuotaTest_AvailableResourcesAfterRescinding_TNjwCO/credential'
I0128 12:20:27.414650 299909120 slave.cpp:323] Slave using credential for: test-principal
I0128 12:20:27.416365 299909120 resources.cpp:564] Parsing resources as JSON failed: cpus:2;mem:1024;disk:1024;ports:[31000-32000]
Trying semicolon-delimited string format instead
I0128 12:20:27.418332 299909120 slave.cpp:463] Slave resources: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I0128 12:20:27.418475 299909120 slave.cpp:471] Slave attributes: [  ]
I0128 12:20:27.418500 299909120 slave.cpp:476] Slave hostname: alexr.fritz.box
I0128 12:20:27.419270 298835968 state.cpp:58] Recovering state from '/tmp/MasterQuotaTest_AvailableResourcesAfterRescinding_TNjwCO/meta'
I0128 12:20:27.420717 298299392 status_update_manager.cpp:200] Recovering status update manager
I0128 12:20:27.420953 299909120 containerizer.cpp:390] Recovering containerizer
I0128 12:20:27.421883 298835968 provisioner.cpp:245] Provisioner recovery complete
I0128 12:20:27.422145 299909120 slave.cpp:4495] Finished recovery
I0128 12:20:27.422695 299909120 slave.cpp:4667] Querying resource estimator for oversubscribable resources
I0128 12:20:27.422929 299909120 slave.cpp:795] New master detected at master@192.168.178.24:51278
I0128 12:20:27.422935 299372544 status_update_manager.cpp:174] Pausing sending status updates
I0128 12:20:27.422971 299909120 slave.cpp:858] Authenticating with master master@192.168.178.24:51278
I0128 12:20:27.422986 299909120 slave.cpp:863] Using default CRAM-MD5 authenticatee
I0128 12:20:27.423073 299909120 slave.cpp:831] Detecting new master
I0128 12:20:27.423151 298835968 authenticatee.cpp:121] Creating new client SASL connection
I0128 12:20:27.423187 299909120 slave.cpp:4681] Received oversubscribable resources  from the resource estimator
I0128 12:20:27.423333 299909120 master.cpp:5521] Authenticating slave(869)@192.168.178.24:51278
I0128 12:20:27.423431 299909120 authenticator.cpp:413] Starting authentication session for crammd5_authenticatee(1736)@192.168.178.24:51278
I0128 12:20:27.423530 297762816 authenticator.cpp:98] Creating new server SASL connection
I0128 12:20:27.423630 299909120 authenticatee.cpp:212] Received SASL authentication mechanisms: CRAM-MD5
I0128 12:20:27.423657 299909120 authenticatee.cpp:238] Attempting to authenticate with mechanism 'CRAM-MD5'
I0128 12:20:27.423738 299909120 authenticator.cpp:203] Received SASL authentication start
I0128 12:20:27.423781 299909120 authenticator.cpp:325] Authentication requires more steps
I0128 12:20:27.423833 299909120 authenticatee.cpp:258] Received SASL authentication step
I0128 12:20:27.423897 299909120 authenticator.cpp:231] Received SASL authentication step
I0128 12:20:27.423918 299909120 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: 'alexr.fritz.box' server FQDN: 'alexr.fritz.box' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0128 12:20:27.423931 299909120 auxprop.cpp:179] Looking up auxiliary property '*userPassword'
I0128 12:20:27.423956 299909120 auxprop.cpp:179] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0128 12:20:27.423992 299909120 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: 'alexr.fritz.box' server FQDN: 'alexr.fritz.box' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0128 12:20:27.424006 299909120 auxprop.cpp:129] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0128 12:20:27.424016 299909120 auxprop.cpp:129] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0128 12:20:27.424031 299909120 authenticator.cpp:317] Authentication success
I0128 12:20:27.424095 297762816 authenticatee.cpp:298] Authentication success
I0128 12:20:27.424125 299909120 master.cpp:5551] Successfully authenticated principal 'test-principal' at slave(869)@192.168.178.24:51278
I0128 12:20:27.424180 299909120 authenticator.cpp:431] Authentication session cleanup for crammd5_authenticatee(1736)@192.168.178.24:51278
I0128 12:20:27.424351 297762816 slave.cpp:926] Successfully authenticated with master master@192.168.178.24:51278
I0128 12:20:27.424444 297762816 slave.cpp:1320] Will retry registration in 10.561574ms if necessary
I0128 12:20:27.424577 300982272 master.cpp:4235] Registering slave at slave(869)@192.168.178.24:51278 (alexr.fritz.box) with id bd6f3479-18eb-478d-8a08-a41364ecbd05-S1
I0128 12:20:27.424949 298835968 registrar.cpp:439] Applied 1 operations in 86us; attempting to update the 'registry'
I0128 12:20:27.425549 297226240 log.cpp:683] Attempting to append 527 bytes to the log
I0128 12:20:27.425662 298835968 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 5
I0128 12:20:27.426563 300982272 replica.cpp:537] Replica received write request for position 5 from (18247)@192.168.178.24:51278
I0128 12:20:27.426820 300982272 leveldb.cpp:341] Persisting action (546 bytes) to leveldb took 215us
I0128 12:20:27.426857 300982272 replica.cpp:712] Persisted action at 5
I0128 12:20:27.427487 297226240 replica.cpp:691] Replica received learned notice for position 5 from @0.0.0.0:0
I0128 12:20:27.427820 297226240 leveldb.cpp:341] Persisting action (548 bytes) to leveldb took 197us
I0128 12:20:27.427855 297226240 replica.cpp:712] Persisted action at 5
I0128 12:20:27.427940 297226240 replica.cpp:697] Replica learned APPEND action at position 5
I0128 12:20:27.432288 299372544 registrar.cpp:484] Successfully updated the 'registry' in 7.28704ms
I0128 12:20:27.432765 299372544 log.cpp:702] Attempting to truncate the log to 5
I0128 12:20:27.433254 299372544 master.cpp:4303] Registered slave bd6f3479-18eb-478d-8a08-a41364ecbd05-S1 at slave(869)@192.168.178.24:51278 (alexr.fritz.box) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I0128 12:20:27.433334 299372544 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 6
I0128 12:20:27.433584 299372544 hierarchical.cpp:473] Added slave bd6f3479-18eb-478d-8a08-a41364ecbd05-S1 (alexr.fritz.box) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (allocated: )
I0128 12:20:27.433667 299372544 hierarchical.cpp:1403] No resources available to allocate!
I0128 12:20:27.433686 299372544 hierarchical.cpp:1116] Performed allocation for slave bd6f3479-18eb-478d-8a08-a41364ecbd05-S1 in 78us
I0128 12:20:27.433753 299372544 slave.cpp:970] Registered with master master@192.168.178.24:51278; given slave ID bd6f3479-18eb-478d-8a08-a41364ecbd05-S1
I0128 12:20:27.433770 299372544 fetcher.cpp:81] Clearing fetcher cache
I0128 12:20:27.434334 298299392 status_update_manager.cpp:181] Resuming sending status updates
I0128 12:20:27.435118 298835968 hierarchical.cpp:1403] No resources available to allocate!
I0128 12:20:27.435144 298835968 hierarchical.cpp:1096] Performed allocation for 2 slaves in 146us
I0128 12:20:27.435294 297226240 replica.cpp:537] Replica received write request for position 6 from (18248)@192.168.178.24:51278
I0128 12:20:27.435482 297226240 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 173us
I0128 12:20:27.435516 297226240 replica.cpp:712] Persisted action at 6
I0128 12:20:27.435883 297226240 replica.cpp:691] Replica received learned notice for position 6 from @0.0.0.0:0
I0128 12:20:27.436043 299372544 slave.cpp:993] Checkpointing SlaveInfo to '/tmp/MasterQuotaTest_AvailableResourcesAfterRescinding_TNjwCO/meta/slaves/bd6f3479-18eb-478d-8a08-a41364ecbd05-S1/slave.info'
I0128 12:20:27.436040 297226240 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 148us
I0128 12:20:27.436091 297226240 leveldb.cpp:399] Deleting ~2 keys from leveldb took 29us
I0128 12:20:27.436112 297226240 replica.cpp:712] Persisted action at 6
I0128 12:20:27.436153 297226240 replica.cpp:697] Replica learned TRUNCATE action at position 6
I0128 12:20:27.436594 299372544 slave.cpp:1029] Forwarding total oversubscribed resources 
I0128 12:20:27.436656 299372544 slave.cpp:3435] Received ping from slave-observer(869)@192.168.178.24:51278
I0128 12:20:27.436753 299372544 master.cpp:4644] Received update of slave bd6f3479-18eb-478d-8a08-a41364ecbd05-S1 at slave(869)@192.168.178.24:51278 (alexr.fritz.box) with total oversubscribed resources 
I0128 12:20:27.437094 299372544 hierarchical.cpp:531] Slave bd6f3479-18eb-478d-8a08-a41364ecbd05-S1 (alexr.fritz.box) updated with oversubscribed resources  (total: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000], allocated: )
I0128 12:20:27.437599 299372544 hierarchical.cpp:1403] No resources available to allocate!
I0128 12:20:27.437620 299372544 hierarchical.cpp:1116] Performed allocation for slave bd6f3479-18eb-478d-8a08-a41364ecbd05-S1 in 499us
I0128 12:20:27.438273 2080858880 containerizer.cpp:143] Using isolation: posix/cpu,posix/mem,filesystem/posix
I0128 12:20:27.441591 299372544 slave.cpp:192] Slave started on 870)@192.168.178.24:51278
I0128 12:20:27.441622 299372544 slave.cpp:193] Flags at startup: --appc_store_dir=""/tmp/mesos/store/appc"" --authenticatee=""crammd5"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --credential=""/tmp/MasterQuotaTest_AvailableResourcesAfterRescinding_iHhziB/credential"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_auth_server=""https://auth.docker.io"" --docker_kill_orphans=""true"" --docker_puller_timeout=""60"" --docker_registry=""https://registry-1.docker.io"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/tmp/mesos/store/docker"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/tmp/MasterQuotaTest_AvailableResourcesAfterRescinding_iHhziB/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --hostname_lookup=""true"" --image_provisioner_backend=""copy"" --initialize_driver_logging=""true"" --isolation=""posix/cpu,posix/mem"" --launcher_dir=""/Users/alex/Projects/mesos/build/default/src"" --logbufsecs=""0"" --logging_level=""INFO"" --oversubscribed_resources_interval=""15secs"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""10ms"" --resources=""cpus:2;mem:1024;disk:1024;ports:[31000-32000]"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --version=""false"" --work_dir=""/tmp/MasterQuotaTest_AvailableResourcesAfterRescinding_iHhziB""
I0128 12:20:27.442208 299372544 credentials.hpp:83] Loading credential for authentication from '/tmp/MasterQuotaTest_AvailableResourcesAfterRescinding_iHhziB/credential'
I0128 12:20:27.442344 299372544 slave.cpp:323] Slave using credential for: test-principal
I0128 12:20:27.442428 299372544 resources.cpp:564] Parsing resources as JSON failed: cpus:2;mem:1024;disk:1024;ports:[31000-32000]
Trying semicolon-delimited string format instead
I0128 12:20:27.443301 299372544 slave.cpp:463] Slave resources: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I0128 12:20:27.443349 299372544 slave.cpp:471] Slave attributes: [  ]
I0128 12:20:27.443361 299372544 slave.cpp:476] Slave hostname: alexr.fritz.box
I0128 12:20:27.443904 298835968 state.cpp:58] Recovering state from '/tmp/MasterQuotaTest_AvailableResourcesAfterRescinding_iHhziB/meta'
I0128 12:20:27.444108 297226240 status_update_manager.cpp:200] Recovering status update manager
I0128 12:20:27.444362 300982272 containerizer.cpp:390] Recovering containerizer
I0128 12:20:27.446213 297226240 provisioner.cpp:245] Provisioner recovery complete
I0128 12:20:27.446542 297226240 slave.cpp:4495] Finished recovery
I0128 12:20:27.447430 297226240 slave.cpp:4667] Querying resource estimator for oversubscribable resources
I0128 12:20:27.447684 300445696 status_update_manager.cpp:174] Pausing sending status updates
I0128 12:20:27.447726 297226240 slave.cpp:795] New master detected at master@192.168.178.24:51278
I0128 12:20:27.447760 297226240 slave.cpp:858] Authenticating with master master@192.168.178.24:51278
I0128 12:20:27.447773 297226240 slave.cpp:863] Using default CRAM-MD5 authenticatee
I0128 12:20:27.448114 297762816 authenticatee.cpp:121] Creating new client SASL connection
I0128 12:20:27.448084 297226240 slave.cpp:831] Detecting new master
I0128 12:20:27.448473 297226240 slave.cpp:4681] Received oversubscribable resources  from the resource estimator
I0128 12:20:27.448545 300982272 master.cpp:5521] Authenticating slave(870)@192.168.178.24:51278
I0128 12:20:27.449328 297762816 authenticator.cpp:413] Starting authentication session for crammd5_authenticatee(1737)@192.168.178.24:51278
I0128 12:20:27.449581 298299392 authenticator.cpp:98] Creating new server SASL connection
I0128 12:20:27.449713 300982272 authenticatee.cpp:212] Received SASL authentication mechanisms: CRAM-MD5
I0128 12:20:27.449743 300982272 authenticatee.cpp:238] Attempting to authenticate with mechanism 'CRAM-MD5'
I0128 12:20:27.449923 297762816 authenticator.cpp:203] Received SASL authentication start
I0128 12:20:27.449990 297762816 authenticator.cpp:325] Authentication requires more steps
I0128 12:20:27.450142 300445696 authenticatee.cpp:258] Received SASL authentication step
I0128 12:20:27.450389 300982272 authenticator.cpp:231] Received SASL authentication step
I0128 12:20:27.450413 300982272 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: 'alexr.fritz.box' server FQDN: 'alexr.fritz.box' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0128 12:20:27.450489 300982272 auxprop.cpp:179] Looking up auxiliary property '*userPassword'
I0128 12:20:27.450513 300982272 auxprop.cpp:179] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0128 12:20:27.450531 300982272 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: 'alexr.fritz.box' server FQDN: 'alexr.fritz.box' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0128 12:20:27.450558 300982272 auxprop.cpp:129] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0128 12:20:27.451807 300982272 auxprop.cpp:129] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0128 12:20:27.451854 300982272 authenticator.cpp:317] Authentication success
I0128 12:20:27.451932 298299392 authenticatee.cpp:298] Authentication success
I0128 12:20:27.452075 298835968 master.cpp:5551] Successfully authenticated principal 'test-principal' at slave(870)@192.168.178.24:51278
I0128 12:20:27.452178 297762816 authenticator.cpp:431] Authentication session cleanup for crammd5_authenticatee(1737)@192.168.178.24:51278
I0128 12:20:27.452294 298299392 slave.cpp:926] Successfully authenticated with master master@192.168.178.24:51278
I0128 12:20:27.452376 298299392 slave.cpp:1320] Will retry registration in 15.028781ms if necessary
I0128 12:20:27.452481 297226240 master.cpp:4235] Registering slave at slave(870)@192.168.178.24:51278 (alexr.fritz.box) with id bd6f3479-18eb-478d-8a08-a41364ecbd05-S2
I0128 12:20:27.452783 298299392 registrar.cpp:439] Applied 1 operations in 86us; attempting to update the 'registry'
I0128 12:20:27.455883 298299392 log.cpp:683] Attempting to append 696 bytes to the log
I0128 12:20:27.455989 299909120 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 7
I0128 12:20:27.456845 300445696 replica.cpp:537] Replica received write request for position 7 from (18262)@192.168.178.24:51278
I0128 12:20:27.457120 300445696 leveldb.cpp:341] Persisting action (715 bytes) to leveldb took 276us
I0128 12:20:27.457268 300445696 replica.cpp:712] Persisted action at 7
I0128 12:20:27.459020 298835968 replica.cpp:691] Replica received learned notice for position 7 from @0.0.0.0:0
I0128 12:20:27.459255 298835968 leveldb.cpp:341] Persisting action (717 bytes) to leveldb took 210us
I0128 12:20:27.459290 298835968 replica.cpp:712] Persisted action at 7
I0128 12:20:27.459307 298835968 replica.cpp:697] Replica learned APPEND action at position 7
I0128 12:20:27.467411 299372544 registrar.cpp:484] Successfully updated the 'registry' in 12.083968ms
I0128 12:20:27.467475 300982272 log.cpp:702] Attempting to truncate the log to 7
I0128 12:20:27.467695 297226240 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 8
I0128 12:20:27.467697 300982272 slave.cpp:1320] Will retry registration in 16.052253ms if necessary
I0128 12:20:27.468121 297762816 slave.cpp:3435] Received ping from slave-observer(870)@192.168.178.24:51278
I0128 12:20:27.468437 299909120 master.cpp:4303] Registered slave bd6f3479-18eb-478d-8a08-a41364ecbd05-S2 at slave(870)@192.168.178.24:51278 (alexr.fritz.box) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I0128 12:20:27.468613 297226240 replica.cpp:537] Replica received write request for position 8 from (18263)@192.168.178.24:51278
I0128 12:20:27.468813 298299392 hierarchical.cpp:473] Added slave bd6f3479-18eb-478d-8a08-a41364ecbd05-S2 (alexr.fritz.box) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (allocated: )
I0128 12:20:27.469081 297226240 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 230us
I0128 12:20:27.469113 297226240 replica.cpp:712] Persisted action at 8
I0128 12:20:27.469173 299909120 master.cpp:4205] Slave bd6f3479-18eb-478d-8a08-a41364ecbd05-S2 at slave(870)@192.168.178.24:51278 (alexr.fritz.box) already registered, resending acknowledgement
I0128 12:20:27.469702 298299392 hierarchical.cpp:1403] No resources available to allocate!
I0128 12:20:27.470820 298299392 hierarchical.cpp:1116] Performed allocation for slave bd6f3479-18eb-478d-8a08-a41364ecbd05-S2 in 1920us
I0128 12:20:27.471000 299372544 slave.cpp:970] Registered with master master@192.168.178.24:51278; given slave ID bd6f3479-18eb-478d-8a08-a41364ecbd05-S2
I0128 12:20:27.471022 299372544 fetcher.cpp:81] Clearing fetcher cache
I0128 12:20:27.471740 299372544 slave.cpp:993] Checkpointing SlaveInfo to '/tmp/MasterQuotaTest_AvailableResourcesAfterRescinding_iHhziB/meta/slaves/bd6f3479-18eb-478d-8a08-a41364ecbd05-S2/slave.info'
I0128 12:20:27.471943 297762816 status_update_manager.cpp:181] Resuming sending status updates
I0128 12:20:27.472684 298835968 replica.cpp:691] Replica received learned notice for position 8 from @0.0.0.0:0
I0128 12:20:27.472903 298835968 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 201us
I0128 12:20:27.472951 298835968 leveldb.cpp:399] Deleting ~2 keys from leveldb took 27us
I0128 12:20:27.472970 298835968 replica.cpp:712] Persisted action at 8
I0128 12:20:27.473085 299372544 slave.cpp:1029] Forwarding total oversubscribed resources 
I0128 12:20:27.473274 298835968 replica.cpp:697] Replica learned TRUNCATE action at position 8
W0128 12:20:27.473520 299372544 slave.cpp:1015] Already registered with master master@192.168.178.24:51278
I0128 12:20:27.473541 299372544 slave.cpp:1029] Forwarding total oversubscribed resources 
I0128 12:20:27.473604 297226240 master.cpp:4644] Received update of slave bd6f3479-18eb-478d-8a08-a41364ecbd05-S2 at slave(870)@192.168.178.24:51278 (alexr.fritz.box) with total oversubscribed resources 
I0128 12:20:27.473798 297226240 master.cpp:4644] Received update of slave bd6f3479-18eb-478d-8a08-a41364ecbd05-S2 at slave(870)@192.168.178.24:51278 (alexr.fritz.box) with total oversubscribed resources 
I0128 12:20:27.473970 300982272 hierarchical.cpp:531] Slave bd6f3479-18eb-478d-8a08-a41364ecbd05-S2 (alexr.fritz.box) updated with oversubscribed resources  (total: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000], allocated: )
I0128 12:20:27.474051 300982272 hierarchical.cpp:1403] No resources available to allocate!
I0128 12:20:27.474071 300982272 hierarchical.cpp:1116] Performed allocation for slave bd6f3479-18eb-478d-8a08-a41364ecbd05-S2 in 74us
I0128 12:20:27.474261 300982272 hierarchical.cpp:531] Slave bd6f3479-18eb-478d-8a08-a41364ecbd05-S2 (alexr.fritz.box) updated with oversubscribed resources  (total: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000], allocated: )
I0128 12:20:27.474347 300982272 hierarchical.cpp:1403] No resources available to allocate!
I0128 12:20:27.474370 300982272 hierarchical.cpp:1116] Performed allocation for slave bd6f3479-18eb-478d-8a08-a41364ecbd05-S2 in 69us
I0128 12:20:27.476686 2080858880 sched.cpp:222] Version: 0.28.0
I0128 12:20:27.477182 299372544 sched.cpp:326] New master detected at master@192.168.178.24:51278
I0128 12:20:27.477264 299372544 sched.cpp:382] Authenticating with master master@192.168.178.24:51278
I0128 12:20:27.477284 299372544 sched.cpp:389] Using default CRAM-MD5 authenticatee
I0128 12:20:27.477462 298835968 authenticatee.cpp:121] Creating new client SASL connection
I0128 12:20:27.477645 298299392 master.cpp:5521] Authenticating scheduler-5f6db5d6-b088-4d13-a14c-94734bb39bf7@192.168.178.24:51278
I0128 12:20:27.477763 299909120 authenticator.cpp:413] Starting authentication session for crammd5_authenticatee(1738)@192.168.178.24:51278
I0128 12:20:27.477926 298299392 authenticator.cpp:98] Creating new server SASL connection
I0128 12:20:27.478040 300982272 authenticatee.cpp:212] Received SASL authentication mechanisms: CRAM-MD5
I0128 12:20:27.478076 300982272 authenticatee.cpp:238] Attempting to authenticate with mechanism 'CRAM-MD5'
I0128 12:20:27.478152 298299392 authenticator.cpp:203] Received SASL authentication start
I0128 12:20:27.478196 298299392 authenticator.cpp:325] Authentication requires more steps
I0128 12:20:27.478247 298299392 authenticatee.cpp:258] Received SASL authentication step
I0128 12:20:27.478337 299909120 authenticator.cpp:231] Received SASL authentication step
I0128 12:20:27.478356 299909120 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: 'alexr.fritz.box' server FQDN: 'alexr.fritz.box' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0128 12:20:27.478369 299909120 auxprop.cpp:179] Looking up auxiliary property '*userPassword'
I0128 12:20:27.478392 299909120 auxprop.cpp:179] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0128 12:20:27.478462 299909120 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: 'alexr.fritz.box' server FQDN: 'alexr.fritz.box' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0128 12:20:27.478478 299909120 auxprop.cpp:129] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0128 12:20:27.478487 299909120 auxprop.cpp:129] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0128 12:20:27.478502 299909120 authenticator.cpp:317] Authentication success
I0128 12:20:27.478562 300982272 authenticatee.cpp:298] Authentication success
I0128 12:20:27.478618 297226240 master.cpp:5551] Successfully authenticated principal 'test-principal' at scheduler-5f6db5d6-b088-4d13-a14c-94734bb39bf7@192.168.178.24:51278
I0128 12:20:27.478713 298835968 authenticator.cpp:431] Authentication session cleanup for crammd5_authenticatee(1738)@192.168.178.24:51278
I0128 12:20:27.478826 297762816 sched.cpp:471] Successfully authenticated with master master@192.168.178.24:51278
I0128 12:20:27.478871 297762816 sched.cpp:780] Sending SUBSCRIBE call to master@192.168.178.24:51278
I0128 12:20:27.478971 297762816 sched.cpp:813] Will retry registration in 250.550205ms if necessary
I0128 12:20:27.479032 298835968 master.cpp:2278] Received SUBSCRIBE call for framework 'framework(18264)' at scheduler-5f6db5d6-b088-4d13-a14c-94734bb39bf7@192.168.178.24:51278
W0128 12:20:27.479054 298835968 master.cpp:2285] Framework at scheduler-5f6db5d6-b088-4d13-a14c-94734bb39bf7@192.168.178.24:51278 (authenticated as 'test-principal') does not set 'principal' in FrameworkInfo
I0128 12:20:27.479069 298835968 master.cpp:1749] Authorizing framework principal '' to receive offers for role 'role1'
I0128 12:20:27.479228 298835968 master.cpp:2349] Subscribing framework framework(18264) with checkpointing disabled and capabilities [  ]
I0128 12:20:27.479449 300445696 hierarchical.cpp:265] Added framework framework(18264)
I0128 12:20:27.479570 298299392 sched.cpp:707] Framework registered with framework(18264)
I0128 12:20:27.479625 298299392 sched.cpp:721] Scheduler::registered took 45us
W0128 12:20:27.479655 297226240 slave.cpp:2236] Ignoring updating pid for framework framework(18264) because it does not exist
W0128 12:20:27.479701 297762816 slave.cpp:2236] Ignoring updating pid for framework framework(18264) because it does not exist
W0128 12:20:27.479756 297226240 slave.cpp:2236] Ignoring updating pid for framework framework(18264) because it does not exist
I0128 12:20:27.480363 300445696 hierarchical.cpp:1498] No inverse offers to send out!
I0128 12:20:27.480406 300445696 hierarchical.cpp:1096] Performed allocation for 3 slaves in 948us
I0128 12:20:27.480887 300982272 master.cpp:5350] Sending 3 offers to framework framework(18264) (framework(18264)) at scheduler-5f6db5d6-b088-4d13-a14c-94734bb39bf7@192.168.178.24:51278
I0128 12:20:27.481411 299909120 sched.cpp:877] Scheduler::resourceOffers took 180us
I0128 12:20:27.485344 300445696 hierarchical.cpp:1403] No resources available to allocate!
I0128 12:20:27.485376 300445696 hierarchical.cpp:1498] No inverse offers to send out!
I0128 12:20:27.485397 300445696 hierarchical.cpp:1096] Performed allocation for 3 slaves in 135us
I0128 12:20:27.486169 2080858880 sched.cpp:222] Version: 0.28.0
I0128 12:20:27.486497 300445696 sched.cpp:326] New master detected at master@192.168.178.24:51278
I0128 12:20:27.486548 300445696 sched.cpp:382] Authenticating with master master@192.168.178.24:51278
I0128 12:20:27.486562 300445696 sched.cpp:389] Using default CRAM-MD5 authenticatee
I0128 12:20:27.486661 299909120 authenticatee.cpp:121] Creating new client SASL connection
I0128 12:20:27.486810 297226240 master.cpp:5521] Authenticating scheduler-ee0b23b6-ac62-4d64-86ce-daf6a4e30f92@192.168.178.24:51278
I0128 12:20:27.486898 300982272 authenticator.cpp:413] Starting authentication session for crammd5_authenticatee(1739)@192.168.178.24:51278
I0128 12:20:27.487023 299909120 authenticator.cpp:98] Creating new server SASL connection
I0128 12:20:27.487139 297762816 authenticatee.cpp:212] Received SASL authentication mechanisms: CRAM-MD5
I0128 12:20:27.487190 297762816 authenticatee.cpp:238] Attempting to authenticate with mechanism 'CRAM-MD5'
I0128 12:20:27.487296 300445696 authenticator.cpp:203] Received SASL authentication start
I0128 12:20:27.487432 300445696 authenticator.cpp:325] Authentication requires more steps
I0128 12:20:27.487490 300982272 authenticatee.cpp:258] Received SASL authentication step
I0128 12:20:27.488359 299909120 authenticator.cpp:231] Received SASL authentication step
I0128 12:20:27.488391 299909120 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: 'alexr.fritz.box' server FQDN: 'alexr.fritz.box' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0128 12:20:27.488405 299909120 auxprop.cpp:179] Looking up auxiliary property '*userPassword'
I0128 12:20:27.488428 299909120 auxprop.cpp:179] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0128 12:20:27.488445 299909120 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: 'alexr.fritz.box' server FQDN: 'alexr.fritz.box' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0128 12:20:27.488471 299909120 auxprop.cpp:129] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0128 12:20:27.488481 299909120 auxprop.cpp:129] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0128 12:20:27.488507 2080858880 sched.cpp:222] Version: 0.28.0
I0128 12:20:27.488694 299909120 authenticator.cpp:317] Authentication success
I0128 12:20:27.488770 300445696 authenticatee.cpp:298] Authentication success
I0128 12:20:27.488823 298299392 master.cpp:5551] Successfully authenticated principal 'test-principal' at scheduler-ee0b23b6-ac62-4d64-86ce-daf6a4e30f92@192.168.178.24:51278
I0128 12:20:27.488911 298835968 authenticator.cpp:431] Authentication session cleanup for crammd5_authenticatee(1739)@192.168.178.24:51278
I0128 12:20:27.489013 298299392 sched.cpp:326] New master detected at master@192.168.178.24:51278
I0128 12:20:27.489037 297762816 sched.cpp:471] Successfully authenticated with master master@192.168.178.24:51278
I0128 12:20:27.489061 297762816 sched.cpp:780] Sending SUBSCRIBE call to master@192.168.178.24:51278
I0128 12:20:27.489102 298299392 sched.cpp:382] Authenticating with master master@192.168.178.24:51278
I0128 12:20:27.489122 298299392 sched.cpp:389] Using default CRAM-MD5 authenticatee
I0128 12:20:27.489151 297762816 sched.cpp:813] Will retry registration in 955.991763ms if necessary
I0128 12:20:27.489215 299909120 master.cpp:2278] Received SUBSCRIBE call for framework 'framework(18265)' at scheduler-ee0b23b6-ac62-4d64-86ce-daf6a4e30f92@192.168.178.24:51278
W0128 12:20:27.489248 299909120 master.cpp:2285] Framework at scheduler-ee0b23b6-ac62-4d64-86ce-daf6a4e30f92@192.168.178.24:51278 (authenticated as 'test-principal') does not set 'principal' in FrameworkInfo
I0128 12:20:27.489266 299909120 master.cpp:1749] Authorizing framework principal '' to receive offers for role 'role2'
I0128 12:20:27.489315 297762816 authenticatee.cpp:121] Creating new client SASL connection
I0128 12:20:27.489439 299909120 master.cpp:5521] Authenticating scheduler-a5c3a776-b3e5-42dc-a9fa-63686dca3249@192.168.178.24:51278
I0128 12:20:27.489513 299372544 authenticator.cpp:413] Starting authentication session for crammd5_authenticatee(1740)@192.168.178.24:51278
I0128 12:20:27.489545 299909120 master.cpp:2349] Subscribing framework framework(18265) with checkpointing disabled and capabilities [  ]
I0128 12:20:27.489681 298835968 authenticator.cpp:98] Creating new server SASL connection
I0128 12:20:27.489807 298299392 authenticatee.cpp:212] Received SASL authentication mechanisms: CRAM-MD5
I0128 12:20:27.489835 298299392 authenticatee.cpp:238] Attempting to authenticate with mechanism 'CRAM-MD5'
I0128 12:20:27.489869 299372544 hierarchical.cpp:265] Added framework framework(18265)
I0128 12:20:27.489958 298299392 authenticator.cpp:203] Received SASL authentication start
I0128 12:20:27.489969 297762816 sched.cpp:707] Framework registered with framework(18265)
I0128 12:20:27.489982 299372544 hierarchical.cpp:1403] No resources available to allocate!
I0128 12:20:27.490000 298299392 authenticator.cpp:325] Authentication requires more steps
I0128 12:20:27.490015 299372544 hierarchical.cpp:1498] No inverse offers to send out!
I0128 12:20:27.490034 299372544 hierarchical.cpp:1096] Performed allocation for 3 slaves in 151us
I0128 12:20:27.490067 297762816 sched.cpp:721] Scheduler::registered took 92us
W0128 12:20:27.490088 300445696 slave.cpp:2236] Ignoring updating pid for framework framework(18265) because it does not exist
I0128 12:20:27.490102 297226240 authenticatee.cpp:258] Received SASL authentication step
W0128 12:20:27.490145 298299392 slave.cpp:2236] Ignoring updating pid for framework framework(18265) because it does not exist
I0128 12:20:27.490262 300445696 authenticator.cpp:231] Received SASL authentication step
W0128 12:20:27.490279 298299392 slave.cpp:2236] Ignoring updating pid for framework framework(18265) because it does not exist
I0128 12:20:27.490291 300445696 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: 'alexr.fritz.box' server FQDN: 'alexr.fritz.box' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0128 12:20:27.490304 300445696 auxprop.cpp:179] Looking up auxiliary property '*userPassword'
I0128 12:20:27.490324 300445696 auxprop.cpp:179] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0128 12:20:27.490341 300445696 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: 'alexr.fritz.box' server FQDN: 'alexr.fritz.box' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0128 12:20:27.490352 300445696 auxprop.cpp:129] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0128 12:20:27.490362 300445696 auxprop.cpp:129] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0128 12:20:27.490375 300445696 authenticator.cpp:317] Authentication success
I0128 12:20:27.490464 299372544 authenticatee.cpp:298] Authentication success
I0128 12:20:27.490517 297226240 master.cpp:5551] Successfully authenticated principal 'test-principal' at scheduler-a5c3a776-b3e5-42dc-a9fa-63686dca3249@192.168.178.24:51278
I0128 12:20:27.490598 299909120 authenticator.cpp:431] Authentication session cleanup for crammd5_authenticatee(1740)@192.168.178.24:51278
I0128 12:20:27.490722 298299392 sched.cpp:471] Successfully authenticated with master master@192.168.178.24:51278
I0128 12:20:27.490741 298299392 sched.cpp:780] Sending SUBSCRIBE call to master@192.168.178.24:51278
I0128 12:20:27.490811 298299392 sched.cpp:813] Will retry registration in 922.212316ms if necessary
I0128 12:20:27.490871 300982272 master.cpp:2278] Received SUBSCRIBE call for framework 'framework(18266)' at scheduler-a5c3a776-b3e5-42dc-a9fa-63686dca3249@192.168.178.24:51278
W0128 12:20:27.490893 300982272 master.cpp:2285] Framework at scheduler-a5c3a776-b3e5-42dc-a9fa-63686dca3249@192.168.178.24:51278 (authenticated as 'test-principal') does not set 'principal' in FrameworkInfo
I0128 12:20:27.490907 300982272 master.cpp:1749] Authorizing framework principal '' to receive offers for role 'role2'
I0128 12:20:27.491086 299909120 master.cpp:2349] Subscribing framework framework(18266) with checkpointing disabled and capabilities [  ]
I0128 12:20:27.491291 297226240 hierarchical.cpp:265] Added framework framework(18266)
I0128 12:20:27.491345 300445696 sched.cpp:707] Framework registered with framework(18266)
I0128 12:20:27.491374 297226240 hierarchical.cpp:1403] No resources available to allocate!
W0128 12:20:27.491379 300982272 slave.cpp:2236] Ignoring updating pid for framework framework(18266) because it does not exist
I0128 12:20:27.491402 297226240 hierarchical.cpp:1498] No inverse offers to send out!
I0128 12:20:27.491418 297226240 hierarchical.cpp:1096] Performed allocation for 3 slaves in 116us
I0128 12:20:27.491427 300445696 sched.cpp:721] Scheduler::registered took 73us
W0128 12:20:27.491439 299372544 slave.cpp:2236] Ignoring updating pid for framework framework(18266) because it does not exist
W0128 12:20:27.491523 297762816 slave.cpp:2236] Ignoring updating pid for framework framework(18266) because it does not exist
I0128 12:20:27.491674 2080858880 resources.cpp:564] Parsing resources as JSON failed: cpus:1;mem:512
Trying semicolon-delimited string format instead
I0128 12:20:27.493041 297226240 process.cpp:3141] Handling HTTP event for process 'master' with path: '/master/quota'
I0128 12:20:27.493536 299909120 http.cpp:503] HTTP POST for /master/quota from 192.168.178.24:51640
I0128 12:20:27.493561 299909120 quota_handler.cpp:211] Setting quota from request: '{""force"":false,""guarantee"":[{""name"":""cpus"",""role"":""*"",""scalar"":{""value"":1.0},""type"":""SCALAR""},{""name"":""mem"",""role"":""*"",""scalar"":{""value"":512.0},""type"":""SCALAR""}],""role"":""role2""}'
I0128 12:20:27.493954 299909120 quota_handler.cpp:446] Authorizing principal 'test-principal' to request quota for role 'role2'
I0128 12:20:27.494176 299909120 quota_handler.cpp:70] Performing capacity heuristic check for a set quota request
I0128 12:20:27.494499 300445696 registrar.cpp:439] Applied 1 operations in 80us; attempting to update the 'registry'
I0128 12:20:27.494951 297226240 log.cpp:683] Attempting to append 770 bytes to the log
I0128 12:20:27.495055 298299392 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 9
I0128 12:20:27.495833 297226240 replica.cpp:537] Replica received write request for position 9 from (18269)@192.168.178.24:51278
I0128 12:20:27.496049 297226240 leveldb.cpp:341] Persisting action (789 bytes) to leveldb took 197us
I0128 12:20:27.496085 297226240 replica.cpp:712] Persisted action at 9
I0128 12:20:27.496620 298835968 replica.cpp:691] Replica received learned notice for position 9 from @0.0.0.0:0
I0128 12:20:27.496810 298835968 leveldb.cpp:341] Persisting action (791 bytes) to leveldb took 177us
I0128 12:20:27.496841 298835968 replica.cpp:712] Persisted action at 9
I0128 12:20:27.496857 298835968 replica.cpp:697] Replica learned APPEND action at position 9
I0128 12:20:27.498211 300445696 registrar.cpp:484] Successfully updated the 'registry' in 3.668992ms
I0128 12:20:27.498492 300982272 log.cpp:702] Attempting to truncate the log to 9
I0128 12:20:27.498584 298835968 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 10
I0128 12:20:27.498679 298299392 hierarchical.cpp:1024] Set quota cpus(*):1; mem(*):512 for role 'role2'
I0128 12:20:27.499006 300982272 sched.cpp:903] Rescinded offer bd6f3479-18eb-478d-8a08-a41364ecbd05-O2
I0128 12:20:27.499060 300982272 sched.cpp:914] Scheduler::offerRescinded took 34us
I0128 12:20:27.499174 298299392 hierarchical.cpp:1403] No resources available to allocate!
I0128 12:20:27.499207 298299392 hierarchical.cpp:1498] No inverse offers to send out!
I0128 12:20:27.499222 299372544 replica.cpp:537] Replica received write request for position 10 from (18270)@192.168.178.24:51278
I0128 12:20:27.499229 298299392 hierarchical.cpp:1096] Performed allocation for 3 slaves in 516us
I0128 12:20:27.499322 300445696 sched.cpp:903] Rescinded offer bd6f3479-18eb-478d-8a08-a41364ecbd05-O1
I0128 12:20:27.499373 300445696 sched.cpp:914] Scheduler::offerRescinded took 35us
I0128 12:20:27.499429 299372544 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 194us
I0128 12:20:27.499469 299372544 replica.cpp:712] Persisted action at 10
I0128 12:20:27.499477 298299392 hierarchical.cpp:892] Recovered cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (total: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000], allocated: ) on slave bd6f3479-18eb-478d-8a08-a41364ecbd05-S2 from framework framework(18264)
I0128 12:20:27.499708 298299392 hierarchical.cpp:892] Recovered cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (total: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000], allocated: ) on slave bd6f3479-18eb-478d-8a08-a41364ecbd05-S1 from framework framework(18264)
I0128 12:20:27.500056 299909120 replica.cpp:691] Replica received learned notice for position 10 from @0.0.0.0:0
I0128 12:20:27.500241 299909120 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 175us
I0128 12:20:27.500306 299909120 leveldb.cpp:399] Deleting ~2 keys from leveldb took 34us
I0128 12:20:27.500331 299909120 replica.cpp:712] Persisted action at 10
I0128 12:20:27.500358 299909120 replica.cpp:697] Replica learned TRUNCATE action at position 10
I0128 12:20:27.501042 297226240 master.cpp:1025] Master terminating
I0128 12:20:27.501637 298835968 hierarchical.cpp:505] Removed slave bd6f3479-18eb-478d-8a08-a41364ecbd05-S2
I0128 12:20:27.501849 300982272 hierarchical.cpp:505] Removed slave bd6f3479-18eb-478d-8a08-a41364ecbd05-S1
I0128 12:20:27.502151 300982272 hierarchical.cpp:505] Removed slave bd6f3479-18eb-478d-8a08-a41364ecbd05-S0
I0128 12:20:27.502595 300982272 hierarchical.cpp:326] Removed framework framework(18266)
I0128 12:20:27.502773 299909120 hierarchical.cpp:326] Removed framework framework(18265)
I0128 12:20:27.503046 299909120 hierarchical.cpp:326] Removed framework framework(18264)
I0128 12:20:27.503330 299372544 slave.cpp:3481] master@192.168.178.24:51278 exited
W0128 12:20:27.503353 299372544 slave.cpp:3484] Master disconnected! Waiting for a new master to be elected
I0128 12:20:27.503422 299372544 slave.cpp:3481] master@192.168.178.24:51278 exited
W0128 12:20:27.503446 299372544 slave.cpp:3484] Master disconnected! Waiting for a new master to be elected
I0128 12:20:27.503530 299372544 slave.cpp:3481] master@192.168.178.24:51278 exited
W0128 12:20:27.503548 299372544 slave.cpp:3484] Master disconnected! Waiting for a new master to be elected
I0128 12:20:27.512778 2080858880 slave.cpp:667] Slave terminating
I0128 12:20:27.515347 2080858880 slave.cpp:667] Slave terminating
I0128 12:20:27.517685 2080858880 slave.cpp:667] Slave terminating
[       OK ] MasterQuotaTest.AvailableResourcesAfterRescinding (205 ms)
{code}",Bug,Major,alexr,2016-02-11T00:39:13.000+0000,5,Resolved,Complete,MasterQuotaTest.AvailableResourcesAfterRescinding is flaky.,2016-02-11T00:39:13.000+0000,MESOS-4542,3.0,mesos,Mesosphere Sprint 28
avinash@mesosphere.io,2016-01-28T01:40:23.000+0000,greggomann,"This test fails in my CentOS 6 VM due to a cgroups issue:

{code}
[ RUN      ] NetClsIsolatorTest.ROOT_CGROUPS_NetClsIsolate
I0127 19:15:06.637328 25347 exec.cpp:134] Version: 0.28.0
I0127 19:15:06.648378 25378 exec.cpp:208] Executor registered on slave 6edafba0-9dbd-4e6e-b10e-c6f935e58d41-S0
Registered executor on localhost
Starting task b745d88e-3fbe-4af9-80b3-e43484e37acf
sh -c 'sleep 1000'
Forked command at 25385
../../src/tests/containerizer/isolator_tests.cpp:926: Failure
pids: Failed to read cgroups control 'cgroup.procs': '/sys/fs/cgroup/net_cls' is not a valid hierarchy
I0127 19:15:06.662083 25376 exec.cpp:381] Executor asked to shutdown
Shutting down
Sending SIGTERM to process tree at pid 25385
[  FAILED  ] NetClsIsolatorTest.ROOT_CGROUPS_NetClsIsolate (335 ms)
{code}",Bug,Major,greggomann,2016-01-28T01:56:19.000+0000,5,Resolved,Complete,NetClsIsolatorTest.ROOT_CGROUPS_NetClsIsolate fails on CentOS 6,2016-01-29T04:45:22.000+0000,MESOS-4540,1.0,mesos,Mesosphere Sprint 27
jieyu,2016-01-28T01:13:11.000+0000,jieyu,"Since du --exclude uses pattern matching. A relative path might accidentally matches an irrelevant directory/file. For instance,

{noformat}
/tmp/testpath $ tree
.
├── aaa
│   └── exc
│       └── file
└── exc
    └── file

3 directories, 2 files
/tmp/testpath $ du --exclude /tmp/testpath/exc /tmp/testpath/
8    /tmp/testpath/aaa/exc
12    /tmp/testpath/aaa
16    /tmp/testpath/
/tmp/testpath $ du --exclude exc /tmp/testpath/
4    /tmp/testpath/aaa
8    /tmp/testpath/
/tmp/testpath $
{noformat}",Task,Major,jieyu,2016-01-28T02:25:35.000+0000,5,Resolved,Complete,Exclude paths in Posix disk isolator should be absolute paths.,2016-01-29T02:07:05.000+0000,MESOS-4539,2.0,mesos,Mesosphere Sprint 27
kaysoky,2016-01-27T23:07:58.000+0000,kaysoky,"Libprocess currently manages file descriptors as plain {{int}} s.  This leads to some easily missed bugs regarding duplicated or closed FDs.

We should introduce an abstraction (like {{unique_ptr}} and {{shared_ptr}}) so that FD ownership can be expressed alongside the affected code.",Improvement,Major,kaysoky,,1,Open,New,"Add abstractions of ""owned"" and ""shared"" file descriptors to libprocess.",2016-01-28T00:20:23.000+0000,MESOS-4536,3.0,mesos,
kaysoky,2016-01-27T20:12:33.000+0000,kaysoky,"One of the patches for [MESOS-4136] introduced the {{FDType::OWNED}} enum for {{Subprocess::IO::FD}}.

The way the logrotate module uses this is slightly incorrect:
# The module starts a subprocess with an output {{Subprocess::PIPE()}}.
# That pipe's FD is passed into another subprocess via {{Subprocess::IO::FD(pipe, IO::OWNED)}}.
# When the second subprocess starts, the pipe's FD is closed in the parent.
# When the first subprocess terminates, the existing code will try to close the pipe again.  This effectively closes a random FD.",Bug,Blocker,kaysoky,2016-01-28T00:26:36.000+0000,5,Resolved,Complete,Logrotate ContainerLogger may not handle FD ownership correctly,2016-01-28T03:56:41.000+0000,MESOS-4535,1.0,mesos,Mesosphere Sprint 27
mcypark,2016-01-27T19:54:06.000+0000,jvanremoortere,"The {{Resources}} object current allows mutation of it's internal state through the public mutable iterator interface.
This can cause issues when the mutation involved stripping certain qualifiers on a {{Resource}}, as they will not be summed together at the end of the mutation (even though they should be).

The {{contains()}} math will not work correctly if two {{addable}} resources are not summed together on the {{lhs}} of the contains check.",Bug,Blocker,jvanremoortere,2016-01-28T00:38:53.000+0000,5,Resolved,Complete,Resources object can be mutated through the public API,2016-01-28T00:38:53.000+0000,MESOS-4534,3.0,mesos,Mesosphere Sprint 27
avinash@mesosphere.io,2016-01-27T14:40:13.000+0000,arojas,"While running the command
{noformat}
sudo ./bin/mesos-tests.sh --gtest_filter=""-CgroupsAnyHierarchyWithCpuMemoryTest.ROOT_CGROUPS_Listen:CgroupsAnyHierarchyMemoryPressureTest.ROOT_IncreaseRSS"" --gtest_repeat=10 --gtest_break_on_failure
{noformat}
One eventually gets the following output:
{noformat}
[ RUN      ] NetClsIsolatorTest.ROOT_CGROUPS_NetClsIsolate
../../src/tests/containerizer/isolator_tests.cpp:870: Failure
containerizer: Could not create isolator 'cgroups/net_cls': Unexpected subsystems found attached to the hierarchy /sys/fs/cgroup/net_cls,net_prio
[  FAILED  ] NetClsIsolatorTest.ROOT_CGROUPS_NetClsIsolate (75 ms)
{noformat}",Bug,Major,arojas,2016-01-28T01:57:41.000+0000,5,Resolved,Complete,NetClsIsolatorTest.ROOT_CGROUPS_NetClsIsolate is flaky,2016-01-29T04:44:33.000+0000,MESOS-4530,1.0,mesos,Mesosphere Sprint 27
mcypark,2016-01-27T05:15:14.000+0000,mcypark,"Eventually, we will want to offer unreserved resources as revocable beyond the role's quota. Rather than offering non-revocable resources beyond the role's quota's guarantee, in the short term, we choose to not offer resources beyond a role's quota.",Task,Major,mcypark,2016-01-27T05:16:14.000+0000,5,Resolved,Complete,Update the allocator to not offer unreserved resources beyond quota.,2016-01-27T05:16:14.000+0000,MESOS-4529,2.0,mesos,Mesosphere Sprint 27
mcypark,2016-01-27T05:07:28.000+0000,mcypark,Reserved resources should be accounted for in the quota guarantee check so that frameworks cannot continually reserve resources to pull them out of the quota pool.,Task,Major,mcypark,2016-01-27T05:08:40.000+0000,5,Resolved,Complete,Account for reserved resources in the quota guarantee check.,2016-01-27T05:11:53.000+0000,MESOS-4528,2.0,mesos,Mesosphere Sprint 27
mcypark,2016-01-27T05:03:30.000+0000,mcypark,"Similar to MESOS-4526, reserved resources should be accounted for in the quota role sorter regardless of their allocation state. In the short-term, we should at least account them if they are allocated.",Task,Major,mcypark,2016-01-27T05:04:05.000+0000,5,Resolved,Complete,Include allocated portion of the reserved resources in the quota role sorter for DRF.,2016-01-27T05:11:57.000+0000,MESOS-4527,1.0,mesos,Mesosphere Sprint 27
mcypark,2016-01-27T04:55:27.000+0000,mcypark,"Reserved resources should be accounted for fairness calculation whether they are allocated or not, since they model a long or forever running task. That is, the effect of reserving resources is equivalent to launching a task in that the resources that make up the reservation are not available to other roles as non-revocable.

In the short-term, we should at least account for the allocated portion of the reservation.",Task,Major,mcypark,2016-01-27T04:57:22.000+0000,5,Resolved,Complete,Include the allocated portion of reserved resources in the role sorter for DRF.,2016-01-27T05:12:01.000+0000,MESOS-4526,1.0,mesos,Mesosphere Sprint 27
vinodkone,2016-01-27T01:16:09.000+0000,vinodkone,It would be nice to enable benchmark tests in the ASF CI so that we can catch performance regressions (esp. during releases).,Improvement,Major,vinodkone,2016-01-29T22:15:08.000+0000,5,Resolved,Complete,Enable benchmark tests in ASF CI,2016-01-29T22:15:08.000+0000,MESOS-4523,3.0,mesos,Mesosphere Sprint 27
avinash@mesosphere.io,2016-01-26T21:53:02.000+0000,avinash@mesosphere.io,"While launching a container mesos isolators end up configuring/modifying various properties of the container. For e.g., cgroup isolators (mem, cpu, net_cls) configure/change the properties associated with their respective subsystems before launching a container. Similary network isolator (net-modules, port mapping) configure the IP address and ports associated with a container. 

Currently, there are not interface in the isolator to extract the run time state of these properties for a given container. Therefore a status() method needs to be implemented in the isolators to allow the containerizer to extract the container status information from the isolator. ",Improvement,Major,avinash@mesosphere.io,2016-01-28T18:26:47.000+0000,5,Resolved,Complete,Introduce a status() interface for isolators,2016-01-28T18:27:11.000+0000,MESOS-4520,1.0,mesos,Mesosphere Sprint 27
gilbert,2016-01-26T21:35:48.000+0000,gilbert,"Currently docker image default configuration are included in `ProvisionInfo`. We should grab necessary config from `ProvisionInfo` into `ContainerInfo`, and handle all these runtime informations inside of docker runtime isolator. Return a `ContainerLaunchInfo` containing `working_dir`, `env` and merged `commandInfo`, etc.",Bug,Major,gilbert,2016-02-05T21:47:05.000+0000,5,Resolved,Complete,Introduce docker runtime isolator.,2016-02-05T21:47:05.000+0000,MESOS-4517,3.0,mesos,Mesosphere Sprint 28
kaysoky,2016-01-26T17:39:04.000+0000,tillt,"{noformat}
[17:24:58][Step 7/7] logrotate: bad argument --version: unknown error
[17:24:58][Step 7/7] F0126 17:24:57.913729  4503 container_logger_tests.cpp:380] CHECK_SOME(containerizer): Failed to create container logger: Failed to create container logger module 'org_apache_mesos_LogrotateContainerLogger': Error creating Module instance for 'org_apache_mesos_LogrotateContainerLogger' 
[17:24:58][Step 7/7] *** Check failure stack trace: ***
[17:24:58][Step 7/7]     @     0x7f11ae0d2d40  google::LogMessage::Fail()
[17:24:58][Step 7/7]     @     0x7f11ae0d2c9c  google::LogMessage::SendToLog()
[17:24:58][Step 7/7]     @     0x7f11ae0d2692  google::LogMessage::Flush()
[17:24:58][Step 7/7]     @     0x7f11ae0d544c  google::LogMessageFatal::~LogMessageFatal()
[17:24:58][Step 7/7]     @           0x983927  _CheckFatal::~_CheckFatal()
[17:24:58][Step 7/7]     @           0xa9a18b  mesos::internal::tests::ContainerLoggerTest_LOGROTATE_RotateInSandbox_Test::TestBody()
[17:24:58][Step 7/7]     @          0x1623a4e  testing::internal::HandleSehExceptionsInMethodIfSupported<>()
[17:24:58][Step 7/7]     @          0x161eab2  testing::internal::HandleExceptionsInMethodIfSupported<>()
[17:24:58][Step 7/7]     @          0x15ffdfd  testing::Test::Run()
[17:24:58][Step 7/7]     @          0x160058b  testing::TestInfo::Run()
[17:24:58][Step 7/7]     @          0x1600bc6  testing::TestCase::Run()
[17:24:58][Step 7/7]     @          0x1607515  testing::internal::UnitTestImpl::RunAllTests()
[17:24:58][Step 7/7]     @          0x16246dd  testing::internal::HandleSehExceptionsInMethodIfSupported<>()
[17:24:58][Step 7/7]     @          0x161f608  testing::internal::HandleExceptionsInMethodIfSupported<>()
[17:24:58][Step 7/7]     @          0x1606245  testing::UnitTest::Run()
[17:24:58][Step 7/7]     @           0xde36b6  RUN_ALL_TESTS()
[17:24:58][Step 7/7]     @           0xde32cc  main
[17:24:58][Step 7/7]     @     0x7f11a8896d5d  __libc_start_main
[17:24:58][Step 7/7]     @           0x981fc9  (unknown)
{noformat}",Bug,Blocker,tillt,2016-01-27T00:01:47.000+0000,5,Resolved,Complete,ContainerLoggerTest.LOGROTATE_RotateInSandbox breaks when running on Centos6.,2016-02-01T18:39:34.000+0000,MESOS-4515,1.0,mesos,Mesosphere Sprint 27
tillt,2016-01-26T16:41:46.000+0000,tillt,"When building the current master, the following happens when using gcc-4.9:

{noformat}
mv -f examples/.deps/persistent_volume_framework-persistent_volume_framework.Tpo examples/.deps/persistent_volume_framework-persistent_volume_framework.Po
g++-4.9 -DPACKAGE_NAME=\""mesos\"" -DPACKAGE_TARNAME=\""mesos\"" -DPACKAGE_VERSION=\""0.27.0\"" -DPACKAGE_STRING=\""mesos\ 0.27.0\"" -DPACKAGE_BUGREPORT=\""\"" -DPACKAGE_URL=\""\"" -DPACKAGE=\""mesos\"" -DVERSION=\""0.27.0\"" -DSTDC_HEADERS=1 -DHAVE_SYS_TYPES_H=1 -DHAVE_SYS_STAT_H=1 -DHAVE_STDLIB_H=1 -DHAVE_STRING_H=1 -DHAVE_MEMORY_H=1 -DHAVE_STRINGS_H=1 -DHAVE_INTTYPES_H=1 -DHAVE_STDINT_H=1 -DHAVE_UNISTD_H=1 -DHAVE_DLFCN_H=1 -DLT_OBJDIR=\"".libs/\"" -DHAVE_PTHREAD_PRIO_INHERIT=1 -DHAVE_PTHREAD=1 -DHAVE_LIBZ=1 -DHAVE_LIBCURL=1 -DHAVE_APR_POOLS_H=1 -DHAVE_LIBAPR_1=1 -DHAVE_SVN_VERSION_H=1 -DHAVE_LIBSVN_SUBR_1=1 -DHAVE_SVN_DELTA_H=1 -DHAVE_LIBSVN_DELTA_1=1 -DHAVE_LIBSASL2=1 -I. -I../../src   -Wall -Werror -DLIBDIR=\""/usr/local/lib\"" -DPKGLIBEXECDIR=\""/usr/local/libexec/mesos\"" -DPKGDATADIR=\""/usr/local/share/mesos\"" -I../../include -I../../3rdparty/libprocess/include -I../../3rdparty/libprocess/3rdparty/stout/include -I../include -I../include/mesos -isystem ../3rdparty/libprocess/3rdparty/boost-1.53.0 -I../3rdparty/libprocess/3rdparty/picojson-1.3.0 -DPICOJSON_USE_INT64 -D__STDC_FORMAT_MACROS -I../3rdparty/libprocess/3rdparty/protobuf-2.5.0/src -I../3rdparty/libprocess/3rdparty/glog-0.3.3/src -I../3rdparty/libprocess/3rdparty/glog-0.3.3/src -I../3rdparty/leveldb/include -I../3rdparty/zookeeper-3.4.5/src/c/include -I../3rdparty/zookeeper-3.4.5/src/c/generated -I../3rdparty/libprocess/3rdparty/protobuf-2.5.0/src -DSOURCE_DIR=\""/Users/till/Development/mesos-private/build/..\"" -DBUILD_DIR=\""/Users/till/Development/mesos-private/build\"" -I../3rdparty/libprocess/3rdparty/gmock-1.7.0/gtest/include -I../3rdparty/libprocess/3rdparty/gmock-1.7.0/include  -I/usr/local/opt/openssl/include -I/usr/local/opt/libevent/include -I/usr/local/opt/subversion/include/subversion-1 -I/usr/include/apr-1 -I/usr/include/apr-1.0  -D_THREAD_SAFE -pthread -g1 -O0 -Wno-unused-local-typedefs -std=c++11 -DGTEST_USE_OWN_TR1_TUPLE=1 -DGTEST_LANG_CXX11 -MT tests/mesos_tests-container_logger_tests.o -MD -MP -MF tests/.deps/mesos_tests-container_logger_tests.Tpo -c -o tests/mesos_tests-container_logger_tests.o `test -f 'tests/container_logger_tests.cpp' || echo '../../src/'`tests/container_logger_tests.cpp
mv -f slave/qos_controllers/.deps/mesos_tests-load.Tpo slave/qos_controllers/.deps/mesos_tests-load.Po
g++-4.9 -DPACKAGE_NAME=\""mesos\"" -DPACKAGE_TARNAME=\""mesos\"" -DPACKAGE_VERSION=\""0.27.0\"" -DPACKAGE_STRING=\""mesos\ 0.27.0\"" -DPACKAGE_BUGREPORT=\""\"" -DPACKAGE_URL=\""\"" -DPACKAGE=\""mesos\"" -DVERSION=\""0.27.0\"" -DSTDC_HEADERS=1 -DHAVE_SYS_TYPES_H=1 -DHAVE_SYS_STAT_H=1 -DHAVE_STDLIB_H=1 -DHAVE_STRING_H=1 -DHAVE_MEMORY_H=1 -DHAVE_STRINGS_H=1 -DHAVE_INTTYPES_H=1 -DHAVE_STDINT_H=1 -DHAVE_UNISTD_H=1 -DHAVE_DLFCN_H=1 -DLT_OBJDIR=\"".libs/\"" -DHAVE_PTHREAD_PRIO_INHERIT=1 -DHAVE_PTHREAD=1 -DHAVE_LIBZ=1 -DHAVE_LIBCURL=1 -DHAVE_APR_POOLS_H=1 -DHAVE_LIBAPR_1=1 -DHAVE_SVN_VERSION_H=1 -DHAVE_LIBSVN_SUBR_1=1 -DHAVE_SVN_DELTA_H=1 -DHAVE_LIBSVN_DELTA_1=1 -DHAVE_LIBSASL2=1 -I. -I../../src   -Wall -Werror -DLIBDIR=\""/usr/local/lib\"" -DPKGLIBEXECDIR=\""/usr/local/libexec/mesos\"" -DPKGDATADIR=\""/usr/local/share/mesos\"" -I../../include -I../../3rdparty/libprocess/include -I../../3rdparty/libprocess/3rdparty/stout/include -I../include -I../include/mesos -isystem ../3rdparty/libprocess/3rdparty/boost-1.53.0 -I../3rdparty/libprocess/3rdparty/picojson-1.3.0 -DPICOJSON_USE_INT64 -D__STDC_FORMAT_MACROS -I../3rdparty/libprocess/3rdparty/protobuf-2.5.0/src -I../3rdparty/libprocess/3rdparty/glog-0.3.3/src -I../3rdparty/libprocess/3rdparty/glog-0.3.3/src -I../3rdparty/leveldb/include -I../3rdparty/zookeeper-3.4.5/src/c/include -I../3rdparty/zookeeper-3.4.5/src/c/generated -I../3rdparty/libprocess/3rdparty/protobuf-2.5.0/src -DSOURCE_DIR=\""/Users/till/Development/mesos-private/build/..\"" -DBUILD_DIR=\""/Users/till/Development/mesos-private/build\"" -I../3rdparty/libprocess/3rdparty/gmock-1.7.0/gtest/include -I../3rdparty/libprocess/3rdparty/gmock-1.7.0/include  -I/usr/local/opt/openssl/include -I/usr/local/opt/libevent/include -I/usr/local/opt/subversion/include/subversion-1 -I/usr/include/apr-1 -I/usr/include/apr-1.0  -D_THREAD_SAFE -pthread -g1 -O0 -Wno-unused-local-typedefs -std=c++11 -DGTEST_USE_OWN_TR1_TUPLE=1 -DGTEST_LANG_CXX11 -MT tests/mesos_tests-containerizer.o -MD -MP -MF tests/.deps/mesos_tests-containerizer.Tpo -c -o tests/mesos_tests-containerizer.o `test -f 'tests/containerizer.cpp' || echo '../../src/'`tests/containerizer.cpp
In file included from ../3rdparty/libprocess/3rdparty/gmock-1.7.0/include/gmock/internal/gmock-internal-utils.h:47:0,
                 from ../3rdparty/libprocess/3rdparty/gmock-1.7.0/include/gmock/gmock-actions.h:46,
                 from ../3rdparty/libprocess/3rdparty/gmock-1.7.0/include/gmock/gmock.h:58,
                 from ../../src/tests/container_logger_tests.cpp:21:
../3rdparty/libprocess/3rdparty/gmock-1.7.0/gtest/include/gtest/gtest.h: In instantiation of 'testing::AssertionResult testing::internal::CmpHelperLE(const char*, const char*, const T1&, const T2&) [with T1 = int; T2 = long long unsigned int]':
../../src/tests/container_logger_tests.cpp:467:3:   required from here
../3rdparty/libprocess/3rdparty/gmock-1.7.0/gtest/include/gtest/gtest.h:1579:28: error: comparison between signed and unsigned integer expressions [-Werror=sign-compare]
 GTEST_IMPL_CMP_HELPER_(LE, <=);
                            ^
../3rdparty/libprocess/3rdparty/gmock-1.7.0/gtest/include/gtest/gtest.h:1562:12: note: in definition of macro 'GTEST_IMPL_CMP_HELPER_'
   if (val1 op val2) {\
            ^
../3rdparty/libprocess/3rdparty/gmock-1.7.0/gtest/include/gtest/gtest.h: In instantiation of 'testing::AssertionResult testing::internal::CmpHelperGE(const char*, const char*, const T1&, const T2&) [with T1 = int; T2 = long long unsigned int]':
../../src/tests/container_logger_tests.cpp:468:3:   required from here
../3rdparty/libprocess/3rdparty/gmock-1.7.0/gtest/include/gtest/gtest.h:1583:28: error: comparison between signed and unsigned integer expressions [-Werror=sign-compare]
 GTEST_IMPL_CMP_HELPER_(GE, >=);
                            ^
../3rdparty/libprocess/3rdparty/gmock-1.7.0/gtest/include/gtest/gtest.h:1562:12: note: in definition of macro 'GTEST_IMPL_CMP_HELPER_'
   if (val1 op val2) {\
            ^
mv -f tests/.deps/mesos_tests-anonymous_tests.Tpo tests/.deps/mesos_tests-anonymous_tests.Po
{noformat}",Bug,Major,tillt,2016-01-26T17:25:30.000+0000,5,Resolved,Complete,Build failure when using gcc-4.9 - signed/unsigned mismatch.,2016-01-26T17:25:30.000+0000,MESOS-4513,1.0,mesos,Mesosphere Sprint 27
alexr,2016-01-26T16:22:49.000+0000,alexr,"Currently quota status endpoint returns a collection of {{QuotaInfo}} protos converted to JSON. An example response looks like this:
{code:xml}
{
  ""infos"": [
    {
      ""role"": ""role1"",
      ""guarantee"": [
        {
          ""name"": ""cpus"",
          ""role"": ""*"",
          ""type"": ""SCALAR"",
          ""scalar"": { ""value"": 12 }
        },
        {
          ""name"": ""mem"",
          ""role"": ""*"",
          ""type"": ""SCALAR"",
          ""scalar"": { ""value"": 6144 }
        }
      ]
    }
  ]
}
{code}

Presence of some fields, e.g. ""role"", is misleading. To address this issue and make the output more informative, we should probably introduce a  {{model()}} function for {{QuotaStatus}}.",Bug,Major,alexr,,1,Open,New,Render quota status consistently with other endpoints.,2016-04-25T22:06:23.000+0000,MESOS-4512,3.0,mesos,
jieyu,2016-01-26T00:09:58.000+0000,jieyu,"We assume MOUNT type disk is exclusive and the underlying filesystem will enforce the quota (i.e., the application won't be able to exceed the quota, and will get a write error it the disk is full).

Therefore, there's no need to enforce it's quota in posix disk isolator.",Task,Major,jieyu,2016-01-27T04:08:24.000+0000,5,Resolved,Complete,Posix disk isolator should ignore disk quota enforcement for MOUNT type disk resources.,2016-01-27T16:55:01.000+0000,MESOS-4506,2.0,mesos,Mesosphere Sprint 27
jvanremoortere,2016-01-25T23:50:52.000+0000,jvanremoortere,"Since we do not strip the non-scalar resources during the resource arithmetic for quota, the performance can degrade significantly, as currently resource arithmetic is expensive.

One approach to resolving this is to filter the resources we use to perform this arithmetic to only use scalars. This is valid as quota can currently only be set for scalar resource types.",Improvement,Blocker,jvanremoortere,2016-01-27T04:18:06.000+0000,5,Resolved,Complete,Hierarchical allocator performance is slow due to Quota,2016-02-27T05:30:20.000+0000,MESOS-4505,3.0,mesos,Mesosphere Sprint 27
gilbert,2016-01-25T17:19:20.000+0000,gilbert,"Currently we do not have these info for isolator. Image once we have docker runtime isolator, CommandInfo is necessary to support either custom executor or command executor. ",Bug,Major,gilbert,2016-01-28T19:57:17.000+0000,5,Resolved,Complete,Expose ExecutorInfo and TaskInfo for isolators.,2016-01-28T19:57:17.000+0000,MESOS-4500,2.0,mesos,Mesosphere Sprint 27
jieyu,2016-01-25T17:04:40.000+0000,jieyu,"Currently, the docker provisioner store will download all the layers associated with an image if the image is not found locally, even though some layers of it might already exist in the cache.

This is problematic because anytime a user deploys a new image, Mesos will fetch all layers of that new image, even though most of the layers are already cached locally.

",Bug,Major,jieyu,2016-02-23T23:12:31.000+0000,5,Resolved,Complete,Docker provisioner store should reuse existing layers in the cache.,2016-02-23T23:12:31.000+0000,MESOS-4499,5.0,mesos,Mesosphere Sprint 27
avinash@mesosphere.io,2016-01-24T19:42:20.000+0000,avinash@mesosphere.io,"As part of MESOS-4487 an interface will be introduce into the `Containerizer` to allow agents to retrieve container state information. The agent needs to use this interface to retrieve container state information during status updates from the executor. The container state information can be then use by the agent to expose various isolator specific configuration (for e.g., IP address allocated by network isolators, net_cls handles allocated by `cgroups/net_cls` isolator), that has been applied to the container, in the state.json endpoint.  ",Improvement,Major,avinash@mesosphere.io,2016-02-10T17:08:28.000+0000,5,Resolved,Complete,Get container status information in slave. ,2016-02-10T17:08:28.000+0000,MESOS-4490,3.0,mesos,Mesosphere Sprint 28
avinash@mesosphere.io,2016-01-24T19:33:09.000+0000,avinash@mesosphere.io,The `cgroup/net_cls` isolator is responsible for allocating network handles to containers launched within a net_cls cgroup. The `cgroup/net_cls` isolator needs to expose these handles to the containerizer as part of the `ContainerStatus` when the containerizer queries the status() method of the isolator. The information itself will go as part of a `CgroupInfo` protobuf that will be defined as part of MESOS-4488 .  ,Improvement,Major,avinash@mesosphere.io,2016-02-09T18:18:59.000+0000,5,Resolved,Complete,The `cgroups/net_cls` isolator needs to expose handles in the ContainerStatus,2016-02-09T18:18:59.000+0000,MESOS-4489,1.0,mesos,Mesosphere Sprint 28
avinash@mesosphere.io,2016-01-24T19:23:18.000+0000,avinash@mesosphere.io,"Within `MesosContainerizer` we have an isolator associated with each linux cgroup subsystem. The isolators apply subsystem specific configuration on the containers before launching the containers. For e.g cgroup/net_cls isolator applies net_cls handles, cgroup/mem isolator applies memory quotas, cgroups/cpu-share isolator configures cpu shares. 

Currently, there is no message structure defined to capture the configuration information of the container, for each cgroup isolator that has been applied to the container. We therefore need to define a protobuf that can capture the cgroup configuration of each cgroup isolator that has been applied to the container. This protobuf will be filled in by the cgroup isolator and will be stored as part of `ContainerConfig` in the containerizer. ",Improvement,Major,avinash@mesosphere.io,2016-01-28T21:40:46.000+0000,5,Resolved,Complete,Define a CgroupInfo protobuf to expose cgroup isolator configuration.,2016-01-28T21:40:57.000+0000,MESOS-4488,1.0,mesos,Mesosphere Sprint 27
avinash@mesosphere.io,2016-01-24T19:07:08.000+0000,avinash@mesosphere.io,"In the Containerizer, during container isolation, the isolators end up modifying the state of the containers. Examples would be IP address allocation to a container by the 'network isolator, or net_cls handle allocation by the cgroup/net_cls isolator. 

Often times the state of the container, needs to be exposed to operators through the state.json end-point. For e.g. operators or frameworks might want to know the IP-address configured on a particular container, or the net_cls handle associated with a container to configure the right TC rules. However, at present, there is no clean interface for the slave to retrieve the state of a container from the Containerizer for any of the launched containers. Thus, we need to introduce a `status` interface in the `Containerizer` base class, in order for the slave to expose container state information in its state.json.   ",Improvement,Major,avinash@mesosphere.io,2016-02-09T18:16:43.000+0000,5,Resolved,Complete,Introduce status() interface in `Containerizer`,2016-02-09T18:16:43.000+0000,MESOS-4487,2.0,mesos,Mesosphere Sprint 28
vinodkone,2016-01-22T22:57:04.000+0000,vinodkone,"The bot is currently tripping on this review  https://reviews.apache.org/r/42506/ (see builds #10973 to #10978).

[~jfarrell] looked at the server logs and said he saw 'MySQL going away' message when the mesos bot was making these requests. I think that error is a bit misleading because it happens only for this review (which has a huge error log due to bad patch). The bot has successfully posted reviews for other review requests which had no error log (good patch).

One way to fix this would be to just post a tail of the error log (and perhaps link to Jenkins Console or some other service for the longer error text).",Bug,Major,vinodkone,2016-01-29T22:08:51.000+0000,5,Resolved,Complete,ReviewBot seemed to be crashing ReviewBoard server when posting large reviews,2016-01-29T22:08:51.000+0000,MESOS-4478,2.0,mesos,Mesosphere Sprint 27
,2016-01-22T20:09:32.000+0000,anandmazumdar,"Currently, we support sending executor->framework messages directly as an optimization. This is not currently possible with using the Scheduler HTTP API. We should think about exploring possible alternatives for supporting this optimization.",Task,Major,anandmazumdar,,1,Open,New,Enable Executor->Framework message optimization for HTTP API,2016-03-03T21:58:44.000+0000,MESOS-4461,13.0,mesos,
,2016-01-22T20:07:46.000+0000,anandmazumdar,"Currently, we support sending framework->executor messages directly as an optimization. This is not currently possible with using the Scheduler HTTP API.  We should think about exploring possible alternatives for supporting this optimization.",Task,Major,anandmazumdar,,1,Open,New,Enable Framework->Executor message optimization for HTTP API,2016-01-22T20:09:29.000+0000,MESOS-4460,13.0,mesos,
anandmazumdar,2016-01-22T19:59:20.000+0000,anandmazumdar,"Currently, we do not have the ability of passing {{Credentials}} via the scheduler library. Once the master supports AuthN handling for the {{/scheduler}} endpoint, we would need to add this support to the library.",Task,Major,anandmazumdar,2016-04-15T21:01:45.000+0000,5,Resolved,Complete,Implement AuthN handling on the scheduler library,2016-04-15T21:01:45.000+0000,MESOS-4459,3.0,mesos,Mesosphere Sprint 33
anandmazumdar,2016-01-22T19:44:33.000+0000,anandmazumdar,We need to add tests for the executor library {{src/executor/executor.cpp}}. One possible approach would be to use the existing tests in {{src/tests/scheduler_tests.cpp}} and make them use the new executor library.,Task,Major,anandmazumdar,2016-02-09T19:19:31.000+0000,5,Resolved,Complete,Implement tests for the new Executor library,2016-02-27T00:05:05.000+0000,MESOS-4457,3.0,mesos,Mesosphere Sprint 28
jojy,2016-01-22T17:51:59.000+0000,jojy,Add common utility function for computing digests. Start with `sha512` since its immediately needed by appc image fetcher. ,Bug,Major,jojy,2016-01-29T02:04:41.000+0000,5,Resolved,Complete,Create common sha512 compute utility function.,2016-02-01T22:51:26.000+0000,MESOS-4454,2.0,mesos,Mesosphere Sprint 27
greggomann,2016-01-21T22:07:25.000+0000,neilc,"* What is the difference between a role and a principal?
* Why do some ACL entities reference ""roles"" but others reference ""principals""? In a typical organization, what real-world entities would my roles vs. principals map to? The ACL documentation could use more information about the motivation of ACLs and examples of configuring ACLs to meet real-world security policies.
* We should give some examples of making reservations when the role and principal are different, and why you would want to do that
* We should add an example to the ACL page that includes setting ACLs for reservations and/or persistent volumes",Documentation,Minor,neilc,2016-02-11T20:18:31.000+0000,5,Resolved,Complete,"Improve documentation around roles, principals, authz, and reservations",2016-02-11T20:18:31.000+0000,MESOS-4452,2.0,mesos,Mesosphere Sprint 28
anandmazumdar,2016-01-21T16:16:08.000+0000,philwinder,"When repeatedly performing our system tests we have found that we get a segfault on one of the agents. It probably occurs about one time in ten. I have attached the full log from that agent. I've attached the log from the agent that failed and the master (although I think this is less helpful).

To reproduce
- I have no idea. It seems to occur at certain times. E.g. like if a packet is created right on a minute boundary or something. But I don't think it's something caused by our code because the timestamps are stamped by mesos. I was surprised not to find a bug already open.",Bug,Blocker,philwinder,2016-01-22T21:14:50.000+0000,5,Resolved,Complete,SegFault on agent during executor startup,2016-02-27T00:18:54.000+0000,MESOS-4449,1.0,mesos,Mesosphere Sprint 27
neilc,2016-01-20T21:18:26.000+0000,neilc,"{noformat}
TEST(RevocableResourceTest, LabelSemantics)
{
  Labels labels1;
  Labels labels2;

  labels1.add_labels()->CopyFrom(createLabel(""foo"", ""bar""));
  labels1.add_labels()->CopyFrom(createLabel(""foo"", ""bar""));

  labels2.add_labels()->CopyFrom(createLabel(""foo"", ""bar""));
  labels2.add_labels()->CopyFrom(createLabel(""baz"", ""qux""));

  bool eq = (labels1 == labels2);
  LOG(INFO) << ""Equal? "" << (eq ? ""true"" : ""false"");
}
{noformat}

Output:
{noformat}
[ RUN      ] RevocableResourceTest.LabelSemantics
I0120 13:15:25.207223 2078158848 resources_tests.cpp:1990] Equal? true
[       OK ] RevocableResourceTest.LabelSemantics (0 ms)
{noformat}

This behavior seems pretty problematic.",Bug,Minor,neilc,,1,Open,New,Labels equality behavior is wrong,2016-02-23T23:34:04.000+0000,MESOS-4445,5.0,mesos,Mesosphere Sprint 28
alexr,2016-01-20T13:36:28.000+0000,alexr,Allocator recovery code can be improved for readability. [~bmahler] left some thoughts about it in https://reviews.apache.org/r/42222/.,Improvement,Major,alexr,,3,In Progress,In Progress,Refactor allocator recovery.,2016-04-18T11:35:30.000+0000,MESOS-4443,3.0,mesos,Mesosphere Sprint 28
mcypark,2016-01-20T11:15:28.000+0000,alexr,"h4. Status Quo
Currently resources allocated to frameworks in a role with quota (aka quota'ed role) beyond quota guarantee are marked non-revocable. This impacts our flexibility for revoking them if we decide so in the future.

h4. Proposal
Once quota guarantee is satisfied we must not necessarily further allocate resources as non-revocable. Instead we can mark all offers resources beyond guarantee as revocable. When in the future {{RevocableInfo}} evolves frameworks will get additional information about ""revocability"" of the resource (i.e. allocation slack)

h4. Caveats
Though it seems like a simple change, it has several implications.

h6. Fairness
Currently the hierarchical allocator considers revocable resources as regular resources when doing fairness calculations. This may prevent frameworks getting non-revocable resources as part of their role's quota guarantee if they accept some revocable resources as well.

Consider the following scenario. A single framework in a role with quota set to {{10}} CPUs is allocated {{10}} CPUs as non-revocable resources as part of its quota and additionally {{2}} revocable CPUs. Now a task using {{2}} non-revocable CPUs finishes and its resources are returned. Total allocation for the role is {{8}} non-revocable + {{2}} revocable. However, the role may not be offered additional {{2}} non-revocable since its total allocation satisfies quota.

h6. Resource math
If we allocate non-revocable resources as revocable, we should make sure we do accounting right: either we should update total agent resources and mark them as revocable as well, or bookkeep resources as non-revocable and convert them to revocable when necessary.

h6. Coarse-grained nature of allocation
The hierarchical allocator performs ""coarse-grained"" allocation, meaning it always allocates the entire remaining agent resources to a single framework. This may lead to over-allocating some resources as non-revocable beyond quota guarantee.

h6. Quotas smaller than fair share
If a quota set for a role is smaller than its fair share, it may reduce the amount of resources offered to this role, if frameworks in it do not accept revocable resources. This is probably the most important consequence of the proposed change. Operators may set quota to get guarantees, but may observe a decrease in amount of resources a role gets, which is not intuitive.",Improvement,Blocker,alexr,,3,In Progress,In Progress,Allocate revocable resources beyond quota guarantee.,2016-02-12T11:26:54.000+0000,MESOS-4441,8.0,mesos,
jojy,2016-01-20T01:10:56.000+0000,jojy,"Currently image validation is done assuming that the image's filename will have  digest (SHA-512) information. This is not part of the spec
    (https://github.com/appc/spec/blob/master/spec/discovery.md).
    
    The spec specifies the tuple <image name, labels> as unique identifier for  discovering an image.
",Task,Major,jojy,2016-02-11T23:28:08.000+0000,5,Resolved,Complete,Fix appc CachedImage image validation,2016-02-11T23:28:08.000+0000,MESOS-4439,1.0,mesos,Mesosphere Sprint 27
jojy,2016-01-20T01:06:44.000+0000,jojy,AppcImageManifest protobuf currently lacks 'dependencies' which is necessary for image discovery.,Task,Major,jojy,2016-01-29T01:27:51.000+0000,5,Resolved,Complete,Add 'dependency' message to 'AppcImageManifest' protobuf.,2016-01-29T01:27:51.000+0000,MESOS-4438,1.0,mesos,Mesosphere Sprint 27
jojy,2016-01-20T00:44:45.000+0000,jojy,"As we are retiring registry client, disable this test which looks flaky.",Task,Major,jojy,2016-02-01T04:22:44.000+0000,5,Resolved,Complete,Disable the test RegistryClientTest.BadTokenServerAddress.,2016-02-01T04:22:44.000+0000,MESOS-4437,1.0,mesos,Mesosphere Sprint 27
mcypark,2016-01-20T00:05:30.000+0000,mcypark,Update {{state-summary}} to use {{jsonify}} to stay consistent with {{state}} HTTP endpoint.,Task,Major,mcypark,2016-02-01T04:35:43.000+0000,5,Resolved,Complete,Update `Master::Http::stateSummary` to use `jsonify`.,2016-02-27T00:23:12.000+0000,MESOS-4435,3.0,mesos,Mesosphere Sprint 26
karya,2016-01-19T22:01:09.000+0000,karya,Mesos modules depend on having these packages installed with the exact version as Mesos was compiled with.,Bug,Major,karya,,10006,Reviewable,New,"Install 3rdparty package boost, glog, protobuf and picojson when installing Mesos",2016-04-27T16:13:17.000+0000,MESOS-4434,3.0,mesos,Mesosphere Sprint 33
anandmazumdar,2016-01-19T20:04:00.000+0000,anandmazumdar,"Currently, we do not have a mocking based callback interface for the executor library. This should look similar to the ongoing work for MESOS-3339 i.e. the corresponding issue for the scheduler library.

The interface should allow us to set expectations like we do for the driver. An example:

{code}
EXPECT_CALL(executor, connected())
  .Times(1)
{code}",Task,Major,anandmazumdar,2016-02-09T19:20:01.000+0000,5,Resolved,Complete,Implement a callback testing interface for the Executor Library,2016-02-27T00:05:07.000+0000,MESOS-4433,3.0,mesos,Mesosphere Sprint 28
anandmazumdar,2016-01-19T02:07:26.000+0000,anandmazumdar,"We need a test abstraction for {{HttpEvent}} similar to the already existing one's for {{DispatchEvent}}, {{MessageEvent}} in libprocess.

The abstraction can look similar in semantics to the already existing {{FUTURE_DISPATCH}}/{{FUTURE_MESSAGE}}.",Bug,Major,anandmazumdar,2016-01-27T01:22:29.000+0000,5,Resolved,Complete,Introduce filtering test abstractions for HTTP events to libprocess,2016-04-14T19:20:11.000+0000,MESOS-4425,3.0,mesos,Mesosphere Sprint 27
neilc,2016-01-18T22:34:19.000+0000,neilc,"The docs for the {{/reserve}} endpoint say:

{noformat}
200 OK: Success (the requested resources have been reserved).
{noformat}

This is not true: the master returns {{200}} when the request has been validated and a {{CheckpointResourcesMessage}} has been sent to the agent, but the master does not attempt to verify that the message has been received or that the agent successfully checkpointed. Same behavior applies to {{/unreserve}}, {{/create-volumes}}, and {{/destroy-volumes}}.

We should _either_:

1. Accurately document what {{200}} return code means.
2. Change the implementation to wait for the agent's next checkpoint to succeed (and to include the effect of the operation) before returning success to the HTTP client.",Task,Major,neilc,2016-02-12T20:56:03.000+0000,5,Resolved,Complete,"Document that /reserve, /create-volumes endpoints can return misleading ""success""",2016-02-12T20:56:03.000+0000,MESOS-4421,3.0,mesos,Mesosphere Sprint 27
alexr,2016-01-17T08:43:44.000+0000,alexr,"There might be a bug that may crash the master as pointed out by [~bmahler] in https://reviews.apache.org/r/42222/:
{noformat}
It looks like if we trip the resume call in addSlave, this delayed resume will crash the master 
due to the CHECK(paused) that currently resides in resume.
{noformat}",Bug,Blocker,alexr,2016-01-21T07:48:09.000+0000,5,Resolved,Complete,Prevent allocator from crashing on successful recovery.,2016-01-21T07:48:09.000+0000,MESOS-4417,3.0,mesos,Mesosphere Sprint 27
gyliu,2016-01-15T23:43:54.000+0000,alexr,There might be a bug in how resources are allocated to multiple quota'ed roles if one role's quota is met. We need to investigate this behavior.,Bug,Blocker,alexr,2016-01-19T23:47:25.000+0000,5,Resolved,Complete,Traverse all roles for quota allocation.,2016-01-19T23:47:25.000+0000,MESOS-4411,3.0,mesos,Mesosphere Sprint 27
alexr,2016-01-15T23:34:28.000+0000,alexr,"To document quota request JSON schema and simplify request processing, introduce a {{QuotaRequest}} protobuf wrapper.",Improvement,Blocker,alexr,2016-01-26T23:58:12.000+0000,5,Resolved,Complete,Introduce protobuf for quota set request.,2016-01-26T23:58:12.000+0000,MESOS-4410,3.0,mesos,Mesosphere Sprint 27
jvanremoortere,2016-01-15T18:50:57.000+0000,jieyu,We have two options here. We can either check and fail if it does not exists. Or we can create if it does not exist like we did for slave.work_dir.,Task,Major,jieyu,2016-01-24T01:25:47.000+0000,5,Resolved,Complete,Check paths in DiskInfo.Source.Path exist during slave initialization.,2016-01-24T01:28:55.000+0000,MESOS-4403,2.0,mesos,Mesosphere Sprint 27
jvanremoortere,2016-01-15T18:40:25.000+0000,jieyu,"This is related to MESOS-4400.

Since persistent volume directories can be created from non root disk now. We need to adjust both posix and linux filesystem isolator to look for volumes from the correct location based on the information in DiskInfo.Source.

See relevant code in:
{code}
Future<Nothing> PosixFilesystemIsolatorProcess::update(..);
Future<Nothing> LinuxFilesystemIsolatorProcess::update(..);
{code}",Task,Major,jieyu,2016-01-24T01:24:46.000+0000,5,Resolved,Complete,Update filesystem isolators to look for persistent volume directories from the correct location.,2016-01-24T01:24:46.000+0000,MESOS-4402,2.0,mesos,Mesosphere Sprint 27
jvanremoortere,2016-01-15T18:33:10.000+0000,jieyu,"Currently, we always create persistent volumes from root disk, and the persistent volumes are directories. With DiskInfo.Source being added, we should create the persistent volume accordingly based on the information in DiskInfo.Source.

This ticket handles the case where DiskInfo.Source.type is PATH. In that case, we should create sub-directories and use the same layout as slave.work_dir.

See the relevant code here:
{code}
void Slave::checkpointResources(...)
{
  // Creates persistent volumes that do not exist and schedules
  // releasing those persistent volumes that are no longer needed.
  //
  // TODO(jieyu): Consider introducing a volume manager once we start
  // to support multiple disks, or raw disks. Depending on the
  // DiskInfo, we may want to create either directories under a root
  // directory, or LVM volumes from a given device.
  Resources volumes = newCheckpointedResources.persistentVolumes();

  foreach (const Resource& volume, volumes) {
    // This is validated in master.
    CHECK_NE(volume.role(), ""*"");

    string path = paths::getPersistentVolumePath(
        flags.work_dir,
        volume.role(),
        volume.disk().persistence().id());

    if (!os::exists(path)) {
      CHECK_SOME(os::mkdir(path, true))
        << ""Failed to create persistent volume at '"" << path << ""'"";
    }
  }
}
{code}",Task,Major,jieyu,2016-01-24T01:23:59.000+0000,5,Resolved,Complete,Create persistent volume directories based on DiskInfo.Source.,2016-01-24T01:23:59.000+0000,MESOS-4400,2.0,mesos,Mesosphere Sprint 27
,2016-01-15T18:14:10.000+0000,anandmazumdar,"Currently, any AuthZ errors for the {{/scheduler}} endpoint are handled asynchronously as {{FrameworkErrorMessage}}. Here is an example:

{code}
  if (authorizationError.isSome()) {
    LOG(INFO) << ""Refusing subscription of framework""
              << "" '"" << frameworkInfo.name() << ""'""
              << "": "" << authorizationError.get().message;

    FrameworkErrorMessage message;
    message.set_message(authorizationError.get().message);
    http.send(message);
    http.close();
    return;
  }
{code}

We would like to handle such errors synchronously when the request is received similar to what other endpoints like {{/reserve}}/{{/quota}} do. We already have the relevant functions {{authorizeXXX}} etc in {{master.cpp}}. We should just make the requests pass through once the relevant {{Future}} from the {{authorizeXXX}} function is fulfilled.",Bug,Major,anandmazumdar,,1,Open,New,Synchronously handle AuthZ errors for the Scheduler endpoint.,2016-01-22T20:11:25.000+0000,MESOS-4398,5.0,mesos,
gilbert,2016-01-15T17:32:22.000+0000,jieyu,"The name ""ContainerPrepareInfo"" does not really capture the purpose of this struct. ContainerLaunchInfo better captures the purpose of this struct. ContainerLaunchInfo is returned by the isolator 'prepare' function. It contains information about how a container should be launched (e.g., environment variables, namespaces, commands, etc.). The information will be used by the Mesos Containerizer when launching the container.",Task,Major,jieyu,2016-01-15T17:33:29.000+0000,5,Resolved,Complete,Rename ContainerPrepareInfo to ContainerLaunchInfo for isolators.,2016-01-15T18:11:29.000+0000,MESOS-4397,2.0,mesos,Mesosphere Sprint 26
greggomann,2016-01-15T16:28:11.000+0000,greggomann,There are currently no persistent volume endpoint tests that do not use a principal; they should be added.,Bug,Major,greggomann,2016-01-29T20:56:49.000+0000,5,Resolved,Complete,Add persistent volume endpoint tests with no principal,2016-01-29T21:02:19.000+0000,MESOS-4395,1.0,mesos,Mesosphere Sprint 27
alexr,2016-01-15T13:50:10.000+0000,bernd-mesos,"Create a design document for setting offered resources as ""revocable by default"". Greedy frameworks can then temporarily use resources set aside to satisfy quota.
",Task,Major,bernd-mesos,,10020,Accepted,In Progress,Draft design document for resource revocability by default.,2016-02-16T23:53:41.000+0000,MESOS-4393,8.0,mesos,Mesosphere Sprint 28
anindya.sinha,2016-01-15T10:16:12.000+0000,adam-mesos,Review & Approve design doc,Task,Major,adam-mesos,,10006,Reviewable,New,Shared Volumes Design Doc,2016-03-31T08:04:46.000+0000,MESOS-4390,3.0,mesos,Mesosphere Sprint 27
vinodkone,2016-01-15T00:58:20.000+0000,vinodkone,"To be consistent with `authenticate_slaves` and `authenticate_http` flags, we should rename `authenticate` to `authenticate_frameworks` flag.

This should be done via deprecation cycle. 

1) Release X supports both `authenticate` and `authenticate_frameworks` flags

2)  Release X + n supports only `authenticate_frameworks` flag. 
",Improvement,Major,vinodkone,,10006,Reviewable,New,Deprecate 'authenticate' master flag in favor of 'authenticate_frameworks' flag,2016-04-27T16:13:16.000+0000,MESOS-4386,1.0,mesos,Mesosphere Sprint 33
kaysoky,2016-01-14T21:32:03.000+0000,kaysoky,"*Problem*
* In {{Master::accept}}, {{validation::offer::validate}} returns an error when an {{InverseOffer}} is included in the list of {{OfferIDs}} in an {{ACCEPT}} call.
* If an {{Offer}} is part of the same {{ACCEPT}}, the master sees {{error.isSome()}} and returns a {{TASK_LOST}} for normal offers.  (https://github.com/apache/mesos/blob/fafbdca610d0a150b9fa9cb62d1c63cb7a6fdaf3/src/master/master.cpp#L3117)

Here's a regression test:
https://reviews.apache.org/r/42092/

*Proprosal*
The question is whether we want to allow the mixing of {{Offers}} and {{InverseOffers}}.

Arguments for mixing:
* The design/structure of the maintenance originally intended to overload {{ACCEPT}} and {{DECLINE}} to take inverse offers.
* Enforcing non-mixing may require breaking changes to {{scheduler.proto}}.

Arguments against mixing:
* Some semantics are difficult to explain.  What does it mean to supply {{InverseOffers}} with {{Offer::Operations}}?  What about {{DECLINE}} with {{Offers}} and {{InverseOffers}}, including a ""reason""?
* What happens if we presumably add a third type of offer?
* Does it make sense to {{TASK_LOST}} valid normal offers if {{InverseOffers}} are invalid?",Bug,Major,kaysoky,,1,Open,New,Offers and InverseOffers cannot be accepted in the same ACCEPT call,2016-01-14T21:32:26.000+0000,MESOS-4385,2.0,mesos,
gilbert,2016-01-14T21:09:16.000+0000,gilbert,We need to support env var configuration returned from docker image in mesos containerizer.,Bug,Major,gilbert,2016-02-05T21:46:15.000+0000,5,Resolved,Complete,Support docker runtime configuration env var from image.,2016-02-05T21:46:15.000+0000,MESOS-4383,2.0,mesos,Mesosphere Sprint 27
greggomann,2016-01-14T20:48:06.000+0000,greggomann,"With the addition of HTTP endpoints for {{/reserve}} and {{/unreserve}}, it is now desirable to allow dynamic reservations without a principal, in the case where HTTP authentication is disabled. To allow for this, we will change the {{principal}} field in {{ReservationInfo}} from required to optional. For backwards-compatibility, however, the master should currently invalidate any {{ReservationInfo}} messages that do not have this field set.",Improvement,Major,greggomann,2016-01-19T17:07:53.000+0000,5,Resolved,Complete,Change the `principal` in `ReservationInfo` to optional,2016-04-26T03:34:04.000+0000,MESOS-4382,1.0,mesos,Mesosphere Sprint 26
js84,2016-01-14T19:37:20.000+0000,js84,Investigate and document upgrade compatibility for 0.27 release.,Documentation,Major,js84,2016-03-10T17:38:48.000+0000,5,Resolved,Complete,Improve upgrade compatibility documentation.,2016-03-10T17:38:49.000+0000,MESOS-4381,3.0,mesos,Mesosphere Sprint 29
jvanremoortere,2016-01-14T19:05:13.000+0000,jieyu,"Since we added the Source for DiskInfo, we need to adjust the Resource arithmetics for that. That includes equality check, addable check, subtractable check, etc.",Task,Major,jieyu,2016-01-24T01:29:30.000+0000,5,Resolved,Complete,Adjust Resource arithmetics for DiskInfo.Source.,2016-01-24T01:29:30.000+0000,MESOS-4380,2.0,mesos,Mesosphere Sprint 27
jvanremoortere,2016-01-14T18:29:11.000+0000,jieyu,"Source is used to describe the extra information about the source of a Disk resource. We will support 'PATH' type first and then 'BLOCK' later.

{noformat}
message Source {
      enum Type {
        PATH = 1;
        BLOCK = 2,
      }

      message Path {
        // Path to the folder (e.g., /mnt/raid/disk0).
        required string root = 1;
        required double total_size = 2;
      }

      message Block {
        // Path to the device file (e.g., /dev/sda1, /dev/vg/v1).
        // It can be a physical partition, or a logical volume (LVM).
        required string device = 1;
      }

      required Type type = 1;
      optional Path path = 2;
      optional Block block = 3;
    }
}
{noformat}",Task,Major,jieyu,2016-01-24T01:21:56.000+0000,5,Resolved,Complete,Add Source to Resource.DiskInfo.,2016-01-24T01:21:56.000+0000,MESOS-4378,1.0,mesos,Mesosphere Sprint 27
neilc,2016-01-14T18:21:22.000+0000,neilc,We should document the units associated with memory and disk resources.,Documentation,Minor,neilc,2016-02-04T19:10:31.000+0000,5,Resolved,Complete,Document units associated with resource types,2016-02-04T19:10:31.000+0000,MESOS-4377,1.0,mesos,Mesosphere Sprint 27
vinodkone,2016-01-14T18:19:55.000+0000,neilc,"We should clarify the semantics of this callback:

* Is it always invoked, or just a hint?
* Can a slave ever come back from `slaveLost`?
* What happens to persistent resources on a lost slave?

The new HA framework development guide might be a good place to put (some of?) this information.
",Documentation,Major,neilc,2016-01-28T03:05:56.000+0000,5,Resolved,Complete,Document semantics of `slaveLost`,2016-01-28T03:05:56.000+0000,MESOS-4376,2.0,mesos,Mesosphere Sprint 27
nfnt,2016-01-14T12:35:06.000+0000,bbannier,The concrete implementation here depends on the implementation strategy used to solve MESOS-4367.,Improvement,Major,bbannier,,10020,Accepted,In Progress,Make HierarchicalAllocatorProcess set a Resource's active role during allocation,2016-02-17T08:31:44.000+0000,MESOS-4368,3.0,mesos,Mesosphere Sprint 27
bbannier,2016-01-14T12:33:57.000+0000,bbannier,"If a framework can have multiple roles, we need a way to identify for which of the framework's role a resource was offered for (e.g., for resource recovery and reconciliation).",Improvement,Major,bbannier,,10020,Accepted,In Progress,Add tracking of the role a Resource was offered for,2016-02-17T08:30:16.000+0000,MESOS-4367,5.0,mesos,Mesosphere Sprint 27
bbannier,2016-01-14T12:27:34.000+0000,bbannier,"If only the {{role}} field is given, add it as single entry to {{roles}}. Add a note to {{CHANGELOG}}/release notes on deprecation of the existing {{role}} field. File a JIRA issue for removal of that migration code once the deprecation cycle is over.
",Improvement,Major,bbannier,,10020,Accepted,In Progress,Add internal migration from role to roles to master,2016-02-17T08:30:27.000+0000,MESOS-4365,3.0,mesos,Mesosphere Sprint 27
qianzhang,2016-01-14T12:25:43.000+0000,bbannier,A {{FrameworkInfo}} can only have one of role or roles. A natural location for this appears to be under {{validation::operation::validate}}.,Improvement,Major,bbannier,,10006,Reviewable,New,Add roles validation code to master,2016-02-17T08:31:34.000+0000,MESOS-4364,5.0,mesos,Mesosphere Sprint 27
qianzhang,2016-01-14T12:24:20.000+0000,bbannier,To represent multiple roles per framework a new repeated string field for roles is needed.,Improvement,Major,bbannier,,10006,Reviewable,New,Add a roles field to FrameworkInfo,2016-02-17T08:31:19.000+0000,MESOS-4363,1.0,mesos,Mesosphere Sprint 27
js84,2016-01-14T10:45:47.000+0000,js84,"The online documentation has a number of bad formatting issues and broken links (e.g., mesos-provider.md).",Improvement,Major,js84,2016-01-15T01:03:51.000+0000,5,Resolved,Complete,Formating issues and broken links in documentation.,2016-01-15T01:03:51.000+0000,MESOS-4362,1.0,mesos,Mesosphere Sprint 26
jojy,2016-01-13T23:18:16.000+0000,jojy,"As part of refactoring and creating a common place to add all command utilities, add *tar* and *untar* as the first POC.",Bug,Major,jojy,2016-01-28T02:20:49.000+0000,5,Resolved,Complete,Create common tar/untar utility function.,2016-01-28T02:20:49.000+0000,MESOS-4360,3.0,mesos,Mesosphere Sprint 26
,2016-01-13T22:33:26.000+0000,greggomann,"The following GMock warning was seen on CentOS 7.1:

{code}
[ RUN      ] DockerContainerizerTest.ROOT_DOCKER_DockerInspectDiscard

GMOCK WARNING:
Uninteresting mock function call - returning directly.
    Function call: executorLost(0x7ffdd74f73e0, @0x7f3e3c00fa20 e1, @0x7f3e3c00f4b0 cf212bb4-c8c5-4a43-b71f-c17b27458627-S0, -1)
Stack trace:
[       OK ] DockerContainerizerTest.ROOT_DOCKER_DockerInspectDiscard (405 ms)
{code}",Bug,Minor,greggomann,,10020,Accepted,In Progress,GMock warning in DockerContainerizerTest.ROOT_DOCKER_DockerInspectDiscard,2016-01-13T23:03:36.000+0000,MESOS-4359,2.0,mesos,
avinash@mesosphere.io,2016-01-13T20:04:07.000+0000,avinash@mesosphere.io,"We need to expose net_cls network handles, associated with containers, to operators and network utilities that would use these network handles to enforce network policy. 

In order to achieve the above we need to add a new field in the `NetworkInfo` protobuf (say NetHandles) and update this field when a container gets assigned to a net_cls cgroup. The `ContainerStatus` protobuf already has the `NetworkInfo` protobuf as a nested message, and the `ContainerStatus` itself is exposed to operators as part of TaskInfo (for tasks associated with the container) in an agent's state.json. ",Task,Major,avinash@mesosphere.io,2016-02-10T17:10:35.000+0000,5,Resolved,Complete,Expose net_cls network handles in agent's state endpoint,2016-02-10T17:10:42.000+0000,MESOS-4358,2.0,mesos,Mesosphere Sprint 28
neilc,2016-01-13T19:07:36.000+0000,neilc,"{noformat}
[ RUN      ] RoleTest.ImplicitRoleStaticReservation

GMOCK WARNING:
Uninteresting mock function call - returning directly.
    Function call: shutdown(0x7fe37a4752f0)
Stack trace:
[       OK ] RoleTest.ImplicitRoleStaticReservation (52 ms)
{noformat}",Bug,Trivial,neilc,2016-01-19T20:03:30.000+0000,5,Resolved,Complete,GMock warning in RoleTest.ImplicitRoleStaticReservation,2016-01-19T20:03:30.000+0000,MESOS-4357,1.0,mesos,Mesosphere Sprint 26
magedm,2016-01-13T13:17:13.000+0000,qianzhang,"Currently libprocess will create {{max(8, number of CPU cores)}} processes during the initialization, see https://github.com/apache/mesos/blob/0.26.0/3rdparty/libprocess/src/process.cpp#L2146 for details. This should be OK for a normal machine which has no much cores (e.g., 16, 32), but for a powerful machine which may have a large number of cores (e.g., an IBM Power machine may have 192 cores), this will cause too much worker threads which are not necessary.

And since libprocess is widely used in Mesos (master, agent, scheduler, executor), it may also cause some performance issue. For example, when user creates a Docker container via Mesos in a Mesos agent which is running on a powerful machine with 192 cores, the DockerContainerizer in Mesos agent will create a dedicated executor for the container, and there will be 192 worker threads in that executor. And if user creates 1000 Docker containers in that machine, then there will be 1000 executors, i.e., 1000 * 192 worker threads which is a large number and may thrash the OS.

",Improvement,Major,qianzhang,2016-04-04T23:13:26.000+0000,5,Resolved,Complete,Limit the number of processes created by libprocess,2016-04-04T23:13:26.000+0000,MESOS-4353,1.0,mesos,Mesosphere Sprint 32
,2016-01-12T22:26:30.000+0000,greggomann,"Several tests involving checkpointing of resources in the {{ReservationTest}} fixture are throwing GMock warnings occasionally. Here is the output of {{GTEST_FILTER=""ReservationTest.*"" bin/mesos-tests.sh --gtest_repeat=10000 --gtest_break_on_failure=1 | grep -B 3 -A 6 WARNING}}:

{code}
-------------------------------------------------------------
We cannot run any Docker tests because:
Docker tests not supported on non-Linux systems
-------------------------------------------------------------
[       OK ] ReservationTest.MasterFailover (89 ms)
[ RUN      ] ReservationTest.CompatibleCheckpointedResources

GMOCK WARNING:
Uninteresting mock function call - returning directly.
    Function call: offerRescinded(0x7fff5f014960, @0x7feec320fab0 65537c10-285c-419e-b89f-191283402d85-O1)
Stack trace:
[       OK ] ReservationTest.CompatibleCheckpointedResources (52 ms)
[ RUN      ] ReservationTest.CompatibleCheckpointedResourcesWithPersistentVolumes
[       OK ] ReservationTest.CompatibleCheckpointedResourcesWithPersistentVolumes (45 ms)
--
--
[       OK ] ReservationTest.CompatibleCheckpointedResources (46 ms)
[ RUN      ] ReservationTest.CompatibleCheckpointedResourcesWithPersistentVolumes

GMOCK WARNING:
Uninteresting mock function call - returning directly.
    Function call: offerRescinded(0x7fff5f014960, @0x7feec796f220 bf4e1b52-02db-4763-8be0-3c759c80f1ba-O1)
Stack trace:
[       OK ] ReservationTest.CompatibleCheckpointedResourcesWithPersistentVolumes (63 ms)
[ RUN      ] ReservationTest.IncompatibleCheckpointedResources
[       OK ] ReservationTest.IncompatibleCheckpointedResources (45 ms)
--
--
[       OK ] ReservationTest.CompatibleCheckpointedResources (42 ms)
[ RUN      ] ReservationTest.CompatibleCheckpointedResourcesWithPersistentVolumes

GMOCK WARNING:
Uninteresting mock function call - returning directly.
    Function call: offerRescinded(0x7fff5f014960, @0x7feec7ad92b0 42a9f1ff-122e-4df7-9530-a96126e36f84-O1)
Stack trace:
[       OK ] ReservationTest.CompatibleCheckpointedResourcesWithPersistentVolumes (65 ms)
[ RUN      ] ReservationTest.IncompatibleCheckpointedResources
[       OK ] ReservationTest.IncompatibleCheckpointedResources (46 ms)
--
--
[       OK ] ReservationTest.CompatibleCheckpointedResourcesWithPersistentVolumes (49 ms)
[ RUN      ] ReservationTest.IncompatibleCheckpointedResources

GMOCK WARNING:
Uninteresting mock function call - returning directly.
    Function call: offerRescinded(0x7fff5f014960, @0x7feec7af4310 d5e1005f-abb8-4bfd-92e0-3976ee150fbf-O1)
Stack trace:
[       OK ] ReservationTest.IncompatibleCheckpointedResources (94 ms)
[ RUN      ] ReservationTest.GoodACLReserveThenUnreserve
[       OK ] ReservationTest.GoodACLReserveThenUnreserve (57 ms)
--
--
[       OK ] ReservationTest.CompatibleCheckpointedResources (43 ms)
[ RUN      ] ReservationTest.CompatibleCheckpointedResourcesWithPersistentVolumes

GMOCK WARNING:
Uninteresting mock function call - returning directly.
    Function call: offerRescinded(0x7fff5f014960, @0x7feec7cdadc0 36e15f52-3299-46fa-850d-970097fef8e2-O1)
Stack trace:
[       OK ] ReservationTest.CompatibleCheckpointedResourcesWithPersistentVolumes (62 ms)
[ RUN      ] ReservationTest.IncompatibleCheckpointedResources
[       OK ] ReservationTest.IncompatibleCheckpointedResources (46 ms)
--
--
[       OK ] ReservationTest.CompatibleCheckpointedResources (47 ms)
[ RUN      ] ReservationTest.CompatibleCheckpointedResourcesWithPersistentVolumes

GMOCK WARNING:
Uninteresting mock function call - returning directly.
    Function call: offerRescinded(0x7fff5f014960, @0x7feec8c1b580 c8dd35ab-7363-40e0-8e20-8c7dc76a8497-O1)
Stack trace:
[       OK ] ReservationTest.CompatibleCheckpointedResourcesWithPersistentVolumes (62 ms)
[ RUN      ] ReservationTest.IncompatibleCheckpointedResources
[       OK ] ReservationTest.IncompatibleCheckpointedResources (45 ms)
--
--
[       OK ] ReservationTest.CompatibleCheckpointedResources (47 ms)
[ RUN      ] ReservationTest.CompatibleCheckpointedResourcesWithPersistentVolumes

GMOCK WARNING:
Uninteresting mock function call - returning directly.
    Function call: offerRescinded(0x7fff5f014960, @0x7feecbd9b5b0 031c2148-8a20-4532-b77f-b6200c3791c8-O1)
Stack trace:
[       OK ] ReservationTest.CompatibleCheckpointedResourcesWithPersistentVolumes (62 ms)
[ RUN      ] ReservationTest.IncompatibleCheckpointedResources
[       OK ] ReservationTest.IncompatibleCheckpointedResources (46 ms)
--
--
[       OK ] ReservationTest.CompatibleCheckpointedResourcesWithPersistentVolumes (47 ms)
[ RUN      ] ReservationTest.IncompatibleCheckpointedResources

GMOCK WARNING:
Uninteresting mock function call - returning directly.
    Function call: offerRescinded(0x7fff5f014960, @0x7feecd52adb0 edc5a322-b220-4b13-a39b-99a523b172ba-O1)
Stack trace:
[       OK ] ReservationTest.IncompatibleCheckpointedResources (76 ms)
[ RUN      ] ReservationTest.GoodACLReserveThenUnreserve
[       OK ] ReservationTest.GoodACLReserveThenUnreserve (63 ms)
--
--
[       OK ] ReservationTest.SendingCheckpointResourcesMessage (45 ms)
[ RUN      ] ReservationTest.ResourcesCheckpointing

GMOCK WARNING:
Uninteresting mock function call - returning directly.
    Function call: offerRescinded(0x7fff5f015df8, @0x7feecfe16f00 09a90e67-a40f-4e42-8802-1a5644733a06-O1)
Stack trace:
[       OK ] ReservationTest.ResourcesCheckpointing (60 ms)
[ RUN      ] ReservationTest.MasterFailover
[       OK ] ReservationTest.MasterFailover (89 ms)
--
--
[       OK ] ReservationTest.CompatibleCheckpointedResources (43 ms)
[ RUN      ] ReservationTest.CompatibleCheckpointedResourcesWithPersistentVolumes

GMOCK WARNING:
Uninteresting mock function call - returning directly.
    Function call: offerRescinded(0x7fff5f014960, @0x7feecacceba0 84965984-28cd-4bc8-b25b-746583477d09-O1)
Stack trace:
[       OK ] ReservationTest.CompatibleCheckpointedResourcesWithPersistentVolumes (58 ms)
[ RUN      ] ReservationTest.IncompatibleCheckpointedResources
[       OK ] ReservationTest.IncompatibleCheckpointedResources (68 ms)
{code}",Bug,Major,greggomann,,10020,Accepted,In Progress,GMock warning on `offerRescinded` in `ReservationTest` fixture,2016-01-12T22:37:05.000+0000,MESOS-4350,2.0,mesos,
neilc,2016-01-12T21:16:30.000+0000,neilc,"{noformat}
[ RUN      ] SlaveTest.ContainerUpdatedBeforeTaskReachesExecutor

GMOCK WARNING:
Uninteresting mock function call - returning directly.
    Function call: shutdown(0x7fe189cae850)
Stack trace:
[       OK ] SlaveTest.ContainerUpdatedBeforeTaskReachesExecutor (51 ms)
{noformat}

Occurs non-deterministically for me on OSX 10.10, perhaps one run in ten.",Bug,Major,neilc,2016-01-20T07:00:35.000+0000,5,Resolved,Complete,GMock warning in SlaveTest.ContainerUpdatedBeforeTaskReachesExecutor,2016-01-20T07:00:35.000+0000,MESOS-4349,1.0,mesos,Mesosphere Sprint 26
anandmazumdar,2016-01-12T21:13:18.000+0000,neilc,"{noformat}
[ RUN      ] HookTest.VerifySlaveRunTaskHook

GMOCK WARNING:
Uninteresting mock function call - returning directly.
    Function call: shutdown(0x7ff079cb2420)
Stack trace:
[       OK ] HookTest.VerifySlaveRunTaskHook (51 ms)
[ RUN      ] HookTest.VerifySlaveTaskStatusDecorator

GMOCK WARNING:
Uninteresting mock function call - returning directly.
    Function call: shutdown(0x7ff079cbb790)
Stack trace:
[       OK ] HookTest.VerifySlaveTaskStatusDecorator (54 ms)
{noformat}

Occurs non-deterministically for me. OSX 10.10.",Bug,Minor,neilc,2016-01-14T23:32:24.000+0000,5,Resolved,Complete,"GMock warning in HookTest.VerifySlaveRunTaskHook, HookTest.VerifySlaveTaskStatusDecorator",2016-01-15T00:36:34.000+0000,MESOS-4348,1.0,mesos,Mesosphere Sprint 26
greggomann,2016-01-12T21:08:10.000+0000,neilc,"{noformat}
[ RUN      ] ReservationTest.ACLMultipleOperations

GMOCK WARNING:
Uninteresting mock function call - returning directly.
    Function call: shutdown(0x7fa2a311b300)
Stack trace:
[       OK ] ReservationTest.ACLMultipleOperations (174 ms)
[----------] 1 test from ReservationTest (174 ms total)
{noformat}

Seems to occur non-deterministically for me, maybe once per 50 runs or so. OSX 10.10",Bug,Minor,neilc,2016-01-13T22:17:55.000+0000,5,Resolved,Complete,GMock warning in ReservationTest.ACLMultipleOperations,2016-01-13T22:17:55.000+0000,MESOS-4347,1.0,mesos,Mesosphere Sprint 26
avinash@mesosphere.io,2016-01-12T17:09:56.000+0000,avinash@mesosphere.io,"As part of implementing the net_cls cgroup isolator we need a mechanism to manage the minor handles that will be allocated to containers when they are associated with a net_cls cgroup. The network-handle manager needs to provide the following functionality:

a) During normal operation keep track of the free and allocated network handles. There can be a total of 64K such network handles.
b) On startup, learn the allocated network handle by walking the net_cls cgroup tree for mesos and build a map of free network handles available to the agent. ",Task,Major,avinash@mesosphere.io,2016-02-06T19:28:58.000+0000,5,Resolved,Complete,Implement a network-handle manager for net_cls cgroup subsystem,2016-02-06T19:28:59.000+0000,MESOS-4345,3.0,mesos,Mesosphere Sprint 27
avinash@mesosphere.io,2016-01-12T17:00:32.000+0000,avinash@mesosphere.io,"The net_cls cgroup associates a 16-bit major and 16-bit minor network handle to packets originating from tasks associated with a specific net_cls cgroup. In mesos we need to give the operator the ability to fix the 16-bit major handle used in an agent (the minor handle will be allocated by the agent. See MESOS-4345). Fixing the parent handle on the agent allows operators to install default firewall rules using the parent handle to enforce a default policy (say DENY ALL) for all container traffic till the container is allocated a minor handle. 

A simple way to achieve this requirement is to pass the major handle as a flag to the agent at startup. ",Improvement,Major,avinash@mesosphere.io,2016-02-09T18:18:08.000+0000,5,Resolved,Complete,Allow operators to assign net_cls major handles to mesos agents,2016-02-09T18:18:08.000+0000,MESOS-4344,1.0,mesos,Mesosphere Sprint 27
klaus1982,2016-01-12T12:45:01.000+0000,klaus1982,Added a parameters to apply the patches quiet; so it's easy for contributor to apply patches with -c.,Bug,Minor,klaus1982,2016-01-13T01:28:33.000+0000,5,Resolved,Complete,Add parameters to apply patches quiet,2016-01-13T01:28:33.000+0000,MESOS-4342,1.0,mesos,
jojy,2016-01-12T01:37:50.000+0000,jojy,"We spawn shell for command line utilities like tar, untar, sha256 etc. Would be great for resuse if we can create a common utilities class/file for all these utilities.

",Bug,Major,jojy,2016-01-29T02:05:02.000+0000,5,Resolved,Complete,Create utilities for common shell commands used.,2016-03-11T19:00:50.000+0000,MESOS-4338,5.0,mesos,
bernd-mesos,2016-01-11T23:46:50.000+0000,ssk2hd,"The Mesos fetcher extracts specified URIs if requested to do so by the scheduler. However, the documentation at http://mesos.apache.org/documentation/latest/fetcher/ doesn't list the file types /extensions that will be extracted by the fetcher.

[The relevant code|https://github.com/apache/mesos/blob/master/src/launcher/fetcher.cpp#L63] specifies an exhaustive list of extensions that will be extracted, the documentation should be updated to match.",Documentation,Trivial,ssk2hd,2016-02-01T11:59:47.000+0000,5,Resolved,Complete,Document supported file types for archive extraction by fetcher,2016-02-01T11:59:47.000+0000,MESOS-4336,1.0,mesos,Mesosphere Sprint 27
jojy,2016-01-11T20:08:06.000+0000,jojy,Current tests can be refactored so that we can reuse some common tasks like test image creation. This will benefit future tests like appc image puller tests.,Improvement,Major,jojy,2016-02-08T23:39:23.000+0000,5,Resolved,Complete,Refactor Appc provisioner tests  ,2016-02-08T23:39:23.000+0000,MESOS-4333,2.0,mesos,Mesosphere Sprint 26
bbannier,2016-01-11T12:38:15.000+0000,bbannier,"Executing {{SlaveTest.LaunchTaskInfoWithContainerInfo}} from {{468b8ec}} under OS X 10.10.5 in isolation fails due to missing cleanup,
{code}
% ./bin/mesos-tests.sh --gtest_filter=SlaveTest.LaunchTaskInfoWithContainerInfo
Source directory: /ABC/DEF/src/mesos
Build directory: /ABC/DEF/src/mesos/build
-------------------------------------------------------------
We cannot run any Docker tests because:
Docker tests not supported on non-Linux systems
-------------------------------------------------------------
/usr/bin/nc
/usr/bin/curl
Note: Google Test filter = SlaveTest.LaunchTaskInfoWithContainerInfo-HealthCheckTest.ROOT_DOCKER_DockerHealthyTask:HealthCheckTest.ROOT_DOCKER_DockerHealthStatusChange:HierarchicalAllocator_BENCHMARK_Test.DeclineOffers:HookTest.ROOT_DOCKER_VerifySlavePreLaunchDockerHook:SlaveTest.ROOT_RunTaskWithCommandInfoWithoutUser:SlaveTest.DISABLED_ROOT_RunTaskWithCommandInfoWithUser:DockerContainerizerTest.ROOT_DOCKER_Launch:DockerContainerizerTest.ROOT_DOCKER_Kill:DockerContainerizerTest.ROOT_DOCKER_Usage:DockerContainerizerTest.ROOT_DOCKER_Recover:DockerContainerizerTest.ROOT_DOCKER_SkipRecoverNonDocker:DockerContainerizerTest.ROOT_DOCKER_Logs:DockerContainerizerTest.ROOT_DOCKER_Default_CMD:DockerContainerizerTest.ROOT_DOCKER_Default_CMD_Override:DockerContainerizerTest.ROOT_DOCKER_Default_CMD_Args:DockerContainerizerTest.ROOT_DOCKER_SlaveRecoveryTaskContainer:DockerContainerizerTest.DISABLED_ROOT_DOCKER_SlaveRecoveryExecutorContainer:DockerContainerizerTest.ROOT_DOCKER_NC_PortMapping:DockerContainerizerTest.ROOT_DOCKER_LaunchSandboxWithColon:DockerContainerizerTest.ROOT_DOCKER_DestroyWhileFetching:DockerContainerizerTest.ROOT_DOCKER_DestroyWhilePulling:DockerContainerizerTest.ROOT_DOCKER_ExecutorCleanupWhenLaunchFailed:DockerContainerizerTest.ROOT_DOCKER_FetchFailure:DockerContainerizerTest.ROOT_DOCKER_DockerPullFailure:DockerContainerizerTest.ROOT_DOCKER_DockerInspectDiscard:DockerTest.ROOT_DOCKER_interface:DockerTest.ROOT_DOCKER_parsing_version:DockerTest.ROOT_DOCKER_CheckCommandWithShell:DockerTest.ROOT_DOCKER_CheckPortResource:DockerTest.ROOT_DOCKER_CancelPull:DockerTest.ROOT_DOCKER_MountRelative:DockerTest.ROOT_DOCKER_MountAbsolute:CopyBackendTest.ROOT_CopyBackend:SlaveAndFrameworkCount/HierarchicalAllocator_BENCHMARK_Test.AddAndUpdateSlave/0:SlaveAndFrameworkCount/HierarchicalAllocator_BENCHMARK_Test.AddAndUpdateSlave/1:SlaveAndFrameworkCount/HierarchicalAllocator_BENCHMARK_Test.AddAndUpdateSlave/2:SlaveAndFrameworkCount/HierarchicalAllocator_BENCHMARK_Test.AddAndUpdateSlave/3:SlaveAndFrameworkCount/HierarchicalAllocator_BENCHMARK_Test.AddAndUpdateSlave/4:SlaveAndFrameworkCount/HierarchicalAllocator_BENCHMARK_Test.AddAndUpdateSlave/5:SlaveAndFrameworkCount/HierarchicalAllocator_BENCHMARK_Test.AddAndUpdateSlave/6:SlaveAndFrameworkCount/HierarchicalAllocator_BENCHMARK_Test.AddAndUpdateSlave/7:SlaveAndFrameworkCount/HierarchicalAllocator_BENCHMARK_Test.AddAndUpdateSlave/8:SlaveAndFrameworkCount/HierarchicalAllocator_BENCHMARK_Test.AddAndUpdateSlave/9:SlaveAndFrameworkCount/HierarchicalAllocator_BENCHMARK_Test.AddAndUpdateSlave/10:SlaveAndFrameworkCount/HierarchicalAllocator_BENCHMARK_Test.AddAndUpdateSlave/11:SlaveAndFrameworkCount/HierarchicalAllocator_BENCHMARK_Test.AddAndUpdateSlave/12:SlaveAndFrameworkCount/HierarchicalAllocator_BENCHMARK_Test.AddAndUpdateSlave/13:SlaveAndFrameworkCount/HierarchicalAllocator_BENCHMARK_Test.AddAndUpdateSlave/14:SlaveAndFrameworkCount/HierarchicalAllocator_BENCHMARK_Test.AddAndUpdateSlave/15:SlaveAndFrameworkCount/HierarchicalAllocator_BENCHMARK_Test.AddAndUpdateSlave/16:SlaveAndFrameworkCount/HierarchicalAllocator_BENCHMARK_Test.AddAndUpdateSlave/17:SlaveAndFrameworkCount/HierarchicalAllocator_BENCHMARK_Test.AddAndUpdateSlave/18:SlaveAndFrameworkCount/HierarchicalAllocator_BENCHMARK_Test.AddAndUpdateSlave/19:SlaveAndFrameworkCount/HierarchicalAllocator_BENCHMARK_Test.AddAndUpdateSlave/20:SlaveAndFrameworkCount/HierarchicalAllocator_BENCHMARK_Test.AddAndUpdateSlave/21:SlaveAndFrameworkCount/HierarchicalAllocator_BENCHMARK_Test.AddAndUpdateSlave/22:SlaveAndFrameworkCount/HierarchicalAllocator_BENCHMARK_Test.AddAndUpdateSlave/23:SlaveAndFrameworkCount/HierarchicalAllocator_BENCHMARK_Test.AddAndUpdateSlave/24:SlaveAndFrameworkCount/HierarchicalAllocator_BENCHMARK_Test.AddAndUpdateSlave/25:SlaveAndFrameworkCount/HierarchicalAllocator_BENCHMARK_Test.AddAndUpdateSlave/26:SlaveAndFrameworkCount/HierarchicalAllocator_BENCHMARK_Test.AddAndUpdateSlave/27:SlaveAndFrameworkCount/HierarchicalAllocator_BENCHMARK_Test.AddAndUpdateSlave/28:SlaveAndFrameworkCount/HierarchicalAllocator_BENCHMARK_Test.AddAndUpdateSlave/29:SlaveAndFrameworkCount/HierarchicalAllocator_BENCHMARK_Test.AddAndUpdateSlave/30:SlaveAndFrameworkCount/HierarchicalAllocator_BENCHMARK_Test.AddAndUpdateSlave/31:SlaveAndFrameworkCount/HierarchicalAllocator_BENCHMARK_Test.AddAndUpdateSlave/32:SlaveAndFrameworkCount/HierarchicalAllocator_BENCHMARK_Test.AddAndUpdateSlave/33:SlaveAndFrameworkCount/HierarchicalAllocator_BENCHMARK_Test.AddAndUpdateSlave/34:SlaveAndFrameworkCount/HierarchicalAllocator_BENCHMARK_Test.AddAndUpdateSlave/35:SlaveCount/Registrar_BENCHMARK_Test.Performance/0:SlaveCount/Registrar_BENCHMARK_Test.Performance/1:SlaveCount/Registrar_BENCHMARK_Test.Performance/2:SlaveCount/Registrar_BENCHMARK_Test.Performance/3
[==========] Running 1 test from 1 test case.
[----------] Global test environment set-up.
[----------] 1 test from SlaveTest
[ RUN      ] SlaveTest.LaunchTaskInfoWithContainerInfo
[       OK ] SlaveTest.LaunchTaskInfoWithContainerInfo (79 ms)
[----------] 1 test from SlaveTest (79 ms total)

[----------] Global test environment tear-down
../../src/tests/environment.cpp:569: Failure
Failed
Tests completed with child processes remaining:
-+- 54487 /ABC/DEF/src/mesos/build/src/.libs/mesos-tests --gtest_filter=SlaveTest.LaunchTaskInfoWithContainerInfo
 \--- 54503 /bin/sh /ABC/DEF/src/mesos/build/src/mesos-containerizer launch --command={""shell"":true,""value"":""\/ABC\/DEF\/src\/mesos\/build\/src\/mesos-executor""} --commands={""commands"":[]} --directory=/tmp --help=false --pipe_read=10 --pipe_write=13 --user=test
[==========] 1 test from 1 test case ran. (87 ms total)
[  PASSED  ] 1 test.
[  FAILED  ] 0 tests, listed below:

 0 FAILED TESTS
{code}
",Bug,Major,bbannier,2016-02-01T00:43:22.000+0000,5,Resolved,Complete,SlaveTest.LaunchTaskInfoWithContainerInfo cannot be execute in isolation,2016-02-29T09:36:57.000+0000,MESOS-4329,1.0,mesos,Mesosphere Sprint 27
greggomann,2016-01-08T21:27:04.000+0000,jieyu,"https://builds.apache.org/job/Mesos/1457/COMPILER=gcc,CONFIGURATION=--verbose%20--enable-libevent%20--enable-ssl,OS=centos:7,label_exp=docker%7C%7CHadoop/consoleFull

{noformat}
[ RUN      ] PersistentVolumeTest.BadACLNoPrincipal
I0108 01:13:16.117883  1325 leveldb.cpp:174] Opened db in 2.614722ms
I0108 01:13:16.118650  1325 leveldb.cpp:181] Compacted db in 706567ns
I0108 01:13:16.118702  1325 leveldb.cpp:196] Created db iterator in 24489ns
I0108 01:13:16.118723  1325 leveldb.cpp:202] Seeked to beginning of db in 2436ns
I0108 01:13:16.118738  1325 leveldb.cpp:271] Iterated through 0 keys in the db in 397ns
I0108 01:13:16.118793  1325 replica.cpp:779] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned
I0108 01:13:16.119627  1348 recover.cpp:447] Starting replica recovery
I0108 01:13:16.120352  1348 recover.cpp:473] Replica is in EMPTY status
I0108 01:13:16.121750  1357 replica.cpp:673] Replica in EMPTY status received a broadcasted recover request from (7084)@172.17.0.2:32801
I0108 01:13:16.122297  1353 recover.cpp:193] Received a recover response from a replica in EMPTY status
I0108 01:13:16.122747  1350 recover.cpp:564] Updating replica status to STARTING
I0108 01:13:16.123625  1354 master.cpp:365] Master 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2 (d9632dd1c41e) started on 172.17.0.2:32801
I0108 01:13:16.123946  1347 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 728242ns
I0108 01:13:16.123999  1347 replica.cpp:320] Persisted replica status to STARTING
I0108 01:13:16.123708  1354 master.cpp:367] Flags at startup: --acls=""create_volumes {
  principals {
    values: ""test-principal""
  }
  volume_types {
    type: ANY
  }
}
create_volumes {
  principals {
    type: ANY
  }
  volume_types {
    type: NONE
  }
}
"" --allocation_interval=""1secs"" --allocator=""HierarchicalDRF"" --authenticate=""false"" --authenticate_slaves=""true"" --authenticators=""crammd5"" --authorizers=""local"" --credentials=""/tmp/f2rA75/credentials"" --framework_sorter=""drf"" --help=""false"" --hostname_lookup=""true"" --initialize_driver_logging=""true"" --log_auto_initialize=""true"" --logbufsecs=""0"" --logging_level=""INFO"" --max_slave_ping_timeouts=""5"" --quiet=""false"" --recovery_slave_removal_limit=""100%"" --registry=""replicated_log"" --registry_fetch_timeout=""1mins"" --registry_store_timeout=""25secs"" --registry_strict=""true"" --roles=""role1"" --root_submissions=""true"" --slave_ping_timeout=""15secs"" --slave_reregister_timeout=""10mins"" --user_sorter=""drf"" --version=""false"" --webui_dir=""/mesos/mesos-0.27.0/_inst/share/mesos/webui"" --work_dir=""/tmp/f2rA75/master"" --zk_session_timeout=""10secs""
I0108 01:13:16.124219  1354 master.cpp:414] Master allowing unauthenticated frameworks to register
I0108 01:13:16.124236  1354 master.cpp:417] Master only allowing authenticated slaves to register
I0108 01:13:16.124248  1354 credentials.hpp:35] Loading credentials for authentication from '/tmp/f2rA75/credentials'
I0108 01:13:16.124294  1358 recover.cpp:473] Replica is in STARTING status
I0108 01:13:16.124644  1354 master.cpp:456] Using default 'crammd5' authenticator
I0108 01:13:16.124820  1354 master.cpp:493] Authorization enabled
W0108 01:13:16.124843  1354 master.cpp:553] The '--roles' flag is deprecated. This flag will be removed in the future. See the Mesos 0.27 upgrade notes for more information
I0108 01:13:16.125154  1348 hierarchical.cpp:147] Initialized hierarchical allocator process
I0108 01:13:16.125334  1345 whitelist_watcher.cpp:77] No whitelist given
I0108 01:13:16.126065  1346 replica.cpp:673] Replica in STARTING status received a broadcasted recover request from (7085)@172.17.0.2:32801
I0108 01:13:16.126806  1348 recover.cpp:193] Received a recover response from a replica in STARTING status
I0108 01:13:16.128237  1354 recover.cpp:564] Updating replica status to VOTING
I0108 01:13:16.128402  1359 master.cpp:1629] The newly elected leader is master@172.17.0.2:32801 with id 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2
I0108 01:13:16.128489  1359 master.cpp:1642] Elected as the leading master!
I0108 01:13:16.128523  1359 master.cpp:1387] Recovering from registrar
I0108 01:13:16.128756  1355 registrar.cpp:307] Recovering registrar
I0108 01:13:16.129259  1344 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 531437ns
I0108 01:13:16.129292  1344 replica.cpp:320] Persisted replica status to VOTING
I0108 01:13:16.129425  1358 recover.cpp:578] Successfully joined the Paxos group
I0108 01:13:16.129680  1358 recover.cpp:462] Recover process terminated
I0108 01:13:16.130187  1358 log.cpp:659] Attempting to start the writer
I0108 01:13:16.131613  1352 replica.cpp:493] Replica received implicit promise request from (7086)@172.17.0.2:32801 with proposal 1
I0108 01:13:16.131983  1352 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 333646ns
I0108 01:13:16.132004  1352 replica.cpp:342] Persisted promised to 1
I0108 01:13:16.132627  1348 coordinator.cpp:238] Coordinator attempting to fill missing positions
I0108 01:13:16.133896  1349 replica.cpp:388] Replica received explicit promise request from (7087)@172.17.0.2:32801 for position 0 with proposal 2
I0108 01:13:16.134289  1349 leveldb.cpp:341] Persisting action (8 bytes) to leveldb took 349652ns
I0108 01:13:16.134317  1349 replica.cpp:712] Persisted action at 0
I0108 01:13:16.135470  1351 replica.cpp:537] Replica received write request for position 0 from (7088)@172.17.0.2:32801
I0108 01:13:16.135537  1351 leveldb.cpp:436] Reading position from leveldb took 36181ns
I0108 01:13:16.135901  1351 leveldb.cpp:341] Persisting action (14 bytes) to leveldb took 308752ns
I0108 01:13:16.135924  1351 replica.cpp:712] Persisted action at 0
I0108 01:13:16.136529  1347 replica.cpp:691] Replica received learned notice for position 0 from @0.0.0.0:0
I0108 01:13:16.136889  1347 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 327106ns
I0108 01:13:16.136916  1347 replica.cpp:712] Persisted action at 0
I0108 01:13:16.136943  1347 replica.cpp:697] Replica learned NOP action at position 0
I0108 01:13:16.137707  1359 log.cpp:675] Writer started with ending position 0
I0108 01:13:16.138844  1348 leveldb.cpp:436] Reading position from leveldb took 31371ns
I0108 01:13:16.139878  1356 registrar.cpp:340] Successfully fetched the registry (0B) in 0ns
I0108 01:13:16.140012  1356 registrar.cpp:439] Applied 1 operations in 42063ns; attempting to update the 'registry'
I0108 01:13:16.140797  1355 log.cpp:683] Attempting to append 170 bytes to the log
I0108 01:13:16.140974  1345 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 1
I0108 01:13:16.141744  1354 replica.cpp:537] Replica received write request for position 1 from (7089)@172.17.0.2:32801
I0108 01:13:16.142226  1354 leveldb.cpp:341] Persisting action (189 bytes) to leveldb took 441971ns
I0108 01:13:16.142251  1354 replica.cpp:712] Persisted action at 1
I0108 01:13:16.142860  1351 replica.cpp:691] Replica received learned notice for position 1 from @0.0.0.0:0
I0108 01:13:16.143198  1351 leveldb.cpp:341] Persisting action (191 bytes) to leveldb took 305928ns
I0108 01:13:16.143223  1351 replica.cpp:712] Persisted action at 1
I0108 01:13:16.143241  1351 replica.cpp:697] Replica learned APPEND action at position 1
I0108 01:13:16.144271  1354 registrar.cpp:484] Successfully updated the 'registry' in 0ns
I0108 01:13:16.144435  1354 registrar.cpp:370] Successfully recovered registrar
I0108 01:13:16.144567  1359 log.cpp:702] Attempting to truncate the log to 1
I0108 01:13:16.144780  1359 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 2
I0108 01:13:16.144989  1348 hierarchical.cpp:165] Skipping recovery of hierarchical allocator: nothing to recover
I0108 01:13:16.144928  1354 master.cpp:1439] Recovered 0 slaves from the Registry (131B) ; allowing 10mins for slaves to re-register
I0108 01:13:16.145690  1357 replica.cpp:537] Replica received write request for position 2 from (7090)@172.17.0.2:32801
I0108 01:13:16.146072  1357 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 345113ns
I0108 01:13:16.146097  1357 replica.cpp:712] Persisted action at 2
I0108 01:13:16.146667  1358 replica.cpp:691] Replica received learned notice for position 2 from @0.0.0.0:0
I0108 01:13:16.147060  1358 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 283648ns
I0108 01:13:16.147116  1358 leveldb.cpp:399] Deleting ~1 keys from leveldb took 32174ns
I0108 01:13:16.147135  1358 replica.cpp:712] Persisted action at 2
I0108 01:13:16.147153  1358 replica.cpp:697] Replica learned TRUNCATE action at position 2
I0108 01:13:16.166832  1325 containerizer.cpp:139] Using isolation: posix/cpu,posix/mem,filesystem/posix
W0108 01:13:16.167556  1325 backend.cpp:48] Failed to create 'bind' backend: BindBackend requires root privileges
I0108 01:13:16.170526  1349 slave.cpp:191] Slave started on 231)@172.17.0.2:32801
I0108 01:13:16.170718  1349 slave.cpp:192] Flags at startup: --appc_store_dir=""/tmp/mesos/store/appc"" --authenticatee=""crammd5"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --credential=""/tmp/PersistentVolumeTest_BadACLNoPrincipal_yqJjLY/credential"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_auth_server=""https://auth.docker.io"" --docker_kill_orphans=""true"" --docker_puller_timeout=""60"" --docker_registry=""https://registry-1.docker.io"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/tmp/mesos/store/docker"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/tmp/PersistentVolumeTest_BadACLNoPrincipal_yqJjLY/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --hostname_lookup=""true"" --image_provisioner_backend=""copy"" --initialize_driver_logging=""true"" --isolation=""posix/cpu,posix/mem"" --launcher_dir=""/mesos/mesos-0.27.0/_build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""10ms"" --resources=""cpus:2;mem:1024;disk(role1):2048"" --revocable_cpu_low_priority=""true"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --systemd_runtime_directory=""/run/systemd/system"" --version=""false"" --work_dir=""/tmp/PersistentVolumeTest_BadACLNoPrincipal_yqJjLY""
I0108 01:13:16.171269  1349 credentials.hpp:83] Loading credential for authentication from '/tmp/PersistentVolumeTest_BadACLNoPrincipal_yqJjLY/credential'
I0108 01:13:16.171505  1349 slave.cpp:322] Slave using credential for: test-principal
I0108 01:13:16.171747  1349 resources.cpp:481] Parsing resources as JSON failed: cpus:2;mem:1024;disk(role1):2048
Trying semicolon-delimited string format instead
I0108 01:13:16.172266  1349 slave.cpp:392] Slave resources: cpus(*):2; mem(*):1024; disk(role1):2048; ports(*):[31000-32000]
I0108 01:13:16.172327  1349 slave.cpp:400] Slave attributes: [  ]
I0108 01:13:16.172340  1349 slave.cpp:405] Slave hostname: d9632dd1c41e
I0108 01:13:16.172353  1349 slave.cpp:410] Slave checkpoint: true
I0108 01:13:16.173418  1353 state.cpp:58] Recovering state from '/tmp/PersistentVolumeTest_BadACLNoPrincipal_yqJjLY/meta'
I0108 01:13:16.173521  1325 sched.cpp:164] Version: 0.27.0
I0108 01:13:16.174054  1345 status_update_manager.cpp:200] Recovering status update manager
I0108 01:13:16.174289  1353 containerizer.cpp:387] Recovering containerizer
I0108 01:13:16.174295  1356 sched.cpp:268] New master detected at master@172.17.0.2:32801
I0108 01:13:16.174387  1356 sched.cpp:278] No credentials provided. Attempting to register without authentication
I0108 01:13:16.174409  1356 sched.cpp:722] Sending SUBSCRIBE call to master@172.17.0.2:32801
I0108 01:13:16.174515  1356 sched.cpp:755] Will retry registration in 1.699889272secs if necessary
I0108 01:13:16.174653  1349 master.cpp:2197] Received SUBSCRIBE call for framework 'no-principal' at scheduler-bf0ed267-b4c4-412d-9fb0-84c85cd2fbce@172.17.0.2:32801
I0108 01:13:16.174823  1349 master.cpp:1668] Authorizing framework principal '' to receive offers for role 'role1'
I0108 01:13:16.175250  1347 master.cpp:2268] Subscribing framework no-principal with checkpointing disabled and capabilities [  ]
I0108 01:13:16.175359  1353 slave.cpp:4429] Finished recovery
I0108 01:13:16.175715  1345 hierarchical.cpp:260] Added framework 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-0000
I0108 01:13:16.175734  1351 sched.cpp:649] Framework registered with 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-0000
I0108 01:13:16.175792  1345 hierarchical.cpp:1329] No resources available to allocate!
I0108 01:13:16.175833  1345 hierarchical.cpp:1423] No inverse offers to send out!
I0108 01:13:16.175853  1353 slave.cpp:4601] Querying resource estimator for oversubscribable resources
I0108 01:13:16.175869  1345 hierarchical.cpp:1079] Performed allocation for 0 slaves in 127881ns
I0108 01:13:16.175923  1351 sched.cpp:663] Scheduler::registered took 27956ns
I0108 01:13:16.176110  1353 slave.cpp:729] New master detected at master@172.17.0.2:32801
I0108 01:13:16.176187  1353 slave.cpp:792] Authenticating with master master@172.17.0.2:32801
I0108 01:13:16.176216  1353 slave.cpp:797] Using default CRAM-MD5 authenticatee
I0108 01:13:16.176398  1357 status_update_manager.cpp:174] Pausing sending status updates
I0108 01:13:16.176404  1353 slave.cpp:765] Detecting new master
I0108 01:13:16.176463  1358 authenticatee.cpp:121] Creating new client SASL connection
I0108 01:13:16.176553  1353 slave.cpp:4615] Received oversubscribable resources  from the resource estimator
I0108 01:13:16.176709  1353 master.cpp:5445] Authenticating slave(231)@172.17.0.2:32801
I0108 01:13:16.176823  1359 authenticator.cpp:413] Starting authentication session for crammd5_authenticatee(516)@172.17.0.2:32801
I0108 01:13:16.177135  1348 authenticator.cpp:98] Creating new server SASL connection
I0108 01:13:16.177373  1356 authenticatee.cpp:212] Received SASL authentication mechanisms: CRAM-MD5
I0108 01:13:16.177399  1356 authenticatee.cpp:238] Attempting to authenticate with mechanism 'CRAM-MD5'
I0108 01:13:16.177502  1344 authenticator.cpp:203] Received SASL authentication start
I0108 01:13:16.177563  1344 authenticator.cpp:325] Authentication requires more steps
I0108 01:13:16.177680  1346 authenticatee.cpp:258] Received SASL authentication step
I0108 01:13:16.177848  1354 authenticator.cpp:231] Received SASL authentication step
I0108 01:13:16.177883  1354 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: 'd9632dd1c41e' server FQDN: 'd9632dd1c41e' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0108 01:13:16.177894  1354 auxprop.cpp:179] Looking up auxiliary property '*userPassword'
I0108 01:13:16.177944  1354 auxprop.cpp:179] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0108 01:13:16.177994  1354 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: 'd9632dd1c41e' server FQDN: 'd9632dd1c41e' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0108 01:13:16.178014  1354 auxprop.cpp:129] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0108 01:13:16.178040  1354 auxprop.cpp:129] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0108 01:13:16.178066  1354 authenticator.cpp:317] Authentication success
I0108 01:13:16.178256  1355 authenticatee.cpp:298] Authentication success
I0108 01:13:16.178315  1354 master.cpp:5475] Successfully authenticated principal 'test-principal' at slave(231)@172.17.0.2:32801
I0108 01:13:16.178356  1355 authenticator.cpp:431] Authentication session cleanup for crammd5_authenticatee(516)@172.17.0.2:32801
I0108 01:13:16.178710  1354 slave.cpp:860] Successfully authenticated with master master@172.17.0.2:32801
I0108 01:13:16.178865  1354 slave.cpp:1254] Will retry registration in 13.009431ms if necessary
I0108 01:13:16.179138  1350 master.cpp:4154] Registering slave at slave(231)@172.17.0.2:32801 (d9632dd1c41e) with id 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-S0
I0108 01:13:16.179628  1345 registrar.cpp:439] Applied 1 operations in 71663ns; attempting to update the 'registry'
I0108 01:13:16.180505  1356 log.cpp:683] Attempting to append 343 bytes to the log
I0108 01:13:16.180711  1352 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 3
I0108 01:13:16.181499  1350 replica.cpp:537] Replica received write request for position 3 from (7103)@172.17.0.2:32801
I0108 01:13:16.182080  1350 leveldb.cpp:341] Persisting action (362 bytes) to leveldb took 537757ns
I0108 01:13:16.182112  1350 replica.cpp:712] Persisted action at 3
I0108 01:13:16.182749  1351 replica.cpp:691] Replica received learned notice for position 3 from @0.0.0.0:0
I0108 01:13:16.183120  1351 leveldb.cpp:341] Persisting action (364 bytes) to leveldb took 340999ns
I0108 01:13:16.183151  1351 replica.cpp:712] Persisted action at 3
I0108 01:13:16.183177  1351 replica.cpp:697] Replica learned APPEND action at position 3
I0108 01:13:16.184787  1348 registrar.cpp:484] Successfully updated the 'registry' in 0ns
I0108 01:13:16.185287  1348 log.cpp:702] Attempting to truncate the log to 3
I0108 01:13:16.185484  1349 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 4
I0108 01:13:16.186043  1353 slave.cpp:3371] Received ping from slave-observer(230)@172.17.0.2:32801
I0108 01:13:16.186074  1345 master.cpp:4222] Registered slave 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-S0 at slave(231)@172.17.0.2:32801 (d9632dd1c41e) with cpus(*):2; mem(*):1024; disk(role1):2048; ports(*):[31000-32000]
I0108 01:13:16.186224  1353 slave.cpp:904] Registered with master master@172.17.0.2:32801; given slave ID 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-S0
I0108 01:13:16.186441  1353 fetcher.cpp:81] Clearing fetcher cache
I0108 01:13:16.186486  1349 hierarchical.cpp:465] Added slave 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-S0 (d9632dd1c41e) with cpus(*):2; mem(*):1024; disk(role1):2048; ports(*):[31000-32000] (allocated: )
I0108 01:13:16.186658  1346 status_update_manager.cpp:181] Resuming sending status updates
I0108 01:13:16.186885  1353 slave.cpp:927] Checkpointing SlaveInfo to '/tmp/PersistentVolumeTest_BadACLNoPrincipal_yqJjLY/meta/slaves/773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-S0/slave.info'
I0108 01:13:16.186905  1350 replica.cpp:537] Replica received write request for position 4 from (7104)@172.17.0.2:32801
I0108 01:13:16.187595  1350 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 645704ns
I0108 01:13:16.187628  1350 replica.cpp:712] Persisted action at 4
I0108 01:13:16.188347  1349 hierarchical.cpp:1423] No inverse offers to send out!
I0108 01:13:16.188475  1349 hierarchical.cpp:1101] Performed allocation for slave 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-S0 in 1.861833ms
I0108 01:13:16.188560  1348 replica.cpp:691] Replica received learned notice for position 4 from @0.0.0.0:0
I0108 01:13:16.188385  1353 slave.cpp:963] Forwarding total oversubscribed resources 
I0108 01:13:16.189275  1344 master.cpp:5274] Sending 1 offers to framework 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-0000 (no-principal) at scheduler-bf0ed267-b4c4-412d-9fb0-84c85cd2fbce@172.17.0.2:32801
I0108 01:13:16.189792  1344 master.cpp:4564] Received update of slave 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-S0 at slave(231)@172.17.0.2:32801 (d9632dd1c41e) with total oversubscribed resources 
I0108 01:13:16.189851  1348 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 1.204958ms
I0108 01:13:16.190150  1348 leveldb.cpp:399] Deleting ~2 keys from leveldb took 62381ns
I0108 01:13:16.190265  1348 replica.cpp:712] Persisted action at 4
I0108 01:13:16.190402  1348 replica.cpp:697] Replica learned TRUNCATE action at position 4
I0108 01:13:16.191192  1349 sched.cpp:819] Scheduler::resourceOffers took 126783ns
I0108 01:13:16.191253  1359 hierarchical.cpp:521] Slave 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-S0 (d9632dd1c41e) updated with oversubscribed resources  (total: cpus(*):2; mem(*):1024; disk(role1):2048; ports(*):[31000-32000], allocated: cpus(*):2; mem(*):1024; ports(*):[31000-32000]; disk(role1):2048)
I0108 01:13:16.191529  1359 hierarchical.cpp:1329] No resources available to allocate!
I0108 01:13:16.191591  1359 hierarchical.cpp:1423] No inverse offers to send out!
I0108 01:13:16.191627  1359 hierarchical.cpp:1101] Performed allocation for slave 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-S0 in 310808ns
I0108 01:13:16.195103  1349 hierarchical.cpp:1329] No resources available to allocate!
I0108 01:13:16.195171  1349 hierarchical.cpp:1423] No inverse offers to send out!
I0108 01:13:16.195205  1349 hierarchical.cpp:1079] Performed allocation for 1 slaves in 368834ns
I0108 01:13:16.205402  1351 master.cpp:3055] Processing ACCEPT call for offers: [ 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-O0 ] on slave 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-S0 at slave(231)@172.17.0.2:32801 (d9632dd1c41e) for framework 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-0000 (no-principal) at scheduler-bf0ed267-b4c4-412d-9fb0-84c85cd2fbce@172.17.0.2:32801
I0108 01:13:16.205471  1351 master.cpp:2843] Authorizing principal 'ANY' to create volumes
E0108 01:13:16.206641  1351 master.cpp:1737] Dropping CREATE offer operation from framework 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-0000 (no-principal) at scheduler-bf0ed267-b4c4-412d-9fb0-84c85cd2fbce@172.17.0.2:32801: Not authorized to create persistent volumes as ''
I0108 01:13:16.207283  1351 hierarchical.cpp:880] Recovered cpus(*):2; mem(*):1024; ports(*):[31000-32000]; disk(role1):2048 (total: cpus(*):2; mem(*):1024; disk(role1):2048; ports(*):[31000-32000], allocated: ) on slave 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-S0 from framework 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-0000
I0108 01:13:16.216485  1348 hierarchical.cpp:1423] No inverse offers to send out!
I0108 01:13:16.216562  1348 hierarchical.cpp:1079] Performed allocation for 1 slaves in 983574ns
I0108 01:13:16.216915  1345 master.cpp:5274] Sending 1 offers to framework 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-0000 (no-principal) at scheduler-bf0ed267-b4c4-412d-9fb0-84c85cd2fbce@172.17.0.2:32801
I0108 01:13:16.217514  1345 sched.cpp:819] Scheduler::resourceOffers took 82354ns
I0108 01:13:16.227466  1348 master.cpp:3592] Processing DECLINE call for offers: [ 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-O1 ] for framework 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-0000 (no-principal) at scheduler-bf0ed267-b4c4-412d-9fb0-84c85cd2fbce@172.17.0.2:32801
I0108 01:13:16.227843  1325 sched.cpp:164] Version: 0.27.0
I0108 01:13:16.228489  1344 hierarchical.cpp:880] Recovered cpus(*):2; mem(*):1024; ports(*):[31000-32000]; disk(role1):2048 (total: cpus(*):2; mem(*):1024; disk(role1):2048; ports(*):[31000-32000], allocated: ) on slave 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-S0 from framework 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-0000
I0108 01:13:16.228989  1346 sched.cpp:268] New master detected at master@172.17.0.2:32801
I0108 01:13:16.229118  1346 sched.cpp:278] No credentials provided. Attempting to register without authentication
I0108 01:13:16.229143  1346 sched.cpp:722] Sending SUBSCRIBE call to master@172.17.0.2:32801
I0108 01:13:16.229277  1346 sched.cpp:755] Will retry registration in 1.383902465secs if necessary
I0108 01:13:16.229912  1348 master.cpp:2650] Processing SUPPRESS call for framework 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-0000 (no-principal) at scheduler-bf0ed267-b4c4-412d-9fb0-84c85cd2fbce@172.17.0.2:32801
I0108 01:13:16.230171  1346 hierarchical.cpp:953] Suppressed offers for framework 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-0000
I0108 01:13:16.230262  1348 master.cpp:2197] Received SUBSCRIBE call for framework 'default' at scheduler-4eccbeb6-560d-41be-b1d6-1e2971db4bb3@172.17.0.2:32801
I0108 01:13:16.230370  1348 master.cpp:1668] Authorizing framework principal 'test-principal' to receive offers for role 'role1'
I0108 01:13:16.230788  1348 master.cpp:2268] Subscribing framework default with checkpointing disabled and capabilities [  ]
I0108 01:13:16.231477  1346 hierarchical.cpp:260] Added framework 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-0001
I0108 01:13:16.232698  1346 hierarchical.cpp:1423] No inverse offers to send out!
I0108 01:13:16.232795  1346 hierarchical.cpp:1079] Performed allocation for 1 slaves in 1.282992ms
I0108 01:13:16.233512  1348 master.cpp:5274] Sending 1 offers to framework 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-0001 (default) at scheduler-4eccbeb6-560d-41be-b1d6-1e2971db4bb3@172.17.0.2:32801
I0108 01:13:16.233728  1351 sched.cpp:649] Framework registered with 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-0001
I0108 01:13:16.233800  1351 sched.cpp:663] Scheduler::registered took 29498ns
I0108 01:13:16.234381  1359 sched.cpp:819] Scheduler::resourceOffers took 113212ns
I0108 01:13:16.239941  1348 hierarchical.cpp:1329] No resources available to allocate!
I0108 01:13:16.240223  1348 hierarchical.cpp:1423] No inverse offers to send out!
I0108 01:13:16.240275  1348 hierarchical.cpp:1079] Performed allocation for 1 slaves in 633949ns
I0108 01:13:16.251688  1357 master.cpp:3055] Processing ACCEPT call for offers: [ 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-O2 ] on slave 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-S0 at slave(231)@172.17.0.2:32801 (d9632dd1c41e) for framework 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-0001 (default) at scheduler-4eccbeb6-560d-41be-b1d6-1e2971db4bb3@172.17.0.2:32801
I0108 01:13:16.251785  1357 master.cpp:2843] Authorizing principal 'test-principal' to create volumes
I0108 01:13:16.253445  1352 master.cpp:3384] Applying CREATE operation for volumes disk(role1)[id1:path1]:128 from framework 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-0001 (default) at scheduler-4eccbeb6-560d-41be-b1d6-1e2971db4bb3@172.17.0.2:32801 to slave 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-S0 at slave(231)@172.17.0.2:32801 (d9632dd1c41e)
I0108 01:13:16.253911  1352 master.cpp:6508] Sending checkpointed resources disk(role1)[id1:path1]:128 to slave 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-S0 at slave(231)@172.17.0.2:32801 (d9632dd1c41e)
I0108 01:13:16.255210  1352 slave.cpp:2277] Updated checkpointed resources from  to disk(role1)[id1:path1]:128
I0108 01:13:16.257128  1356 hierarchical.cpp:642] Updated allocation of framework 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-0001 on slave 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-S0 from cpus(*):2; mem(*):1024; ports(*):[31000-32000]; disk(role1):2048 to cpus(*):2; mem(*):1024; ports(*):[31000-32000]; disk(role1):1920; disk(role1)[id1:path1]:128
I0108 01:13:16.257844  1356 hierarchical.cpp:880] Recovered cpus(*):2; mem(*):1024; ports(*):[31000-32000]; disk(role1):1920; disk(role1)[id1:path1]:128 (total: cpus(*):2; mem(*):1024; disk(role1):1920; ports(*):[31000-32000]; disk(role1)[id1:path1]:128, allocated: ) on slave 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-S0 from framework 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-0001
I0108 01:13:16.262976  1344 hierarchical.cpp:1423] No inverse offers to send out!
I0108 01:13:16.263068  1344 hierarchical.cpp:1079] Performed allocation for 1 slaves in 1.435723ms
I0108 01:13:16.263535  1353 master.cpp:5274] Sending 1 offers to framework 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-0001 (default) at scheduler-4eccbeb6-560d-41be-b1d6-1e2971db4bb3@172.17.0.2:32801
I0108 01:13:16.264181  1356 sched.cpp:819] Scheduler::resourceOffers took 139353ns
I0108 01:13:16.271931  1355 master.cpp:3671] Processing REVIVE call for framework 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-0000 (no-principal) at scheduler-bf0ed267-b4c4-412d-9fb0-84c85cd2fbce@172.17.0.2:32801
I0108 01:13:16.272141  1359 hierarchical.cpp:973] Removed offer filters for framework 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-0000
I0108 01:13:16.272177  1355 master.cpp:3592] Processing DECLINE call for offers: [ 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-O3 ] for framework 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-0001 (default) at scheduler-4eccbeb6-560d-41be-b1d6-1e2971db4bb3@172.17.0.2:32801
I0108 01:13:16.272423  1359 hierarchical.cpp:1329] No resources available to allocate!
I0108 01:13:16.272483  1359 hierarchical.cpp:1423] No inverse offers to send out!
I0108 01:13:16.272514  1359 hierarchical.cpp:1079] Performed allocation for 1 slaves in 344563ns
I0108 01:13:16.272924  1355 master.cpp:2650] Processing SUPPRESS call for framework 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-0001 (default) at scheduler-4eccbeb6-560d-41be-b1d6-1e2971db4bb3@172.17.0.2:32801
I0108 01:13:16.272989  1359 hierarchical.cpp:880] Recovered cpus(*):2; mem(*):1024; ports(*):[31000-32000]; disk(role1):1920; disk(role1)[id1:path1]:128 (total: cpus(*):2; mem(*):1024; disk(role1):1920; ports(*):[31000-32000]; disk(role1)[id1:path1]:128, allocated: ) on slave 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-S0 from framework 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-0001
I0108 01:13:16.273309  1359 hierarchical.cpp:953] Suppressed offers for framework 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-0001
2016-01-08 01:13:18,959:1325(0x7fb7cd6ae700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:50826] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2016-01-08 01:13:22,295:1325(0x7fb7cd6ae700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:50826] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2016-01-08 01:13:25,631:1325(0x7fb7cd6ae700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:50826] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2016-01-08 01:13:28,968:1325(0x7fb7cd6ae700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:50826] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
../../src/tests/persistent_volume_tests.cpp:1211: Failure
Failed to wait 15secs for offers
I0108 01:13:31.277577  1354 master.cpp:1130] Framework 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-0001 (default) at scheduler-4eccbeb6-560d-41be-b1d6-1e2971db4bb3@172.17.0.2:32801 disconnected
I../../src/tests/persistent_volume_tests.cpp:1204: Failure
Actual function call count doesn't match EXPECT_CALL(sched1, resourceOffers(&driver1, _))...
         Expected: to be called once
           Actual: never called - unsatisfied and active
0108 01:13:31.277909  1354 master.cpp:2493] Disconnecting framework 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-0001 (default) at scheduler-4eccbeb6-560d-41be-b1d6-1e2971db4bb3@172.17.0.2:32801
I0108 01:13:31.279088  1354 master.cpp:2517] Deactivating framework 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-0001 (default) at scheduler-4eccbeb6-560d-41be-b1d6-1e2971db4bb3@172.17.0.2:32801
I0108 01:13:31.279496  1354 master.cpp:1154] Giving framework 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-0001 (default) at scheduler-4eccbeb6-560d-41be-b1d6-1e2971db4bb3@172.17.0.2:32801 0ns to failover
I0108 01:13:31.280046  1354 master.cpp:1130] Framework 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-0000 (no-principal) at scheduler-bf0ed267-b4c4-412d-9fb0-84c85cd2fbce@172.17.0.2:32801 disconnected
I0108 01:13:31.280603  1354 master.cpp:2493] Disconnecting framework 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-0000 (no-principal) at scheduler-bf0ed267-b4c4-412d-9fb0-84c85cd2fbce@172.17.0.2:32801
I0108 01:13:31.280644  1354 master.cpp:2517] Deactivating framework 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-0000 (no-principal) at scheduler-bf0ed267-b4c4-412d-9fb0-84c85cd2fbce@172.17.0.2:32801
I0108 01:13:31.280863  1354 master.cpp:1154] Giving framework 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-0000 (no-principal) at scheduler-bf0ed267-b4c4-412d-9fb0-84c85cd2fbce@172.17.0.2:32801 0ns to failover
I0108 01:13:31.280563  1348 hierarchical.cpp:366] Deactivated framework 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-0001
I0108 01:13:31.281056  1348 hierarchical.cpp:366] Deactivated framework 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-0000
I0108 01:13:31.281097  1354 master.cpp:930] Master terminating
I0108 01:13:31.281910  1355 hierarchical.cpp:496] Removed slave 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-S0
I0108 01:13:31.282516  1352 hierarchical.cpp:321] Removed framework 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-0001
I0108 01:13:31.282817  1352 hierarchical.cpp:321] Removed framework 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-0000
I0108 01:13:31.282985  1352 slave.cpp:3417] master@172.17.0.2:32801 exited
W0108 01:13:31.283144  1352 slave.cpp:3420] Master disconnected! Waiting for a new master to be elected
I0108 01:13:31.313812  1346 slave.cpp:601] Slave terminating
[  FAILED  ] PersistentVolumeTest.BadACLNoPrincipal (15203 ms)
{noformat}",Bug,Major,jieyu,2016-01-13T00:08:23.000+0000,5,Resolved,Complete,PersistentVolumeTest.BadACLNoPrincipal is flaky,2016-01-13T00:08:23.000+0000,MESOS-4318,1.0,mesos,Mesosphere Sprint 26
gradywang,2016-01-08T14:46:55.000+0000,gradywang,"Like /quota, we should also add query logic for /weights to keep consistent. Then /roles no longer needs to show weight information.",Task,Minor,gradywang,2016-04-25T17:16:04.000+0000,5,Resolved,Complete,Support get non-default weights by /weights,2016-04-25T17:16:04.000+0000,MESOS-4316,5.0,mesos,Mesosphere Sprint 31
js84,2016-01-08T13:06:39.000+0000,js84,Publish and finish the operator guide draft  for quota which describes basic usage of the endpoints and few basic and advanced usage cases.,Documentation,Major,js84,2016-01-16T23:31:27.000+0000,5,Resolved,Complete,Publish Quota Documentation,2016-01-16T23:32:01.000+0000,MESOS-4314,3.0,mesos,Mesosphere Sprint 26
gilbert,2016-01-08T02:04:10.000+0000,gilbert,"Currently when protobuf::parse handles nested JSON objects, it cannot pass any error message out. We should enable showing those error messages.",Bug,Major,gilbert,2016-01-12T01:23:33.000+0000,5,Resolved,Complete,Protobuf parse should pass error messages when parsing nested JSON.,2016-02-27T01:26:23.000+0000,MESOS-4311,1.0,mesos,Mesosphere Sprint 26
,2016-01-07T18:47:48.000+0000,adam-mesos,"Now that executor terminations are reported (unreliably), we should investigate queuing up these messages (on the agent?) and resending them periodically until we get an acknowledgement, much like status updates do.

From MESOS-313: The Scheduler interface has a callback for executorLost, but currently it is never called.",Improvement,Major,woggle,,10020,Accepted,In Progress,Reliably report executor terminations to framework schedulers.,2016-05-02T17:28:24.000+0000,MESOS-4308,5.0,mesos,Mesosphere Sprint 24
,2016-01-07T17:34:30.000+0000,greggomann,"The ""Getting Started"" documentation currently contains basic instructions to prepare several platforms for compilation and installation of Mesos. However, these instructions are not sufficient to run and pass all tests in the test suite, using all configuration options. The installation instructions should be made comprehensive in this respect.

It may also be desirable to provide scripts that have been verified to prepare a particular base OS to build, install, and test Mesos. This would be very useful for both developers and users of Mesos.

Note that using some features on some platforms requires the installation of software packages from sources that may not be completely reliable in the long-term; for example, packages which are maintained as personal projects of individuals. This should be noted in the instructions accordingly.",Improvement,Major,greggomann,,10020,Accepted,In Progress,"Expand the ""Getting Started"" installation instructions",2016-01-07T18:23:56.000+0000,MESOS-4307,5.0,mesos,
bernd-mesos,2016-01-07T02:23:23.000+0000,JTCunning,"This bug was resolved for the hdfs protocol for MESOS-3602 but since the process checks for the ""hdfs"" protocol at the beginning of the URI, the fix does not extend itself to non-hdfs hadoop clients.
{code}
I0107 01:22:01.259490 17678 logging.cpp:172] INFO level logging started!
I0107 01:22:01.259856 17678 fetcher.cpp:422] Fetcher Info: {""cache_directory"":""\/tmp\/mesos\/fetch\/slaves\/530dda5a-481a-4117-8154-3aee637d3b38-S3\/root"",""items"":[{""action"":""BYPASS_CACHE"",""uri"":{""extract"":true,""value"":""maprfs:\/\/\/mesos\/storm-mesos-0.9.3.tgz""}},{""action"":""BYPASS_CACHE"",""uri"":{""extract"":true,""value"":""http:\/\/s0121.stag.urbanairship.com:36373\/conf\/storm.yaml""}}],""sandbox_directory"":""\/mnt\/data\/mesos\/slaves\/530dda5a-481a-4117-8154-3aee637d3b38-S3\/frameworks\/530dda5a-481a-4117-8154-3aee637d3b38-0000\/executors\/word-count-1-1452129714\/runs\/4443d5ac-d034-49b3-bf12-08fb9b0d92d0"",""user"":""root""}
I0107 01:22:01.262171 17678 fetcher.cpp:377] Fetching URI 'maprfs:///mesos/storm-mesos-0.9.3.tgz'
I0107 01:22:01.262212 17678 fetcher.cpp:248] Fetching directly into the sandbox directory
I0107 01:22:01.262243 17678 fetcher.cpp:185] Fetching URI 'maprfs:///mesos/storm-mesos-0.9.3.tgz'
I0107 01:22:01.671777 17678 fetcher.cpp:110] Downloading resource with Hadoop client from 'maprfs:///mesos/storm-mesos-0.9.3.tgz' to '/mnt/data/mesos/slaves/530dda5a-481a-4117-8154-3aee637d3b38-S3/frameworks/530dda5a-481a-4117-8154-3aee637d3b38-0000/executors/word-count-1-1452129714/runs/4443d5ac-d034-49b3-bf12-08fb9b0d92d0/storm-mesos-0.9.3.tgz'
copyToLocal: java.net.URISyntaxException: Expected scheme-specific part at index 7: maprfs:
Usage: java FsShell [-copyToLocal [-ignoreCrc] [-crc] <src> <localdst>]
E0107 01:22:02.435556 17678 shell.hpp:90] Command 'hadoop fs -copyToLocal '/maprfs:///mesos/storm-mesos-0.9.3.tgz' '/mnt/data/mesos/slaves/530dda5a-481a-4117-8154-3aee637d3b38-S3/frameworks/530dda5a-481a-4117-8154-3aee637d3b38-0000/executors/word-count-1-1452129714/runs/4443d5ac-d034-49b3-bf12-08fb9b0d92d0/storm-mesos-0.9.3.tgz'' failed; this is the output:
Failed to fetch 'maprfs:///mesos/storm-mesos-0.9.3.tgz': HDFS copyToLocal failed: Failed to execute 'hadoop fs -copyToLocal '/maprfs:///mesos/storm-mesos-0.9.3.tgz' '/mnt/data/mesos/slaves/530dda5a-481a-4117-8154-3aee637d3b38-S3/frameworks/530dda5a-481a-4117-8154-3aee637d3b38-0000/executors/word-count-1-1452129714/runs/4443d5ac-d034-49b3-bf12-08fb9b0d92d0/storm-mesos-0.9.3.tgz''; the command was either not found or exited with a non-zero exit status: 255
Failed to synchronize with slave (it's probably exited)
{code}

After a brief chat with [~jieyu], it was recommended to fix the current hdfs client code because the new hadoop fetcher plugin is slated to use it.",Bug,Blocker,JTCunning,2016-01-22T10:38:01.000+0000,5,Resolved,Complete,hdfs operations fail due to prepended / on path for non-hdfs hadoop clients.,2016-01-25T19:02:13.000+0000,MESOS-4304,1.0,mesos,Mesosphere Sprint 26
kaysoky,2016-01-06T16:21:14.000+0000,kaysoky,"Whenever a scheduler accepts an inverse offer, Mesos will print a line like this in the master logs:
{code}
W1125 10:05:53.155109 29362 master.cpp:2897] ACCEPT call used invalid offers '[ 932f7d7b-f2d4-42c7-9391-222c19b9d35b-O2 ]': Offer 932f7d7b-f2d4-42c7-9391-222c19b9d35b-O2 is no longer valid
{code}

Inverse offers should not trigger this warning.",Bug,Major,kaysoky,2016-01-19T21:44:10.000+0000,5,Resolved,Complete,Accepting an inverse offer prints misleading logs,2016-01-19T21:44:10.000+0000,MESOS-4301,1.0,mesos,Mesosphere Sprint 26
,2016-01-06T16:12:48.000+0000,kaysoky,Maintenance endpoints are currently only restricted by firewall settings.  They should also support authentication/authorization like other HTTP endpoints.,Task,Major,kaysoky,2016-03-03T05:16:22.000+0000,5,Resolved,Complete,Add AuthN and AuthZ to maintenance endpoints.,2016-03-03T08:42:44.000+0000,MESOS-4300,3.0,mesos,
greggomann,2016-01-06T14:11:18.000+0000,gyliu,"The https://reviews.apache.org/r/39923/ made some clean up for configuration.md but the related flags.cpp was not updated, we should update those files as well.",Bug,Major,gyliu,2016-01-29T21:35:06.000+0000,5,Resolved,Complete,Sync up configuration.md and flags.cpp,2016-01-29T21:35:06.000+0000,MESOS-4298,1.0,mesos,Mesosphere Sprint 27
jieyu,2016-01-06T01:39:50.000+0000,jieyu,"The existing registry client for docker assumes that Mesos is built using SSL support and SSL is enabled. That means Mesos built with libev (or if SSL is disabled) won't be able to use docker registry client to provision docker images.

Given the new URI fetcher (MESOS-3918) work has been committed, we can add a new URI fetcher plugin for docker. The plugin will be based on curl so that https and 3xx redirects will be handled automatically. The docker registry puller will just use the URI fetcher to get docker images.",Task,Major,jieyu,2016-01-21T17:39:22.000+0000,5,Resolved,Complete,Add docker URI fetcher plugin based on curl.,2016-01-21T17:39:22.000+0000,MESOS-4296,8.0,mesos,Mesosphere Sprint 27
js84,2016-01-05T20:05:12.000+0000,neilc,"Right now, links either use the form {noformat}[label](/documentation/latest/foo/){noformat} or {noformat}[label](foo.md){noformat}. We should probably switch to using the latter form consistently -- it previews better on Github, and it will make it easier to have multiple versions of the docs on the website at once in the future.",Task,Minor,neilc,2016-01-25T14:30:26.000+0000,5,Resolved,Complete,"Change documentation links to ""*.md""",2016-01-25T14:30:26.000+0000,MESOS-4295,3.0,mesos,Mesosphere Sprint 27
gilbert,2016-01-05T19:48:35.000+0000,gilbert,"(This bug was exposed by MESOS-4184, when serializing docker v1 image manifest as protobuf).

Currently protobuf::parse returns failures when parsing any JSON containing JSON::Null. If we have any protobuf field set as `JSON::Null`, any other non-repeated field cannot capture their value. For example, assuming we have a protobuf message:
{noformat}
message Nested {
  optional string str = 1;
  repeated string json_null = 2;
}
{noformat}

If there exists any field containing JSON::Null, like below:
{noformat}
      {
        \""str\"": \""message\"",
        \""json_null\"": null
      }
{noformat}
When we do protobuf::parse, it would return the following failure:
{noformat}
Failure parse: Not expecting a JSON null
{noformat}",Bug,Major,gilbert,2016-01-12T01:22:59.000+0000,5,Resolved,Complete,Protobuf parse should support parsing JSON object containing JSON Null.,2016-01-19T17:46:07.000+0000,MESOS-4294,1.0,mesos,Mesosphere Sprint 26
,2016-01-05T11:29:34.000+0000,alexr,"With the introduction of implicit roles (MESOS-3988), we should make sure quota can be set for an inactive role (unknown to the master) and maybe transition it to the active state.",Task,Major,alexr,,1,Open,New,Tests for quota with implicit roles.,2016-01-13T14:57:07.000+0000,MESOS-4292,3.0,mesos,
jieyu,2016-01-05T01:06:02.000+0000,jieyu,"I noticed this when I was testing the unified containerizer with the bind mount backend and no volumes.

The current implementation of fs::enter will put the old root under /tmp/._old_root_.XXXXXX in the new rootfs. It assumes that /tmp is writable in the new rootfs, but this might not be true, especially if the bind mount backend is used.

To solve the problem, what we can do is to mount tmpfs to /tmp in the new rootfs and umount it after pivot_root.",Bug,Major,jieyu,2016-02-23T23:13:12.000+0000,5,Resolved,Complete,fs::enter(rootfs) does not work if 'rootfs' is read only.,2016-02-23T23:13:12.000+0000,MESOS-4291,2.0,mesos,Mesosphere Sprint 26
jojy,2016-01-04T22:42:04.000+0000,jojy,"Create a design document describing the following:

- Model and abstraction of the Discoverer
- Workflow of the discovery process
",Task,Major,jojy,2016-01-14T00:17:18.000+0000,5,Resolved,Complete,Design doc for simple appc image discovery,2016-01-14T00:17:18.000+0000,MESOS-4289,5.0,mesos,Mesosphere Sprint 26
tnachen,2016-01-04T20:13:26.000+0000,tnachen,Currently volumes are stripped when an image is specified running a command task with Mesos containerizer. ,Bug,Major,tnachen,2016-02-08T17:29:42.000+0000,5,Resolved,Complete,Mesos command task doesn't support volumes with image,2016-02-08T17:29:42.000+0000,MESOS-4285,3.0,mesos,Mesosphere Sprint 26
bbannier,2016-01-04T19:35:13.000+0000,bernd-mesos,Create a document that describes the problems with having only single-role frameworks and proposes an MVP solution and implementation approach.,Story,Major,bernd-mesos,2016-01-14T12:00:49.000+0000,5,Resolved,Complete,Draft design doc for multi-role frameworks,2016-01-14T12:00:49.000+0000,MESOS-4284,8.0,mesos,Mesosphere Sprint 26
gilbert,2016-01-04T17:16:19.000+0000,gilbert,"Currently we have the isolator's prepare function returning ContainerPrepareInfo protobuf. We should enable ContainerLaunchInfo (contains environment variables, namespaces, etc.) to be returned which will be used by Mesos containerize to launch containers. 

By doing this (ContainerPrepareInfo -> ContainerLaunchInfo), we can select any necessary information and passing then to launcher.",Bug,Blocker,gilbert,2016-01-15T18:11:29.000+0000,5,Resolved,Complete,Update isolator prepare function to use ContainerLaunchInfo,2016-01-15T18:11:29.000+0000,MESOS-4282,2.0,mesos,
hartem,2016-01-04T16:59:45.000+0000,hartem,In its current implementation disk quota enforcement on the task sandbox will not work correctly when disk volumes are bind mounted into the task sandbox (this happens when Linux filesystem isolator is used).,Bug,Major,hartem,2016-01-26T22:00:55.000+0000,5,Resolved,Complete,Correctly handle disk quota usage when volumes are bind mounted into the container.,2016-01-26T22:03:00.000+0000,MESOS-4281,3.0,mesos,Mesosphere Sprint 26
qianzhang,2016-01-04T15:57:28.000+0000,bydga,"I'm implementing a graceful restarts of our mesos-marathon-docker setup and I came to a following issue:

(it was already discussed on https://github.com/mesosphere/marathon/issues/2876 and guys form mesosphere got to a point that its probably a docker containerizer problem...)
To sum it up:

When i deploy simple python script to all mesos-slaves:
{code}
#!/usr/bin/python

from time import sleep
import signal
import sys
import datetime

def sigterm_handler(_signo, _stack_frame):
    print ""got %i"" % _signo
    print datetime.datetime.now().time()
    sys.stdout.flush()
    sleep(2)
    print datetime.datetime.now().time()
    print ""ending""
    sys.stdout.flush()
    sys.exit(0)

signal.signal(signal.SIGTERM, sigterm_handler)
signal.signal(signal.SIGINT, sigterm_handler)

try:
    print ""Hello""
    i = 0
    while True:
        i += 1
        print datetime.datetime.now().time()
        print ""Iteration #%i"" % i
        sys.stdout.flush()
        sleep(1)
finally:
    print ""Goodbye""
{code}

and I run it through Marathon like
{code:javascript}
data = {
	args: [""/tmp/script.py""],
	instances: 1,
	cpus: 0.1,
	mem: 256,
	id: ""marathon-test-api""
}
{code}

During the app restart I get expected result - the task receives sigterm and dies peacefully (during my script-specified 2 seconds period)

But when i wrap this python script in a docker:
{code}
FROM node:4.2

RUN mkdir /app
ADD . /app
WORKDIR /app
ENTRYPOINT []
{code}
and run appropriate application by Marathon:
{code:javascript}
data = {
	args: [""./script.py""],
	container: {
		type: ""DOCKER"",
		docker: {
			image: ""bydga/marathon-test-api""
		},
		forcePullImage: yes
	},
	cpus: 0.1,
	mem: 256,
	instances: 1,
	id: ""marathon-test-api""
}
{code}

The task during restart (issued from marathon) dies immediately without having a chance to do any cleanup.
",Bug,Major,bydga,,10020,Accepted,In Progress,Docker executor truncates task's output when the task is killed.,2016-04-25T17:21:54.000+0000,MESOS-4279,5.0,mesos,
hartem,2015-12-30T23:14:37.000+0000,hartem,POSIX disk isolator does not currently report volume usage through ResourceStatistics. {{PosixDiskIsolatorProcess::usage()}} should be amended to take into account volume usage as well. ,Bug,Major,hartem,,10020,Accepted,In Progress,Report volume usage through ResourceStatistics.,2016-01-07T16:53:13.000+0000,MESOS-4263,3.0,mesos,
avinash@mesosphere.io,2015-12-30T16:07:23.000+0000,avinash@mesosphere.io,"Currently the control group infrastructure within mesos supports only the memory and CPU subsystems. We need to enhance this infrastructure to support the net_cls subsystem as well. Details of the net_cls subsystem and its use-cases can be found here:
https://www.kernel.org/doc/Documentation/cgroups/net_cls.txt

Enabling the net_cls will allow us to provide operators to, potentially, regulate framework traffic on a per-container basis.  ",Improvement,Major,avinash@mesosphere.io,2016-01-20T18:16:14.000+0000,5,Resolved,Complete,Enable net_cls subsytem in cgroup infrastructure,2016-01-20T18:16:14.000+0000,MESOS-4262,5.0,mesos,Mesosphere Sprint 26
jieyu,2015-12-30T08:26:01.000+0000,tnachen,"We currently use a configured docker auth server from a slave flag to get token auth for docker registry. However this doesn't work for private registries as docker registry supports sending down the correct auth server to contact.

We should remove docker auth server flag completely and ask the docker registry for auth server.",Improvement,Major,tnachen,2016-02-23T23:12:48.000+0000,5,Resolved,Complete,Remove docker auth server flag,2016-02-23T23:12:48.000+0000,MESOS-4261,3.0,mesos,Mesosphere Sprint 25
js84,2015-12-29T19:49:32.000+0000,bmahler,"{noformat: title=Good Run}
[ RUN      ] ExamplesTest.NoExecutorFramework
I1221 23:10:02.721617 32528 exec.cpp:444] Ignoring exited event because the driver is aborted!
Using temporary directory '/tmp/ExamplesTest_NoExecutorFramework_fCmFLn'
I1221 23:10:02.721675 32539 exec.cpp:444] Ignoring exited event because the driver is aborted!
I1221 23:10:02.722024 32554 exec.cpp:444] Ignoring exited event because the driver is aborted!
WARNING: Logging before InitGoogleLogging() is written to STDERR
I1221 23:10:05.179466 32569 resources.cpp:478] Parsing resources as JSON failed: cpus:0.1;mem:32;disk:32
Trying semicolon-delimited string format instead
I1221 23:10:05.180269 32569 logging.cpp:172] Logging to STDERR
I1221 23:10:05.185768 32569 process.cpp:998] libprocess is initialized on 172.17.0.2:40874 for 16 cpus
I1221 23:10:05.200728 32569 leveldb.cpp:174] Opened db in 4.184362ms
I1221 23:10:05.202234 32569 leveldb.cpp:181] Compacted db in 1.459268ms
I1221 23:10:05.202353 32569 leveldb.cpp:196] Created db iterator in 73761ns
I1221 23:10:05.202383 32569 leveldb.cpp:202] Seeked to beginning of db in 3382ns
I1221 23:10:05.202405 32569 leveldb.cpp:271] Iterated through 0 keys in the db in 633ns
I1221 23:10:05.202674 32569 replica.cpp:779] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned
I1221 23:10:05.205301 32604 recover.cpp:447] Starting replica recovery
I1221 23:10:05.206414 32569 local.cpp:239] Using 'local' authorizer
I1221 23:10:05.206405 32604 recover.cpp:473] Replica is in EMPTY status
I1221 23:10:05.209595 32594 replica.cpp:673] Replica in EMPTY status received a broadcasted recover request from (4)@172.17.0.2:40874
I1221 23:10:05.210916 32596 recover.cpp:193] Received a recover response from a replica in EMPTY status
I1221 23:10:05.211515 32597 master.cpp:365] Master 3931c1a8-1cd6-49eb-94c8-d01b33bb008e (6ccf2ee56b13) started on 172.17.0.2:40874
I1221 23:10:05.211699 32605 recover.cpp:564] Updating replica status to STARTING
I1221 23:10:05.211539 32597 master.cpp:367] Flags at startup: --acls=""permissive: false
register_frameworks {
  principals {
    type: SOME
    values: ""test-principal""
  }
  roles {
    type: SOME
    values: ""*""
  }
}
run_tasks {
  principals {
    type: SOME
    values: ""test-principal""
  }
  users {
    type: SOME
    values: ""mesos""
  }
}
"" --allocation_interval=""1secs"" --allocator=""HierarchicalDRF"" --authenticate=""true"" --authenticate_slaves=""false"" --authenticators=""crammd5"" --authorizers=""local"" --credentials=""/tmp/ExamplesTest_NoExecutorFramework_fCmFLn/credentials"" --framework_sorter=""drf"" --help=""true"" --hostname_lookup=""true"" --initialize_driver_logging=""true"" --log_auto_initialize=""true"" --logbufsecs=""0"" --logging_level=""INFO"" --max_slave_ping_timeouts=""5"" --quiet=""false"" --recovery_slave_removal_limit=""100%"" --registry=""replicated_log"" --registry_fetch_timeout=""1mins"" --registry_store_timeout=""5secs"" --registry_strict=""false"" --root_submissions=""true"" --slave_ping_timeout=""15secs"" --slave_reregister_timeout=""10mins"" --user_sorter=""drf"" --version=""false"" --webui_dir=""/mesos/mesos-0.27.0/src/webui"" --work_dir=""/tmp/mesos-otpdch"" --zk_session_timeout=""10secs""
I1221 23:10:05.212323 32597 master.cpp:412] Master only allowing authenticated frameworks to register
I1221 23:10:05.212337 32597 master.cpp:419] Master allowing unauthenticated slaves to register
I1221 23:10:05.212347 32597 credentials.hpp:35] Loading credentials for authentication from '/tmp/ExamplesTest_NoExecutorFramework_fCmFLn/credentials'
W1221 23:10:05.212442 32597 credentials.hpp:50] Permissions on credentials file '/tmp/ExamplesTest_NoExecutorFramework_fCmFLn/credentials' are too open. It is recommended that your credentials file is NOT accessible by others.
I1221 23:10:05.212606 32600 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 656857ns
I1221 23:10:05.212620 32597 master.cpp:456] Using default 'crammd5' authenticator
I1221 23:10:05.212631 32600 replica.cpp:320] Persisted replica status to STARTING
I1221 23:10:05.212893 32597 authenticator.cpp:518] Initializing server SASL
I1221 23:10:05.213091 32608 recover.cpp:473] Replica is in STARTING status
I1221 23:10:05.213958 32595 replica.cpp:673] Replica in STARTING status received a broadcasted recover request from (5)@172.17.0.2:40874
I1221 23:10:05.214323 32594 recover.cpp:193] Received a recover response from a replica in STARTING status
I1221 23:10:05.214689 32595 recover.cpp:564] Updating replica status to VOTING
I1221 23:10:05.215353 32596 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 487419ns
I1221 23:10:05.215384 32596 replica.cpp:320] Persisted replica status to VOTING
I1221 23:10:05.215481 32605 recover.cpp:578] Successfully joined the Paxos group
I1221 23:10:05.215867 32605 recover.cpp:462] Recover process terminated
I1221 23:10:05.216111 32569 containerizer.cpp:141] Using isolation: filesystem/posix,posix/cpu,posix/mem
W1221 23:10:05.217021 32569 backend.cpp:48] Failed to create 'bind' backend: BindBackend requires root privileges
I1221 23:10:05.221482 32608 slave.cpp:191] Slave started on 1)@172.17.0.2:40874
I1221 23:10:05.221521 32608 slave.cpp:192] Flags at startup: --appc_store_dir=""/tmp/mesos/store/appc"" --authenticatee=""crammd5"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_auth_server=""auth.docker.io"" --docker_auth_server_port=""443"" --docker_kill_orphans=""true"" --docker_local_archives_dir=""/tmp/mesos/images/docker"" --docker_puller=""local"" --docker_puller_timeout=""60"" --docker_registry=""registry-1.docker.io"" --docker_registry_port=""443"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/tmp/mesos/store/docker"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/tmp/mesos/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --hostname_lookup=""true"" --image_provisioner_backend=""copy"" --initialize_driver_logging=""true"" --isolation=""filesystem/posix,posix/cpu,posix/mem"" --launcher=""posix"" --launcher_dir=""/mesos/mesos-0.27.0/_build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""1secs"" --resources=""cpus:2;mem:10240"" --revocable_cpu_low_priority=""true"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --systemd_runtime_directory=""/run/systemd/system"" --version=""false"" --work_dir=""/tmp/mesos-otpdch/0""
I1221 23:10:05.222578 32608 resources.cpp:478] Parsing resources as JSON failed: cpus:2;mem:10240
Trying semicolon-delimited string format instead
I1221 23:10:05.223465 32608 slave.cpp:392] Slave resources: cpus(*):2; mem(*):10240; disk(*):3.70122e+06; ports(*):[31000-32000]
I1221 23:10:05.223621 32569 containerizer.cpp:141] Using isolation: filesystem/posix,posix/cpu,posix/mem
I1221 23:10:05.223610 32608 slave.cpp:400] Slave attributes: [  ]
I1221 23:10:05.223677 32608 slave.cpp:405] Slave hostname: 6ccf2ee56b13
I1221 23:10:05.223697 32608 slave.cpp:410] Slave checkpoint: true
W1221 23:10:05.224143 32569 backend.cpp:48] Failed to create 'bind' backend: BindBackend requires root privileges
I1221 23:10:05.226668 32604 slave.cpp:191] Slave started on 2)@172.17.0.2:40874
I1221 23:10:05.226692 32604 slave.cpp:192] Flags at startup: --appc_store_dir=""/tmp/mesos/store/appc"" --authenticatee=""crammd5"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_auth_server=""auth.docker.io"" --docker_auth_server_port=""443"" --docker_kill_orphans=""true"" --docker_local_archives_dir=""/tmp/mesos/images/docker"" --docker_puller=""local"" --docker_puller_timeout=""60"" --docker_registry=""registry-1.docker.io"" --docker_registry_port=""443"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/tmp/mesos/store/docker"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/tmp/mesos/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --hostname_lookup=""true"" --image_provisioner_backend=""copy"" --initialize_driver_logging=""true"" --isolation=""filesystem/posix,posix/cpu,posix/mem"" --launcher=""posix"" --launcher_dir=""/mesos/mesos-0.27.0/_build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""1secs"" --resources=""cpus:2;mem:10240"" --revocable_cpu_low_priority=""true"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --systemd_runtime_directory=""/run/systemd/system"" --version=""false"" --work_dir=""/tmp/mesos-otpdch/1""
I1221 23:10:05.227520 32604 resources.cpp:478] Parsing resources as JSON failed: cpus:2;mem:10240
Trying semicolon-delimited string format instead
I1221 23:10:05.228037 32604 slave.cpp:392] Slave resources: cpus(*):2; mem(*):10240; disk(*):3.70122e+06; ports(*):[31000-32000]
I1221 23:10:05.228148 32604 slave.cpp:400] Slave attributes: [  ]
I1221 23:10:05.228169 32604 slave.cpp:405] Slave hostname: 6ccf2ee56b13
I1221 23:10:05.228184 32604 slave.cpp:410] Slave checkpoint: true
I1221 23:10:05.229123 32569 containerizer.cpp:141] Using isolation: filesystem/posix,posix/cpu,posix/mem
I1221 23:10:05.229641 32605 state.cpp:58] Recovering state from '/tmp/mesos-otpdch/0/meta'
W1221 23:10:05.229645 32569 backend.cpp:48] Failed to create 'bind' backend: BindBackend requires root privileges
I1221 23:10:05.229636 32595 state.cpp:58] Recovering state from '/tmp/mesos-otpdch/1/meta'
I1221 23:10:05.230242 32605 status_update_manager.cpp:200] Recovering status update manager
I1221 23:10:05.230254 32598 status_update_manager.cpp:200] Recovering status update manager
I1221 23:10:05.230515 32601 containerizer.cpp:383] Recovering containerizer
I1221 23:10:05.230562 32602 containerizer.cpp:383] Recovering containerizer
I1221 23:10:05.232681 32597 auxprop.cpp:71] Initialized in-memory auxiliary property plugin
I1221 23:10:05.232803 32597 master.cpp:493] Authorization enabled
I1221 23:10:05.232867 32600 slave.cpp:4427] Finished recovery
I1221 23:10:05.232980 32598 slave.cpp:191] Slave started on 3)@172.17.0.2:40874
I1221 23:10:05.233039 32594 slave.cpp:4427] Finished recovery
I1221 23:10:05.233376 32599 whitelist_watcher.cpp:77] No whitelist given
I1221 23:10:05.233428 32601 hierarchical.cpp:147] Initialized hierarchical allocator process
I1221 23:10:05.233003 32598 slave.cpp:192] Flags at startup: --appc_store_dir=""/tmp/mesos/store/appc"" --authenticatee=""crammd5"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_auth_server=""auth.docker.io"" --docker_auth_server_port=""443"" --docker_kill_orphans=""true"" --docker_local_archives_dir=""/tmp/mesos/images/docker"" --docker_puller=""local"" --docker_puller_timeout=""60"" --docker_registry=""registry-1.docker.io"" --docker_registry_port=""443"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/tmp/mesos/store/docker"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/tmp/mesos/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --hostname_lookup=""true"" --image_provisioner_backend=""copy"" --initialize_driver_logging=""true"" --isolation=""filesystem/posix,posix/cpu,posix/mem"" --launcher=""posix"" --launcher_dir=""/mesos/mesos-0.27.0/_build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""1secs"" --resources=""cpus:2;mem:10240"" --revocable_cpu_low_priority=""true"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --systemd_runtime_directory=""/run/systemd/system"" --version=""false"" --work_dir=""/tmp/mesos-otpdch/2""
I1221 23:10:05.233744 32600 slave.cpp:4599] Querying resource estimator for oversubscribable resources
I1221 23:10:05.233749 32598 resources.cpp:478] Parsing resources as JSON failed: cpus:2;mem:10240
Trying semicolon-delimited string format instead
I1221 23:10:05.234222 32598 slave.cpp:392] Slave resources: cpus(*):2; mem(*):10240; disk(*):3.70122e+06; ports(*):[31000-32000]
I1221 23:10:05.234284 32598 slave.cpp:400] Slave attributes: [  ]
I1221 23:10:05.234299 32598 slave.cpp:405] Slave hostname: 6ccf2ee56b13
I1221 23:10:05.234311 32598 slave.cpp:410] Slave checkpoint: true
I1221 23:10:05.234338 32600 slave.cpp:729] New master detected at master@172.17.0.2:40874
I1221 23:10:05.234376 32604 status_update_manager.cpp:174] Pausing sending status updates
I1221 23:10:05.234424 32600 slave.cpp:754] No credentials provided. Attempting to register without authentication
I1221 23:10:05.234522 32600 slave.cpp:765] Detecting new master
I1221 23:10:05.234616 32569 sched.cpp:164] Version: 0.27.0
I1221 23:10:05.234658 32600 slave.cpp:4613] Received oversubscribable resources  from the resource estimator
I1221 23:10:05.234671 32594 slave.cpp:4599] Querying resource estimator for oversubscribable resources
I1221 23:10:05.234884 32606 slave.cpp:4613] Received oversubscribable resources  from the resource estimator
I1221 23:10:05.235038 32595 status_update_manager.cpp:174] Pausing sending status updates
I1221 23:10:05.235043 32606 slave.cpp:729] New master detected at master@172.17.0.2:40874
I1221 23:10:05.235111 32606 slave.cpp:754] No credentials provided. Attempting to register without authentication
I1221 23:10:05.235147 32606 slave.cpp:765] Detecting new master
I1221 23:10:05.235240 32594 state.cpp:58] Recovering state from '/tmp/mesos-otpdch/2/meta'
I1221 23:10:05.235443 32608 status_update_manager.cpp:200] Recovering status update manager
I1221 23:10:05.235625 32594 containerizer.cpp:383] Recovering containerizer
I1221 23:10:05.236549 32599 slave.cpp:4427] Finished recovery
I1221 23:10:05.236984 32593 sched.cpp:262] New master detected at master@172.17.0.2:40874
I1221 23:10:05.237004 32599 slave.cpp:4599] Querying resource estimator for oversubscribable resources
I1221 23:10:05.237221 32593 sched.cpp:318] Authenticating with master master@172.17.0.2:40874
I1221 23:10:05.237277 32593 sched.cpp:325] Using default CRAM-MD5 authenticatee
I1221 23:10:05.237285 32604 status_update_manager.cpp:174] Pausing sending status updates
I1221 23:10:05.237288 32599 slave.cpp:729] New master detected at master@172.17.0.2:40874
I1221 23:10:05.237361 32599 slave.cpp:754] No credentials provided. Attempting to register without authentication
I1221 23:10:05.237433 32599 slave.cpp:765] Detecting new master
I1221 23:10:05.237565 32599 slave.cpp:4613] Received oversubscribable resources  from the resource estimator
I1221 23:10:05.238154 32605 authenticatee.cpp:97] Initializing client SASL
I1221 23:10:05.238315 32605 authenticatee.cpp:121] Creating new client SASL connection
I1221 23:10:05.239640 32597 master.cpp:1200] Dropping 'mesos.internal.AuthenticateMessage' message since not elected yet
I1221 23:10:05.239765 32597 master.cpp:1629] The newly elected leader is master@172.17.0.2:40874 with id 3931c1a8-1cd6-49eb-94c8-d01b33bb008e
I1221 23:10:05.239794 32597 master.cpp:1642] Elected as the leading master!
I1221 23:10:05.239843 32597 master.cpp:1387] Recovering from registrar
I1221 23:10:05.240056 32600 registrar.cpp:307] Recovering registrar
I1221 23:10:05.241477 32608 log.cpp:659] Attempting to start the writer
I1221 23:10:05.244540 32600 replica.cpp:493] Replica received implicit promise request from (39)@172.17.0.2:40874 with proposal 1
I1221 23:10:05.245358 32600 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 776937ns
I1221 23:10:05.245393 32600 replica.cpp:342] Persisted promised to 1
I1221 23:10:05.246625 32601 coordinator.cpp:238] Coordinator attempting to fill missing positions
I1221 23:10:05.248757 32605 replica.cpp:388] Replica received explicit promise request from (40)@172.17.0.2:40874 for position 0 with proposal 2
I1221 23:10:05.249214 32605 leveldb.cpp:341] Persisting action (8 bytes) to leveldb took 366567ns
I1221 23:10:05.249246 32605 replica.cpp:712] Persisted action at 0
I1221 23:10:05.250998 32599 replica.cpp:537] Replica received write request for position 0 from (41)@172.17.0.2:40874
I1221 23:10:05.251111 32599 leveldb.cpp:436] Reading position from leveldb took 66773ns
I1221 23:10:05.251734 32599 leveldb.cpp:341] Persisting action (14 bytes) to leveldb took 379612ns
I1221 23:10:05.251759 32599 replica.cpp:712] Persisted action at 0
I1221 23:10:05.252555 32601 replica.cpp:691] Replica received learned notice for position 0 from @0.0.0.0:0
I1221 23:10:05.253010 32601 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 381858ns
I1221 23:10:05.253036 32601 replica.cpp:712] Persisted action at 0
I1221 23:10:05.253068 32601 replica.cpp:697] Replica learned NOP action at position 0
I1221 23:10:05.254043 32595 log.cpp:675] Writer started with ending position 0
I1221 23:10:05.256741 32595 leveldb.cpp:436] Reading position from leveldb took 48607ns
I1221 23:10:05.260617 32601 registrar.cpp:340] Successfully fetched the registry (0B) in 20.47616ms
I1221 23:10:05.260988 32601 registrar.cpp:439] Applied 1 operations in 103123ns; attempting to update the 'registry'
I1221 23:10:05.264700 32604 log.cpp:683] Attempting to append 170 bytes to the log
I1221 23:10:05.265138 32601 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 1
I1221 23:10:05.266208 32603 replica.cpp:537] Replica received write request for position 1 from (42)@172.17.0.2:40874
I1221 23:10:05.266829 32603 leveldb.cpp:341] Persisting action (189 bytes) to leveldb took 551087ns
I1221 23:10:05.266861 32603 replica.cpp:712] Persisted action at 1
I1221 23:10:05.267918 32605 replica.cpp:691] Replica received learned notice for position 1 from @0.0.0.0:0
I1221 23:10:05.268442 32605 leveldb.cpp:341] Persisting action (191 bytes) to leveldb took 453416ns
I1221 23:10:05.268470 32605 replica.cpp:712] Persisted action at 1
I1221 23:10:05.268506 32605 replica.cpp:697] Replica learned APPEND action at position 1
I1221 23:10:05.270512 32606 registrar.cpp:484] Successfully updated the 'registry' in 9.375232ms
I1221 23:10:05.270705 32606 registrar.cpp:370] Successfully recovered registrar
I1221 23:10:05.271045 32602 log.cpp:702] Attempting to truncate the log to 1
I1221 23:10:05.271178 32603 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 2
I1221 23:10:05.271695 32605 master.cpp:1439] Recovered 0 slaves from the Registry (131B) ; allowing 10mins for slaves to re-register
I1221 23:10:05.271751 32603 hierarchical.cpp:165] Skipping recovery of hierarchical allocator: nothing to recover
I1221 23:10:05.272274 32596 replica.cpp:537] Replica received write request for position 2 from (43)@172.17.0.2:40874
I1221 23:10:05.272838 32596 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 451122ns
I1221 23:10:05.272867 32596 replica.cpp:712] Persisted action at 2
I1221 23:10:05.273483 32607 replica.cpp:691] Replica received learned notice for position 2 from @0.0.0.0:0
I1221 23:10:05.273919 32607 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 405444ns
I1221 23:10:05.273977 32607 leveldb.cpp:399] Deleting ~1 keys from leveldb took 33953ns
I1221 23:10:05.273998 32607 replica.cpp:712] Persisted action at 2
I1221 23:10:05.274024 32607 replica.cpp:697] Replica learned TRUNCATE action at position 2
I1221 23:10:05.288020 32593 slave.cpp:1254] Will retry registration in 1.091568855secs if necessary
I1221 23:10:05.288321 32605 master.cpp:4132] Registering slave at slave(3)@172.17.0.2:40874 (6ccf2ee56b13) with id 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S0
I1221 23:10:05.289105 32607 registrar.cpp:439] Applied 1 operations in 104776ns; attempting to update the 'registry'
I1221 23:10:05.291494 32605 log.cpp:683] Attempting to append 339 bytes to the log
I1221 23:10:05.291594 32595 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 3
I1221 23:10:05.292273 32602 replica.cpp:537] Replica received write request for position 3 from (44)@172.17.0.2:40874
I1221 23:10:05.292811 32602 leveldb.cpp:341] Persisting action (358 bytes) to leveldb took 499449ns
I1221 23:10:05.292845 32602 replica.cpp:712] Persisted action at 3
I1221 23:10:05.293373 32594 replica.cpp:691] Replica received learned notice for position 3 from @0.0.0.0:0
I1221 23:10:05.293776 32594 leveldb.cpp:341] Persisting action (360 bytes) to leveldb took 376184ns
I1221 23:10:05.293799 32594 replica.cpp:712] Persisted action at 3
I1221 23:10:05.293833 32594 replica.cpp:697] Replica learned APPEND action at position 3
I1221 23:10:05.295142 32603 registrar.cpp:484] Successfully updated the 'registry' in 5.945088ms
I1221 23:10:05.295403 32602 log.cpp:702] Attempting to truncate the log to 3
I1221 23:10:05.295513 32603 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 4
I1221 23:10:05.296146 32598 replica.cpp:537] Replica received write request for position 4 from (45)@172.17.0.2:40874
I1221 23:10:05.296443 32608 slave.cpp:3371] Received ping from slave-observer(1)@172.17.0.2:40874
I1221 23:10:05.296552 32598 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 368428ns
I1221 23:10:05.296604 32598 replica.cpp:712] Persisted action at 4
I1221 23:10:05.296744 32594 master.cpp:4200] Registered slave 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S0 at slave(3)@172.17.0.2:40874 (6ccf2ee56b13) with cpus(*):2; mem(*):10240; disk(*):3.70122e+06; ports(*):[31000-32000]
I1221 23:10:05.297025 32597 slave.cpp:904] Registered with master master@172.17.0.2:40874; given slave ID 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S0
I1221 23:10:05.297056 32597 fetcher.cpp:81] Clearing fetcher cache
I1221 23:10:05.297175 32599 status_update_manager.cpp:181] Resuming sending status updates
I1221 23:10:05.297184 32600 hierarchical.cpp:465] Added slave 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S0 (6ccf2ee56b13) with cpus(*):2; mem(*):10240; disk(*):3.70122e+06; ports(*):[31000-32000] (allocated: )
I1221 23:10:05.297473 32597 slave.cpp:927] Checkpointing SlaveInfo to '/tmp/mesos-otpdch/2/meta/slaves/3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S0/slave.info'
I1221 23:10:05.297618 32600 hierarchical.cpp:1329] No resources available to allocate!
I1221 23:10:05.297688 32600 hierarchical.cpp:1101] Performed allocation for slave 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S0 in 454887ns
I1221 23:10:05.298058 32597 slave.cpp:963] Forwarding total oversubscribed resources 
I1221 23:10:05.298235 32597 master.cpp:4542] Received update of slave 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S0 at slave(3)@172.17.0.2:40874 (6ccf2ee56b13) with total oversubscribed resources 
I1221 23:10:05.298396 32604 replica.cpp:691] Replica received learned notice for position 4 from @0.0.0.0:0
I1221 23:10:05.298765 32597 hierarchical.cpp:521] Slave 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S0 (6ccf2ee56b13) updated with oversubscribed resources  (total: cpus(*):2; mem(*):10240; disk(*):3.70122e+06; ports(*):[31000-32000], allocated: )
I1221 23:10:05.298907 32597 hierarchical.cpp:1329] No resources available to allocate!
I1221 23:10:05.298933 32597 hierarchical.cpp:1101] Performed allocation for slave 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S0 in 134328ns
I1221 23:10:05.298965 32604 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 505213ns
I1221 23:10:05.299031 32604 leveldb.cpp:399] Deleting ~2 keys from leveldb took 37007ns
I1221 23:10:05.299054 32604 replica.cpp:712] Persisted action at 4
I1221 23:10:05.299103 32604 replica.cpp:697] Replica learned TRUNCATE action at position 4
I1221 23:10:05.350281 32598 slave.cpp:1254] Will retry registration in 1.181298785secs if necessary
I1221 23:10:05.350510 32608 master.cpp:4132] Registering slave at slave(1)@172.17.0.2:40874 (6ccf2ee56b13) with id 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S1
I1221 23:10:05.351222 32607 registrar.cpp:439] Applied 1 operations in 118623ns; attempting to update the 'registry'
I1221 23:10:05.352174 32604 log.cpp:683] Attempting to append 505 bytes to the log
I1221 23:10:05.352375 32595 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 5
I1221 23:10:05.353365 32601 replica.cpp:537] Replica received write request for position 5 from (46)@172.17.0.2:40874
I1221 23:10:05.353960 32601 leveldb.cpp:341] Persisting action (524 bytes) to leveldb took 552132ns
I1221 23:10:05.353986 32601 replica.cpp:712] Persisted action at 5
I1221 23:10:05.354867 32606 replica.cpp:691] Replica received learned notice for position 5 from @0.0.0.0:0
I1221 23:10:05.355370 32606 leveldb.cpp:341] Persisting action (526 bytes) to leveldb took 456354ns
I1221 23:10:05.355399 32606 replica.cpp:712] Persisted action at 5
I1221 23:10:05.355432 32606 replica.cpp:697] Replica learned APPEND action at position 5
I1221 23:10:05.357318 32595 registrar.cpp:484] Successfully updated the 'registry' in 6.016768ms
I1221 23:10:05.357708 32595 log.cpp:702] Attempting to truncate the log to 5
I1221 23:10:05.357805 32606 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 6
I1221 23:10:05.358273 32602 slave.cpp:3371] Received ping from slave-observer(2)@172.17.0.2:40874
I1221 23:10:05.358331 32599 master.cpp:4200] Registered slave 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S1 at slave(1)@172.17.0.2:40874 (6ccf2ee56b13) with cpus(*):2; mem(*):10240; disk(*):3.70122e+06; ports(*):[31000-32000]
I1221 23:10:05.358405 32602 slave.cpp:904] Registered with master master@172.17.0.2:40874; given slave ID 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S1
I1221 23:10:05.358428 32602 fetcher.cpp:81] Clearing fetcher cache
I1221 23:10:05.358722 32599 status_update_manager.cpp:181] Resuming sending status updates
I1221 23:10:05.358736 32606 hierarchical.cpp:465] Added slave 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S1 (6ccf2ee56b13) with cpus(*):2; mem(*):10240; disk(*):3.70122e+06; ports(*):[31000-32000] (allocated: )
I1221 23:10:05.358813 32599 replica.cpp:537] Replica received write request for position 6 from (47)@172.17.0.2:40874
I1221 23:10:05.358952 32602 slave.cpp:927] Checkpointing SlaveInfo to '/tmp/mesos-otpdch/0/meta/slaves/3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S1/slave.info'
I1221 23:10:05.358969 32606 hierarchical.cpp:1329] No resources available to allocate!
I1221 23:10:05.358996 32606 hierarchical.cpp:1101] Performed allocation for slave 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S1 in 217083ns
I1221 23:10:05.359350 32599 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 464454ns
I1221 23:10:05.359374 32599 replica.cpp:712] Persisted action at 6
I1221 23:10:05.359591 32602 slave.cpp:963] Forwarding total oversubscribed resources 
I1221 23:10:05.359740 32606 master.cpp:4542] Received update of slave 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S1 at slave(1)@172.17.0.2:40874 (6ccf2ee56b13) with total oversubscribed resources 
I1221 23:10:05.360227 32605 replica.cpp:691] Replica received learned notice for position 6 from @0.0.0.0:0
I1221 23:10:05.360288 32606 hierarchical.cpp:521] Slave 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S1 (6ccf2ee56b13) updated with oversubscribed resources  (total: cpus(*):2; mem(*):10240; disk(*):3.70122e+06; ports(*):[31000-32000], allocated: )
I1221 23:10:05.360539 32606 hierarchical.cpp:1329] No resources available to allocate!
I1221 23:10:05.360591 32606 hierarchical.cpp:1101] Performed allocation for slave 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S1 in 256841ns
I1221 23:10:05.360702 32605 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 444736ns
I1221 23:10:05.360759 32605 leveldb.cpp:399] Deleting ~2 keys from leveldb took 30869ns
I1221 23:10:05.360777 32605 replica.cpp:712] Persisted action at 6
I1221 23:10:05.360800 32605 replica.cpp:697] Replica learned TRUNCATE action at position 6
I1221 23:10:05.957257 32601 slave.cpp:1254] Will retry registration in 627.120173ms if necessary
I1221 23:10:05.957504 32597 master.cpp:4132] Registering slave at slave(2)@172.17.0.2:40874 (6ccf2ee56b13) with id 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S2
I1221 23:10:05.958284 32607 registrar.cpp:439] Applied 1 operations in 174321ns; attempting to update the 'registry'
I1221 23:10:05.959336 32594 log.cpp:683] Attempting to append 671 bytes to the log
I1221 23:10:05.959477 32606 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 7
I1221 23:10:05.960484 32604 replica.cpp:537] Replica received write request for position 7 from (48)@172.17.0.2:40874
I1221 23:10:05.960891 32604 leveldb.cpp:341] Persisting action (690 bytes) to leveldb took 362101ns
I1221 23:10:05.960917 32604 replica.cpp:712] Persisted action at 7
I1221 23:10:05.961642 32597 replica.cpp:691] Replica received learned notice for position 7 from @0.0.0.0:0
I1221 23:10:05.962502 32597 leveldb.cpp:341] Persisting action (692 bytes) to leveldb took 828414ns
I1221 23:10:05.962532 32597 replica.cpp:712] Persisted action at 7
I1221 23:10:05.962563 32597 replica.cpp:697] Replica learned APPEND action at position 7
I1221 23:10:05.964241 32598 registrar.cpp:484] Successfully updated the 'registry' in 5.87392ms
I1221 23:10:05.964552 32593 log.cpp:702] Attempting to truncate the log to 7
I1221 23:10:05.964643 32606 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 8
I1221 23:10:05.964964 32593 slave.cpp:3371] Received ping from slave-observer(3)@172.17.0.2:40874
I1221 23:10:05.965152 32602 slave.cpp:904] Registered with master master@172.17.0.2:40874; given slave ID 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S2
I1221 23:10:05.965111 32600 master.cpp:4200] Registered slave 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S2 at slave(2)@172.17.0.2:40874 (6ccf2ee56b13) with cpus(*):2; mem(*):10240; disk(*):3.70122e+06; ports(*):[31000-32000]
I1221 23:10:05.965178 32602 fetcher.cpp:81] Clearing fetcher cache
I1221 23:10:05.965293 32607 status_update_manager.cpp:181] Resuming sending status updates
I1221 23:10:05.965293 32593 replica.cpp:537] Replica received write request for position 8 from (49)@172.17.0.2:40874
I1221 23:10:05.965637 32603 hierarchical.cpp:465] Added slave 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S2 (6ccf2ee56b13) with cpus(*):2; mem(*):10240; disk(*):3.70122e+06; ports(*):[31000-32000] (allocated: )
I1221 23:10:05.965734 32602 slave.cpp:927] Checkpointing SlaveInfo to '/tmp/mesos-otpdch/1/meta/slaves/3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S2/slave.info'
I1221 23:10:05.965751 32593 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 424617ns
I1221 23:10:05.965772 32593 replica.cpp:712] Persisted action at 8
I1221 23:10:05.965847 32603 hierarchical.cpp:1329] No resources available to allocate!
I1221 23:10:05.965880 32603 hierarchical.cpp:1101] Performed allocation for slave 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S2 in 209313ns
I1221 23:10:05.966145 32602 slave.cpp:963] Forwarding total oversubscribed resources 
I1221 23:10:05.966325 32601 replica.cpp:691] Replica received learned notice for position 8 from @0.0.0.0:0
I1221 23:10:05.966342 32596 master.cpp:4542] Received update of slave 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S2 at slave(2)@172.17.0.2:40874 (6ccf2ee56b13) with total oversubscribed resources 
I1221 23:10:05.966703 32601 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 347667ns
I1221 23:10:05.966753 32601 leveldb.cpp:399] Deleting ~2 keys from leveldb took 29042ns
I1221 23:10:05.966770 32601 replica.cpp:712] Persisted action at 8
I1221 23:10:05.966791 32601 replica.cpp:697] Replica learned TRUNCATE action at position 8
I1221 23:10:05.966792 32602 hierarchical.cpp:521] Slave 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S2 (6ccf2ee56b13) updated with oversubscribed resources  (total: cpus(*):2; mem(*):10240; disk(*):3.70122e+06; ports(*):[31000-32000], allocated: )
I1221 23:10:05.966943 32602 hierarchical.cpp:1329] No resources available to allocate!
I1221 23:10:05.966969 32602 hierarchical.cpp:1101] Performed allocation for slave 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S2 in 137680ns
I1221 23:10:06.235074 32595 hierarchical.cpp:1329] No resources available to allocate!
I1221 23:10:06.235142 32595 hierarchical.cpp:1079] Performed allocation for 3 slaves in 594477ns
I1221 23:10:07.237238 32599 hierarchical.cpp:1329] No resources available to allocate!
I1221 23:10:07.237309 32599 hierarchical.cpp:1079] Performed allocation for 3 slaves in 506418ns
I1221 23:10:07.702738 32564 exec.cpp:88] Committing suicide by killing the process group
II1221 23:10:07.703501 32546 exec.cpp:88] Committing suicide by killing the process group
1221 23:10:07.703501 32525 exec.cpp:88] Committing suicide by killing the process group
I1221 23:10:08.239282 32608 hierarchical.cpp:1329] No resources available to allocate!
I1221 23:10:08.239356 32608 hierarchical.cpp:1079] Performed allocation for 3 slaves in 671976ns
I1221 23:10:09.241236 32600 hierarchical.cpp:1329] No resources available to allocate!
I1221 23:10:09.241303 32600 hierarchical.cpp:1079] Performed allocation for 3 slaves in 520741ns
W1221 23:10:10.239298 32601 sched.cpp:429] Authentication timed out
I1221 23:10:10.239653 32596 sched.cpp:387] Failed to authenticate with master master@172.17.0.2:40874: Authentication discarded
I1221 23:10:10.239714 32596 sched.cpp:318] Authenticating with master master@172.17.0.2:40874
I1221 23:10:10.239732 32596 sched.cpp:325] Using default CRAM-MD5 authenticatee
I1221 23:10:10.240026 32602 authenticatee.cpp:121] Creating new client SASL connection
I1221 23:10:10.240432 32594 master.cpp:5423] Authenticating scheduler-7caeff0b-9ca0-4c4c-935b-843b0edf2650@172.17.0.2:40874
I1221 23:10:10.240576 32596 authenticator.cpp:413] Starting authentication session for crammd5_authenticatee(2)@172.17.0.2:40874
I1221 23:10:10.240986 32605 authenticator.cpp:98] Creating new server SASL connection
I1221 23:10:10.241237 32607 authenticatee.cpp:212] Received SASL authentication mechanisms: CRAM-MD5
I1221 23:10:10.241272 32607 authenticatee.cpp:238] Attempting to authenticate with mechanism 'CRAM-MD5'
I1221 23:10:10.241382 32607 authenticator.cpp:203] Received SASL authentication start
I1221 23:10:10.241453 32607 authenticator.cpp:325] Authentication requires more steps
I1221 23:10:10.241591 32608 authenticatee.cpp:258] Received SASL authentication step
I1221 23:10:10.241724 32602 authenticator.cpp:231] Received SASL authentication step
I1221 23:10:10.241765 32602 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: '6ccf2ee56b13' server FQDN: '6ccf2ee56b13' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I1221 23:10:10.241783 32602 auxprop.cpp:179] Looking up auxiliary property '*userPassword'
I1221 23:10:10.241968 32602 auxprop.cpp:179] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I1221 23:10:10.242019 32602 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: '6ccf2ee56b13' server FQDN: '6ccf2ee56b13' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I1221 23:10:10.242038 32602 auxprop.cpp:129] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I1221 23:10:10.242048 32602 auxprop.cpp:129] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I1221 23:10:10.242076 32602 authenticator.cpp:317] Authentication success
I1221 23:10:10.242184 32604 authenticatee.cpp:298] Authentication success
I1221 23:10:10.242332 32604 master.cpp:5453] Successfully authenticated principal 'test-principal' at scheduler-7caeff0b-9ca0-4c4c-935b-843b0edf2650@172.17.0.2:40874
I1221 23:10:10.242419 32596 authenticator.cpp:431] Authentication session cleanup for crammd5_authenticatee(2)@172.17.0.2:40874
I1221 23:10:10.242563 32597 hierarchical.cpp:1329] No resources available to allocate!
I1221 23:10:10.242589 32608 sched.cpp:407] Successfully authenticated with master master@172.17.0.2:40874
I1221 23:10:10.242607 32597 hierarchical.cpp:1079] Performed allocation for 3 slaves in 417370ns
I1221 23:10:10.242616 32608 sched.cpp:714] Sending SUBSCRIBE call to master@172.17.0.2:40874
I1221 23:10:10.242784 32608 sched.cpp:747] Will retry registration in 1.218358899secs if necessary
I1221 23:10:10.242954 32595 master.cpp:2197] Received SUBSCRIBE call for framework 'No Executor Framework' at scheduler-7caeff0b-9ca0-4c4c-935b-843b0edf2650@172.17.0.2:40874
I1221 23:10:10.243087 32595 master.cpp:1668] Authorizing framework principal 'test-principal' to receive offers for role '*'
I1221 23:10:10.243455 32606 master.cpp:2268] Subscribing framework No Executor Framework with checkpointing enabled and capabilities [  ]
I1221 23:10:10.244477 32599 sched.cpp:641] Framework registered with 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000
I1221 23:10:10.244581 32608 hierarchical.cpp:260] Added framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000
I1221 23:10:10.244503 32599 no_executor_framework.cpp:81] Registered with master id: ""3931c1a8-1cd6-49eb-94c8-d01b33bb008e""
ip: 33558956
port: 40874
pid: ""master@172.17.0.2:40874""
hostname: ""6ccf2ee56b13""
version: ""0.27.0""
address {
  hostname: ""6ccf2ee56b13""
  ip: ""172.17.0.2""
  port: 40874
}
 and got framework ID 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000
I1221 23:10:10.244710 32599 sched.cpp:655] Scheduler::registered took 206722ns
I1221 23:10:10.247185 32608 hierarchical.cpp:1423] No inverse offers to send out!
I1221 23:10:10.247257 32608 hierarchical.cpp:1079] Performed allocation for 3 slaves in 2.650323ms
I1221 23:10:10.248139 32607 master.cpp:5252] Sending 3 offers to framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 (No Executor Framework) at scheduler-7caeff0b-9ca0-4c4c-935b-843b0edf2650@172.17.0.2:40874
I1221 23:10:10.248862 32597 no_executor_framework.cpp:105] Received offer 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-O0 from slave 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S1 (6ccf2ee56b13) with cpus(*):2; mem(*):10240; disk(*):3.70122e+06; ports(*):[31000-32000]
I1221 23:10:10.249704 32597 no_executor_framework.cpp:105] Received offer 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-O1 from slave 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S0 (6ccf2ee56b13) with cpus(*):2; mem(*):10240; disk(*):3.70122e+06; ports(*):[31000-32000]
I1221 23:10:10.249862 32597 no_executor_framework.cpp:105] Received offer 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-O2 from slave 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S2 (6ccf2ee56b13) with cpus(*):2; mem(*):10240; disk(*):3.70122e+06; ports(*):[31000-32000]
I1221 23:10:10.250028 32597 sched.cpp:811] Scheduler::resourceOffers took 1.177919ms
I1221 23:10:10.251667 32605 master.cpp:3055] Processing ACCEPT call for offers: [ 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-O0 ] on slave 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S1 at slave(1)@172.17.0.2:40874 (6ccf2ee56b13) for framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 (No Executor Framework) at scheduler-7caeff0b-9ca0-4c4c-935b-843b0edf2650@172.17.0.2:40874
I1221 23:10:10.251718 32605 master.cpp:2742] Authorizing framework principal 'test-principal' to launch task 0 as user 'mesos'
I1221 23:10:10.251905 32605 master.cpp:2742] Authorizing framework principal 'test-principal' to launch task 1 as user 'mesos'
I1221 23:10:10.251987 32605 master.cpp:2742] Authorizing framework principal 'test-principal' to launch task 2 as user 'mesos'
I1221 23:10:10.252104 32605 master.cpp:2742] Authorizing framework principal 'test-principal' to launch task 3 as user 'mesos'
I1221 23:10:10.252215 32605 master.cpp:2742] Authorizing framework principal 'test-principal' to launch task 4 as user 'mesos'
I1221 23:10:10.253389 32605 master.cpp:3055] Processing ACCEPT call for offers: [ 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-O1 ] on slave 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S0 at slave(3)@172.17.0.2:40874 (6ccf2ee56b13) for framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 (No Executor Framework) at scheduler-7caeff0b-9ca0-4c4c-935b-843b0edf2650@172.17.0.2:40874
I1221 23:10:10.254391 32605 master.cpp:3055] Processing ACCEPT call for offers: [ 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-O2 ] on slave 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S2 at slave(2)@172.17.0.2:40874 (6ccf2ee56b13) for framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 (No Executor Framework) at scheduler-7caeff0b-9ca0-4c4c-935b-843b0edf2650@172.17.0.2:40874
I1221 23:10:10.255661 32605 master.hpp:176] Adding task 0 with resources cpus(*):0.1; mem(*):32; disk(*):32 on slave 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S1 (6ccf2ee56b13)
I1221 23:10:10.255782 32605 master.cpp:3518] Launching task 0 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 (No Executor Framework) at scheduler-7caeff0b-9ca0-4c4c-935b-843b0edf2650@172.17.0.2:40874 with resources cpus(*):0.1; mem(*):32; disk(*):32 on slave 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S1 at slave(1)@172.17.0.2:40874 (6ccf2ee56b13)
I1221 23:10:10.256191 32600 slave.cpp:1294] Got assigned task 0 for framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000
I1221 23:10:10.256526 32600 slave.cpp:5133] Checkpointing FrameworkInfo to '/tmp/mesos-otpdch/0/meta/slaves/3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S1/frameworks/3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000/framework.info'
I1221 23:10:10.256664 32605 master.hpp:176] Adding task 1 with resources cpus(*):0.1; mem(*):32; disk(*):32 on slave 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S1 (6ccf2ee56b13)
I1221 23:10:10.256821 32605 master.cpp:3518] Launching task 1 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 (No Executor Framework) at scheduler-7caeff0b-9ca0-4c4c-935b-843b0edf2650@172.17.0.2:40874 with resources cpus(*):0.1; mem(*):32; disk(*):32 on slave 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S1 at slave(1)@172.17.0.2:40874 (6ccf2ee56b13)
I1221 23:10:10.257133 32600 slave.cpp:5144] Checkpointing framework pid 'scheduler-7caeff0b-9ca0-4c4c-935b-843b0edf2650@172.17.0.2:40874' to '/tmp/mesos-otpdch/0/meta/slaves/3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S1/frameworks/3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000/framework.pid'
I1221 23:10:10.257485 32600 resources.cpp:478] Parsing resources as JSON failed: cpus:0.1;mem:32
Trying semicolon-delimited string format instead
I1221 23:10:10.257843 32605 master.hpp:176] Adding task 2 with resources cpus(*):0.1; mem(*):32; disk(*):32 on slave 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S1 (6ccf2ee56b13)
I1221 23:10:10.257989 32605 master.cpp:3518] Launching task 2 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 (No Executor Framework) at scheduler-7caeff0b-9ca0-4c4c-935b-843b0edf2650@172.17.0.2:40874 with resources cpus(*):0.1; mem(*):32; disk(*):32 on slave 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S1 at slave(1)@172.17.0.2:40874 (6ccf2ee56b13)
I1221 23:10:10.258313 32600 slave.cpp:1294] Got assigned task 1 for framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000
I1221 23:10:10.258416 32600 resources.cpp:478] Parsing resources as JSON failed: cpus:0.1;mem:32
Trying semicolon-delimited string format instead
I1221 23:10:10.258853 32600 slave.cpp:1294] Got assigned task 2 for framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000
I1221 23:10:10.258858 32605 master.hpp:176] Adding task 3 with resources cpus(*):0.1; mem(*):32; disk(*):32 on slave 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S1 (6ccf2ee56b13)
I1221 23:10:10.258970 32600 resources.cpp:478] Parsing resources as JSON failed: cpus:0.1;mem:32
Trying semicolon-delimited string format instead
I1221 23:10:10.258997 32605 master.cpp:3518] Launching task 3 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 (No Executor Framework) at scheduler-7caeff0b-9ca0-4c4c-935b-843b0edf2650@172.17.0.2:40874 with resources cpus(*):0.1; mem(*):32; disk(*):32 on slave 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S1 at slave(1)@172.17.0.2:40874 (6ccf2ee56b13)
I1221 23:10:10.259377 32600 slave.cpp:1413] Launching task 0 for framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000
I1221 23:10:10.259452 32600 resources.cpp:478] Parsing resources as JSON failed: cpus:0.1;mem:32
Trying semicolon-delimited string format instead
I1221 23:10:10.259807 32605 master.hpp:176] Adding task 4 with resources cpus(*):0.1; mem(*):32; disk(*):32 on slave 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S1 (6ccf2ee56b13)
I1221 23:10:10.259959 32605 master.cpp:3518] Launching task 4 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 (No Executor Framework) at scheduler-7caeff0b-9ca0-4c4c-935b-843b0edf2650@172.17.0.2:40874 with resources cpus(*):0.1; mem(*):32; disk(*):32 on slave 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S1 at slave(1)@172.17.0.2:40874 (6ccf2ee56b13)
I1221 23:10:10.260562 32596 hierarchical.cpp:880] Recovered cpus(*):1.5; mem(*):10080; disk(*):3.70106e+06; ports(*):[31000-32000] (total: cpus(*):2; mem(*):10240; disk(*):3.70122e+06; ports(*):[31000-32000], allocated: cpus(*):0.5; mem(*):160; disk(*):160) on slave 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S1 from framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000
I1221 23:10:10.260640 32596 hierarchical.cpp:917] Framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 filtered slave 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S1 for 5secs
I1221 23:10:10.261168 32596 hierarchical.cpp:880] Recovered cpus(*):2; mem(*):10240; disk(*):3.70122e+06; ports(*):[31000-32000] (total: cpus(*):2; mem(*):10240; disk(*):3.70122e+06; ports(*):[31000-32000], allocated: ) on slave 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S0 from framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000
I1221 23:10:10.261225 32596 hierarchical.cpp:917] Framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 filtered slave 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S0 for 5secs
I1221 23:10:10.261605 32596 hierarchical.cpp:880] Recovered cpus(*):2; mem(*):10240; disk(*):3.70122e+06; ports(*):[31000-32000] (total: cpus(*):2; mem(*):10240; disk(*):3.70122e+06; ports(*):[31000-32000], allocated: ) on slave 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S2 from framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000
I1221 23:10:10.261659 32596 hierarchical.cpp:917] Framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 filtered slave 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S2 for 5secs
I1221 23:10:10.261741 32600 paths.cpp:434] Trying to chown '/tmp/mesos-otpdch/0/slaves/3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S1/frameworks/3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000/executors/0/runs/b1188a8b-2b23-4bae-9539-4a6e5c844624' to user 'mesos'
I1221 23:10:10.265745 32600 slave.cpp:5582] Checkpointing ExecutorInfo to '/tmp/mesos-otpdch/0/meta/slaves/3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S1/frameworks/3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000/executors/0/executor.info'
I1221 23:10:10.266571 32600 slave.cpp:5211] Launching executor 0 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 with resources cpus(*):0.1; mem(*):32 in work directory '/tmp/mesos-otpdch/0/slaves/3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S1/frameworks/3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000/executors/0/runs/b1188a8b-2b23-4bae-9539-4a6e5c844624'
I1221 23:10:10.267230 32604 containerizer.cpp:612] Starting container 'b1188a8b-2b23-4bae-9539-4a6e5c844624' for executor '0' of framework '3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000'
I1221 23:10:10.267544 32600 slave.cpp:5605] Checkpointing TaskInfo to '/tmp/mesos-otpdch/0/meta/slaves/3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S1/frameworks/3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000/executors/0/runs/b1188a8b-2b23-4bae-9539-4a6e5c844624/tasks/0/task.info'
I1221 23:10:10.267994 32600 slave.cpp:1631] Queuing task '0' for executor '0' of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000
I1221 23:10:10.268184 32600 slave.cpp:1413] Launching task 1 for framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000
I1221 23:10:10.268286 32600 resources.cpp:478] Parsing resources as JSON failed: cpus:0.1;mem:32
Trying semicolon-delimited string format instead
I1221 23:10:10.268862 32600 paths.cpp:434] Trying to chown '/tmp/mesos-otpdch/0/slaves/3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S1/frameworks/3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000/executors/1/runs/468fb0f8-4dd0-448c-8e64-207cc4c81c63' to user 'mesos'
I1221 23:10:10.272169 32600 slave.cpp:5582] Checkpointing ExecutorInfo to '/tmp/mesos-otpdch/0/meta/slaves/3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S1/frameworks/3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000/executors/1/executor.info'
I1221 23:10:10.274011 32600 slave.cpp:5211] Launching executor 1 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 with resources cpus(*):0.1; mem(*):32 in work directory '/tmp/mesos-otpdch/0/slaves/3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S1/frameworks/3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000/executors/1/runs/468fb0f8-4dd0-448c-8e64-207cc4c81c63'
I1221 23:10:10.274662 32600 slave.cpp:5605] Checkpointing TaskInfo to '/tmp/mesos-otpdch/0/meta/slaves/3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S1/frameworks/3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000/executors/1/runs/468fb0f8-4dd0-448c-8e64-207cc4c81c63/tasks/1/task.info'
I1221 23:10:10.274734 32605 launcher.cpp:132] Forked child with pid '32615' for container 'b1188a8b-2b23-4bae-9539-4a6e5c844624'
I1221 23:10:10.275048 32605 containerizer.cpp:845] Checkpointing executor's forked pid 32615 to '/tmp/mesos-otpdch/0/meta/slaves/3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S1/frameworks/3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000/executors/0/runs/b1188a8b-2b23-4bae-9539-4a6e5c844624/pids/forked.pid'
I1221 23:10:10.275127 32600 slave.cpp:1631] Queuing task '1' for executor '1' of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000
I1221 23:10:10.275367 32600 slave.cpp:1294] Got assigned task 3 for framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000
I1221 23:10:10.275490 32600 resources.cpp:478] Parsing resources as JSON failed: cpus:0.1;mem:32
Trying semicolon-delimited string format instead
I1221 23:10:10.275995 32600 slave.cpp:1413] Launching task 2 for framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000
I1221 23:10:10.276074 32600 resources.cpp:478] Parsing resources as JSON failed: cpus:0.1;mem:32
Trying semicolon-delimited string format instead
I1221 23:10:10.276592 32600 paths.cpp:434] Trying to chown '/tmp/mesos-otpdch/0/slaves/3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S1/frameworks/3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000/executors/2/runs/63bf8626-189d-44f9-a961-b51b046d5057' to user 'mesos'
I1221 23:10:10.278714 32605 containerizer.cpp:612] Starting container '468fb0f8-4dd0-448c-8e64-207cc4c81c63' for executor '1' of framework '3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000'
I1221 23:10:10.280418 32600 slave.cpp:5582] Checkpointing ExecutorInfo to '/tmp/mesos-otpdch/0/meta/slaves/3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S1/frameworks/3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000/executors/2/executor.info'
I1221 23:10:10.281625 32593 launcher.cpp:132] Forked child with pid '32624' for container '468fb0f8-4dd0-448c-8e64-207cc4c81c63'
I1221 23:10:10.281893 32593 containerizer.cpp:845] Checkpointing executor's forked pid 32624 to '/tmp/mesos-otpdch/0/meta/slaves/3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S1/frameworks/3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000/executors/1/runs/468fb0f8-4dd0-448c-8e64-207cc4c81c63/pids/forked.pid'
I1221 23:10:10.281858 32600 slave.cpp:5211] Launching executor 2 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 with resources cpus(*):0.1; mem(*):32 in work directory '/tmp/mesos-otpdch/0/slaves/3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S1/frameworks/3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000/executors/2/runs/63bf8626-189d-44f9-a961-b51b046d5057'
I1221 23:10:10.282771 32600 slave.cpp:5605] Checkpointing TaskInfo to '/tmp/mesos-otpdch/0/meta/slaves/3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S1/frameworks/3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000/executors/2/runs/63bf8626-189d-44f9-a961-b51b046d5057/tasks/2/task.info'
I1221 23:10:10.283249 32593 containerizer.cpp:612] Starting container '63bf8626-189d-44f9-a961-b51b046d5057' for executor '2' of framework '3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000'
I1221 23:10:10.283433 32600 slave.cpp:1631] Queuing task '2' for executor '2' of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000
I1221 23:10:10.283763 32600 slave.cpp:1294] Got assigned task 4 for framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000
I1221 23:10:10.283947 32600 resources.cpp:478] Parsing resources as JSON failed: cpus:0.1;mem:32
Trying semicolon-delimited string format instead
I1221 23:10:10.284603 32600 slave.cpp:682] Successfully attached file '/tmp/mesos-otpdch/0/slaves/3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S1/frameworks/3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000/executors/0/runs/b1188a8b-2b23-4bae-9539-4a6e5c844624'
I1221 23:10:10.284701 32600 slave.cpp:682] Successfully attached file '/tmp/mesos-otpdch/0/slaves/3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S1/frameworks/3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000/executors/1/runs/468fb0f8-4dd0-448c-8e64-207cc4c81c63'
I1221 23:10:10.284961 32600 slave.cpp:1413] Launching task 3 for framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000
I1221 23:10:10.285212 32600 resources.cpp:478] Parsing resources as JSON failed: cpus:0.1;mem:32
Trying semicolon-delimited string format instead
I1221 23:10:10.286070 32600 paths.cpp:434] Trying to chown '/tmp/mesos-otpdch/0/slaves/3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S1/frameworks/3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000/executors/3/runs/7f37bf6f-50f1-45a9-ae7a-20b1b829cc38' to user 'mesos'
I1221 23:10:10.288195 32607 launcher.cpp:132] Forked child with pid '32633' for container '63bf8626-189d-44f9-a961-b51b046d5057'
I1221 23:10:10.288494 32607 containerizer.cpp:845] Checkpointing executor's forked pid 32633 to '/tmp/mesos-otpdch/0/meta/slaves/3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S1/frameworks/3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000/executors/2/runs/63bf8626-189d-44f9-a961-b51b046d5057/pids/forked.pid'
I1221 23:10:10.292366 32600 slave.cpp:5582] Checkpointing ExecutorInfo to '/tmp/mesos-otpdch/0/meta/slaves/3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S1/frameworks/3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000/executors/3/executor.info'
I1221 23:10:10.293144 32600 slave.cpp:5211] Launching executor 3 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 with resources cpus(*):0.1; mem(*):32 in work directory '/tmp/mesos-otpdch/0/slaves/3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S1/frameworks/3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000/executors/3/runs/7f37bf6f-50f1-45a9-ae7a-20b1b829cc38'
I1221 23:10:10.293701 32605 containerizer.cpp:612] Starting container '7f37bf6f-50f1-45a9-ae7a-20b1b829cc38' for executor '3' of framework '3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000'
I1221 23:10:10.293853 32600 slave.cpp:5605] Checkpointing TaskInfo to '/tmp/mesos-otpdch/0/meta/slaves/3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S1/frameworks/3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000/executors/3/runs/7f37bf6f-50f1-45a9-ae7a-20b1b829cc38/tasks/3/task.info'
I1221 23:10:10.294319 32600 slave.cpp:1631] Queuing task '3' for executor '3' of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000
I1221 23:10:10.294919 32600 slave.cpp:682] Successfully attached file '/tmp/mesos-otpdch/0/slaves/3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S1/frameworks/3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000/executors/2/runs/63bf8626-189d-44f9-a961-b51b046d5057'
I1221 23:10:10.295056 32600 slave.cpp:1413] Launching task 4 for framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000
I1221 23:10:10.295173 32600 resources.cpp:478] Parsing resources as JSON failed: cpus:0.1;mem:32
Trying semicolon-delimited string format instead
I1221 23:10:10.296032 32600 paths.cpp:434] Trying to chown '/tmp/mesos-otpdch/0/slaves/3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S1/frameworks/3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000/executors/4/runs/bc8306b3-d4ba-4bd6-b5f5-03dfb4759738' to user 'mesos'
I1221 23:10:10.298739 32605 launcher.cpp:132] Forked child with pid '32658' for container '7f37bf6f-50f1-45a9-ae7a-20b1b829cc38'
I1221 23:10:10.299033 32605 containerizer.cpp:845] Checkpointing executor's forked pid 32658 to '/tmp/mesos-otpdch/0/meta/slaves/3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S1/frameworks/3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000/executors/3/runs/7f37bf6f-50f1-45a9-ae7a-20b1b829cc38/pids/forked.pid'
I1221 23:10:10.300787 32600 slave.cpp:5582] Checkpointing ExecutorInfo to '/tmp/mesos-otpdch/0/meta/slaves/3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S1/frameworks/3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000/executors/4/executor.info'
I1221 23:10:10.301609 32600 slave.cpp:5211] Launching executor 4 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 with resources cpus(*):0.1; mem(*):32 in work directory '/tmp/mesos-otpdch/0/slaves/3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S1/frameworks/3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000/executors/4/runs/bc8306b3-d4ba-4bd6-b5f5-03dfb4759738'
I1221 23:10:10.302110 32602 containerizer.cpp:612] Starting container 'bc8306b3-d4ba-4bd6-b5f5-03dfb4759738' for executor '4' of framework '3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000'
I1221 23:10:10.302317 32600 slave.cpp:5605] Checkpointing TaskInfo to '/tmp/mesos-otpdch/0/meta/slaves/3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S1/frameworks/3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000/executors/4/runs/bc8306b3-d4ba-4bd6-b5f5-03dfb4759738/tasks/4/task.info'
I1221 23:10:10.302917 32600 slave.cpp:1631] Queuing task '4' for executor '4' of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000
I1221 23:10:10.303236 32600 slave.cpp:682] Successfully attached file '/tmp/mesos-otpdch/0/slaves/3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S1/frameworks/3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000/executors/3/runs/7f37bf6f-50f1-45a9-ae7a-20b1b829cc38'
I1221 23:10:10.303436 32600 slave.cpp:682] Successfully attached file '/tmp/mesos-otpdch/0/slaves/3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S1/frameworks/3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000/executors/4/runs/bc8306b3-d4ba-4bd6-b5f5-03dfb4759738'
I1221 23:10:10.307138 32598 launcher.cpp:132] Forked child with pid '32667' for container 'bc8306b3-d4ba-4bd6-b5f5-03dfb4759738'
I1221 23:10:10.307472 32598 containerizer.cpp:845] Checkpointing executor's forked pid 32667 to '/tmp/mesos-otpdch/0/meta/slaves/3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S1/frameworks/3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000/executors/4/runs/bc8306b3-d4ba-4bd6-b5f5-03dfb4759738/pids/forked.pid'
WARNING: Logging before InitGoogleLogging() is written to STDERR
I1221 23:10:10.415683 32691 process.cpp:998] libprocess is initialized on 172.17.0.2:60995 for 16 cpus
I1221 23:10:10.416345 32691 logging.cpp:172] Logging to STDERR
I1221 23:10:10.419946 32691 exec.cpp:134] Version: 0.27.0
WARNING: Logging before InitGoogleLogging() is written to STDERR
I1221 23:10:10.422938 32696 process.cpp:998] libprocess is initialized on 172.17.0.2:33894 for 16 cpus
WARNING: Logging before InitGoogleLogging() is written to STDERR
I1221 23:10:10.423743 32695 process.cpp:998] libprocess is initialized on 172.17.0.2:51530 for 16 cpus
I1221 23:10:10.424510 32695 logging.cpp:172] Logging to STDERR
I1221 23:10:10.424516 32696 logging.cpp:172] Logging to STDERR
I1221 23:10:10.425381   304 exec.cpp:184] Executor started at: executor(1)@172.17.0.2:60995 with pid 32691
I1221 23:10:10.427569 32696 exec.cpp:134] Version: 0.27.0
I1221 23:10:10.427634 32695 exec.cpp:134] Version: 0.27.0
I1221 23:10:10.429059 32596 slave.cpp:2578] Got registration for executor '0' of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 from executor(1)@172.17.0.2:60995
I1221 23:10:10.429922 32596 slave.cpp:2664] Checkpointing executor pid 'executor(1)@172.17.0.2:60995' to '/tmp/mesos-otpdch/0/meta/slaves/3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S1/frameworks/3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000/executors/0/runs/b1188a8b-2b23-4bae-9539-4a6e5c844624/pids/libprocess.pid'
I1221 23:10:10.432864 32597 slave.cpp:1796] Sending queued task '0' to executor '0' of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 at executor(1)@172.17.0.2:60995
I1221 23:10:10.433215 32765 exec.cpp:208] Executor registered on slave 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S1
I1221 23:10:10.435856 32765 exec.cpp:220] Executor::registered took 343507ns
Registered executor on 6ccf2ee56b13
I1221 23:10:10.436198 32765 exec.cpp:295] Executor asked to run task '0'
I1221 23:10:10.436317 32765 exec.cpp:304] Executor::launchTask took 86293ns
Starting task 0
Forked command at 361
sh -c 'echo hello'
I1221 23:10:10.437671   335 exec.cpp:184] Executor started at: executor(1)@172.17.0.2:33894 with pid 32696
I1221 23:10:10.438062   315 exec.cpp:184] Executor started at: executor(1)@172.17.0.2:51530 with pid 32695
hello
I1221 23:10:10.440052 32599 slave.cpp:2578] Got registration for executor '2' of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 from executor(1)@172.17.0.2:33894
I1221 23:10:10.440507 32599 slave.cpp:2664] Checkpointing executor pid 'executor(1)@172.17.0.2:33894' to '/tmp/mesos-otpdch/0/meta/slaves/3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S1/frameworks/3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000/executors/2/runs/63bf8626-189d-44f9-a961-b51b046d5057/pids/libprocess.pid'
WARNING: Logging before InitGoogleLogging() is written to STDERR
I1221 23:10:10.440497 32733 process.cpp:998] libprocess is initialized on 172.17.0.2:33285 for 16 cpus
I1221 23:10:10.441426 32733 logging.cpp:172] Logging to STDERR
I1221 23:10:10.441916 32599 slave.cpp:2578] Got registration for executor '1' of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 from executor(1)@172.17.0.2:51530
I1221 23:10:10.442277 32599 slave.cpp:2664] Checkpointing executor pid 'executor(1)@172.17.0.2:51530' to '/tmp/mesos-otpdch/0/meta/slaves/3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S1/frameworks/3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000/executors/1/runs/468fb0f8-4dd0-448c-8e64-207cc4c81c63/pids/libprocess.pid'
I1221 23:10:10.442314   303 exec.cpp:517] Executor sending status update TASK_RUNNING (UUID: 61620dd2-95c0-4570-b174-24ed26e0a289) for task 0 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000
I1221 23:10:10.442684   339 exec.cpp:208] Executor registered on slave 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S1
I1221 23:10:10.443305 32599 slave.cpp:1796] Sending queued task '2' to executor '2' of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 at executor(1)@172.17.0.2:33894
I1221 23:10:10.443668 32733 exec.cpp:134] Version: 0.27.0
I1221 23:10:10.443768 32599 slave.cpp:2937] Handling status update TASK_RUNNING (UUID: 61620dd2-95c0-4570-b174-24ed26e0a289) for task 0 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 from executor(1)@172.17.0.2:60995
I1221 23:10:10.444797   320 exec.cpp:208] Executor registered on slave 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S1
I1221 23:10:10.445159   339 exec.cpp:220] Executor::registered took 241955ns
Registered executor on 6ccf2ee56b13
I1221 23:10:10.445430   339 exec.cpp:295] Executor asked to run task '2'
Starting task 2
I1221 23:10:10.445519   339 exec.cpp:304] Executor::launchTask took 65600ns
I1221 23:10:10.445794 32598 status_update_manager.cpp:320] Received status update TASK_RUNNING (UUID: 61620dd2-95c0-4570-b174-24ed26e0a289) for task 0 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000
I1221 23:10:10.445888 32598 status_update_manager.cpp:497] Creating StatusUpdate stream for task 0 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000
I1221 23:10:10.446015 32599 slave.cpp:1796] Sending queued task '1' to executor '1' of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 at executor(1)@172.17.0.2:51530
Forked command at 362
I1221 23:10:10.446622 32598 status_update_manager.cpp:824] Checkpointing UPDATE for status update TASK_RUNNING (UUID: 61620dd2-95c0-4570-b174-24ed26e0a289) for task 0 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000
I1221 23:10:10.446990   320 exec.cpp:220] Executor::registered took 324561ns
sh -c 'echo hello'
Registered executor on 6ccf2ee56b13
I1221 23:10:10.447427   320 exec.cpp:295] Executor asked to run task '1'
Starting task 1
I1221 23:10:10.447540   320 exec.cpp:304] Executor::launchTask took 84828ns
Forked command at 363
sh -c 'echo hello'
hello
I1221 23:10:10.449244   334 exec.cpp:517] Executor sending status update TASK_RUNNING (UUID: b774dc4c-940c-41d5-be2a-ed3a9e8379a7) for task 2 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000
hello
I1221 23:10:10.450145 32597 slave.cpp:2937] Handling status update TASK_RUNNING (UUID: b774dc4c-940c-41d5-be2a-ed3a9e8379a7) for task 2 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 from executor(1)@172.17.0.2:33894
I1221 23:10:10.451092 32598 status_update_manager.cpp:374] Forwarding update TASK_RUNNING (UUID: 61620dd2-95c0-4570-b174-24ed26e0a289) for task 0 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 to the slave
I1221 23:10:10.451117   324 exec.cpp:517] Executor sending status update TASK_RUNNING (UUID: 7a5d5a7f-975d-4970-949e-cfa4d08a4a30) for task 1 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000
I1221 23:10:10.451370 32595 slave.cpp:3289] Forwarding the update TASK_RUNNING (UUID: 61620dd2-95c0-4570-b174-24ed26e0a289) for task 0 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 to master@172.17.0.2:40874
I1221 23:10:10.451637 32598 status_update_manager.cpp:320] Received status update TASK_RUNNING (UUID: b774dc4c-940c-41d5-be2a-ed3a9e8379a7) for task 2 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000
I1221 23:10:10.451710 32598 status_update_manager.cpp:497] Creating StatusUpdate stream for task 2 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000
I1221 23:10:10.451652 32595 slave.cpp:3183] Status update manager successfully handled status update TASK_RUNNING (UUID: 61620dd2-95c0-4570-b174-24ed26e0a289) for task 0 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000
I1221 23:10:10.451838 32593 master.cpp:4687] Status update TASK_RUNNING (UUID: 61620dd2-95c0-4570-b174-24ed26e0a289) for task 0 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 from slave 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S1 at slave(1)@172.17.0.2:40874 (6ccf2ee56b13)
I1221 23:10:10.451974 32593 master.cpp:4735] Forwarding status update TASK_RUNNING (UUID: 61620dd2-95c0-4570-b174-24ed26e0a289) for task 0 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000
I1221 23:10:10.452203 32593 master.cpp:6347] Updating the state of task 0 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 (latest state: TASK_RUNNING, status update state: TASK_RUNNING)
I1221 23:10:10.451875 32595 slave.cpp:3199] Sending acknowledgement for status update TASK_RUNNING (UUID: 61620dd2-95c0-4570-b174-24ed26e0a289) for task 0 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 to executor(1)@172.17.0.2:60995
I1221 23:10:10.452308 32598 status_update_manager.cpp:824] Checkpointing UPDATE for status update TASK_RUNNING (UUID: b774dc4c-940c-41d5-be2a-ed3a9e8379a7) for task 2 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000
I1221 23:10:10.452432 32603 no_executor_framework.cpp:160] Task '0' is in state TASK_RUNNING
I1221 23:10:10.452468 32603 sched.cpp:919] Scheduler::statusUpdate took 46157ns
I1221 23:10:10.452905 32606 slave.cpp:2937] Handling status update TASK_RUNNING (UUID: 7a5d5a7f-975d-4970-949e-cfa4d08a4a30) for task 1 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 from executor(1)@172.17.0.2:51530
I1221 23:10:10.453151 32598 status_update_manager.cpp:374] Forwarding update TASK_RUNNING (UUID: b774dc4c-940c-41d5-be2a-ed3a9e8379a7) for task 2 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 to the slave
I1221 23:10:10.453403   302 exec.cpp:341] Executor received status update acknowledgement 61620dd2-95c0-4570-b174-24ed26e0a289 for task 0 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000
I1221 23:10:10.453454 32598 status_update_manager.cpp:320] Received status update TASK_RUNNING (UUID: 7a5d5a7f-975d-4970-949e-cfa4d08a4a30) for task 1 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000
I1221 23:10:10.453496 32598 status_update_manager.cpp:497] Creating StatusUpdate stream for task 1 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000
I1221 23:10:10.453488 32606 slave.cpp:3289] Forwarding the update TASK_RUNNING (UUID: b774dc4c-940c-41d5-be2a-ed3a9e8379a7) for task 2 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 to master@172.17.0.2:40874
I1221 23:10:10.453840 32598 status_update_manager.cpp:824] Checkpointing UPDATE for status update TASK_RUNNING (UUID: 7a5d5a7f-975d-4970-949e-cfa4d08a4a30) for task 1 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000
I1221 23:10:10.453954 32603 master.cpp:3844] Processing ACKNOWLEDGE call 61620dd2-95c0-4570-b174-24ed26e0a289 for task 0 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 (No Executor Framework) at scheduler-7caeff0b-9ca0-4c4c-935b-843b0edf2650@172.17.0.2:40874 on slave 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S1
I1221 23:10:10.454144 32606 slave.cpp:3183] Status update manager successfully handled status update TASK_RUNNING (UUID: b774dc4c-940c-41d5-be2a-ed3a9e8379a7) for task 2 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000
I1221 23:10:10.454195 32606 slave.cpp:3199] Sending acknowledgement for status update TASK_RUNNING (UUID: b774dc4c-940c-41d5-be2a-ed3a9e8379a7) for task 2 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 to executor(1)@172.17.0.2:33894
I1221 23:10:10.454293 32600 master.cpp:4687] Status update TASK_RUNNING (UUID: b774dc4c-940c-41d5-be2a-ed3a9e8379a7) for task 2 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 from slave 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S1 at slave(1)@172.17.0.2:40874 (6ccf2ee56b13)
I1221 23:10:10.454344 32600 master.cpp:4735] Forwarding status update TASK_RUNNING (UUID: b774dc4c-940c-41d5-be2a-ed3a9e8379a7) for task 2 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000
I1221 23:10:10.454502 32600 master.cpp:6347] Updating the state of task 2 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 (latest state: TASK_RUNNING, status update state: TASK_RUNNING)
I1221 23:10:10.454665 32600 no_executor_framework.cpp:160] Task '2' is in state TASK_RUNNING
I1221 23:10:10.454691 32600 sched.cpp:919] Scheduler::statusUpdate took 31056ns
I1221 23:10:10.454856   335 exec.cpp:341] Executor received status update acknowledgement b774dc4c-940c-41d5-be2a-ed3a9e8379a7 for task 2 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000
I1221 23:10:10.454898 32598 status_update_manager.cpp:374] Forwarding update TASK_RUNNING (UUID: 7a5d5a7f-975d-4970-949e-cfa4d08a4a30) for task 1 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 to the slave
I1221 23:10:10.455098 32600 master.cpp:3844] Processing ACKNOWLEDGE call b774dc4c-940c-41d5-be2a-ed3a9e8379a7 for task 2 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 (No Executor Framework) at scheduler-7caeff0b-9ca0-4c4c-935b-843b0edf2650@172.17.0.2:40874 on slave 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S1
I1221 23:10:10.455250 32605 slave.cpp:3289] Forwarding the update TASK_RUNNING (UUID: 7a5d5a7f-975d-4970-949e-cfa4d08a4a30) for task 1 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 to master@172.17.0.2:40874
I1221 23:10:10.455487 32598 status_update_manager.cpp:392] Received status update acknowledgement (UUID: 61620dd2-95c0-4570-b174-24ed26e0a289) for task 0 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000
I1221 23:10:10.455533 32605 slave.cpp:3183] Status update manager successfully handled status update TASK_RUNNING (UUID: 7a5d5a7f-975d-4970-949e-cfa4d08a4a30) for task 1 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000
I1221 23:10:10.455538 32607 master.cpp:4687] Status update TASK_RUNNING (UUID: 7a5d5a7f-975d-4970-949e-cfa4d08a4a30) for task 1 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 from slave 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S1 at slave(1)@172.17.0.2:40874 (6ccf2ee56b13)
I1221 23:10:10.455574 32607 master.cpp:4735] Forwarding status update TASK_RUNNING (UUID: 7a5d5a7f-975d-4970-949e-cfa4d08a4a30) for task 1 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000
I1221 23:10:10.455574 32605 slave.cpp:3199] Sending acknowledgement for status update TASK_RUNNING (UUID: 7a5d5a7f-975d-4970-949e-cfa4d08a4a30) for task 1 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 to executor(1)@172.17.0.2:51530
I1221 23:10:10.455685 32598 status_update_manager.cpp:824] Checkpointing ACK for status update TASK_RUNNING (UUID: 61620dd2-95c0-4570-b174-24ed26e0a289) for task 0 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000
I1221 23:10:10.455713 32607 master.cpp:6347] Updating the state of task 1 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 (latest state: TASK_RUNNING, status update state: TASK_RUNNING)
I1221 23:10:10.455813 32606 no_executor_framework.cpp:160] Task '1' is in state TASK_RUNNING
I1221 23:10:10.455916 32606 sched.cpp:919] Scheduler::statusUpdate took 105226ns
I1221 23:10:10.455812   344 exec.cpp:184] Executor started at: executor(1)@172.17.0.2:33285 with pid 32733
I1221 23:10:10.456140 32607 master.cpp:3844] Processing ACKNOWLEDGE call 7a5d5a7f-975d-4970-949e-cfa4d08a4a30 for task 1 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 (No Executor Framework) at scheduler-7caeff0b-9ca0-4c4c-935b-843b0edf2650@172.17.0.2:40874 on slave 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S1
I1221 23:10:10.456163   312 exec.cpp:341] Executor received status update acknowledgement 7a5d5a7f-975d-4970-949e-cfa4d08a4a30 for task 1 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000
I1221 23:10:10.456812 32598 status_update_manager.cpp:392] Received status update acknowledgement (UUID: b774dc4c-940c-41d5-be2a-ed3a9e8379a7) for task 2 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000
I1221 23:10:10.456807 32606 slave.cpp:2348] Status update manager successfully handled status update acknowledgement (UUID: 61620dd2-95c0-4570-b174-24ed26e0a289) for task 0 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000
I1221 23:10:10.457012 32598 status_update_manager.cpp:824] Checkpointing ACK for status update TASK_RUNNING (UUID: b774dc4c-940c-41d5-be2a-ed3a9e8379a7) for task 2 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000
I1221 23:10:10.457720 32598 status_update_manager.cpp:392] Received status update acknowledgement (UUID: 7a5d5a7f-975d-4970-949e-cfa4d08a4a30) for task 1 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000
I1221 23:10:10.457751 32607 slave.cpp:2348] Status update manager successfully handled status update acknowledgement (UUID: b774dc4c-940c-41d5-be2a-ed3a9e8379a7) for task 2 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000
I1221 23:10:10.457883 32598 status_update_manager.cpp:824] Checkpointing ACK for status update TASK_RUNNING (UUID: 7a5d5a7f-975d-4970-949e-cfa4d08a4a30) for task 1 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000
I1221 23:10:10.458319 32604 slave.cpp:2578] Got registration for executor '3' of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 from executor(1)@172.17.0.2:33285
I1221 23:10:10.458766 32604 slave.cpp:2664] Checkpointing executor pid 'executor(1)@172.17.0.2:33285' to '/tmp/mesos-otpdch/0/meta/slaves/3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S1/frameworks/3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000/executors/3/runs/7f37bf6f-50f1-45a9-ae7a-20b1b829cc38/pids/libprocess.pid'
I1221 23:10:10.460311 32604 slave.cpp:2348] Status update manager successfully handled status update acknowledgement (UUID: 7a5d5a7f-975d-4970-949e-cfa4d08a4a30) for task 1 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000
I1221 23:10:10.461024   349 exec.cpp:208] Executor registered on slave 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S1
I1221 23:10:10.461304 32603 slave.cpp:1796] Sending queued task '3' to executor '3' of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 at executor(1)@172.17.0.2:33285
I1221 23:10:10.462968   349 exec.cpp:220] Executor::registered took 273965ns
Registered executor on 6ccf2ee56b13
I1221 23:10:10.463317   349 exec.cpp:295] Executor asked to run task '3'
Starting task 3
I1221 23:10:10.463429   349 exec.cpp:304] Executor::launchTask took 87510ns
Forked command at 381
sh -c 'echo hello'
hello
WARNING: Logging before InitGoogleLogging() is written to STDERR
I1221 23:10:10.466104 32747 process.cpp:998] libprocess is initialized on 172.17.0.2:39520 for 16 cpus
I1221 23:10:10.466836   351 exec.cpp:517] Executor sending status update TASK_RUNNING (UUID: f4c28f0e-5d99-46e2-8e97-be35f47a8447) for task 3 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000
I1221 23:10:10.467958 32604 slave.cpp:2937] Handling status update TASK_RUNNING (UUID: f4c28f0e-5d99-46e2-8e97-be35f47a8447) for task 3 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 from executor(1)@172.17.0.2:33285
I1221 23:10:10.468389 32595 status_update_manager.cpp:320] Received status update TASK_RUNNING (UUID: f4c28f0e-5d99-46e2-8e97-be35f47a8447) for task 3 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000
I1221 23:10:10.468441 32595 status_update_manager.cpp:497] Creating StatusUpdate stream for task 3 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000
I1221 23:10:10.469084 32595 status_update_manager.cpp:824] Checkpointing UPDATE for status update TASK_RUNNING (UUID: f4c28f0e-5d99-46e2-8e97-be35f47a8447) for task 3 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000
I1221 23:10:10.470094 32595 status_update_manager.cpp:374] Forwarding update TASK_RUNNING (UUID: f4c28f0e-5d99-46e2-8e97-be35f47a8447) for task 3 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 to the slave
I1221 23:10:10.470365 32597 slave.cpp:3289] Forwarding the update TASK_RUNNING (UUID: f4c28f0e-5d99-46e2-8e97-be35f47a8447) for task 3 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 to master@172.17.0.2:40874
I1221 23:10:10.470433 32747 logging.cpp:172] Logging to STDERR
I1221 23:10:10.470557 32597 slave.cpp:3183] Status update manager successfully handled status update TASK_RUNNING (UUID: f4c28f0e-5d99-46e2-8e97-be35f47a8447) for task 3 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000
I1221 23:10:10.470603 32597 slave.cpp:3199] Sending acknowledgement for status update TASK_RUNNING (UUID: f4c28f0e-5d99-46e2-8e97-be35f47a8447) for task 3 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 to executor(1)@172.17.0.2:33285
I1221 23:10:10.470706 32604 master.cpp:4687] Status update TASK_RUNNING (UUID: f4c28f0e-5d99-46e2-8e97-be35f47a8447) for task 3 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 from slave 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S1 at slave(1)@172.17.0.2:40874 (6ccf2ee56b13)
I1221 23:10:10.470757 32604 master.cpp:4735] Forwarding status update TASK_RUNNING (UUID: f4c28f0e-5d99-46e2-8e97-be35f47a8447) for task 3 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000
I1221 23:10:10.470984 32604 master.cpp:6347] Updating the state of task 3 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 (latest state: TASK_RUNNING, status update state: TASK_RUNNING)
I1221 23:10:10.471132 32595 no_executor_framework.cpp:160] Task '3' is in state TASK_RUNNING
I1221 23:10:10.471163 32595 sched.cpp:919] Scheduler::statusUpdate took 46297ns
I1221 23:10:10.471405 32595 master.cpp:3844] Processing ACKNOWLEDGE call f4c28f0e-5d99-46e2-8e97-be35f47a8447 for task 3 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 (No Executor Framework) at scheduler-7caeff0b-9ca0-4c4c-935b-843b0edf2650@172.17.0.2:40874 on slave 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S1
I1221 23:10:10.471519   346 exec.cpp:341] Executor received status update acknowledgement f4c28f0e-5d99-46e2-8e97-be35f47a8447 for task 3 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000
I1221 23:10:10.471691 32595 status_update_manager.cpp:392] Received status update acknowledgement (UUID: f4c28f0e-5d99-46e2-8e97-be35f47a8447) for task 3 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000
I1221 23:10:10.471897 32595 status_update_manager.cpp:824] Checkpointing ACK for status update TASK_RUNNING (UUID: f4c28f0e-5d99-46e2-8e97-be35f47a8447) for task 3 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000
I1221 23:10:10.472843 32608 slave.cpp:2348] Status update manager successfully handled status update acknowledgement (UUID: f4c28f0e-5d99-46e2-8e97-be35f47a8447) for task 3 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000
I1221 23:10:10.476374 32747 exec.cpp:134] Version: 0.27.0
I1221 23:10:10.478024   373 exec.cpp:184] Executor started at: executor(1)@172.17.0.2:39520 with pid 32747
I1221 23:10:10.480216 32607 slave.cpp:2578] Got registration for executor '4' of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 from executor(1)@172.17.0.2:39520
I1221 23:10:10.480746 32607 slave.cpp:2664] Checkpointing executor pid 'executor(1)@172.17.0.2:39520' to '/tmp/mesos-otpdch/0/meta/slaves/3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S1/frameworks/3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000/executors/4/runs/bc8306b3-d4ba-4bd6-b5f5-03dfb4759738/pids/libprocess.pid'
I1221 23:10:10.482753 32597 slave.cpp:1796] Sending queued task '4' to executor '4' of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 at executor(1)@172.17.0.2:39520
I1221 23:10:10.483325   379 exec.cpp:208] Executor registered on slave 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S1
Registered executor on 6ccf2ee56b13
I1221 23:10:10.485102   379 exec.cpp:220] Executor::registered took 269656ns
I1221 23:10:10.485415   379 exec.cpp:295] Executor asked to run task '4'
Starting task 4
I1221 23:10:10.485508   379 exec.cpp:304] Executor::launchTask took 63624ns
sh -c 'echo hello'
Forked command at 382
hello
I1221 23:10:10.488653   377 exec.cpp:517] Executor sending status update TASK_RUNNING (UUID: c8c2c7b7-8040-48e9-84d6-75b9fbbb3419) for task 4 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000
I1221 23:10:10.489480 32593 slave.cpp:2937] Handling status update TASK_RUNNING (UUID: c8c2c7b7-8040-48e9-84d6-75b9fbbb3419) for task 4 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 from executor(1)@172.17.0.2:39520
I1221 23:10:10.489858 32595 status_update_manager.cpp:320] Received status update TASK_RUNNING (UUID: c8c2c7b7-8040-48e9-84d6-75b9fbbb3419) for task 4 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000
I1221 23:10:10.489914 32595 status_update_manager.cpp:497] Creating StatusUpdate stream for task 4 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000
I1221 23:10:10.490532 32595 status_update_manager.cpp:824] Checkpointing UPDATE for status update TASK_RUNNING (UUID: c8c2c7b7-8040-48e9-84d6-75b9fbbb3419) for task 4 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000
I1221 23:10:10.491937 32595 status_update_manager.cpp:374] Forwarding update TASK_RUNNING (UUID: c8c2c7b7-8040-48e9-84d6-75b9fbbb3419) for task 4 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 to the slave
I1221 23:10:10.492247 32597 slave.cpp:3289] Forwarding the update TASK_RUNNING (UUID: c8c2c7b7-8040-48e9-84d6-75b9fbbb3419) for task 4 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 to master@172.17.0.2:40874
I1221 23:10:10.492422 32597 slave.cpp:3183] Status update manager successfully handled status update TASK_RUNNING (UUID: c8c2c7b7-8040-48e9-84d6-75b9fbbb3419) for task 4 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000
I1221 23:10:10.492470 32597 slave.cpp:3199] Sending acknowledgement for status update TASK_RUNNING (UUID: c8c2c7b7-8040-48e9-84d6-75b9fbbb3419) for task 4 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 to executor(1)@172.17.0.2:39520
I1221 23:10:10.492741 32606 master.cpp:4687] Status update TASK_RUNNING (UUID: c8c2c7b7-8040-48e9-84d6-75b9fbbb3419) for task 4 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 from slave 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S1 at slave(1)@172.17.0.2:40874 (6ccf2ee56b13)
I1221 23:10:10.492871 32606 master.cpp:4735] Forwarding status update TASK_RUNNING (UUID: c8c2c7b7-8040-48e9-84d6-75b9fbbb3419) for task 4 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000
I1221 23:10:10.493093   372 exec.cpp:341] Executor received status update acknowledgement c8c2c7b7-8040-48e9-84d6-75b9fbbb3419 for task 4 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000
I1221 23:10:10.493377 32606 master.cpp:6347] Updating the state of task 4 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 (latest state: TASK_RUNNING, status update state: TASK_RUNNING)
I1221 23:10:10.493441 32595 no_executor_framework.cpp:160] Task '4' is in state TASK_RUNNING
I1221 23:10:10.493469 32595 sched.cpp:919] Scheduler::statusUpdate took 37984ns
I1221 23:10:10.493696 32595 master.cpp:3844] Processing ACKNOWLEDGE call c8c2c7b7-8040-48e9-84d6-75b9fbbb3419 for task 4 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 (No Executor Framework) at scheduler-7caeff0b-9ca0-4c4c-935b-843b0edf2650@172.17.0.2:40874 on slave 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S1
I1221 23:10:10.493988 32599 status_update_manager.cpp:392] Received status update acknowledgement (UUID: c8c2c7b7-8040-48e9-84d6-75b9fbbb3419) for task 4 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000
I1221 23:10:10.494120 32599 status_update_manager.cpp:824] Checkpointing ACK for status update TASK_RUNNING (UUID: c8c2c7b7-8040-48e9-84d6-75b9fbbb3419) for task 4 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000
I1221 23:10:10.495306 32598 slave.cpp:2348] Status update manager successfully handled status update acknowledgement (UUID: c8c2c7b7-8040-48e9-84d6-75b9fbbb3419) for task 4 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000
Command exited with status 0 (pid: 361)
I1221 23:10:10.543287   304 exec.cpp:517] Executor sending status update TASK_FINISHED (UUID: 133ecf74-5795-4890-a47d-02b397e9084c) for task 0 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000
I1221 23:10:10.544509 32593 slave.cpp:2937] Handling status update TASK_FINISHED (UUID: 133ecf74-5795-4890-a47d-02b397e9084c) for task 0 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 from executor(1)@172.17.0.2:60995
I1221 23:10:10.544711 32593 slave.cpp:5520] Terminating task 0
I1221 23:10:10.546165 32608 status_update_manager.cpp:320] Received status update TASK_FINISHED (UUID: 133ecf74-5795-4890-a47d-02b397e9084c) for task 0 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000
I1221 23:10:10.546311 32608 status_update_manager.cpp:824] Checkpointing UPDATE for status update TASK_FINISHED (UUID: 133ecf74-5795-4890-a47d-02b397e9084c) for task 0 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000
I1221 23:10:10.547626 32608 status_update_manager.cpp:374] Forwarding update TASK_FINISHED (UUID: 133ecf74-5795-4890-a47d-02b397e9084c) for task 0 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 to the slave
I1221 23:10:10.547925 32594 slave.cpp:3289] Forwarding the update TASK_FINISHED (UUID: 133ecf74-5795-4890-a47d-02b397e9084c) for task 0 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 to master@172.17.0.2:40874
I1221 23:10:10.548090 32594 slave.cpp:3183] Status update manager successfully handled status update TASK_FINISHED (UUID: 133ecf74-5795-4890-a47d-02b397e9084c) for task 0 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000
I1221 23:10:10.548177 32594 slave.cpp:3199] Sending acknowledgement for status update TASK_FINISHED (UUID: 133ecf74-5795-4890-a47d-02b397e9084c) for task 0 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 to executor(1)@172.17.0.2:60995
I1221 23:10:10.548307 32605 master.cpp:4687] Status update TASK_FINISHED (UUID: 133ecf74-5795-4890-a47d-02b397e9084c) for task 0 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 from slave 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S1 at slave(1)@172.17.0.2:40874 (6ccf2ee56b13)
I1221 23:10:10.548375 32605 master.cpp:4735] Forwarding status update TASK_FINISHED (UUID: 133ecf74-5795-4890-a47d-02b397e9084c) for task 0 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000
I1221 23:10:10.548635 32605 master.cpp:6347] Updating the state of task 0 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 (latest state: TASK_FINISHED, status update state: TASK_FINISHED)
I1221 23:10:10.548717 32608 no_executor_framework.cpp:160] Task '0' is in state TASK_FINISHED
I1221 23:10:10.548748 32608 sched.cpp:919] Scheduler::statusUpdate took 39141ns
I1221 23:10:10.549018   301 exec.cpp:341] Executor received status update acknowledgement 133ecf74-5795-4890-a47d-02b397e9084c for task 0 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000
I1221 23:10:10.549098 32605 master.cpp:3844] Processing ACKNOWLEDGE call 133ecf74-5795-4890-a47d-02b397e9084c for task 0 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 (No Executor Framework) at scheduler-7caeff0b-9ca0-4c4c-935b-843b0edf2650@172.17.0.2:40874 on slave 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S1
I1221 23:10:10.549120 32607 hierarchical.cpp:880] Recovered cpus(*):0.1; mem(*):32; disk(*):32 (total: cpus(*):2; mem(*):10240; disk(*):3.70122e+06; ports(*):[31000-32000], allocated: cpus(*):0.4; mem(*):128; disk(*):128) on slave 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S1 from framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000
I1221 23:10:10.549159 32605 master.cpp:6413] Removing task 0 with resources cpus(*):0.1; mem(*):32; disk(*):32 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 on slave 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S1 at slave(1)@172.17.0.2:40874 (6ccf2ee56b13)
I1221 23:10:10.549618 32593 status_update_manager.cpp:392] Received status update acknowledgement (UUID: 133ecf74-5795-4890-a47d-02b397e9084c) for task 0 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000
Command exited with status 0 (pid: 362)
Command exited with status 0 (pid: 363)
I1221 23:10:10.550003 32593 status_update_manager.cpp:824] Checkpointing ACK for status update TASK_FINISHED (UUID: 133ecf74-5795-4890-a47d-02b397e9084c) for task 0 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000
I1221 23:10:10.550715 32593 status_update_manager.cpp:528] Cleaning up status update stream for task 0 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000
I1221 23:10:10.551157 32599 slave.cpp:2348] Status update manager successfully handled status update acknowledgement (UUID: 133ecf74-5795-4890-a47d-02b397e9084c) for task 0 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000
I1221 23:10:10.551211 32599 slave.cpp:5561] Completing task 0
I1221 23:10:10.551658   341 exec.cpp:517] Executor sending status update TASK_FINISHED (UUID: aa45776d-1ace-433c-84f6-46f8b2f26744) for task 2 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000
I1221 23:10:10.551913   322 exec.cpp:517] Executor sending status update TASK_FINISHED (UUID: 0efe56fd-9140-48a7-8f5a-fb95b52d0ac2) for task 1 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000
I1221 23:10:10.552376 32607 slave.cpp:2937] Handling status update TASK_FINISHED (UUID: aa45776d-1ace-433c-84f6-46f8b2f26744) for task 2 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 from executor(1)@172.17.0.2:33894
I1221 23:10:10.552472 32607 slave.cpp:5520] Terminating task 2
I1221 23:10:10.552793 32607 slave.cpp:2937] Handling status update TASK_FINISHED (UUID: 0efe56fd-9140-48a7-8f5a-fb95b52d0ac2) for task 1 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 from executor(1)@172.17.0.2:51530
I1221 23:10:10.552904 32607 slave.cpp:5520] Terminating task 1
I1221 23:10:10.553864 32605 status_update_manager.cpp:320] Received status update TASK_FINISHED (UUID: aa45776d-1ace-433c-84f6-46f8b2f26744) for task 2 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000
I1221 23:10:10.553944 32605 status_update_manager.cpp:824] Checkpointing UPDATE for status update TASK_FINISHED (UUID: aa45776d-1ace-433c-84f6-46f8b2f26744) for task 2 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000
I1221 23:10:10.554796 32605 status_update_manager.cpp:374] Forwarding update TASK_FINISHED (UUID: aa45776d-1ace-433c-84f6-46f8b2f26744) for task 2 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 to the slave
I1221 23:10:10.555117 32594 slave.cpp:3289] Forwarding the update TASK_FINISHED (UUID: aa45776d-1ace-433c-84f6-46f8b2f26744) for task 2 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 to master@172.17.0.2:40874
I1221 23:10:10.555182 32605 status_update_manager.cpp:320] Received status update TASK_FINISHED (UUID: 0efe56fd-9140-48a7-8f5a-fb95b52d0ac2) for task 1 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000
I1221 23:10:10.555248 32605 status_update_manager.cpp:824] Checkpointing UPDATE for status update TASK_FINISHED (UUID: 0efe56fd-9140-48a7-8f5a-fb95b52d0ac2) for task 1 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000
I1221 23:10:10.555275 32594 slave.cpp:3183] Status update manager successfully handled status update TASK_FINISHED (UUID: aa45776d-1ace-433c-84f6-46f8b2f26744) for task 2 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000
I1221 23:10:10.555327 32594 slave.cpp:3199] Sending acknowledgement for status update TASK_FINISHED (UUID: aa45776d-1ace-433c-84f6-46f8b2f26744) for task 2 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 to executor(1)@172.17.0.2:33894
I1221 23:10:10.555445 32597 master.cpp:4687] Status update TASK_FINISHED (UUID: aa45776d-1ace-433c-84f6-46f8b2f26744) for task 2 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 from slave 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S1 at slave(1)@172.17.0.2:40874 (6ccf2ee56b13)
I1221 23:10:10.555490 32597 master.cpp:4735] Forwarding status update TASK_FINISHED (UUID: aa45776d-1ace-433c-84f6-46f8b2f26744) for task 2 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000
I1221 23:10:10.555722 32600 no_executor_framework.cpp:160] Task '2' is in state TASK_FINISHED
I1221 23:10:10.555730 32597 master.cpp:6347] Updating the state of task 2 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 (latest state: TASK_FINISHED, status update state: TASK_FINISHED)
I1221 23:10:10.555749 32600 sched.cpp:919] Scheduler::statusUpdate took 34947ns
I1221 23:10:10.555935   329 exec.cpp:341] Executor received status update acknowledgement aa45776d-1ace-433c-84f6-46f8b2f26744 for task 2 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000
I1221 23:10:10.556202 32605 status_update_manager.cpp:374] Forwarding update TASK_FINISHED (UUID: 0efe56fd-9140-48a7-8f5a-fb95b52d0ac2) for task 1 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 to the slave
I1221 23:10:10.556202 32597 master.cpp:3844] Processing ACKNOWLEDGE call aa45776d-1ace-433c-84f6-46f8b2f26744 for task 2 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 (No Executor Framework) at scheduler-7caeff0b-9ca0-4c4c-935b-843b0edf2650@172.17.0.2:40874 on slave 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S1
I1221 23:10:10.556238 32604 hierarchical.cpp:880] Recovered cpus(*):0.1; mem(*):32; disk(*):32 (total: cpus(*):2; mem(*):10240; disk(*):3.70122e+06; ports(*):[31000-32000], allocated: cpus(*):0.3; mem(*):96; disk(*):96) on slave 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S1 from framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000
I1221 23:10:10.556287 32597 master.cpp:6413] Removing task 2 with resources cpus(*):0.1; mem(*):32; disk(*):32 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 on slave 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S1 at slave(1)@172.17.0.2:40874 (6ccf2ee56b13)
I1221 23:10:10.556505 32600 slave.cpp:3289] Forwarding the update TASK_FINISHED (UUID: 0efe56fd-9140-48a7-8f5a-fb95b52d0ac2) for task 1 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 to master@172.17.0.2:40874
I1221 23:10:10.556773 32597 status_update_manager.cpp:392] Received status update acknowledgement (UUID: aa45776d-1ace-433c-84f6-46f8b2f26744) for task 2 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000
I1221 23:10:10.556782 32605 master.cpp:4687] Status update TASK_FINISHED (UUID: 0efe56fd-9140-48a7-8f5a-fb95b52d0ac2) for task 1 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 from slave 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S1 at slave(1)@172.17.0.2:40874 (6ccf2ee56b13)
I1221 23:10:10.556799 32600 slave.cpp:3183] Status update manager successfully handled status update TASK_FINISHED (UUID: 0efe56fd-9140-48a7-8f5a-fb95b52d0ac2) for task 1 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000
I1221 23:10:10.556861 32605 master.cpp:4735] Forwarding status update TASK_FINISHED (UUID: 0efe56fd-9140-48a7-8f5a-fb95b52d0ac2) for task 1 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000
I1221 23:10:10.556879 32600 slave.cpp:3199] Sending acknowledgement for status update TASK_FINISHED (UUID: 0efe56fd-9140-48a7-8f5a-fb95b52d0ac2) for task 1 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 to executor(1)@172.17.0.2:51530
I1221 23:10:10.556947 32597 status_update_manager.cpp:824] Checkpointing ACK for status update TASK_FINISHED (UUID: aa45776d-1ace-433c-84f6-46f8b2f26744) for task 2 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000
I1221 23:10:10.557054 32605 master.cpp:6347] Updating the state of task 1 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 (latest state: TASK_FINISHED, status update state: TASK_FINISHED)
I1221 23:10:10.557144 32607 no_executor_framework.cpp:160] Task '1' is in state TASK_FINISHED
I1221 23:10:10.557171 32607 sched.cpp:919] Scheduler::statusUpdate took 39728ns
I1221 23:10:10.557394 32605 master.cpp:3844] Processing ACKNOWLEDGE call 0efe56fd-9140-48a7-8f5a-fb95b52d0ac2 for task 1 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 (No Executor Framework) at scheduler-7caeff0b-9ca0-4c4c-935b-843b0edf2650@172.17.0.2:40874 on slave 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S1
I1221 23:10:10.557454 32605 master.cpp:6413] Removing task 1 with resources cpus(*):0.1; mem(*):32; disk(*):32 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 on slave 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S1 at slave(1)@172.17.0.2:40874 (6ccf2ee56b13)
I1221 23:10:10.557556 32597 status_update_manager.cpp:528] Cleaning up status update stream for task 2 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000
I1221 23:10:10.557639   311 exec.cpp:341] Executor received status update acknowledgement 0efe56fd-9140-48a7-8f5a-fb95b52d0ac2 for task 1 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000
I1221 23:10:10.557924 32597 status_update_manager.cpp:392] Received status update acknowledgement (UUID: 0efe56fd-9140-48a7-8f5a-fb95b52d0ac2) for task 1 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000
I1221 23:10:10.557929 32600 slave.cpp:2348] Status update manager successfully handled status update acknowledgement (UUID: aa45776d-1ace-433c-84f6-46f8b2f26744) for task 2 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000
I1221 23:10:10.557991 32600 slave.cpp:5561] Completing task 2
I1221 23:10:10.558094 32597 status_update_manager.cpp:824] Checkpointing ACK for status update TASK_FINISHED (UUID: 0efe56fd-9140-48a7-8f5a-fb95b52d0ac2) for task 1 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000
I1221 23:10:10.558291 32598 hierarchical.cpp:880] Recovered cpus(*):0.1; mem(*):32; disk(*):32 (total: cpus(*):2; mem(*):10240; disk(*):3.70122e+06; ports(*):[31000-32000], allocated: cpus(*):0.2; mem(*):64; disk(*):64) on slave 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S1 from framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000
I1221 23:10:10.558727 32597 status_update_manager.cpp:528] Cleaning up status update stream for task 1 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000
I1221 23:10:10.558970 32595 slave.cpp:2348] Status update manager successfully handled status update acknowledgement (UUID: 0efe56fd-9140-48a7-8f5a-fb95b52d0ac2) for task 1 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000
I1221 23:10:10.559031 32595 slave.cpp:5561] Completing task 1
Command exited with status 0 (pid: 381)
I1221 23:10:10.568011   358 exec.cpp:517] Executor sending status update TASK_FINISHED (UUID: 6ed3878a-a7ad-4eab-9df4-1e1ef7f0ca67) for task 3 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000
I1221 23:10:10.568733 32599 slave.cpp:2937] Handling status update TASK_FINISHED (UUID: 6ed3878a-a7ad-4eab-9df4-1e1ef7f0ca67) for task 3 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 from executor(1)@172.17.0.2:33285
I1221 23:10:10.568866 32599 slave.cpp:5520] Terminating task 3
I1221 23:10:10.570035 32594 status_update_manager.cpp:320] Received status update TASK_FINISHED (UUID: 6ed3878a-a7ad-4eab-9df4-1e1ef7f0ca67) for task 3 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000
I1221 23:10:10.570112 32594 status_update_manager.cpp:824] Checkpointing UPDATE for status update TASK_FINISHED (UUID: 6ed3878a-a7ad-4eab-9df4-1e1ef7f0ca67) for task 3 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000
I1221 23:10:10.571346 32594 status_update_manager.cpp:374] Forwarding update TASK_FINISHED (UUID: 6ed3878a-a7ad-4eab-9df4-1e1ef7f0ca67) for task 3 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 to the slave
I1221 23:10:10.571598 32601 slave.cpp:3289] Forwarding the update TASK_FINISHED (UUID: 6ed3878a-a7ad-4eab-9df4-1e1ef7f0ca67) for task 3 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 to master@172.17.0.2:40874
I1221 23:10:10.571753 32601 slave.cpp:3183] Status update manager successfully handled status update TASK_FINISHED (UUID: 6ed3878a-a7ad-4eab-9df4-1e1ef7f0ca67) for task 3 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000
I1221 23:10:10.571802 32601 slave.cpp:3199] Sending acknowledgement for status update TASK_FINISHED (UUID: 6ed3878a-a7ad-4eab-9df4-1e1ef7f0ca67) for task 3 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 to executor(1)@172.17.0.2:33285
I1221 23:10:10.572021 32594 master.cpp:4687] Status update TASK_FINISHED (UUID: 6ed3878a-a7ad-4eab-9df4-1e1ef7f0ca67) for task 3 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 from slave 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S1 at slave(1)@172.17.0.2:40874 (6ccf2ee56b13)
I1221 23:10:10.572069 32594 master.cpp:4735] Forwarding status update TASK_FINISHED (UUID: 6ed3878a-a7ad-4eab-9df4-1e1ef7f0ca67) for task 3 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000
I1221 23:10:10.572232 32594 master.cpp:6347] Updating the state of task 3 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 (latest state: TASK_FINISHED, status update state: TASK_FINISHED)
I1221 23:10:10.572475 32601 no_executor_framework.cpp:160] Task '3' is in state TASK_FINISHED
I1221 23:10:10.572506 32601 sched.cpp:919] Scheduler::statusUpdate took 33954ns
I1221 23:10:10.572589   347 exec.cpp:341] Executor received status update acknowledgement 6ed3878a-a7ad-4eab-9df4-1e1ef7f0ca67 for task 3 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000
I1221 23:10:10.572710 32601 master.cpp:3844] Processing ACKNOWLEDGE call 6ed3878a-a7ad-4eab-9df4-1e1ef7f0ca67 for task 3 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 (No Executor Framework) at scheduler-7caeff0b-9ca0-4c4c-935b-843b0edf2650@172.17.0.2:40874 on slave 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S1
I1221 23:10:10.572775 32594 hierarchical.cpp:880] Recovered cpus(*):0.1; mem(*):32; disk(*):32 (total: cpus(*):2; mem(*):10240; disk(*):3.70122e+06; ports(*):[31000-32000], allocated: cpus(*):0.1; mem(*):32; disk(*):32) on slave 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S1 from framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000
I1221 23:10:10.572775 32601 master.cpp:6413] Removing task 3 with resources cpus(*):0.1; mem(*):32; disk(*):32 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 on slave 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S1 at slave(1)@172.17.0.2:40874 (6ccf2ee56b13)
I1221 23:10:10.573259 32608 status_update_manager.cpp:392] Received status update acknowledgement (UUID: 6ed3878a-a7ad-4eab-9df4-1e1ef7f0ca67) for task 3 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000
I1221 23:10:10.573410 32608 status_update_manager.cpp:824] Checkpointing ACK for status update TASK_FINISHED (UUID: 6ed3878a-a7ad-4eab-9df4-1e1ef7f0ca67) for task 3 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000
I1221 23:10:10.574010 32608 status_update_manager.cpp:528] Cleaning up status update stream for task 3 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000
I1221 23:10:10.574340 32606 slave.cpp:2348] Status update manager successfully handled status update acknowledgement (UUID: 6ed3878a-a7ad-4eab-9df4-1e1ef7f0ca67) for task 3 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000
I1221 23:10:10.574443 32606 slave.cpp:5561] Completing task 3
Command exited with status 0 (pid: 382)
I1221 23:10:10.589720   368 exec.cpp:517] Executor sending status update TASK_FINISHED (UUID: 303c6b60-9df4-405f-b62c-a1b11dc95d7a) for task 4 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000
I1221 23:10:10.590697 32595 slave.cpp:2937] Handling status update TASK_FINISHED (UUID: 303c6b60-9df4-405f-b62c-a1b11dc95d7a) for task 4 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 from executor(1)@172.17.0.2:39520
I1221 23:10:10.590865 32595 slave.cpp:5520] Terminating task 4
I1221 23:10:10.592167 32596 status_update_manager.cpp:320] Received status update TASK_FINISHED (UUID: 303c6b60-9df4-405f-b62c-a1b11dc95d7a) for task 4 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000
I1221 23:10:10.592253 32596 status_update_manager.cpp:824] Checkpointing UPDATE for status update TASK_FINISHED (UUID: 303c6b60-9df4-405f-b62c-a1b11dc95d7a) for task 4 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000
I1221 23:10:10.593219 32596 status_update_manager.cpp:374] Forwarding update TASK_FINISHED (UUID: 303c6b60-9df4-405f-b62c-a1b11dc95d7a) for task 4 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 to the slave
I1221 23:10:10.593444 32593 slave.cpp:3289] Forwarding the update TASK_FINISHED (UUID: 303c6b60-9df4-405f-b62c-a1b11dc95d7a) for task 4 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 to master@172.17.0.2:40874
I1221 23:10:10.593598 32593 slave.cpp:3183] Status update manager successfully handled status update TASK_FINISHED (UUID: 303c6b60-9df4-405f-b62c-a1b11dc95d7a) for task 4 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000
I1221 23:10:10.593649 32593 slave.cpp:3199] Sending acknowledgement for status update TASK_FINISHED (UUID: 303c6b60-9df4-405f-b62c-a1b11dc95d7a) for task 4 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 to executor(1)@172.17.0.2:39520
I1221 23:10:10.593793 32606 master.cpp:4687] Status update TASK_FINISHED (UUID: 303c6b60-9df4-405f-b62c-a1b11dc95d7a) for task 4 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 from slave 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S1 at slave(1)@172.17.0.2:40874 (6ccf2ee56b13)
I1221 23:10:10.593868 32606 master.cpp:4735] Forwarding status update TASK_FINISHED (UUID: 303c6b60-9df4-405f-b62c-a1b11dc95d7a) for task 4 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000
I1221 23:10:10.594050 32606 master.cpp:6347] Updating the state of task 4 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 (latest state: TASK_FINISHED, status update state: TASK_FINISHED)
I1221 23:10:10.594197 32607 no_executor_framework.cpp:160] Task '4' is in state TASK_FINISHED
I1221 23:10:10.594233 32607 sched.cpp:1803] Asked to stop the driver
I1221 23:10:10.594293 32607 sched.cpp:919] Scheduler::statusUpdate took 138437ns
I1221 23:10:10.594322 32607 sched.cpp:926] Not sending status update acknowledgment message because the driver is not running!
I1221 23:10:10.594398 32607 sched.cpp:1041] Stopping framework '3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000'
I1221 23:10:10.594574 32594 hierarchical.cpp:880] Recovered cpus(*):0.1; mem(*):32; disk(*):32 (total: cpus(*):2; mem(*):10240; disk(*):3.70122e+06; ports(*):[31000-32000], allocated: cpus(*):4.71845e-16) on slave 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S1 from framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000
I1221 23:10:10.594632 32601 master.cpp:5823] Processing TEARDOWN call for framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 (No Executor Framework) at scheduler-7caeff0b-9ca0-4c4c-935b-843b0edf2650@172.17.0.2:40874
I1221 23:10:10.594660 32601 master.cpp:5835] Removing framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 (No Executor Framework) at scheduler-7caeff0b-9ca0-4c4c-935b-843b0edf2650@172.17.0.2:40874
I1221 23:10:10.594743 32569 sched.cpp:1803] Asked to stop the driver
I1221 23:10:10.594738   378 exec.cpp:341] Executor received status update acknowledgement 303c6b60-9df4-405f-b62c-a1b11dc95d7a for task 4 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000
I1221 23:10:10.594775 32569 sched.cpp:1806] Ignoring stop because the status of the driver is DRIVER_STOPPED
I1221 23:10:10.594985 32603 hierarchical.cpp:366] Deactivated framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000
I1221 23:10:10.595082 32606 slave.cpp:2012] Asked to shut down framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 by master@172.17.0.2:40874
I1221 23:10:10.595089 32602 slave.cpp:2012] Asked to shut down framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 by master@172.17.0.2:40874
I1221 23:10:10.595098 32597 slave.cpp:2012] Asked to shut down framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 by master@172.17.0.2:40874
I1221 23:10:10.595124 32602 slave.cpp:2037] Shutting down framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000
W1221 23:10:10.595314 32597 slave.cpp:2027] Cannot shut down unknown framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000
I1221 23:10:10.595219 32601 master.cpp:6347] Updating the state of task 4 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 (latest state: TASK_FINISHED, status update state: TASK_KILLED)
W1221 23:10:10.595125 32606 slave.cpp:2027] Cannot shut down unknown framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000
I1221 23:10:10.595356 32601 master.cpp:6413] Removing task 4 with resources cpus(*):0.1; mem(*):32; disk(*):32 of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 on slave 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S1 at slave(1)@172.17.0.2:40874 (6ccf2ee56b13)
I1221 23:10:10.595501 32602 slave.cpp:4060] Shutting down executor '0' of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 at executor(1)@172.17.0.2:60995
I1221 23:10:10.595871 32602 slave.cpp:4060] Shutting down executor '1' of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 at executor(1)@172.17.0.2:51530
I1221 23:10:10.596016 32601 master.cpp:930] Master terminating
I1221 23:10:10.596148 32602 slave.cpp:4060] Shutting down executor '2' of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 at executor(1)@172.17.0.2:33894
I1221 23:10:10.596225 32766 exec.cpp:381] Executor asked to shutdown
I1221 23:10:10.596321 32602 slave.cpp:4060] Shutting down executor '3' of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 at executor(1)@172.17.0.2:33285
I1221 23:10:10.596387   316 exec.cpp:381] Executor asked to shutdown
I1221 23:10:10.596487 32762 exec.cpp:80] Scheduling shutdown of the executor
I1221 23:10:10.596493 32602 slave.cpp:4060] Shutting down executor '4' of framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000 at executor(1)@172.17.0.2:39520
I1221 23:10:10.596551   316 exec.cpp:396] Executor::shutdown took 16507ns
I1221 23:10:10.596624 32766 exec.cpp:396] Executor::shutdown took 217407ns
I1221 23:10:10.596698   325 exec.cpp:80] Scheduling shutdown of the executor
I1221 23:10:10.597014   356 exec.cpp:381] Executor asked to shutdown
I1221 23:10:10.597086 32595 hierarchical.cpp:321] Removed framework 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-0000
I1221 23:10:10.597214   356 exec.cpp:396] Executor::shutdown took 12938ns
I1221 23:10:10.597326   356 exec.cpp:80] Scheduling shutdown of the executor
I1221 23:10:10.597384   333 exec.cpp:381] Executor asked to shutdown
I1221 23:10:10.597462 32595 hierarchical.cpp:496] Removed slave 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S2
I1221 23:10:10.597538   333 exec.cpp:396] Executor::shutdown took 16568ns
I1221 23:10:10.597635   332 exec.cpp:80] Scheduling shutdown of the executor
I1221 23:10:10.597754 32595 hierarchical.cpp:496] Removed slave 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S1
I1221 23:10:10.598021 32603 slave.cpp:3417] master@172.17.0.2:40874 exited
I1221 23:10:10.598033   374 exec.cpp:381] Executor asked to shutdown
I1221 23:10:10.598038 32595 hierarchical.cpp:496] Removed slave 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S0
I1221 23:10:10.598048 32597 slave.cpp:3417] master@172.17.0.2:40874 exited
W1221 23:10:10.598047 32603 slave.cpp:3420] Master disconnected! Waiting for a new master to be elected
W1221 23:10:10.598078 32597 slave.cpp:3420] Master disconnected! Waiting for a new master to be elected
I1221 23:10:10.598156   374 exec.cpp:396] Executor::shutdown took 14264ns
I1221 23:10:10.598224 32605 slave.cpp:3417] master@172.17.0.2:40874 exited
W1221 23:10:10.598253 32605 slave.cpp:3420] Master disconnected! Waiting for a new master to be elected
I1221 23:10:10.598274   374 exec.cpp:80] Scheduling shutdown of the executor
I1221 23:10:10.600291 32593 slave.cpp:601] Slave terminating
I1221 23:10:10.603379 32569 slave.cpp:601] Slave terminating
I1221 23:10:10.605034 32604 slave.cpp:601] Slave terminating
[       OK ] ExamplesTest.NoExecutorFramework (7895 ms)
{noformat}

{noformat: title=Bad Run}
[ RUN      ] ExamplesTest.NoExecutorFramework
WARNING: Logging before InitGoogleLogging() is written to STDERR
I1221 17:48:57.199091 32567 resources.cpp:478] Parsing resources as JSON failed: cpus:0.1;mem:32;disk:32
Trying semicolon-delimited string format instead
I1221 17:48:57.200100 32567 logging.cpp:172] Logging to STDERR
I1221 17:48:57.207792 32567 process.cpp:998] libprocess is initialized on 172.17.0.2:34597 for 16 cpus
I1221 17:48:57.235429 32567 leveldb.cpp:174] Opened db in 3.93451ms
I1221 17:48:57.236927 32567 leveldb.cpp:181] Compacted db in 1.417647ms
I1221 17:48:57.237071 32567 leveldb.cpp:196] Created db iterator in 92710ns
I1221 17:48:57.237102 32567 leveldb.cpp:202] Seeked to beginning of db in 6258ns
I1221 17:48:57.237119 32567 leveldb.cpp:271] Iterated through 0 keys in the db in 473ns
I1221 17:48:57.237413 32567 replica.cpp:779] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned
I1221 17:48:57.240393 32594 recover.cpp:447] Starting replica recovery
I1221 17:48:57.241396 32594 recover.cpp:473] Replica is in EMPTY status
I1221 17:48:57.242455 32567 local.cpp:239] Using 'local' authorizer
I1221 17:48:57.245549 32600 replica.cpp:673] Replica in EMPTY status received a broadcasted recover request from (4)@172.17.0.2:34597
I1221 17:48:57.246796 32592 recover.cpp:193] Received a recover response from a replica in EMPTY status
I1221 17:48:57.247524 32595 master.cpp:365] Master f9d7159c-0e99-4adc-8090-abc4b60b98f5 (be9bd3a71092) started on 172.17.0.2:34597
I1221 17:48:57.247854 32601 recover.cpp:564] Updating replica status to STARTING
I1221 17:48:57.247812 32595 master.cpp:367] Flags at startup: --acls=""permissive: false
register_frameworks {
  principals {
    type: SOME
    values: ""test-principal""
  }
  roles {
    type: SOME
    values: ""*""
  }
}
run_tasks {
  principals {
    type: SOME
    values: ""test-principal""
  }
  users {
    type: SOME
    values: ""mesos""
  }
}
"" --allocation_interval=""1secs"" --allocator=""HierarchicalDRF"" --authenticate=""true"" --authenticate_slaves=""false"" --authenticators=""crammd5"" --authorizers=""local"" --credentials=""/tmp/ExamplesTest_NoExecutorFramework_2wDEhF/credentials"" --framework_sorter=""drf"" --help=""true"" --hostname_lookup=""true"" --initialize_driver_logging=""true"" --log_auto_initialize=""true"" --logbufsecs=""0"" --logging_level=""INFO"" --max_slave_ping_timeouts=""5"" --quiet=""false"" --recovery_slave_removal_limit=""100%"" --registry=""replicated_log"" --registry_fetch_timeout=""1mins"" --registry_store_timeout=""5secs"" --registry_strict=""false"" --root_submissions=""true"" --slave_ping_timeout=""15secs"" --slave_reregister_timeout=""10mins"" --user_sorter=""drf"" --version=""false"" --webui_dir=""/mesos/mesos-0.27.0/src/webui"" --work_dir=""/tmp/mesos-Alzbfe"" --zk_session_timeout=""10secs""
I1221 17:48:57.248754 32595 master.cpp:412] Master only allowing authenticated frameworks to register
I1221 17:48:57.248770 32595 master.cpp:419] Master allowing unauthenticated slaves to register
I1221 17:48:57.248783 32595 credentials.hpp:35] Loading credentials for authentication from '/tmp/ExamplesTest_NoExecutorFramework_2wDEhF/credentials'
W1221 17:48:57.248898 32595 credentials.hpp:50] Permissions on credentials file '/tmp/ExamplesTest_NoExecutorFramework_2wDEhF/credentials' are too open. It is recommended that your credentials file is NOT accessible by others.
I1221 17:48:57.249059 32592 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 890391ns
I1221 17:48:57.249110 32595 master.cpp:456] Using default 'crammd5' authenticator
I1221 17:48:57.249126 32592 replica.cpp:320] Persisted replica status to STARTING
I1221 17:48:57.249439 32595 authenticator.cpp:518] Initializing server SASL
I1221 17:48:57.249902 32592 recover.cpp:473] Replica is in STARTING status
I1221 17:48:57.251363 32600 replica.cpp:673] Replica in STARTING status received a broadcasted recover request from (5)@172.17.0.2:34597
I1221 17:48:57.251474 32595 auxprop.cpp:71] Initialized in-memory auxiliary property plugin
I1221 17:48:57.251765 32595 master.cpp:493] Authorization enabled
I1221 17:48:57.251787 32592 recover.cpp:193] Received a recover response from a replica in STARTING status
I1221 17:48:57.252167 32567 containerizer.cpp:141] Using isolation: filesystem/posix,posix/cpu,posix/mem
I1221 17:48:57.252354 32592 recover.cpp:564] Updating replica status to VOTING
I1221 17:48:57.252828 32596 hierarchical.cpp:147] Initialized hierarchical allocator process
I1221 17:48:57.253104 32597 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 602231ns
I1221 17:48:57.253137 32597 replica.cpp:320] Persisted replica status to VOTING
W1221 17:48:57.253180 32567 backend.cpp:48] Failed to create 'bind' backend: BindBackend requires root privileges
I1221 17:48:57.253304 32597 whitelist_watcher.cpp:77] No whitelist given
I1221 17:48:57.253336 32597 recover.cpp:578] Successfully joined the Paxos group
I1221 17:48:57.253697 32597 recover.cpp:462] Recover process terminated
I1221 17:48:57.268092 32595 master.cpp:1629] The newly elected leader is master@172.17.0.2:34597 with id f9d7159c-0e99-4adc-8090-abc4b60b98f5
I1221 17:48:57.268255 32595 master.cpp:1642] Elected as the leading master!
I1221 17:48:57.268411 32595 master.cpp:1387] Recovering from registrar
I1221 17:48:57.268937 32603 registrar.cpp:307] Recovering registrar
I1221 17:48:57.271054 32600 log.cpp:659] Attempting to start the writer
I1221 17:48:57.271824 32595 slave.cpp:191] Slave started on 1)@172.17.0.2:34597
I1221 17:48:57.271908 32595 slave.cpp:192] Flags at startup: --appc_store_dir=""/tmp/mesos/store/appc"" --authenticatee=""crammd5"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_auth_server=""auth.docker.io"" --docker_auth_server_port=""443"" --docker_kill_orphans=""true"" --docker_local_archives_dir=""/tmp/mesos/images/docker"" --docker_puller=""local"" --docker_puller_timeout=""60"" --docker_registry=""registry-1.docker.io"" --docker_registry_port=""443"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/tmp/mesos/store/docker"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/tmp/mesos/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --hostname_lookup=""true"" --image_provisioner_backend=""copy"" --initialize_driver_logging=""true"" --isolation=""filesystem/posix,posix/cpu,posix/mem"" --launcher=""posix"" --launcher_dir=""/mesos/mesos-0.27.0/_build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""1secs"" --resources=""cpus:2;mem:10240"" --revocable_cpu_low_priority=""true"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --systemd_runtime_directory=""/run/systemd/system"" --version=""false"" --work_dir=""/tmp/mesos-Alzbfe/0""
I1221 17:48:57.273372 32595 resources.cpp:478] Parsing resources as JSON failed: cpus:2;mem:10240
Trying semicolon-delimited string format instead
I1221 17:48:57.274093 32567 containerizer.cpp:141] Using isolation: filesystem/posix,posix/cpu,posix/mem
W1221 17:48:57.275342 32567 backend.cpp:48] Failed to create 'bind' backend: BindBackend requires root privileges
I1221 17:48:57.275346 32595 slave.cpp:392] Slave resources: cpus(*):2; mem(*):10240; disk(*):3.70122e+06; ports(*):[31000-32000]
I1221 17:48:57.275511 32595 slave.cpp:400] Slave attributes: [  ]
I1221 17:48:57.275542 32595 slave.cpp:405] Slave hostname: be9bd3a71092
I1221 17:48:57.275553 32595 slave.cpp:410] Slave checkpoint: true
I1221 17:48:57.276523 32594 replica.cpp:493] Replica received implicit promise request from (19)@172.17.0.2:34597 with proposal 1
I1221 17:48:57.277181 32594 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 616640ns
I1221 17:48:57.277209 32594 replica.cpp:342] Persisted promised to 1
I1221 17:48:57.279067 32604 state.cpp:58] Recovering state from '/tmp/mesos-Alzbfe/0/meta'
I1221 17:48:57.279417 32604 slave.cpp:191] Slave started on 2)@172.17.0.2:34597
I1221 17:48:57.279439 32604 slave.cpp:192] Flags at startup: --appc_store_dir=""/tmp/mesos/store/appc"" --authenticatee=""crammd5"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_auth_server=""auth.docker.io"" --docker_auth_server_port=""443"" --docker_kill_orphans=""true"" --docker_local_archives_dir=""/tmp/mesos/images/docker"" --docker_puller=""local"" --docker_puller_timeout=""60"" --docker_registry=""registry-1.docker.io"" --docker_registry_port=""443"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/tmp/mesos/store/docker"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/tmp/mesos/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --hostname_lookup=""true"" --image_provisioner_backend=""copy"" --initialize_driver_logging=""true"" --isolation=""filesystem/posix,posix/cpu,posix/mem"" --launcher=""posix"" --launcher_dir=""/mesos/mesos-0.27.0/_build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""1secs"" --resources=""cpus:2;mem:10240"" --revocable_cpu_low_priority=""true"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --systemd_runtime_directory=""/run/systemd/system"" --version=""false"" --work_dir=""/tmp/mesos-Alzbfe/1""
I1221 17:48:57.280268 32604 resources.cpp:478] Parsing resources as JSON failed: cpus:2;mem:10240
Trying semicolon-delimited string format instead
I1221 17:48:57.280439 32601 status_update_manager.cpp:200] Recovering status update manager
I1221 17:48:57.280815 32603 containerizer.cpp:383] Recovering containerizer
I1221 17:48:57.280939 32604 slave.cpp:392] Slave resources: cpus(*):2; mem(*):10240; disk(*):3.70122e+06; ports(*):[31000-32000]
I1221 17:48:57.281018 32604 slave.cpp:400] Slave attributes: [  ]
I1221 17:48:57.281033 32604 slave.cpp:405] Slave hostname: be9bd3a71092
I1221 17:48:57.281041 32604 slave.cpp:410] Slave checkpoint: true
I1221 17:48:57.281885 32567 containerizer.cpp:141] Using isolation: filesystem/posix,posix/cpu,posix/mem
I1221 17:48:57.282161 32597 state.cpp:58] Recovering state from '/tmp/mesos-Alzbfe/1/meta'
W1221 17:48:57.282383 32567 backend.cpp:48] Failed to create 'bind' backend: BindBackend requires root privileges
I1221 17:48:57.282456 32604 status_update_manager.cpp:200] Recovering status update manager
I1221 17:48:57.282768 32604 containerizer.cpp:383] Recovering containerizer
I1221 17:48:57.284574 32596 slave.cpp:4427] Finished recovery
I1221 17:48:57.284680 32594 coordinator.cpp:238] Coordinator attempting to fill missing positions
I1221 17:48:57.284574 32602 slave.cpp:4427] Finished recovery
I1221 17:48:57.286054 32602 slave.cpp:4599] Querying resource estimator for oversubscribable resources
I1221 17:48:57.286500 32596 slave.cpp:4599] Querying resource estimator for oversubscribable resources
I1221 17:48:57.287051 32596 slave.cpp:729] New master detected at master@172.17.0.2:34597
I1221 17:48:57.287382 32596 slave.cpp:754] No credentials provided. Attempting to register without authentication
I1221 17:48:57.288478 32596 slave.cpp:765] Detecting new master
I1221 17:48:57.288810 32596 slave.cpp:4613] Received oversubscribable resources  from the resource estimator
I1221 17:48:57.289193 32604 replica.cpp:388] Replica received explicit promise request from (37)@172.17.0.2:34597 for position 0 with proposal 2
I1221 17:48:57.289732 32602 slave.cpp:729] New master detected at master@172.17.0.2:34597
I1221 17:48:57.289877 32602 slave.cpp:754] No credentials provided. Attempting to register without authentication
I1221 17:48:57.290024 32602 slave.cpp:765] Detecting new master
I1221 17:48:57.287452 32601 status_update_manager.cpp:174] Pausing sending status updates
I1221 17:48:57.290153 32604 leveldb.cpp:341] Persisting action (8 bytes) to leveldb took 870482ns
I1221 17:48:57.290181 32604 replica.cpp:712] Persisted action at 0
I1221 17:48:57.290182 32601 status_update_manager.cpp:174] Pausing sending status updates
I1221 17:48:57.290633 32602 slave.cpp:4613] Received oversubscribable resources  from the resource estimator
I1221 17:48:57.290832 32601 slave.cpp:191] Slave started on 3)@172.17.0.2:34597
I1221 17:48:57.290853 32601 slave.cpp:192] Flags at startup: --appc_store_dir=""/tmp/mesos/store/appc"" --authenticatee=""crammd5"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_auth_server=""auth.docker.io"" --docker_auth_server_port=""443"" --docker_kill_orphans=""true"" --docker_local_archives_dir=""/tmp/mesos/images/docker"" --docker_puller=""local"" --docker_puller_timeout=""60"" --docker_registry=""registry-1.docker.io"" --docker_registry_port=""443"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/tmp/mesos/store/docker"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/tmp/mesos/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""true"" --hostname_lookup=""true"" --image_provisioner_backend=""copy"" --initialize_driver_logging=""true"" --isolation=""filesystem/posix,posix/cpu,posix/mem"" --launcher=""posix"" --launcher_dir=""/mesos/mesos-0.27.0/_build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""1secs"" --resources=""cpus:2;mem:10240"" --revocable_cpu_low_priority=""true"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --systemd_runtime_directory=""/run/systemd/system"" --version=""false"" --work_dir=""/tmp/mesos-Alzbfe/2""
I1221 17:48:57.291916 32601 resources.cpp:478] Parsing resources as JSON failed: cpus:2;mem:10240
Trying semicolon-delimited string format instead
I1221 17:48:57.293606 32592 replica.cpp:537] Replica received write request for position 0 from (40)@172.17.0.2:34597
I1221 17:48:57.293800 32592 leveldb.cpp:436] Reading position from leveldb took 85274ns
I1221 17:48:57.294116 32601 slave.cpp:392] Slave resources: cpus(*):2; mem(*):10240; disk(*):3.70122e+06; ports(*):[31000-32000]
I1221 17:48:57.294201 32601 slave.cpp:400] Slave attributes: [  ]
I1221 17:48:57.294216 32601 slave.cpp:405] Slave hostname: be9bd3a71092
I1221 17:48:57.294226 32601 slave.cpp:410] Slave checkpoint: true
I1221 17:48:57.294476 32592 leveldb.cpp:341] Persisting action (14 bytes) to leveldb took 574132ns
I1221 17:48:57.294507 32592 replica.cpp:712] Persisted action at 0
I1221 17:48:57.292084 32567 sched.cpp:164] Version: 0.27.0
I1221 17:48:57.295533 32593 state.cpp:58] Recovering state from '/tmp/mesos-Alzbfe/2/meta'
I1221 17:48:57.295897 32593 status_update_manager.cpp:200] Recovering status update manager
I1221 17:48:57.296118 32593 containerizer.cpp:383] Recovering containerizer
I1221 17:48:57.296663 32601 sched.cpp:262] New master detected at master@172.17.0.2:34597
I1221 17:48:57.296844 32601 sched.cpp:318] Authenticating with master master@172.17.0.2:34597
I1221 17:48:57.297032 32601 sched.cpp:325] Using default CRAM-MD5 authenticatee
I1221 17:48:57.297277 32606 replica.cpp:691] Replica received learned notice for position 0 from @0.0.0.0:0
I1221 17:48:57.297690 32591 slave.cpp:4427] Finished recovery
I1221 17:48:57.298143 32604 authenticatee.cpp:97] Initializing client SASL
I1221 17:48:57.298321 32591 slave.cpp:4599] Querying resource estimator for oversubscribable resources
I1221 17:48:57.298390 32604 authenticatee.cpp:121] Creating new client SASL connection
I1221 17:48:57.299451 32606 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 2.136052ms
I1221 17:48:57.299499 32606 replica.cpp:712] Persisted action at 0
I1221 17:48:57.299559 32593 slave.cpp:729] New master detected at master@172.17.0.2:34597
I1221 17:48:57.299649 32593 slave.cpp:754] No credentials provided. Attempting to register without authentication
I1221 17:48:57.299749 32593 slave.cpp:765] Detecting new master
I1221 17:48:57.300058 32591 status_update_manager.cpp:174] Pausing sending status updates
I1221 17:48:57.300135 32591 slave.cpp:4613] Received oversubscribable resources  from the resource estimator
I1221 17:48:57.299549 32606 replica.cpp:697] Replica learned NOP action at position 0
I1221 17:48:57.300899 32594 log.cpp:675] Writer started with ending position 0
I1221 17:48:57.301214 32593 master.cpp:1214] Dropping 'mesos.internal.AuthenticateMessage' message since not recovered yet
I1221 17:48:57.304466 32593 leveldb.cpp:436] Reading position from leveldb took 81139ns
I1221 17:48:57.310492 32593 registrar.cpp:340] Successfully fetched the registry (0B) in 41.458944ms
I1221 17:48:57.311039 32593 registrar.cpp:439] Applied 1 operations in 129600ns; attempting to update the 'registry'
I1221 17:48:57.313105 32593 log.cpp:683] Attempting to append 170 bytes to the log
I1221 17:48:57.313442 32596 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 1
I1221 17:48:57.314694 32596 replica.cpp:537] Replica received write request for position 1 from (42)@172.17.0.2:34597
I1221 17:48:57.315327 32596 leveldb.cpp:341] Persisting action (189 bytes) to leveldb took 577099ns
I1221 17:48:57.315357 32596 replica.cpp:712] Persisted action at 1
I1221 17:48:57.316313 32600 replica.cpp:691] Replica received learned notice for position 1 from @0.0.0.0:0
I1221 17:48:57.316994 32600 leveldb.cpp:341] Persisting action (191 bytes) to leveldb took 480415ns
I1221 17:48:57.317123 32600 replica.cpp:712] Persisted action at 1
I1221 17:48:57.317282 32600 replica.cpp:697] Replica learned APPEND action at position 1
I1221 17:48:57.320767 32592 registrar.cpp:484] Successfully updated the 'registry' in 9.52704ms
I1221 17:48:57.321055 32592 registrar.cpp:370] Successfully recovered registrar
I1221 17:48:57.321439 32600 log.cpp:702] Attempting to truncate the log to 1
I1221 17:48:57.321874 32592 master.cpp:1439] Recovered 0 slaves from the Registry (131B) ; allowing 10mins for slaves to re-register
I1221 17:48:57.322048 32592 hierarchical.cpp:165] Skipping recovery of hierarchical allocator: nothing to recover
I1221 17:48:57.322037 32600 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 2
I1221 17:48:57.323606 32604 replica.cpp:537] Replica received write request for position 2 from (43)@172.17.0.2:34597
I1221 17:48:57.324168 32604 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 509037ns
I1221 17:48:57.324198 32604 replica.cpp:712] Persisted action at 2
I1221 17:48:57.325112 32594 replica.cpp:691] Replica received learned notice for position 2 from @0.0.0.0:0
I1221 17:48:57.325742 32594 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 593646ns
I1221 17:48:57.325814 32594 leveldb.cpp:399] Deleting ~1 keys from leveldb took 41880ns
I1221 17:48:57.325839 32594 replica.cpp:712] Persisted action at 2
I1221 17:48:57.325876 32594 replica.cpp:697] Replica learned TRUNCATE action at position 2
I1221 17:48:57.350574 32606 slave.cpp:1254] Will retry registration in 1.696000194secs if necessary
I1221 17:48:57.351130 32606 master.cpp:4132] Registering slave at slave(3)@172.17.0.2:34597 (be9bd3a71092) with id f9d7159c-0e99-4adc-8090-abc4b60b98f5-S0
I1221 17:48:57.352444 32606 registrar.cpp:439] Applied 1 operations in 180199ns; attempting to update the 'registry'
I1221 17:48:57.356662 32591 log.cpp:683] Attempting to append 339 bytes to the log
I1221 17:48:57.356816 32606 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 3
I1221 17:48:57.358114 32605 replica.cpp:537] Replica received write request for position 3 from (44)@172.17.0.2:34597
I1221 17:48:57.358846 32605 leveldb.cpp:341] Persisting action (358 bytes) to leveldb took 652183ns
I1221 17:48:57.358881 32605 replica.cpp:712] Persisted action at 3
I1221 17:48:57.361673 32596 replica.cpp:691] Replica received learned notice for position 3 from @0.0.0.0:0
I1221 17:48:57.362452 32596 leveldb.cpp:341] Persisting action (360 bytes) to leveldb took 713269ns
I1221 17:48:57.362488 32596 replica.cpp:712] Persisted action at 3
I1221 17:48:57.362535 32596 replica.cpp:697] Replica learned APPEND action at position 3
I1221 17:48:57.364503 32598 registrar.cpp:484] Successfully updated the 'registry' in 11.939072ms
I1221 17:48:57.364987 32599 log.cpp:702] Attempting to truncate the log to 3
I1221 17:48:57.365222 32599 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 4
I1221 17:48:57.366511 32592 replica.cpp:537] Replica received write request for position 4 from (45)@172.17.0.2:34597
I1221 17:48:57.366540 32598 master.cpp:4200] Registered slave f9d7159c-0e99-4adc-8090-abc4b60b98f5-S0 at slave(3)@172.17.0.2:34597 (be9bd3a71092) with cpus(*):2; mem(*):10240; disk(*):3.70122e+06; ports(*):[31000-32000]
I1221 17:48:57.367009 32602 hierarchical.cpp:465] Added slave f9d7159c-0e99-4adc-8090-abc4b60b98f5-S0 (be9bd3a71092) with cpus(*):2; mem(*):10240; disk(*):3.70122e+06; ports(*):[31000-32000] (allocated: )
I1221 17:48:57.367451 32592 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 859317ns
I1221 17:48:57.367491 32592 replica.cpp:712] Persisted action at 4
I1221 17:48:57.367493 32602 hierarchical.cpp:1329] No resources available to allocate!
I1221 17:48:57.367559 32602 hierarchical.cpp:1101] Performed allocation for slave f9d7159c-0e99-4adc-8090-abc4b60b98f5-S0 in 495028ns
I1221 17:48:57.367691 32605 slave.cpp:3371] Received ping from slave-observer(1)@172.17.0.2:34597
I1221 17:48:57.368522 32605 slave.cpp:904] Registered with master master@172.17.0.2:34597; given slave ID f9d7159c-0e99-4adc-8090-abc4b60b98f5-S0
I1221 17:48:57.368553 32605 fetcher.cpp:81] Clearing fetcher cache
I1221 17:48:57.368744 32601 status_update_manager.cpp:181] Resuming sending status updates
I1221 17:48:57.369035 32605 slave.cpp:927] Checkpointing SlaveInfo to '/tmp/mesos-Alzbfe/2/meta/slaves/f9d7159c-0e99-4adc-8090-abc4b60b98f5-S0/slave.info'
I1221 17:48:57.369297 32602 replica.cpp:691] Replica received learned notice for position 4 from @0.0.0.0:0
I1221 17:48:57.369947 32602 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 615887ns
I1221 17:48:57.370028 32602 leveldb.cpp:399] Deleting ~2 keys from leveldb took 47965ns
I1221 17:48:57.370055 32602 replica.cpp:712] Persisted action at 4
I1221 17:48:57.370115 32602 replica.cpp:697] Replica learned TRUNCATE action at position 4
I1221 17:48:57.370586 32605 slave.cpp:963] Forwarding total oversubscribed resources 
I1221 17:48:57.370836 32605 master.cpp:4542] Received update of slave f9d7159c-0e99-4adc-8090-abc4b60b98f5-S0 at slave(3)@172.17.0.2:34597 (be9bd3a71092) with total oversubscribed resources 
I1221 17:48:57.371511 32594 hierarchical.cpp:521] Slave f9d7159c-0e99-4adc-8090-abc4b60b98f5-S0 (be9bd3a71092) updated with oversubscribed resources  (total: cpus(*):2; mem(*):10240; disk(*):3.70122e+06; ports(*):[31000-32000], allocated: )
I1221 17:48:57.371695 32594 hierarchical.cpp:1329] No resources available to allocate!
I1221 17:48:57.371731 32594 hierarchical.cpp:1101] Performed allocation for slave f9d7159c-0e99-4adc-8090-abc4b60b98f5-S0 in 174017ns
I1221 17:48:57.403847 32605 slave.cpp:1254] Will retry registration in 922.499544ms if necessary
I1221 17:48:57.404263 32605 master.cpp:4132] Registering slave at slave(2)@172.17.0.2:34597 (be9bd3a71092) with id f9d7159c-0e99-4adc-8090-abc4b60b98f5-S1
I1221 17:48:57.405129 32605 registrar.cpp:439] Applied 1 operations in 145186ns; attempting to update the 'registry'
I1221 17:48:57.408872 32606 log.cpp:683] Attempting to append 505 bytes to the log
I1221 17:48:57.409003 32601 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 5
I1221 17:48:57.409904 32592 replica.cpp:537] Replica received write request for position 5 from (46)@172.17.0.2:34597
I1221 17:48:57.410267 32592 leveldb.cpp:341] Persisting action (524 bytes) to leveldb took 271200ns
I1221 17:48:57.410295 32592 replica.cpp:712] Persisted action at 5
I1221 17:48:57.411341 32592 replica.cpp:691] Replica received learned notice for position 5 from @0.0.0.0:0
I1221 17:48:57.411941 32592 leveldb.cpp:341] Persisting action (526 bytes) to leveldb took 567605ns
I1221 17:48:57.411965 32592 replica.cpp:712] Persisted action at 5
I1221 17:48:57.412001 32592 replica.cpp:697] Replica learned APPEND action at position 5
I1221 17:48:57.413841 32603 registrar.cpp:484] Successfully updated the 'registry' in 8.61312ms
I1221 17:48:57.414279 32604 log.cpp:702] Attempting to truncate the log to 5
I1221 17:48:57.414530 32604 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 6
I1221 17:48:57.415073 32603 slave.cpp:3371] Received ping from slave-observer(2)@172.17.0.2:34597
I1221 17:48:57.415380 32604 hierarchical.cpp:465] Added slave f9d7159c-0e99-4adc-8090-abc4b60b98f5-S1 (be9bd3a71092) with cpus(*):2; mem(*):10240; disk(*):3.70122e+06; ports(*):[31000-32000] (allocated: )
I1221 17:48:57.415545 32598 replica.cpp:537] Replica received write request for position 6 from (47)@172.17.0.2:34597
I1221 17:48:57.415616 32591 slave.cpp:904] Registered with master master@172.17.0.2:34597; given slave ID f9d7159c-0e99-4adc-8090-abc4b60b98f5-S1
I1221 17:48:57.415639 32591 fetcher.cpp:81] Clearing fetcher cache
I1221 17:48:57.415701 32604 hierarchical.cpp:1329] No resources available to allocate!
I1221 17:48:57.415741 32604 hierarchical.cpp:1101] Performed allocation for slave f9d7159c-0e99-4adc-8090-abc4b60b98f5-S1 in 226908ns
I1221 17:48:57.415596 32599 master.cpp:4200] Registered slave f9d7159c-0e99-4adc-8090-abc4b60b98f5-S1 at slave(2)@172.17.0.2:34597 (be9bd3a71092) with cpus(*):2; mem(*):10240; disk(*):3.70122e+06; ports(*):[31000-32000]
I1221 17:48:57.415772 32601 status_update_manager.cpp:181] Resuming sending status updates
I1221 17:48:57.416055 32598 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 469956ns
I1221 17:48:57.416087 32598 replica.cpp:712] Persisted action at 6
I1221 17:48:57.416223 32591 slave.cpp:927] Checkpointing SlaveInfo to '/tmp/mesos-Alzbfe/1/meta/slaves/f9d7159c-0e99-4adc-8090-abc4b60b98f5-S1/slave.info'
I1221 17:48:57.416697 32591 slave.cpp:963] Forwarding total oversubscribed resources 
I1221 17:48:57.416990 32599 replica.cpp:691] Replica received learned notice for position 6 from @0.0.0.0:0
I1221 17:48:57.417300 32603 master.cpp:4542] Received update of slave f9d7159c-0e99-4adc-8090-abc4b60b98f5-S1 at slave(2)@172.17.0.2:34597 (be9bd3a71092) with total oversubscribed resources 
I1221 17:48:57.417551 32599 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 524015ns
I1221 17:48:57.417635 32599 leveldb.cpp:399] Deleting ~2 keys from leveldb took 45508ns
I1221 17:48:57.417661 32599 replica.cpp:712] Persisted action at 6
I1221 17:48:57.417707 32599 replica.cpp:697] Replica learned TRUNCATE action at position 6
I1221 17:48:57.418038 32591 hierarchical.cpp:521] Slave f9d7159c-0e99-4adc-8090-abc4b60b98f5-S1 (be9bd3a71092) updated with oversubscribed resources  (total: cpus(*):2; mem(*):10240; disk(*):3.70122e+06; ports(*):[31000-32000], allocated: )
I1221 17:48:57.418301 32591 hierarchical.cpp:1329] No resources available to allocate!
I1221 17:48:57.418340 32591 hierarchical.cpp:1101] Performed allocation for slave f9d7159c-0e99-4adc-8090-abc4b60b98f5-S1 in 217594ns
I1221 17:48:58.012727 32599 slave.cpp:1254] Will retry registration in 627.120173ms if necessary
I1221 17:48:58.013154 32599 master.cpp:4132] Registering slave at slave(1)@172.17.0.2:34597 (be9bd3a71092) with id f9d7159c-0e99-4adc-8090-abc4b60b98f5-S2
I1221 17:48:58.014058 32599 registrar.cpp:439] Applied 1 operations in 172284ns; attempting to update the 'registry'
I1221 17:48:58.018355 32606 log.cpp:683] Attempting to append 671 bytes to the log
I1221 17:48:58.018780 32606 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 7
I1221 17:48:58.020308 32599 replica.cpp:537] Replica received write request for position 7 from (48)@172.17.0.2:34597
I1221 17:48:58.020777 32599 leveldb.cpp:341] Persisting action (690 bytes) to leveldb took 403829ns
I1221 17:48:58.020808 32599 replica.cpp:712] Persisted action at 7
I1221 17:48:58.021896 32599 replica.cpp:691] Replica received learned notice for position 7 from @0.0.0.0:0
I1221 17:48:58.022712 32599 leveldb.cpp:341] Persisting action (692 bytes) to leveldb took 779108ns
I1221 17:48:58.022742 32599 replica.cpp:712] Persisted action at 7
I1221 17:48:58.022774 32599 replica.cpp:697] Replica learned APPEND action at position 7
I1221 17:48:58.025650 32592 registrar.cpp:484] Successfully updated the 'registry' in 11.459072ms
I1221 17:48:58.026250 32599 log.cpp:702] Attempting to truncate the log to 7
I1221 17:48:58.026494 32593 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 8
I1221 17:48:58.027056 32599 slave.cpp:3371] Received ping from slave-observer(3)@172.17.0.2:34597
I1221 17:48:58.027214 32599 slave.cpp:904] Registered with master master@172.17.0.2:34597; given slave ID f9d7159c-0e99-4adc-8090-abc4b60b98f5-S2
I1221 17:48:58.027156 32606 master.cpp:4200] Registered slave f9d7159c-0e99-4adc-8090-abc4b60b98f5-S2 at slave(1)@172.17.0.2:34597 (be9bd3a71092) with cpus(*):2; mem(*):10240; disk(*):3.70122e+06; ports(*):[31000-32000]
I1221 17:48:58.027241 32599 fetcher.cpp:81] Clearing fetcher cache
I1221 17:48:58.027559 32593 hierarchical.cpp:465] Added slave f9d7159c-0e99-4adc-8090-abc4b60b98f5-S2 (be9bd3a71092) with cpus(*):2; mem(*):10240; disk(*):3.70122e+06; ports(*):[31000-32000] (allocated: )
I1221 17:48:58.027600 32606 status_update_manager.cpp:181] Resuming sending status updates
I1221 17:48:58.027760 32593 hierarchical.cpp:1329] No resources available to allocate!
I1221 17:48:58.027762 32599 slave.cpp:927] Checkpointing SlaveInfo to '/tmp/mesos-Alzbfe/0/meta/slaves/f9d7159c-0e99-4adc-8090-abc4b60b98f5-S2/slave.info'
I1221 17:48:58.027799 32593 hierarchical.cpp:1101] Performed allocation for slave f9d7159c-0e99-4adc-8090-abc4b60b98f5-S2 in 199647ns
I1221 17:48:58.027900 32606 replica.cpp:537] Replica received write request for position 8 from (49)@172.17.0.2:34597
I1221 17:48:58.028748 32599 slave.cpp:963] Forwarding total oversubscribed resources 
I1221 17:48:58.028779 32606 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 828368ns
I1221 17:48:58.028890 32606 replica.cpp:712] Persisted action at 8
I1221 17:48:58.029263 32599 master.cpp:4542] Received update of slave f9d7159c-0e99-4adc-8090-abc4b60b98f5-S2 at slave(1)@172.17.0.2:34597 (be9bd3a71092) with total oversubscribed resources 
I1221 17:48:58.030340 32600 replica.cpp:691] Replica received learned notice for position 8 from @0.0.0.0:0
I1221 17:48:58.030915 32600 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 525652ns
I1221 17:48:58.030994 32600 leveldb.cpp:399] Deleting ~2 keys from leveldb took 44845ns
I1221 17:48:58.031020 32600 replica.cpp:712] Persisted action at 8
I1221 17:48:58.031065 32600 replica.cpp:697] Replica learned TRUNCATE action at position 8
I1221 17:48:58.031116 32597 hierarchical.cpp:521] Slave f9d7159c-0e99-4adc-8090-abc4b60b98f5-S2 (be9bd3a71092) updated with oversubscribed resources  (total: cpus(*):2; mem(*):10240; disk(*):3.70122e+06; ports(*):[31000-32000], allocated: )
I1221 17:48:58.031369 32597 hierarchical.cpp:1329] No resources available to allocate!
I1221 17:48:58.031419 32597 hierarchical.cpp:1101] Performed allocation for slave f9d7159c-0e99-4adc-8090-abc4b60b98f5-S2 in 235433ns
I1221 17:48:58.254617 32591 hierarchical.cpp:1329] No resources available to allocate!
I1221 17:48:58.254684 32591 hierarchical.cpp:1079] Performed allocation for 3 slaves in 634906ns
I1221 17:48:59.126865 32523 exec.cpp:88] Committing suicide by killing the process group
II1221 17:48:59.130280 32542 exec.cpp:88] Committing suicide by killing the process group
1221 17:48:59.130283 32550 exec.cpp:88] Committing suicide by killing the process group
I1221 17:48:59.256448 32599 hierarchical.cpp:1329] No resources available to allocate!
I1221 17:48:59.256536 32599 hierarchical.cpp:1079] Performed allocation for 3 slaves in 624788ns
I1221 17:49:00.258219 32600 hierarchical.cpp:1329] No resources available to allocate!
I1221 17:49:00.258293 32600 hierarchical.cpp:1079] Performed allocation for 3 slaves in 607689ns
I1221 17:49:01.260308 32600 hierarchical.cpp:1329] No resources available to allocate!
I1221 17:49:01.260392 32600 hierarchical.cpp:1079] Performed allocation for 3 slaves in 657847ns
I1221 17:49:02.262225 32600 hierarchical.cpp:1329] No resources available to allocate!
I1221 17:49:02.262302 32600 hierarchical.cpp:1079] Performed allocation for 3 slaves in 594437ns
W1221 17:49:02.299283 32600 sched.cpp:429] Authentication timed out
I1221 17:49:02.299711 32600 sched.cpp:387] Failed to authenticate with master master@172.17.0.2:34597: Authentication discarded
I1221 17:49:02.299768 32600 sched.cpp:318] Authenticating with master master@172.17.0.2:34597
I1221 17:49:02.299787 32600 sched.cpp:325] Using default CRAM-MD5 authenticatee
I1221 17:49:02.300168 32600 authenticatee.cpp:121] Creating new client SASL connection
I1221 17:49:02.300691 32600 master.cpp:5423] Authenticating scheduler-bf375f84-4668-4cc7-9788-476575858f38@172.17.0.2:34597
I1221 17:49:02.301192 32600 authenticator.cpp:413] Starting authentication session for crammd5_authenticatee(2)@172.17.0.2:34597
I1221 17:49:02.302160 32600 authenticator.cpp:98] Creating new server SASL connection
I1221 17:49:02.302534 32597 authenticatee.cpp:212] Received SASL authentication mechanisms: CRAM-MD5
I1221 17:49:02.302569 32597 authenticatee.cpp:238] Attempting to authenticate with mechanism 'CRAM-MD5'
I1221 17:49:02.302848 32600 authenticator.cpp:203] Received SASL authentication start
I1221 17:49:02.302922 32600 authenticator.cpp:325] Authentication requires more steps
I1221 17:49:02.303489 32591 authenticatee.cpp:258] Received SASL authentication step
I1221 17:49:02.303669 32600 authenticator.cpp:231] Received SASL authentication step
I1221 17:49:02.303793 32600 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: 'be9bd3a71092' server FQDN: 'be9bd3a71092' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I1221 17:49:02.303872 32600 auxprop.cpp:179] Looking up auxiliary property '*userPassword'
I1221 17:49:02.304182 32600 auxprop.cpp:179] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I1221 17:49:02.304271 32600 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: 'be9bd3a71092' server FQDN: 'be9bd3a71092' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I1221 17:49:02.304348 32600 auxprop.cpp:129] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I1221 17:49:02.304435 32600 auxprop.cpp:129] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I1221 17:49:02.304523 32600 authenticator.cpp:317] Authentication success
I1221 17:49:02.304816 32596 authenticatee.cpp:298] Authentication success
I1221 17:49:02.314942 32596 sched.cpp:407] Successfully authenticated with master master@172.17.0.2:34597
I1221 17:49:02.315335 32602 master.cpp:5453] Successfully authenticated principal 'test-principal' at scheduler-bf375f84-4668-4cc7-9788-476575858f38@172.17.0.2:34597
I1221 17:49:02.315585 32595 authenticator.cpp:431] Authentication session cleanup for crammd5_authenticatee(2)@172.17.0.2:34597
I1221 17:49:02.316352 32596 sched.cpp:714] Sending SUBSCRIBE call to master@172.17.0.2:34597
I1221 17:49:02.316957 32592 master.cpp:2197] Received SUBSCRIBE call for framework 'No Executor Framework' at scheduler-bf375f84-4668-4cc7-9788-476575858f38@172.17.0.2:34597
I1221 17:49:02.317235 32592 master.cpp:1668] Authorizing framework principal 'test-principal' to receive offers for role '*'
I1221 17:49:02.317922 32592 master.cpp:2268] Subscribing framework No Executor Framework with checkpointing enabled and capabilities [  ]
I1221 17:49:02.320014 32592 hierarchical.cpp:260] Added framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000
I1221 17:49:02.323434 32596 sched.cpp:747] Will retry registration in 189.845934ms if necessary
I1221 17:49:02.325234 32605 master.cpp:5252] Sending 3 offers to framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000 (No Executor Framework) at scheduler-bf375f84-4668-4cc7-9788-476575858f38@172.17.0.2:34597
I1221 17:49:02.325760 32592 hierarchical.cpp:1423] No inverse offers to send out!
I1221 17:49:02.326253 32592 hierarchical.cpp:1079] Performed allocation for 3 slaves in 6.132385ms
I1221 17:49:02.326153 32596 sched.cpp:641] Framework registered with f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000
I1221 17:49:02.326529 32596 no_executor_framework.cpp:81] Registered with master id: ""f9d7159c-0e99-4adc-8090-abc4b60b98f5""
ip: 33558956
port: 34597
pid: ""master@172.17.0.2:34597""
hostname: ""be9bd3a71092""
version: ""0.27.0""
address {
  hostname: ""be9bd3a71092""
  ip: ""172.17.0.2""
  port: 34597
}
 and got framework ID f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000
I1221 17:49:02.326908 32596 sched.cpp:655] Scheduler::registered took 380713ns
I1221 17:49:02.328006 32596 no_executor_framework.cpp:105] Received offer f9d7159c-0e99-4adc-8090-abc4b60b98f5-O0 from slave f9d7159c-0e99-4adc-8090-abc4b60b98f5-S0 (be9bd3a71092) with cpus(*):2; mem(*):10240; disk(*):3.70122e+06; ports(*):[31000-32000]
I1221 17:49:02.329509 32596 no_executor_framework.cpp:105] Received offer f9d7159c-0e99-4adc-8090-abc4b60b98f5-O1 from slave f9d7159c-0e99-4adc-8090-abc4b60b98f5-S1 (be9bd3a71092) with cpus(*):2; mem(*):10240; disk(*):3.70122e+06; ports(*):[31000-32000]
I1221 17:49:02.329823 32596 no_executor_framework.cpp:105] Received offer f9d7159c-0e99-4adc-8090-abc4b60b98f5-O2 from slave f9d7159c-0e99-4adc-8090-abc4b60b98f5-S2 (be9bd3a71092) with cpus(*):2; mem(*):10240; disk(*):3.70122e+06; ports(*):[31000-32000]
I1221 17:49:02.330021 32596 sched.cpp:811] Scheduler::resourceOffers took 2.025496ms
I1221 17:49:02.332299 32597 master.cpp:3055] Processing ACCEPT call for offers: [ f9d7159c-0e99-4adc-8090-abc4b60b98f5-O0 ] on slave f9d7159c-0e99-4adc-8090-abc4b60b98f5-S0 at slave(3)@172.17.0.2:34597 (be9bd3a71092) for framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000 (No Executor Framework) at scheduler-bf375f84-4668-4cc7-9788-476575858f38@172.17.0.2:34597
I1221 17:49:02.332388 32597 master.cpp:2742] Authorizing framework principal 'test-principal' to launch task 0 as user 'mesos'
I1221 17:49:02.332682 32597 master.cpp:2742] Authorizing framework principal 'test-principal' to launch task 1 as user 'mesos'
I1221 17:49:02.332800 32597 master.cpp:2742] Authorizing framework principal 'test-principal' to launch task 2 as user 'mesos'
I1221 17:49:02.332922 32597 master.cpp:2742] Authorizing framework principal 'test-principal' to launch task 3 as user 'mesos'
I1221 17:49:02.333034 32597 master.cpp:2742] Authorizing framework principal 'test-principal' to launch task 4 as user 'mesos'
I1221 17:49:02.334977 32597 master.cpp:3055] Processing ACCEPT call for offers: [ f9d7159c-0e99-4adc-8090-abc4b60b98f5-O1 ] on slave f9d7159c-0e99-4adc-8090-abc4b60b98f5-S1 at slave(2)@172.17.0.2:34597 (be9bd3a71092) for framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000 (No Executor Framework) at scheduler-bf375f84-4668-4cc7-9788-476575858f38@172.17.0.2:34597
I1221 17:49:02.335844 32597 master.cpp:3055] Processing ACCEPT call for offers: [ f9d7159c-0e99-4adc-8090-abc4b60b98f5-O2 ] on slave f9d7159c-0e99-4adc-8090-abc4b60b98f5-S2 at slave(1)@172.17.0.2:34597 (be9bd3a71092) for framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000 (No Executor Framework) at scheduler-bf375f84-4668-4cc7-9788-476575858f38@172.17.0.2:34597
I1221 17:49:02.337626 32597 master.hpp:176] Adding task 0 with resources cpus(*):0.1; mem(*):32; disk(*):32 on slave f9d7159c-0e99-4adc-8090-abc4b60b98f5-S0 (be9bd3a71092)
I1221 17:49:02.337843 32597 master.cpp:3518] Launching task 0 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000 (No Executor Framework) at scheduler-bf375f84-4668-4cc7-9788-476575858f38@172.17.0.2:34597 with resources cpus(*):0.1; mem(*):32; disk(*):32 on slave f9d7159c-0e99-4adc-8090-abc4b60b98f5-S0 at slave(3)@172.17.0.2:34597 (be9bd3a71092)
I1221 17:49:02.338376 32598 slave.cpp:1294] Got assigned task 0 for framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000
I1221 17:49:02.338762 32598 slave.cpp:5133] Checkpointing FrameworkInfo to '/tmp/mesos-Alzbfe/2/meta/slaves/f9d7159c-0e99-4adc-8090-abc4b60b98f5-S0/frameworks/f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000/framework.info'
I1221 17:49:02.338826 32597 master.hpp:176] Adding task 1 with resources cpus(*):0.1; mem(*):32; disk(*):32 on slave f9d7159c-0e99-4adc-8090-abc4b60b98f5-S0 (be9bd3a71092)
I1221 17:49:02.338965 32597 master.cpp:3518] Launching task 1 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000 (No Executor Framework) at scheduler-bf375f84-4668-4cc7-9788-476575858f38@172.17.0.2:34597 with resources cpus(*):0.1; mem(*):32; disk(*):32 on slave f9d7159c-0e99-4adc-8090-abc4b60b98f5-S0 at slave(3)@172.17.0.2:34597 (be9bd3a71092)
I1221 17:49:02.339486 32598 slave.cpp:5144] Checkpointing framework pid 'scheduler-bf375f84-4668-4cc7-9788-476575858f38@172.17.0.2:34597' to '/tmp/mesos-Alzbfe/2/meta/slaves/f9d7159c-0e99-4adc-8090-abc4b60b98f5-S0/frameworks/f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000/framework.pid'
I1221 17:49:02.339838 32597 master.hpp:176] Adding task 2 with resources cpus(*):0.1; mem(*):32; disk(*):32 on slave f9d7159c-0e99-4adc-8090-abc4b60b98f5-S0 (be9bd3a71092)
I1221 17:49:02.339970 32598 resources.cpp:478] Parsing resources as JSON failed: cpus:0.1;mem:32
Trying semicolon-delimited string format instead
I1221 17:49:02.339985 32597 master.cpp:3518] Launching task 2 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000 (No Executor Framework) at scheduler-bf375f84-4668-4cc7-9788-476575858f38@172.17.0.2:34597 with resources cpus(*):0.1; mem(*):32; disk(*):32 on slave f9d7159c-0e99-4adc-8090-abc4b60b98f5-S0 at slave(3)@172.17.0.2:34597 (be9bd3a71092)
I1221 17:49:02.340816 32597 master.hpp:176] Adding task 3 with resources cpus(*):0.1; mem(*):32; disk(*):32 on slave f9d7159c-0e99-4adc-8090-abc4b60b98f5-S0 (be9bd3a71092)
I1221 17:49:02.340962 32597 master.cpp:3518] Launching task 3 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000 (No Executor Framework) at scheduler-bf375f84-4668-4cc7-9788-476575858f38@172.17.0.2:34597 with resources cpus(*):0.1; mem(*):32; disk(*):32 on slave f9d7159c-0e99-4adc-8090-abc4b60b98f5-S0 at slave(3)@172.17.0.2:34597 (be9bd3a71092)
I1221 17:49:02.341094 32598 slave.cpp:1294] Got assigned task 1 for framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000
I1221 17:49:02.341233 32598 resources.cpp:478] Parsing resources as JSON failed: cpus:0.1;mem:32
Trying semicolon-delimited string format instead
I1221 17:49:02.341850 32598 slave.cpp:1294] Got assigned task 2 for framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000
I1221 17:49:02.341856 32597 master.hpp:176] Adding task 4 with resources cpus(*):0.1; mem(*):32; disk(*):32 on slave f9d7159c-0e99-4adc-8090-abc4b60b98f5-S0 (be9bd3a71092)
I1221 17:49:02.341959 32598 resources.cpp:478] Parsing resources as JSON failed: cpus:0.1;mem:32
Trying semicolon-delimited string format instead
I1221 17:49:02.341996 32597 master.cpp:3518] Launching task 4 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000 (No Executor Framework) at scheduler-bf375f84-4668-4cc7-9788-476575858f38@172.17.0.2:34597 with resources cpus(*):0.1; mem(*):32; disk(*):32 on slave f9d7159c-0e99-4adc-8090-abc4b60b98f5-S0 at slave(3)@172.17.0.2:34597 (be9bd3a71092)
I1221 17:49:02.342867 32604 hierarchical.cpp:880] Recovered cpus(*):1.5; mem(*):10080; disk(*):3.70106e+06; ports(*):[31000-32000] (total: cpus(*):2; mem(*):10240; disk(*):3.70122e+06; ports(*):[31000-32000], allocated: cpus(*):0.5; mem(*):160; disk(*):160) on slave f9d7159c-0e99-4adc-8090-abc4b60b98f5-S0 from framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000
I1221 17:49:02.342952 32604 hierarchical.cpp:917] Framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000 filtered slave f9d7159c-0e99-4adc-8090-abc4b60b98f5-S0 for 5secs
I1221 17:49:02.343386 32598 slave.cpp:1413] Launching task 0 for framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000
I1221 17:49:02.343524 32598 resources.cpp:478] Parsing resources as JSON failed: cpus:0.1;mem:32
Trying semicolon-delimited string format instead
I1221 17:49:02.343547 32604 hierarchical.cpp:880] Recovered cpus(*):2; mem(*):10240; disk(*):3.70122e+06; ports(*):[31000-32000] (total: cpus(*):2; mem(*):10240; disk(*):3.70122e+06; ports(*):[31000-32000], allocated: ) on slave f9d7159c-0e99-4adc-8090-abc4b60b98f5-S1 from framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000
I1221 17:49:02.343616 32604 hierarchical.cpp:917] Framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000 filtered slave f9d7159c-0e99-4adc-8090-abc4b60b98f5-S1 for 5secs
I1221 17:49:02.343993 32604 hierarchical.cpp:880] Recovered cpus(*):2; mem(*):10240; disk(*):3.70122e+06; ports(*):[31000-32000] (total: cpus(*):2; mem(*):10240; disk(*):3.70122e+06; ports(*):[31000-32000], allocated: ) on slave f9d7159c-0e99-4adc-8090-abc4b60b98f5-S2 from framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000
I1221 17:49:02.344048 32604 hierarchical.cpp:917] Framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000 filtered slave f9d7159c-0e99-4adc-8090-abc4b60b98f5-S2 for 5secs
I1221 17:49:02.346493 32598 paths.cpp:434] Trying to chown '/tmp/mesos-Alzbfe/2/slaves/f9d7159c-0e99-4adc-8090-abc4b60b98f5-S0/frameworks/f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000/executors/0/runs/b694ed73-200b-4014-aa3b-e2148f1aa2f6' to user 'mesos'
I1221 17:49:02.352700 32598 slave.cpp:5582] Checkpointing ExecutorInfo to '/tmp/mesos-Alzbfe/2/meta/slaves/f9d7159c-0e99-4adc-8090-abc4b60b98f5-S0/frameworks/f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000/executors/0/executor.info'
I1221 17:49:02.354054 32598 slave.cpp:5211] Launching executor 0 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000 with resources cpus(*):0.1; mem(*):32 in work directory '/tmp/mesos-Alzbfe/2/slaves/f9d7159c-0e99-4adc-8090-abc4b60b98f5-S0/frameworks/f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000/executors/0/runs/b694ed73-200b-4014-aa3b-e2148f1aa2f6'
I1221 17:49:02.355726 32598 slave.cpp:5605] Checkpointing TaskInfo to '/tmp/mesos-Alzbfe/2/meta/slaves/f9d7159c-0e99-4adc-8090-abc4b60b98f5-S0/frameworks/f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000/executors/0/runs/b694ed73-200b-4014-aa3b-e2148f1aa2f6/tasks/0/task.info'
I1221 17:49:02.356019 32603 containerizer.cpp:612] Starting container 'b694ed73-200b-4014-aa3b-e2148f1aa2f6' for executor '0' of framework 'f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000'
I1221 17:49:02.356816 32598 slave.cpp:1631] Queuing task '0' for executor '0' of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000
I1221 17:49:02.357427 32598 slave.cpp:1294] Got assigned task 3 for framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000
I1221 17:49:02.357729 32598 resources.cpp:478] Parsing resources as JSON failed: cpus:0.1;mem:32
Trying semicolon-delimited string format instead
I1221 17:49:02.358626 32598 slave.cpp:1413] Launching task 1 for framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000
I1221 17:49:02.358891 32598 resources.cpp:478] Parsing resources as JSON failed: cpus:0.1;mem:32
Trying semicolon-delimited string format instead
I1221 17:49:02.359839 32598 paths.cpp:434] Trying to chown '/tmp/mesos-Alzbfe/2/slaves/f9d7159c-0e99-4adc-8090-abc4b60b98f5-S0/frameworks/f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000/executors/1/runs/32cbc778-7e14-49c5-9549-f1e397aab07f' to user 'mesos'
I1221 17:49:02.364758 32603 launcher.cpp:132] Forked child with pid '32613' for container 'b694ed73-200b-4014-aa3b-e2148f1aa2f6'
I1221 17:49:02.365324 32603 containerizer.cpp:845] Checkpointing executor's forked pid 32613 to '/tmp/mesos-Alzbfe/2/meta/slaves/f9d7159c-0e99-4adc-8090-abc4b60b98f5-S0/frameworks/f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000/executors/0/runs/b694ed73-200b-4014-aa3b-e2148f1aa2f6/pids/forked.pid'
I1221 17:49:02.365481 32598 slave.cpp:5582] Checkpointing ExecutorInfo to '/tmp/mesos-Alzbfe/2/meta/slaves/f9d7159c-0e99-4adc-8090-abc4b60b98f5-S0/frameworks/f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000/executors/1/executor.info'
I1221 17:49:02.366531 32598 slave.cpp:5211] Launching executor 1 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000 with resources cpus(*):0.1; mem(*):32 in work directory '/tmp/mesos-Alzbfe/2/slaves/f9d7159c-0e99-4adc-8090-abc4b60b98f5-S0/frameworks/f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000/executors/1/runs/32cbc778-7e14-49c5-9549-f1e397aab07f'
I1221 17:49:02.367337 32598 slave.cpp:5605] Checkpointing TaskInfo to '/tmp/mesos-Alzbfe/2/meta/slaves/f9d7159c-0e99-4adc-8090-abc4b60b98f5-S0/frameworks/f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000/executors/1/runs/32cbc778-7e14-49c5-9549-f1e397aab07f/tasks/1/task.info'
I1221 17:49:02.367902 32603 containerizer.cpp:612] Starting container '32cbc778-7e14-49c5-9549-f1e397aab07f' for executor '1' of framework 'f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000'
I1221 17:49:02.368161 32598 slave.cpp:1631] Queuing task '1' for executor '1' of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000
I1221 17:49:02.368551 32598 slave.cpp:1294] Got assigned task 4 for framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000
I1221 17:49:02.368794 32598 resources.cpp:478] Parsing resources as JSON failed: cpus:0.1;mem:32
Trying semicolon-delimited string format instead
I1221 17:49:02.369572 32598 slave.cpp:1413] Launching task 2 for framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000
I1221 17:49:02.369730 32598 resources.cpp:478] Parsing resources as JSON failed: cpus:0.1;mem:32
Trying semicolon-delimited string format instead
I1221 17:49:02.370654 32598 paths.cpp:434] Trying to chown '/tmp/mesos-Alzbfe/2/slaves/f9d7159c-0e99-4adc-8090-abc4b60b98f5-S0/frameworks/f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000/executors/2/runs/43e66467-95ea-46d2-be71-510b74ad1c93' to user 'mesos'
I1221 17:49:02.373889 32595 launcher.cpp:132] Forked child with pid '32618' for container '32cbc778-7e14-49c5-9549-f1e397aab07f'
I1221 17:49:02.374148 32595 containerizer.cpp:845] Checkpointing executor's forked pid 32618 to '/tmp/mesos-Alzbfe/2/meta/slaves/f9d7159c-0e99-4adc-8090-abc4b60b98f5-S0/frameworks/f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000/executors/1/runs/32cbc778-7e14-49c5-9549-f1e397aab07f/pids/forked.pid'
I1221 17:49:02.376971 32598 slave.cpp:5582] Checkpointing ExecutorInfo to '/tmp/mesos-Alzbfe/2/meta/slaves/f9d7159c-0e99-4adc-8090-abc4b60b98f5-S0/frameworks/f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000/executors/2/executor.info'
I1221 17:49:02.380004 32598 slave.cpp:5211] Launching executor 2 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000 with resources cpus(*):0.1; mem(*):32 in work directory '/tmp/mesos-Alzbfe/2/slaves/f9d7159c-0e99-4adc-8090-abc4b60b98f5-S0/frameworks/f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000/executors/2/runs/43e66467-95ea-46d2-be71-510b74ad1c93'
I1221 17:49:02.380900 32598 slave.cpp:5605] Checkpointing TaskInfo to '/tmp/mesos-Alzbfe/2/meta/slaves/f9d7159c-0e99-4adc-8090-abc4b60b98f5-S0/frameworks/f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000/executors/2/runs/43e66467-95ea-46d2-be71-510b74ad1c93/tasks/2/task.info'
I1221 17:49:02.380893 32593 containerizer.cpp:612] Starting container '43e66467-95ea-46d2-be71-510b74ad1c93' for executor '2' of framework 'f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000'
I1221 17:49:02.381963 32598 slave.cpp:1631] Queuing task '2' for executor '2' of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000
I1221 17:49:02.382143 32598 slave.cpp:682] Successfully attached file '/tmp/mesos-Alzbfe/2/slaves/f9d7159c-0e99-4adc-8090-abc4b60b98f5-S0/frameworks/f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000/executors/0/runs/b694ed73-200b-4014-aa3b-e2148f1aa2f6'
I1221 17:49:02.382199 32598 slave.cpp:1413] Launching task 3 for framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000
I1221 17:49:02.382339 32598 resources.cpp:478] Parsing resources as JSON failed: cpus:0.1;mem:32
Trying semicolon-delimited string format instead
I1221 17:49:02.383301 32598 paths.cpp:434] Trying to chown '/tmp/mesos-Alzbfe/2/slaves/f9d7159c-0e99-4adc-8090-abc4b60b98f5-S0/frameworks/f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000/executors/3/runs/d18f1e1f-a36e-4262-a8de-8857f8806415' to user 'mesos'
I1221 17:49:02.388300 32592 launcher.cpp:132] Forked child with pid '32636' for container '43e66467-95ea-46d2-be71-510b74ad1c93'
I1221 17:49:02.388861 32592 containerizer.cpp:845] Checkpointing executor's forked pid 32636 to '/tmp/mesos-Alzbfe/2/meta/slaves/f9d7159c-0e99-4adc-8090-abc4b60b98f5-S0/frameworks/f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000/executors/2/runs/43e66467-95ea-46d2-be71-510b74ad1c93/pids/forked.pid'
I1221 17:49:02.389917 32598 slave.cpp:5582] Checkpointing ExecutorInfo to '/tmp/mesos-Alzbfe/2/meta/slaves/f9d7159c-0e99-4adc-8090-abc4b60b98f5-S0/frameworks/f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000/executors/3/executor.info'
I1221 17:49:02.391086 32598 slave.cpp:5211] Launching executor 3 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000 with resources cpus(*):0.1; mem(*):32 in work directory '/tmp/mesos-Alzbfe/2/slaves/f9d7159c-0e99-4adc-8090-abc4b60b98f5-S0/frameworks/f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000/executors/3/runs/d18f1e1f-a36e-4262-a8de-8857f8806415'
I1221 17:49:02.392046 32592 containerizer.cpp:612] Starting container 'd18f1e1f-a36e-4262-a8de-8857f8806415' for executor '3' of framework 'f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000'
I1221 17:49:02.392333 32598 slave.cpp:5605] Checkpointing TaskInfo to '/tmp/mesos-Alzbfe/2/meta/slaves/f9d7159c-0e99-4adc-8090-abc4b60b98f5-S0/frameworks/f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000/executors/3/runs/d18f1e1f-a36e-4262-a8de-8857f8806415/tasks/3/task.info'
I1221 17:49:02.393121 32598 slave.cpp:1631] Queuing task '3' for executor '3' of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000
I1221 17:49:02.393587 32598 slave.cpp:682] Successfully attached file '/tmp/mesos-Alzbfe/2/slaves/f9d7159c-0e99-4adc-8090-abc4b60b98f5-S0/frameworks/f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000/executors/1/runs/32cbc778-7e14-49c5-9549-f1e397aab07f'
I1221 17:49:02.393795 32598 slave.cpp:1413] Launching task 4 for framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000
I1221 17:49:02.393967 32598 resources.cpp:478] Parsing resources as JSON failed: cpus:0.1;mem:32
Trying semicolon-delimited string format instead
I1221 17:49:02.394999 32598 paths.cpp:434] Trying to chown '/tmp/mesos-Alzbfe/2/slaves/f9d7159c-0e99-4adc-8090-abc4b60b98f5-S0/frameworks/f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000/executors/4/runs/45dd8e22-074e-4dcd-b3f8-4074526eb23b' to user 'mesos'
I1221 17:49:02.398633 32592 launcher.cpp:132] Forked child with pid '32644' for container 'd18f1e1f-a36e-4262-a8de-8857f8806415'
I1221 17:49:02.398843 32592 containerizer.cpp:845] Checkpointing executor's forked pid 32644 to '/tmp/mesos-Alzbfe/2/meta/slaves/f9d7159c-0e99-4adc-8090-abc4b60b98f5-S0/frameworks/f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000/executors/3/runs/d18f1e1f-a36e-4262-a8de-8857f8806415/pids/forked.pid'
I1221 17:49:02.402446 32598 slave.cpp:5582] Checkpointing ExecutorInfo to '/tmp/mesos-Alzbfe/2/meta/slaves/f9d7159c-0e99-4adc-8090-abc4b60b98f5-S0/frameworks/f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000/executors/4/executor.info'
I1221 17:49:02.403795 32598 slave.cpp:5211] Launching executor 4 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000 with resources cpus(*):0.1; mem(*):32 in work directory '/tmp/mesos-Alzbfe/2/slaves/f9d7159c-0e99-4adc-8090-abc4b60b98f5-S0/frameworks/f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000/executors/4/runs/45dd8e22-074e-4dcd-b3f8-4074526eb23b'
I1221 17:49:02.404616 32594 containerizer.cpp:612] Starting container '45dd8e22-074e-4dcd-b3f8-4074526eb23b' for executor '4' of framework 'f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000'
I1221 17:49:02.404855 32598 slave.cpp:5605] Checkpointing TaskInfo to '/tmp/mesos-Alzbfe/2/meta/slaves/f9d7159c-0e99-4adc-8090-abc4b60b98f5-S0/frameworks/f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000/executors/4/runs/45dd8e22-074e-4dcd-b3f8-4074526eb23b/tasks/4/task.info'
I1221 17:49:02.405623 32598 slave.cpp:1631] Queuing task '4' for executor '4' of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000
I1221 17:49:02.406750 32598 slave.cpp:682] Successfully attached file '/tmp/mesos-Alzbfe/2/slaves/f9d7159c-0e99-4adc-8090-abc4b60b98f5-S0/frameworks/f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000/executors/2/runs/43e66467-95ea-46d2-be71-510b74ad1c93'
I1221 17:49:02.406944 32598 slave.cpp:682] Successfully attached file '/tmp/mesos-Alzbfe/2/slaves/f9d7159c-0e99-4adc-8090-abc4b60b98f5-S0/frameworks/f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000/executors/3/runs/d18f1e1f-a36e-4262-a8de-8857f8806415'
I1221 17:49:02.407101 32598 slave.cpp:682] Successfully attached file '/tmp/mesos-Alzbfe/2/slaves/f9d7159c-0e99-4adc-8090-abc4b60b98f5-S0/frameworks/f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000/executors/4/runs/45dd8e22-074e-4dcd-b3f8-4074526eb23b'
I1221 17:49:02.410367 32605 launcher.cpp:132] Forked child with pid '32660' for container '45dd8e22-074e-4dcd-b3f8-4074526eb23b'
I1221 17:49:02.410871 32605 containerizer.cpp:845] Checkpointing executor's forked pid 32660 to '/tmp/mesos-Alzbfe/2/meta/slaves/f9d7159c-0e99-4adc-8090-abc4b60b98f5-S0/frameworks/f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000/executors/4/runs/45dd8e22-074e-4dcd-b3f8-4074526eb23b/pids/forked.pid'
WARNING: Logging before InitGoogleLogging() is written to STDERR
I1221 17:49:02.593050 32690 process.cpp:998] libprocess is initialized on 172.17.0.2:40805 for 16 cpus
WARNING: Logging before InitGoogleLogging() is written to STDERR
I1221 17:49:02.594478 32689 process.cpp:998] libprocess is initialized on 172.17.0.2:41236 for 16 cpus
I1221 17:49:02.595391 32689 logging.cpp:172] Logging to STDERR
WARNING: Logging before InitGoogleLogging() is written to STDERR
I1221 17:49:02.606909 32718 process.cpp:998] libprocess is initialized on 172.17.0.2:44314 for 16 cpus
I1221 17:49:02.607516 32690 logging.cpp:172] Logging to STDERR
I1221 17:49:02.608772 32718 logging.cpp:172] Logging to STDERR
I1221 17:49:02.611932 32718 exec.cpp:134] Version: 0.27.0
I1221 17:49:02.613535 32690 exec.cpp:134] Version: 0.27.0
I1221 17:49:02.623258   322 exec.cpp:184] Executor started at: executor(1)@172.17.0.2:40805 with pid 32690
I1221 17:49:02.624547 32689 exec.cpp:134] Version: 0.27.0
I1221 17:49:02.629375 32604 slave.cpp:2578] Got registration for executor '1' of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000 from executor(1)@172.17.0.2:40805
I1221 17:49:02.631279 32604 slave.cpp:2664] Checkpointing executor pid 'executor(1)@172.17.0.2:40805' to '/tmp/mesos-Alzbfe/2/meta/slaves/f9d7159c-0e99-4adc-8090-abc4b60b98f5-S0/frameworks/f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000/executors/1/runs/32cbc778-7e14-49c5-9549-f1e397aab07f/pids/libprocess.pid'
I1221 17:49:02.634460   304 exec.cpp:184] Executor started at: executor(1)@172.17.0.2:41236 with pid 32689
I1221 17:49:02.636802 32605 slave.cpp:1796] Sending queued task '1' to executor '1' of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000 at executor(1)@172.17.0.2:40805
I1221 17:49:02.637688 32605 slave.cpp:2578] Got registration for executor '0' of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000 from executor(1)@172.17.0.2:41236
I1221 17:49:02.638875 32605 slave.cpp:2664] Checkpointing executor pid 'executor(1)@172.17.0.2:41236' to '/tmp/mesos-Alzbfe/2/meta/slaves/f9d7159c-0e99-4adc-8090-abc4b60b98f5-S0/frameworks/f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000/executors/0/runs/b694ed73-200b-4014-aa3b-e2148f1aa2f6/pids/libprocess.pid'
I1221 17:49:02.640518   326 exec.cpp:184] Executor started at: executor(1)@172.17.0.2:44314 with pid 32718
I1221 17:49:02.642498   301 exec.cpp:208] Executor registered on slave f9d7159c-0e99-4adc-8090-abc4b60b98f5-S0
I1221 17:49:02.642885   313 exec.cpp:208] Executor registered on slave f9d7159c-0e99-4adc-8090-abc4b60b98f5-S0
I1221 17:49:02.643174 32605 slave.cpp:1796] Sending queued task '0' to executor '0' of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000 at executor(1)@172.17.0.2:41236
Registered executor on be9bd3a71092
IRegistered executor on be9bd3a71092
1221 17:49:02.646105   313 exec.cpp:220] Executor::registered took 370768ns
I1221 17:49:02.646445   301 exec.cpp:220] Executor::registered took 310909ns
I1221 17:49:02.646821   313 exec.cpp:295] Executor asked to run task '1'
II1221 17:49:02.647033   308 exec.cpp:295] Executor asked to run task '0'
Starting task 1
I1221 17:49:02.647197   308 exec.cpp:304] Executor::launchTask took 129788ns
I1221 17:49:02.647519 32604 slave.cpp:2578] Got registration for executor '2' of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000 from executor(1)@172.17.0.2:44314
Starting task 0
1221 17:49:02.647030   313 exec.cpp:304] Executor::launchTask took 109733ns
I1221 17:49:02.648578 32604 slave.cpp:2664] Checkpointing executor pid 'executor(1)@172.17.0.2:44314' to '/tmp/mesos-Alzbfe/2/meta/slaves/f9d7159c-0e99-4adc-8090-abc4b60b98f5-S0/frameworks/f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000/executors/2/runs/43e66467-95ea-46d2-be71-510b74ad1c93/pids/libprocess.pid'
Forked command at 343
sh -c 'echo hello'
sh -c 'echo hello'
Forked command at 342
hello
hello
I1221 17:49:02.653201 32604 slave.cpp:1796] Sending queued task '2' to executor '2' of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000 at executor(1)@172.17.0.2:44314
I1221 17:49:02.653561   308 exec.cpp:517] Executor sending status update TASK_RUNNING (UUID: 7933b468-ff47-4ec3-95a2-4ac8c5396d7b) for task 0 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000
I1221 17:49:02.653892 32764 exec.cpp:517] Executor sending status update TASK_RUNNING (UUID: cf7921a3-0f46-4d60-bb83-c71a97b2b6f8) for task 1 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000
I1221 17:49:02.654163   327 exec.cpp:208] Executor registered on slave f9d7159c-0e99-4adc-8090-abc4b60b98f5-S0
I1221 17:49:02.655485 32604 slave.cpp:2937] Handling status update TASK_RUNNING (UUID: cf7921a3-0f46-4d60-bb83-c71a97b2b6f8) for task 1 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000 from executor(1)@172.17.0.2:40805
I1221 17:49:02.656472 32605 status_update_manager.cpp:320] Received status update TASK_RUNNING (UUID: cf7921a3-0f46-4d60-bb83-c71a97b2b6f8) for task 1 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000
I1221 17:49:02.656808 32605 status_update_manager.cpp:497] Creating StatusUpdate stream for task 1 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000
I1221 17:49:02.657961 32605 status_update_manager.cpp:824] Checkpointing UPDATE for status update TASK_RUNNING (UUID: cf7921a3-0f46-4d60-bb83-c71a97b2b6f8) for task 1 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000
I1221 17:49:02.658934   327 exec.cpp:220] Executor::registered took 540568ns
Registered executor on be9bd3a71092
I1221 17:49:02.659931   327 exec.cpp:295] Executor asked to run task '2'
IStarting task 2
1221 17:49:02.660373   327 exec.cpp:304] Executor::launchTask took 187926ns
I1221 17:49:02.656481 32604 slave.cpp:2937] Handling status update TASK_RUNNING (UUID: 7933b468-ff47-4ec3-95a2-4ac8c5396d7b) for task 0 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000 from executor(1)@172.17.0.2:41236
sh -c 'echo hello'
Forked command at 361
hello
WARNING: Logging before InitGoogleLogging() is written to STDERR
I1221 17:49:02.663389 32725 process.cpp:998] libprocess is initialized on 172.17.0.2:52442 for 16 cpus
I1221 17:49:02.664667 32725 logging.cpp:172] Logging to STDERR
I1221 17:49:02.665820   328 exec.cpp:517] Executor sending status update TASK_RUNNING (UUID: c872f4b3-c7cd-43d7-949c-34a4d92a6c44) for task 2 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000
I1221 17:49:02.667768 32599 slave.cpp:2937] Handling status update TASK_RUNNING (UUID: c872f4b3-c7cd-43d7-949c-34a4d92a6c44) for task 2 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000 from executor(1)@172.17.0.2:44314
I1221 17:49:02.668275 32605 status_update_manager.cpp:374] Forwarding update TASK_RUNNING (UUID: cf7921a3-0f46-4d60-bb83-c71a97b2b6f8) for task 1 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000 to the slave
I1221 17:49:02.668929 32725 exec.cpp:134] Version: 0.27.0
I1221 17:49:02.669333 32601 slave.cpp:3289] Forwarding the update TASK_RUNNING (UUID: cf7921a3-0f46-4d60-bb83-c71a97b2b6f8) for task 1 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000 to master@172.17.0.2:34597
I1221 17:49:02.669736 32605 status_update_manager.cpp:320] Received status update TASK_RUNNING (UUID: 7933b468-ff47-4ec3-95a2-4ac8c5396d7b) for task 0 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000
I1221 17:49:02.669823 32605 status_update_manager.cpp:497] Creating StatusUpdate stream for task 0 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000
I1221 17:49:02.669803 32601 master.cpp:4687] Status update TASK_RUNNING (UUID: cf7921a3-0f46-4d60-bb83-c71a97b2b6f8) for task 1 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000 from slave f9d7159c-0e99-4adc-8090-abc4b60b98f5-S0 at slave(3)@172.17.0.2:34597 (be9bd3a71092)
I1221 17:49:02.669858 32601 master.cpp:4735] Forwarding status update TASK_RUNNING (UUID: cf7921a3-0f46-4d60-bb83-c71a97b2b6f8) for task 1 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000
I1221 17:49:02.669975 32606 slave.cpp:3183] Status update manager successfully handled status update TASK_RUNNING (UUID: cf7921a3-0f46-4d60-bb83-c71a97b2b6f8) for task 1 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000
I1221 17:49:02.670042 32606 slave.cpp:3199] Sending acknowledgement for status update TASK_RUNNING (UUID: cf7921a3-0f46-4d60-bb83-c71a97b2b6f8) for task 1 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000 to executor(1)@172.17.0.2:40805
I1221 17:49:02.670083 32601 master.cpp:6347] Updating the state of task 1 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000 (latest state: TASK_RUNNING, status update state: TASK_RUNNING)
I1221 17:49:02.670706 32605 status_update_manager.cpp:824] Checkpointing UPDATE for status update TASK_RUNNING (UUID: 7933b468-ff47-4ec3-95a2-4ac8c5396d7b) for task 0 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000
I1221 17:49:02.670958   312 exec.cpp:341] Executor received status update acknowledgement cf7921a3-0f46-4d60-bb83-c71a97b2b6f8 for task 1 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000
I1221 17:49:02.671279 32601 no_executor_framework.cpp:160] Task '1' is in state TASK_RUNNING
I1221 17:49:02.671316 32601 sched.cpp:919] Scheduler::statusUpdate took 44078ns
I1221 17:49:02.671849 32605 status_update_manager.cpp:374] Forwarding update TASK_RUNNING (UUID: 7933b468-ff47-4ec3-95a2-4ac8c5396d7b) for task 0 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000 to the slave
I1221 17:49:02.671898 32596 master.cpp:3844] Processing ACKNOWLEDGE call cf7921a3-0f46-4d60-bb83-c71a97b2b6f8 for task 1 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000 (No Executor Framework) at scheduler-bf375f84-4668-4cc7-9788-476575858f38@172.17.0.2:34597 on slave f9d7159c-0e99-4adc-8090-abc4b60b98f5-S0
I1221 17:49:02.672727 32596 slave.cpp:3289] Forwarding the update TASK_RUNNING (UUID: 7933b468-ff47-4ec3-95a2-4ac8c5396d7b) for task 0 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000 to master@172.17.0.2:34597
I1221 17:49:02.672931 32596 slave.cpp:3183] Status update manager successfully handled status update TASK_RUNNING (UUID: 7933b468-ff47-4ec3-95a2-4ac8c5396d7b) for task 0 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000
I1221 17:49:02.672984 32596 slave.cpp:3199] Sending acknowledgement for status update TASK_RUNNING (UUID: 7933b468-ff47-4ec3-95a2-4ac8c5396d7b) for task 0 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000 to executor(1)@172.17.0.2:41236
I1221 17:49:02.673074 32601 master.cpp:4687] Status update TASK_RUNNING (UUID: 7933b468-ff47-4ec3-95a2-4ac8c5396d7b) for task 0 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000 from slave f9d7159c-0e99-4adc-8090-abc4b60b98f5-S0 at slave(3)@172.17.0.2:34597 (be9bd3a71092)
I1221 17:49:02.673126 32601 master.cpp:4735] Forwarding status update TASK_RUNNING (UUID: 7933b468-ff47-4ec3-95a2-4ac8c5396d7b) for task 0 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000
I1221 17:49:02.673251 32605 status_update_manager.cpp:320] Received status update TASK_RUNNING (UUID: c872f4b3-c7cd-43d7-949c-34a4d92a6c44) for task 2 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000
I1221 17:49:02.673301 32601 master.cpp:6347] Updating the state of task 0 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000 (latest state: TASK_RUNNING, status update state: TASK_RUNNING)
I1221 17:49:02.673322 32605 status_update_manager.cpp:497] Creating StatusUpdate stream for task 2 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000
I1221 17:49:02.673393 32600 no_executor_framework.cpp:160] Task '0' is in state TASK_RUNNING
I1221 17:49:02.673431 32600 sched.cpp:919] Scheduler::statusUpdate took 44557ns
I1221 17:49:02.673645 32596 master.cpp:3844] Processing ACKNOWLEDGE call 7933b468-ff47-4ec3-95a2-4ac8c5396d7b for task 0 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000 (No Executor Framework) at scheduler-bf375f84-4668-4cc7-9788-476575858f38@172.17.0.2:34597 on slave f9d7159c-0e99-4adc-8090-abc4b60b98f5-S0
I1221 17:49:02.674245 32605 status_update_manager.cpp:824] Checkpointing UPDATE for status update TASK_RUNNING (UUID: c872f4b3-c7cd-43d7-949c-34a4d92a6c44) for task 2 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000
I1221 17:49:02.674623 32763 exec.cpp:341] Executor received status update acknowledgement 7933b468-ff47-4ec3-95a2-4ac8c5396d7b for task 0 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000
I1221 17:49:02.675426 32605 status_update_manager.cpp:374] Forwarding update TASK_RUNNING (UUID: c872f4b3-c7cd-43d7-949c-34a4d92a6c44) for task 2 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000 to the slave
I1221 17:49:02.675943 32601 slave.cpp:3289] Forwarding the update TASK_RUNNING (UUID: c872f4b3-c7cd-43d7-949c-34a4d92a6c44) for task 2 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000 to master@172.17.0.2:34597
I1221 17:49:02.676070 32605 status_update_manager.cpp:392] Received status update acknowledgement (UUID: cf7921a3-0f46-4d60-bb83-c71a97b2b6f8) for task 1 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000
I1221 17:49:02.676157 32601 slave.cpp:3183] Status update manager successfully handled status update TASK_RUNNING (UUID: c872f4b3-c7cd-43d7-949c-34a4d92a6c44) for task 2 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000
I1221 17:49:02.676205 32601 slave.cpp:3199] Sending acknowledgement for status update TASK_RUNNING (UUID: c872f4b3-c7cd-43d7-949c-34a4d92a6c44) for task 2 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000 to executor(1)@172.17.0.2:44314
I1221 17:49:02.676233 32605 status_update_manager.cpp:824] Checkpointing ACK for status update TASK_RUNNING (UUID: cf7921a3-0f46-4d60-bb83-c71a97b2b6f8) for task 1 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000
I1221 17:49:02.676892 32591 master.cpp:4687] Status update TASK_RUNNING (UUID: c872f4b3-c7cd-43d7-949c-34a4d92a6c44) for task 2 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000 from slave f9d7159c-0e99-4adc-8090-abc4b60b98f5-S0 at slave(3)@172.17.0.2:34597 (be9bd3a71092)
I1221 17:49:02.677039 32591 master.cpp:4735] Forwarding status update TASK_RUNNING (UUID: c872f4b3-c7cd-43d7-949c-34a4d92a6c44) for task 2 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000
I1221 17:49:02.677346 32591 master.cpp:6347] Updating the state of task 2 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000 (latest state: TASK_RUNNING, status update state: TASK_RUNNING)
I1221 17:49:02.677573 32596 no_executor_framework.cpp:160] Task '2' is in state TASK_RUNNING
I1221 17:49:02.677602 32596 sched.cpp:919] Scheduler::statusUpdate took 32264ns
I1221 17:49:02.677616 32605 status_update_manager.cpp:392] Received status update acknowledgement (UUID: 7933b468-ff47-4ec3-95a2-4ac8c5396d7b) for task 0 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000
I1221 17:49:02.677805 32596 master.cpp:3844] Processing ACKNOWLEDGE call c872f4b3-c7cd-43d7-949c-34a4d92a6c44 for task 2 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000 (No Executor Framework) at scheduler-bf375f84-4668-4cc7-9788-476575858f38@172.17.0.2:34597 on slave f9d7159c-0e99-4adc-8090-abc4b60b98f5-S0
I1221 17:49:02.678067 32605 status_update_manager.cpp:824] Checkpointing ACK for status update TASK_RUNNING (UUID: 7933b468-ff47-4ec3-95a2-4ac8c5396d7b) for task 0 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000
I1221 17:49:02.678205 32601 slave.cpp:2348] Status update manager successfully handled status update acknowledgement (UUID: cf7921a3-0f46-4d60-bb83-c71a97b2b6f8) for task 1 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000
I1221 17:49:02.678587   326 exec.cpp:341] Executor received status update acknowledgement c872f4b3-c7cd-43d7-949c-34a4d92a6c44 for task 2 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000
WARNING: Logging before InitGoogleLogging() is written to STDERR
I1221 17:49:02.679698 32717 process.cpp:998] libprocess is initialized on 172.17.0.2:60087 for 16 cpus
I1221 17:49:02.680353 32592 slave.cpp:2348] Status update manager successfully handled status update acknowledgement (UUID: 7933b468-ff47-4ec3-95a2-4ac8c5396d7b) for task 0 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000
I1221 17:49:02.680680 32605 status_update_manager.cpp:392] Received status update acknowledgement (UUID: c872f4b3-c7cd-43d7-949c-34a4d92a6c44) for task 2 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000
I1221 17:49:02.681006 32605 status_update_manager.cpp:824] Checkpointing ACK for status update TASK_RUNNING (UUID: c872f4b3-c7cd-43d7-949c-34a4d92a6c44) for task 2 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000
I1221 17:49:02.681891 32602 slave.cpp:2348] Status update manager successfully handled status update acknowledgement (UUID: c872f4b3-c7cd-43d7-949c-34a4d92a6c44) for task 2 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000
I1221 17:49:02.682057   357 exec.cpp:184] Executor started at: executor(1)@172.17.0.2:52442 with pid 32725
I1221 17:49:02.682662 32717 logging.cpp:172] Logging to STDERR
I1221 17:49:02.684780 32593 slave.cpp:2578] Got registration for executor '4' of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000 from executor(1)@172.17.0.2:52442
I1221 17:49:02.685559 32593 slave.cpp:2664] Checkpointing executor pid 'executor(1)@172.17.0.2:52442' to '/tmp/mesos-Alzbfe/2/meta/slaves/f9d7159c-0e99-4adc-8090-abc4b60b98f5-S0/frameworks/f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000/executors/4/runs/45dd8e22-074e-4dcd-b3f8-4074526eb23b/pids/libprocess.pid'
I1221 17:49:02.686137 32717 exec.cpp:134] Version: 0.27.0
I1221 17:49:02.687831   350 exec.cpp:208] Executor registered on slave f9d7159c-0e99-4adc-8090-abc4b60b98f5-S0
I1221 17:49:02.688374 32593 slave.cpp:1796] Sending queued task '4' to executor '4' of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000 at executor(1)@172.17.0.2:52442
I1221 17:49:02.692977   350 exec.cpp:220] Executor::registered took 480666ns
Registered executor on be9bd3a71092
I1221 17:49:02.693589   350 exec.cpp:295] Executor asked to run task '4'
I1221 17:49:02.693708   350 exec.cpp:304] Executor::launchTask took 90675ns
Starting task 4
I1221 17:49:02.696471   366 exec.cpp:184] Executor started at: executor(1)@172.17.0.2:60087 with pid 32717
sh -c 'echo hello'
Forked command at 379
I1221 17:49:02.698496   350 exec.cpp:517] Executor sending status update TASK_RUNNING (UUID: 0503112a-acd1-41ef-8f1e-423f26af8554) for task 4 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000
hello
I1221 17:49:02.700582 32591 slave.cpp:2937] Handling status update TASK_RUNNING (UUID: 0503112a-acd1-41ef-8f1e-423f26af8554) for task 4 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000 from executor(1)@172.17.0.2:52442
I1221 17:49:02.701061 32591 status_update_manager.cpp:320] Received status update TASK_RUNNING (UUID: 0503112a-acd1-41ef-8f1e-423f26af8554) for task 4 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000
I1221 17:49:02.701114 32591 status_update_manager.cpp:497] Creating StatusUpdate stream for task 4 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000
I1221 17:49:02.701758 32591 status_update_manager.cpp:824] Checkpointing UPDATE for status update TASK_RUNNING (UUID: 0503112a-acd1-41ef-8f1e-423f26af8554) for task 4 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000
I1221 17:49:02.702018 32599 slave.cpp:2578] Got registration for executor '3' of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000 from executor(1)@172.17.0.2:60087
I1221 17:49:02.702574 32599 slave.cpp:2664] Checkpointing executor pid 'executor(1)@172.17.0.2:60087' to '/tmp/mesos-Alzbfe/2/meta/slaves/f9d7159c-0e99-4adc-8090-abc4b60b98f5-S0/frameworks/f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000/executors/3/runs/d18f1e1f-a36e-4262-a8de-8857f8806415/pids/libprocess.pid'
I1221 17:49:02.703951 32591 status_update_manager.cpp:374] Forwarding update TASK_RUNNING (UUID: 0503112a-acd1-41ef-8f1e-423f26af8554) for task 4 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000 to the slave
I1221 17:49:02.704859 32595 slave.cpp:3289] Forwarding the update TASK_RUNNING (UUID: 0503112a-acd1-41ef-8f1e-423f26af8554) for task 4 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000 to master@172.17.0.2:34597
I1221 17:49:02.705049 32595 slave.cpp:3183] Status update manager successfully handled status update TASK_RUNNING (UUID: 0503112a-acd1-41ef-8f1e-423f26af8554) for task 4 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000
I1221 17:49:02.705101 32595 slave.cpp:3199] Sending acknowledgement for status update TASK_RUNNING (UUID: 0503112a-acd1-41ef-8f1e-423f26af8554) for task 4 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000 to executor(1)@172.17.0.2:52442
I1221 17:49:02.705574 32595 master.cpp:4687] Status update TASK_RUNNING (UUID: 0503112a-acd1-41ef-8f1e-423f26af8554) for task 4 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000 from slave f9d7159c-0e99-4adc-8090-abc4b60b98f5-S0 at slave(3)@172.17.0.2:34597 (be9bd3a71092)
I1221 17:49:02.705628 32595 master.cpp:4735] Forwarding status update TASK_RUNNING (UUID: 0503112a-acd1-41ef-8f1e-423f26af8554) for task 4 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000
I1221 17:49:02.705811 32595 master.cpp:6347] Updating the state of task 4 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000 (latest state: TASK_RUNNING, status update state: TASK_RUNNING)
I1221 17:49:02.705986 32595 no_executor_framework.cpp:160] Task '4' is in state TASK_RUNNING
I1221 17:49:02.706015 32595 sched.cpp:919] Scheduler::statusUpdate took 34916ns
I1221 17:49:02.706490 32606 slave.cpp:1796] Sending queued task '3' to executor '3' of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000 at executor(1)@172.17.0.2:60087
I1221 17:49:02.707698 32597 master.cpp:3844] Processing ACKNOWLEDGE call 0503112a-acd1-41ef-8f1e-423f26af8554 for task 4 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000 (No Executor Framework) at scheduler-bf375f84-4668-4cc7-9788-476575858f38@172.17.0.2:34597 on slave f9d7159c-0e99-4adc-8090-abc4b60b98f5-S0
I1221 17:49:02.708030   344 exec.cpp:341] Executor received status update acknowledgement 0503112a-acd1-41ef-8f1e-423f26af8554 for task 4 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000
I1221 17:49:02.708132 32596 status_update_manager.cpp:392] Received status update acknowledgement (UUID: 0503112a-acd1-41ef-8f1e-423f26af8554) for task 4 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000
II1221 17:49:02.708384 32596 status_update_manager.cpp:824] Checkpointing ACK for status update TASK_RUNNING (UUID: 0503112a-acd1-41ef-8f1e-423f26af8554) for task 4 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000
1221 17:49:02.708255   373 exec.cpp:208] Executor registered on slave f9d7159c-0e99-4adc-8090-abc4b60b98f5-S0
I1221 17:49:02.709542 32596 slave.cpp:2348] Status update manager successfully handled status update acknowledgement (UUID: 0503112a-acd1-41ef-8f1e-423f26af8554) for task 4 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000
IRegistered executor on be9bd3a71092
1221 17:49:02.711908   373 exec.cpp:220] Executor::registered took 462491ns
I1221 17:49:02.712682   373 exec.cpp:295] Executor asked to run task '3'
IStarting task 3
1221 17:49:02.712882   373 exec.cpp:304] Executor::launchTask took 98612ns
Forked command at 380
sh -c 'echo hello'
hello
I1221 17:49:02.719938   375 exec.cpp:517] Executor sending status update TASK_RUNNING (UUID: 8afc2a21-fcf7-4474-88d0-124947945fb9) for task 3 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000
I1221 17:49:02.721655 32596 slave.cpp:2937] Handling status update TASK_RUNNING (UUID: 8afc2a21-fcf7-4474-88d0-124947945fb9) for task 3 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000 from executor(1)@172.17.0.2:60087
I1221 17:49:02.722296 32596 status_update_manager.cpp:320] Received status update TASK_RUNNING (UUID: 8afc2a21-fcf7-4474-88d0-124947945fb9) for task 3 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000
I1221 17:49:02.722362 32596 status_update_manager.cpp:497] Creating StatusUpdate stream for task 3 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000
I1221 17:49:02.723229 32596 status_update_manager.cpp:824] Checkpointing UPDATE for status update TASK_RUNNING (UUID: 8afc2a21-fcf7-4474-88d0-124947945fb9) for task 3 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000
I1221 17:49:02.724555 32596 status_update_manager.cpp:374] Forwarding update TASK_RUNNING (UUID: 8afc2a21-fcf7-4474-88d0-124947945fb9) for task 3 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000 to the slave
I1221 17:49:02.725309 32597 slave.cpp:3289] Forwarding the update TASK_RUNNING (UUID: 8afc2a21-fcf7-4474-88d0-124947945fb9) for task 3 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000 to master@172.17.0.2:34597
I1221 17:49:02.725697 32597 slave.cpp:3183] Status update manager successfully handled status update TASK_RUNNING (UUID: 8afc2a21-fcf7-4474-88d0-124947945fb9) for task 3 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000
I1221 17:49:02.725751 32597 slave.cpp:3199] Sending acknowledgement for status update TASK_RUNNING (UUID: 8afc2a21-fcf7-4474-88d0-124947945fb9) for task 3 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000 to executor(1)@172.17.0.2:60087
I1221 17:49:02.725988 32596 master.cpp:4687] Status update TASK_RUNNING (UUID: 8afc2a21-fcf7-4474-88d0-124947945fb9) for task 3 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000 from slave f9d7159c-0e99-4adc-8090-abc4b60b98f5-S0 at slave(3)@172.17.0.2:34597 (be9bd3a71092)
I1221 17:49:02.726114 32596 master.cpp:4735] Forwarding status update TASK_RUNNING (UUID: 8afc2a21-fcf7-4474-88d0-124947945fb9) for task 3 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000
I1221 17:49:02.726605 32603 no_executor_framework.cpp:160] Task '3' is in state TASK_RUNNING
I1221 17:49:02.726634 32603 sched.cpp:919] Scheduler::statusUpdate took 49701ns
I1221 17:49:02.726335 32596 master.cpp:6347] Updating the state of task 3 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000 (latest state: TASK_RUNNING, status update state: TASK_RUNNING)
I1221 17:49:02.726938 32596 master.cpp:3844] Processing ACKNOWLEDGE call 8afc2a21-fcf7-4474-88d0-124947945fb9 for task 3 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000 (No Executor Framework) at scheduler-bf375f84-4668-4cc7-9788-476575858f38@172.17.0.2:34597 on slave f9d7159c-0e99-4adc-8090-abc4b60b98f5-S0
I1221 17:49:02.727350 32596 status_update_manager.cpp:392] Received status update acknowledgement (UUID: 8afc2a21-fcf7-4474-88d0-124947945fb9) for task 3 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000
I1221 17:49:02.727682 32596 status_update_manager.cpp:824] Checkpointing ACK for status update TASK_RUNNING (UUID: 8afc2a21-fcf7-4474-88d0-124947945fb9) for task 3 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000
I1221 17:49:02.728458   367 exec.cpp:341] Executor received status update acknowledgement 8afc2a21-fcf7-4474-88d0-124947945fb9 for task 3 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000
I1221 17:49:02.728891 32596 slave.cpp:2348] Status update manager successfully handled status update acknowledgement (UUID: 8afc2a21-fcf7-4474-88d0-124947945fb9) for task 3 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000
Command exited with status 0 (pid: 343)
Command exited with status 0 (pid: 342)
I1221 17:49:02.754735   311 exec.cpp:517] Executor sending status update TASK_FINISHED (UUID: 104773bb-c486-4864-aa71-37fa14c444bb) for task 1 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000
I1221 17:49:02.755300   307 exec.cpp:517] Executor sending status update TASK_FINISHED (UUID: b6352200-525b-4124-a654-2ab82cdf2af2) for task 0 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000
I1221 17:49:02.756655 32596 slave.cpp:2937] Handling status update TASK_FINISHED (UUID: 104773bb-c486-4864-aa71-37fa14c444bb) for task 1 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000 from executor(1)@172.17.0.2:40805
I1221 17:49:02.756892 32596 slave.cpp:5520] Terminating task 1
I1221 17:49:02.759013 32592 slave.cpp:2937] Handling status update TASK_FINISHED (UUID: b6352200-525b-4124-a654-2ab82cdf2af2) for task 0 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000 from executor(1)@172.17.0.2:41236
I1221 17:49:02.759171 32592 slave.cpp:5520] Terminating task 0
I1221 17:49:02.759883 32592 status_update_manager.cpp:320] Received status update TASK_FINISHED (UUID: 104773bb-c486-4864-aa71-37fa14c444bb) for task 1 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000
I1221 17:49:02.759965 32592 status_update_manager.cpp:824] Checkpointing UPDATE for status update TASK_FINISHED (UUID: 104773bb-c486-4864-aa71-37fa14c444bb) for task 1 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000
I1221 17:49:02.764642 32592 status_update_manager.cpp:374] Forwarding update TASK_FINISHED (UUID: 104773bb-c486-4864-aa71-37fa14c444bb) for task 1 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000 to the slave
I1221 17:49:02.765254 32592 status_update_manager.cpp:320] Received status update TASK_FINISHED (UUID: b6352200-525b-4124-a654-2ab82cdf2af2) for task 0 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000
I1221 17:49:02.765331 32592 status_update_manager.cpp:824] Checkpointing UPDATE for status update TASK_FINISHED (UUID: b6352200-525b-4124-a654-2ab82cdf2af2) for task 0 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000
I1221 17:49:02.765532 32604 slave.cpp:3289] Forwarding the update TASK_FINISHED (UUID: 104773bb-c486-4864-aa71-37fa14c444bb) for task 1 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000 to master@172.17.0.2:34597
I1221 17:49:02.765739 32604 slave.cpp:3183] Status update manager successfully handled status update TASK_FINISHED (UUID: 104773bb-c486-4864-aa71-37fa14c444bb) for task 1 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000
I1221 17:49:02.765794 32604 slave.cpp:3199] Sending acknowledgement for status update TASK_FINISHED (UUID: 104773bb-c486-4864-aa71-37fa14c444bb) for task 1 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000 to executor(1)@172.17.0.2:40805
I1221 17:49:02.766365 32604 master.cpp:4687] Status update TASK_FINISHED (UUID: 104773bb-c486-4864-aa71-37fa14c444bb) for task 1 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000 from slave f9d7159c-0e99-4adc-8090-abc4b60b98f5-S0 at slave(3)@172.17.0.2:34597 (be9bd3a71092)
I1221 17:49:02.766440 32604 master.cpp:4735] Forwarding status update TASK_FINISHED (UUID: 104773bb-c486-4864-aa71-37fa14c444bb) for task 1 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000
I1221 17:49:02.766636 32604 master.cpp:6347] Updating the state of task 1 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000 (latest state: TASK_FINISHED, status update state: TASK_FINISHED)
Command exited with status 0 (pid: 361)
I1221 17:49:02.767107 32604 no_executor_framework.cpp:160] Task '1' is in state TASK_FINISHED
I1221 17:49:02.767150 32604 sched.cpp:919] Scheduler::statusUpdate took 49238ns
II1221 17:49:02.767559 32606 master.cpp:3844] Processing ACKNOWLEDGE call 104773bb-c486-4864-aa71-37fa14c444bb for task 1 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000 (No Executor Framework) at scheduler-bf375f84-4668-4cc7-9788-476575858f38@172.17.0.2:34597 on slave f9d7159c-0e99-4adc-8090-abc4b60b98f5-S0
1221 17:49:02.767462   323 exec.cpp:341] Executor received status update acknowledgement 104773bb-c486-4864-aa71-37fa14c444bb for task 1 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000
I1221 17:49:02.767626 32606 master.cpp:6413] Removing task 1 with resources cpus(*):0.1; mem(*):32; disk(*):32 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000 on slave f9d7159c-0e99-4adc-8090-abc4b60b98f5-S0 at slave(3)@172.17.0.2:34597 (be9bd3a71092)
I1221 17:49:02.768234 32592 status_update_manager.cpp:374] Forwarding update TASK_FINISHED (UUID: b6352200-525b-4124-a654-2ab82cdf2af2) for task 0 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000 to the slave
I1221 17:49:02.768561 32604 hierarchical.cpp:880] Recovered cpus(*):0.1; mem(*):32; disk(*):32 (total: cpus(*):2; mem(*):10240; disk(*):3.70122e+06; ports(*):[31000-32000], allocated: cpus(*):0.4; mem(*):128; disk(*):128) on slave f9d7159c-0e99-4adc-8090-abc4b60b98f5-S0 from framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000
I1221 17:49:02.768721 32592 status_update_manager.cpp:392] Received status update acknowledgement (UUID: 104773bb-c486-4864-aa71-37fa14c444bb) for task 1 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000
I1221 17:49:02.768883 32592 status_update_manager.cpp:824] Checkpointing ACK for status update TASK_FINISHED (UUID: 104773bb-c486-4864-aa71-37fa14c444bb) for task 1 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000
I1221 17:49:02.768705 32604 slave.cpp:3289] Forwarding the update TASK_FINISHED (UUID: b6352200-525b-4124-a654-2ab82cdf2af2) for task 0 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000 to master@172.17.0.2:34597
I1221 17:49:02.769352   336 exec.cpp:517] Executor sending status update TASK_FINISHED (UUID: 6bdc8f1d-5969-495a-b1dc-c8d1d17df8fd) for task 2 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000
I1221 17:49:02.769563 32604 slave.cpp:3183] Status update manager successfully handled status update TASK_FINISHED (UUID: b6352200-525b-4124-a654-2ab82cdf2af2) for task 0 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000
I1221 17:49:02.769613 32604 slave.cpp:3199] Sending acknowledgement for status update TASK_FINISHED (UUID: b6352200-525b-4124-a654-2ab82cdf2af2) for task 0 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000 to executor(1)@172.17.0.2:41236
I1221 17:49:02.769758 32601 master.cpp:4687] Status update TASK_FINISHED (UUID: b6352200-525b-4124-a654-2ab82cdf2af2) for task 0 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000 from slave f9d7159c-0e99-4adc-8090-abc4b60b98f5-S0 at slave(3)@172.17.0.2:34597 (be9bd3a71092)
I1221 17:49:02.769803 32601 master.cpp:4735] Forwarding status update TASK_FINISHED (UUID: b6352200-525b-4124-a654-2ab82cdf2af2) for task 0 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000
I1221 17:49:02.769979 32601 master.cpp:6347] Updating the state of task 0 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000 (latest state: TASK_FINISHED, status update state: TASK_FINISHED)
I1221 17:49:02.770455 32598 no_executor_framework.cpp:160] Task '0' is in state TASK_FINISHED
I1221 17:49:02.770493 32598 sched.cpp:919] Scheduler::statusUpdate took 62688ns
I1221 17:49:02.770823 32602 hierarchical.cpp:880] Recovered cpus(*):0.1; mem(*):32; disk(*):32 (total: cpus(*):2; mem(*):10240; disk(*):3.70122e+06; ports(*):[31000-32000], allocated: cpus(*):0.3; mem(*):96; disk(*):96) on slave f9d7159c-0e99-4adc-8090-abc4b60b98f5-S0 from framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000
I1221 17:49:02.771128 32598 master.cpp:3844] Processing ACKNOWLEDGE call b6352200-525b-4124-a654-2ab82cdf2af2 for task 0 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000 (No Executor Framework) at scheduler-bf375f84-4668-4cc7-9788-476575858f38@172.17.0.2:34597 on slave f9d7159c-0e99-4adc-8090-abc4b60b98f5-S0
I1221 17:49:02.771245 32598 master.cpp:6413] Removing task 0 with resources cpus(*):0.1; mem(*):32; disk(*):32 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000 on slave f9d7159c-0e99-4adc-8090-abc4b60b98f5-S0 at slave(3)@172.17.0.2:34597 (be9bd3a71092)
I1221 17:49:02.770465 32592 status_update_manager.cpp:528] Cleaning up status update stream for task 1 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000
I1221 17:49:02.771577   301 exec.cpp:341] Executor received status update acknowledgement b6352200-525b-4124-a654-2ab82cdf2af2 for task 0 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000
I1221 17:49:02.771955 32592 status_update_manager.cpp:392] Received status update acknowledgement (UUID: b6352200-525b-4124-a654-2ab82cdf2af2) for task 0 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000
I1221 17:49:02.772097 32592 status_update_manager.cpp:824] Checkpointing ACK for status update TASK_FINISHED (UUID: b6352200-525b-4124-a654-2ab82cdf2af2) for task 0 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000
I1221 17:49:02.772390 32591 slave.cpp:2937] Handling status update TASK_FINISHED (UUID: 6bdc8f1d-5969-495a-b1dc-c8d1d17df8fd) for task 2 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000 from executor(1)@172.17.0.2:44314
I1221 17:49:02.772554 32591 slave.cpp:5520] Terminating task 2
I1221 17:49:02.772975 32591 slave.cpp:2348] Status update manager successfully handled status update acknowledgement (UUID: 104773bb-c486-4864-aa71-37fa14c444bb) for task 1 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000
I1221 17:49:02.773069 32591 slave.cpp:5561] Completing task 1
I1221 17:49:02.774068 32592 status_update_manager.cpp:528] Cleaning up status update stream for task 0 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000
I1221 17:49:02.774581 32592 slave.cpp:2348] Status update manager successfully handled status update acknowledgement (UUID: b6352200-525b-4124-a654-2ab82cdf2af2) for task 0 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000
I1221 17:49:02.774747 32592 slave.cpp:5561] Completing task 0
I1221 17:49:02.775277 32595 status_update_manager.cpp:320] Received status update TASK_FINISHED (UUID: 6bdc8f1d-5969-495a-b1dc-c8d1d17df8fd) for task 2 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000
I1221 17:49:02.775414 32595 status_update_manager.cpp:824] Checkpointing UPDATE for status update TASK_FINISHED (UUID: 6bdc8f1d-5969-495a-b1dc-c8d1d17df8fd) for task 2 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000
I1221 17:49:02.778640 32595 status_update_manager.cpp:374] Forwarding update TASK_FINISHED (UUID: 6bdc8f1d-5969-495a-b1dc-c8d1d17df8fd) for task 2 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000 to the slave
I1221 17:49:02.779016 32592 slave.cpp:3289] Forwarding the update TASK_FINISHED (UUID: 6bdc8f1d-5969-495a-b1dc-c8d1d17df8fd) for task 2 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000 to master@172.17.0.2:34597
I1221 17:49:02.779219 32592 slave.cpp:3183] Status update manager successfully handled status update TASK_FINISHED (UUID: 6bdc8f1d-5969-495a-b1dc-c8d1d17df8fd) for task 2 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000
I1221 17:49:02.779268 32592 slave.cpp:3199] Sending acknowledgement for status update TASK_FINISHED (UUID: 6bdc8f1d-5969-495a-b1dc-c8d1d17df8fd) for task 2 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000 to executor(1)@172.17.0.2:44314
I1221 17:49:02.779527 32595 master.cpp:4687] Status update TASK_FINISHED (UUID: 6bdc8f1d-5969-495a-b1dc-c8d1d17df8fd) for task 2 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000 from slave f9d7159c-0e99-4adc-8090-abc4b60b98f5-S0 at slave(3)@172.17.0.2:34597 (be9bd3a71092)
I1221 17:49:02.779698 32595 master.cpp:4735] Forwarding status update TASK_FINISHED (UUID: 6bdc8f1d-5969-495a-b1dc-c8d1d17df8fd) for task 2 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000
I1221 17:49:02.779952 32595 master.cpp:6347] Updating the state of task 2 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000 (latest state: TASK_FINISHED, status update state: TASK_FINISHED)
I1221 17:49:02.780423 32595 no_executor_framework.cpp:160] Task '2' is in state TASK_FINISHED
I1221 17:49:02.780583   338 exec.cpp:341] Executor received status update acknowledgement 6bdc8f1d-5969-495a-b1dc-c8d1d17df8fd for task 2 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000
I1221 17:49:02.780850 32595 sched.cpp:919] Scheduler::statusUpdate took 432956ns
I1221 17:49:02.781097 32591 hierarchical.cpp:880] Recovered cpus(*):0.1; mem(*):32; disk(*):32 (total: cpus(*):2; mem(*):10240; disk(*):3.70122e+06; ports(*):[31000-32000], allocated: cpus(*):0.2; mem(*):64; disk(*):64) on slave f9d7159c-0e99-4adc-8090-abc4b60b98f5-S0 from framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000
I1221 17:49:02.781183 32601 master.cpp:3844] Processing ACKNOWLEDGE call 6bdc8f1d-5969-495a-b1dc-c8d1d17df8fd for task 2 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000 (No Executor Framework) at scheduler-bf375f84-4668-4cc7-9788-476575858f38@172.17.0.2:34597 on slave f9d7159c-0e99-4adc-8090-abc4b60b98f5-S0
I1221 17:49:02.781383 32601 master.cpp:6413] Removing task 2 with resources cpus(*):0.1; mem(*):32; disk(*):32 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000 on slave f9d7159c-0e99-4adc-8090-abc4b60b98f5-S0 at slave(3)@172.17.0.2:34597 (be9bd3a71092)
I1221 17:49:02.782066 32601 status_update_manager.cpp:392] Received status update acknowledgement (UUID: 6bdc8f1d-5969-495a-b1dc-c8d1d17df8fd) for task 2 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000
I1221 17:49:02.782419 32601 status_update_manager.cpp:824] Checkpointing ACK for status update TASK_FINISHED (UUID: 6bdc8f1d-5969-495a-b1dc-c8d1d17df8fd) for task 2 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000
I1221 17:49:02.783466 32601 status_update_manager.cpp:528] Cleaning up status update stream for task 2 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000
I1221 17:49:02.784078 32602 slave.cpp:2348] Status update manager successfully handled status update acknowledgement (UUID: 6bdc8f1d-5969-495a-b1dc-c8d1d17df8fd) for task 2 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000
I1221 17:49:02.784178 32602 slave.cpp:5561] Completing task 2
Command exited with status 0 (pid: 379)
I1221 17:49:02.801833   358 exec.cpp:517] Executor sending status update TASK_FINISHED (UUID: e54772f7-bf6f-477c-a76d-00c874928fa0) for task 4 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000
I1221 17:49:02.803642 32601 slave.cpp:2937] Handling status update TASK_FINISHED (UUID: e54772f7-bf6f-477c-a76d-00c874928fa0) for task 4 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000 from executor(1)@172.17.0.2:52442
I1221 17:49:02.803870 32601 slave.cpp:5520] Terminating task 4
I1221 17:49:02.806363 32601 status_update_manager.cpp:320] Received status update TASK_FINISHED (UUID: e54772f7-bf6f-477c-a76d-00c874928fa0) for task 4 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000
I1221 17:49:02.806607 32601 status_update_manager.cpp:824] Checkpointing UPDATE for status update TASK_FINISHED (UUID: e54772f7-bf6f-477c-a76d-00c874928fa0) for task 4 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000
I1221 17:49:02.808573 32601 status_update_manager.cpp:374] Forwarding update TASK_FINISHED (UUID: e54772f7-bf6f-477c-a76d-00c874928fa0) for task 4 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000 to the slave
I1221 17:49:02.809147 32601 slave.cpp:3289] Forwarding the update TASK_FINISHED (UUID: e54772f7-bf6f-477c-a76d-00c874928fa0) for task 4 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000 to master@172.17.0.2:34597
I1221 17:49:02.809331 32601 slave.cpp:3183] Status update manager successfully handled status update TASK_FINISHED (UUID: e54772f7-bf6f-477c-a76d-00c874928fa0) for task 4 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000
I1221 17:49:02.809659 32593 master.cpp:4687] Status update TASK_FINISHED (UUID: e54772f7-bf6f-477c-a76d-00c874928fa0) for task 4 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000 from slave f9d7159c-0e99-4adc-8090-abc4b60b98f5-S0 at slave(3)@172.17.0.2:34597 (be9bd3a71092)
I1221 17:49:02.809711 32593 master.cpp:4735] Forwarding status update TASK_FINISHED (UUID: e54772f7-bf6f-477c-a76d-00c874928fa0) for task 4 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000
I1221 17:49:02.809882 32593 master.cpp:6347] Updating the state of task 4 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000 (latest state: TASK_FINISHED, status update state: TASK_FINISHED)
I1221 17:49:02.810335 32593 no_executor_framework.cpp:160] Task '4' is in state TASK_FINISHED
I1221 17:49:02.810369 32593 sched.cpp:919] Scheduler::statusUpdate took 40498ns
I1221 17:49:02.811712 32604 hierarchical.cpp:880] Recovered cpus(*):0.1; mem(*):32; disk(*):32 (total: cpus(*):2; mem(*):10240; disk(*):3.70122e+06; ports(*):[31000-32000], allocated: cpus(*):0.1; mem(*):32; disk(*):32) on slave f9d7159c-0e99-4adc-8090-abc4b60b98f5-S0 from framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000
I1221 17:49:02.812052 32593 master.cpp:3844] Processing ACKNOWLEDGE call e54772f7-bf6f-477c-a76d-00c874928fa0 for task 4 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000 (No Executor Framework) at scheduler-bf375f84-4668-4cc7-9788-476575858f38@172.17.0.2:34597 on slave f9d7159c-0e99-4adc-8090-abc4b60b98f5-S0
I1221 17:49:02.812126 32593 master.cpp:6413] Removing task 4 with resources cpus(*):0.1; mem(*):32; disk(*):32 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000 on slave f9d7159c-0e99-4adc-8090-abc4b60b98f5-S0 at slave(3)@172.17.0.2:34597 (be9bd3a71092)
I1221 17:49:02.809386 32601 slave.cpp:3199] Sending acknowledgement for status update TASK_FINISHED (UUID: e54772f7-bf6f-477c-a76d-00c874928fa0) for task 4 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000 to executor(1)@172.17.0.2:52442
I1221 17:49:02.813179 32601 status_update_manager.cpp:392] Received status update acknowledgement (UUID: e54772f7-bf6f-477c-a76d-00c874928fa0) for task 4 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000
I1221 17:49:02.813343 32601 status_update_manager.cpp:824] Checkpointing ACK for status update TASK_FINISHED (UUID: e54772f7-bf6f-477c-a76d-00c874928fa0) for task 4 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000
I1221 17:49:02.813365   345 exec.cpp:341] Executor received status update acknowledgement e54772f7-bf6f-477c-a76d-00c874928fa0 for task 4 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000
I1221 17:49:02.815651 32601 status_update_manager.cpp:528] Cleaning up status update stream for task 4 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000
I1221 17:49:02.816185 32606 slave.cpp:2348] Status update manager successfully handled status update acknowledgement (UUID: e54772f7-bf6f-477c-a76d-00c874928fa0) for task 4 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000
I1221 17:49:02.816253 32606 slave.cpp:5561] Completing task 4
Command exited with status 0 (pid: 380)
I1221 17:49:02.821643   374 exec.cpp:517] Executor sending status update TASK_FINISHED (UUID: f7609151-4f05-4f8c-9fc1-98ee76dac298) for task 3 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000
I1221 17:49:02.823426 32601 slave.cpp:2937] Handling status update TASK_FINISHED (UUID: f7609151-4f05-4f8c-9fc1-98ee76dac298) for task 3 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000 from executor(1)@172.17.0.2:60087
I1221 17:49:02.823643 32601 slave.cpp:5520] Terminating task 3
I1221 17:49:02.825294 32602 status_update_manager.cpp:320] Received status update TASK_FINISHED (UUID: f7609151-4f05-4f8c-9fc1-98ee76dac298) for task 3 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000
I1221 17:49:02.825374 32602 status_update_manager.cpp:824] Checkpointing UPDATE for status update TASK_FINISHED (UUID: f7609151-4f05-4f8c-9fc1-98ee76dac298) for task 3 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000
I1221 17:49:02.827564 32602 status_update_manager.cpp:374] Forwarding update TASK_FINISHED (UUID: f7609151-4f05-4f8c-9fc1-98ee76dac298) for task 3 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000 to the slave
I1221 17:49:02.827869 32591 slave.cpp:3289] Forwarding the update TASK_FINISHED (UUID: f7609151-4f05-4f8c-9fc1-98ee76dac298) for task 3 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000 to master@172.17.0.2:34597
I1221 17:49:02.828033 32591 slave.cpp:3183] Status update manager successfully handled status update TASK_FINISHED (UUID: f7609151-4f05-4f8c-9fc1-98ee76dac298) for task 3 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000
I1221 17:49:02.828075 32591 slave.cpp:3199] Sending acknowledgement for status update TASK_FINISHED (UUID: f7609151-4f05-4f8c-9fc1-98ee76dac298) for task 3 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000 to executor(1)@172.17.0.2:60087
I1221 17:49:02.828188 32602 master.cpp:4687] Status update TASK_FINISHED (UUID: f7609151-4f05-4f8c-9fc1-98ee76dac298) for task 3 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000 from slave f9d7159c-0e99-4adc-8090-abc4b60b98f5-S0 at slave(3)@172.17.0.2:34597 (be9bd3a71092)
I1221 17:49:02.828230 32602 master.cpp:4735] Forwarding status update TASK_FINISHED (UUID: f7609151-4f05-4f8c-9fc1-98ee76dac298) for task 3 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000
I1221 17:49:02.828552 32606 no_executor_framework.cpp:160] Task '3' is in state TASK_FINISHED
I1221 17:49:02.828582 32606 sched.cpp:1803] Asked to stop the driver
I1221 17:49:02.828631 32606 sched.cpp:919] Scheduler::statusUpdate took 96503ns
I1221 17:49:02.828649 32606 sched.cpp:926] Not sending status update acknowledgment message because the driver is not running!
I1221 17:49:02.828696 32606 sched.cpp:1041] Stopping framework 'f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000'
I1221 17:49:02.828698 32602 master.cpp:6347] Updating the state of task 3 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000 (latest state: TASK_FINISHED, status update state: TASK_FINISHED)
I1221 17:49:02.829473 32567 sched.cpp:1803] Asked to stop the driver
I1221 17:49:02.829541 32567 sched.cpp:1806] Ignoring stop because the status of the driver is DRIVER_STOPPED
I1221 17:49:02.829524   372 exec.cpp:341] Executor received status update acknowledgement f7609151-4f05-4f8c-9fc1-98ee76dac298 for task 3 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000
I1221 17:49:02.829692 32602 master.cpp:5823] Processing TEARDOWN call for framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000 (No Executor Framework) at scheduler-bf375f84-4668-4cc7-9788-476575858f38@172.17.0.2:34597
I1221 17:49:02.829725 32602 master.cpp:5835] Removing framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000 (No Executor Framework) at scheduler-bf375f84-4668-4cc7-9788-476575858f38@172.17.0.2:34597
I1221 17:49:02.829743 32591 hierarchical.cpp:880] Recovered cpus(*):0.1; mem(*):32; disk(*):32 (total: cpus(*):2; mem(*):10240; disk(*):3.70122e+06; ports(*):[31000-32000], allocated: cpus(*):4.71845e-16) on slave f9d7159c-0e99-4adc-8090-abc4b60b98f5-S0 from framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000
I1221 17:49:02.829884 32591 hierarchical.cpp:366] Deactivated framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000
I1221 17:49:02.830040 32602 master.cpp:6347] Updating the state of task 3 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000 (latest state: TASK_FINISHED, status update state: TASK_KILLED)
I1221 17:49:02.830144 32591 slave.cpp:2012] Asked to shut down framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000 by master@172.17.0.2:34597
W1221 17:49:02.830174 32591 slave.cpp:2027] Cannot shut down unknown framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000
I1221 17:49:02.830214 32591 slave.cpp:2012] Asked to shut down framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000 by master@172.17.0.2:34597
W1221 17:49:02.830231 32591 slave.cpp:2027] Cannot shut down unknown framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000
I1221 17:49:02.830263 32591 slave.cpp:2012] Asked to shut down framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000 by master@172.17.0.2:34597
I1221 17:49:02.830288 32591 slave.cpp:2037] Shutting down framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000
I1221 17:49:02.830116 32602 master.cpp:6413] Removing task 3 with resources cpus(*):0.1; mem(*):32; disk(*):32 of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000 on slave f9d7159c-0e99-4adc-8090-abc4b60b98f5-S0 at slave(3)@172.17.0.2:34597 (be9bd3a71092)
I1221 17:49:02.830490 32591 slave.cpp:4060] Shutting down executor '0' of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000 at executor(1)@172.17.0.2:41236
I1221 17:49:02.830888 32591 slave.cpp:4060] Shutting down executor '1' of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000 at executor(1)@172.17.0.2:40805
I1221 17:49:02.831028 32602 master.cpp:930] Master terminating
I1221 17:49:02.831089 32591 slave.cpp:4060] Shutting down executor '2' of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000 at executor(1)@172.17.0.2:44314
I1221 17:49:02.831279 32591 slave.cpp:4060] Shutting down executor '3' of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000 at executor(1)@172.17.0.2:60087
I1221 17:49:02.831519   301 exec.cpp:381] Executor asked to shutdown
I1221 17:49:02.831696   301 exec.cpp:396] Executor::shutdown took 12788ns
I1221 17:49:02.831830   301 exec.cpp:80] Scheduling shutdown of the executor
I1221 17:49:02.831900 32595 hierarchical.cpp:321] Removed framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000
I1221 17:49:02.832252 32595 hierarchical.cpp:496] Removed slave f9d7159c-0e99-4adc-8090-abc4b60b98f5-S2
I1221 17:49:02.832567 32591 slave.cpp:4060] Shutting down executor '4' of framework f9d7159c-0e99-4adc-8090-abc4b60b98f5-0000 at executor(1)@172.17.0.2:52442
I1221 17:49:02.832967 32595 hierarchical.cpp:496] Removed slave f9d7159c-0e99-4adc-8090-abc4b60b98f5-S1
II1221 17:49:02.833258 32595 hierarchical.cpp:496] Removed slave f9d7159c-0e99-4adc-8090-abc4b60b98f5-S0
1221 17:49:02.833251   372 exec.cpp:381] Executor asked to shutdown
I1221 17:49:02.833380   372 exec.cpp:396] Executor::shutdown took 12276ns
I1221 17:49:02.833453   369 exec.cpp:80] Scheduling shutdown of the executor
I1221 17:49:02.833497   314 exec.cpp:381] Executor asked to shutdown
I1221 17:49:02.833633   314 exec.cpp:396] Executor::shutdown took 14634ns
I1221 17:49:02.833751   314 exec.cpp:80] Scheduling shutdown of the executor
I1221 17:49:02.834023   335 exec.cpp:381] Executor asked to shutdown
I1221 17:49:02.834144   335 exec.cpp:396] Executor::shutdown took 11303ns
I1221 17:49:02.834262   335 exec.cpp:80] Scheduling shutdown of the executor
I1221 17:49:02.834300   345 exec.cpp:381] Executor asked to shutdown
I1221 17:49:02.834462   345 exec.cpp:396] Executor::shutdown took 14263ns
I1221 17:49:02.834502   345 exec.cpp:80] Scheduling shutdown of the executor
I1221 17:49:02.834887 32602 slave.cpp:3417] master@172.17.0.2:34597 exited
W1221 17:49:02.834947 32602 slave.cpp:3420] Master disconnected! Waiting for a new master to be elected
I1221 17:49:02.835018 32602 slave.cpp:3417] master@172.17.0.2:34597 exited
W1221 17:49:02.835083 32602 slave.cpp:3420] Master disconnected! Waiting for a new master to be elected
I1221 17:49:02.835146 32602 slave.cpp:3417] master@172.17.0.2:34597 exited
W1221 17:49:02.835213 32602 slave.cpp:3420] Master disconnected! Waiting for a new master to be elected
I1221 17:49:02.839617 32593 slave.cpp:601] Slave terminating
I1221 17:49:02.845129 32567 slave.cpp:601] Slave terminating
I1221 17:49:02.847450 32603 slave.cpp:601] Slave terminating
I1221 17:49:02.879467   314 exec.cpp:444] Ignoring exited event because the driver is aborted!
I1221 17:49:02.879524   301 exec.cpp:444] Ignoring exited event because the driver is aborted!
I[       OK ] ExamplesTest.NoExecutorFramework (8717 ms)
[ RUN      ] ExamplesTest.EventCallFramework
I1221 17:49:02.881482   352 exec.cpp:444] Ignoring exited event because the driver is aborted!
Using temporary directory '/tmp/ExamplesTest_EventCallFramework_9TX8jH'
1221 17:49:02.881122   335 exec.cpp:444] Ignoring exited event because the driver is aborted!
I1221 17:49:02.882071   372 exec.cpp:444] Ignoring exited event because the driver is aborted!
Shutting down
Shutting down
Sending SIGTERM to process tree at pid 343
Sending SIGTERM to process tree at pid 342
Killing the following process trees:
[ 

]
Killing the following process trees:
[ 

]
Shutting down
Sending SIGTERM to process tree at pid 361
Killing the following process trees:
[ 

]
Shutting down
Sending SIGTERM to process tree at pid 379
Killing the following process trees:
[ 

]
Shutting down
Sending SIGTERM to process tree at pid 380
Killing the following process trees:
[ 

]
WARNING: Logging before InitGoogleLogging() is written to STDERR
I1221 17:49:05.786998   381 process.cpp:998] libprocess is initialized on 172.17.0.2:60254 for 16 cpus
I1221 17:49:05.787230   381 logging.cpp:172] Logging to STDERR
I1221 17:49:05.788791   381 scheduler.cpp:154] Version: 0.27.0
I1221 17:49:05.809587   381 leveldb.cpp:174] Opened db in 3.241758ms
I1221 17:49:05.811599   381 leveldb.cpp:181] Compacted db in 1.820335ms
I1221 17:49:05.811724   381 leveldb.cpp:196] Created db iterator in 74530ns
I1221 17:49:05.811749   381 leveldb.cpp:202] Seeked to beginning of db in 5247ns
I1221 17:49:05.811760   381 leveldb.cpp:271] Iterated through 0 keys in the db in 324ns
I1221 17:49:05.811980   381 replica.cpp:779] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned
I1221 17:49:05.816004   381 local.cpp:239] Using 'local' authorizer
I1221 17:49:05.818006   411 recover.cpp:447] Starting replica recovery
I1221 17:49:05.819463   406 recover.cpp:473] Replica is in EMPTY status
I1221 17:49:05.825531   413 replica.cpp:673] Replica in EMPTY status received a broadcasted recover request from (5)@172.17.0.2:60254
I1221 17:49:05.831326   412 recover.cpp:193] Received a recover response from a replica in EMPTY status
I1221 17:49:05.832586   418 recover.cpp:564] Updating replica status to STARTING
I1221 17:49:05.834460   418 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 1.082224ms
I1221 17:49:05.834507   418 replica.cpp:320] Persisted replica status to STARTING
I1221 17:49:05.835194   417 recover.cpp:473] Replica is in STARTING status
I1221 17:49:05.838433   417 replica.cpp:673] Replica in STARTING status received a broadcasted recover request from (6)@172.17.0.2:60254
I1221 17:49:05.838778   405 master.cpp:365] Master 19baa4c9-78aa-4ffb-abb3-e4daf634ec63 (be9bd3a71092) started on 172.17.0.2:60254
I1221 17:49:05.838824   405 master.cpp:367] Flags at startup: --acls=""permissive: false
register_frameworks {
  principals {
    type: SOME
    values: ""test-principal""
  }
  roles {
    type: SOME
    values: ""*""
  }
}
run_tasks {
  principals {
    type: SOME
    values: ""test-principal""
  }
  users {
    type: SOME
    values: ""mesos""
  }
}
"" --allocation_interval=""1secs"" --allocator=""HierarchicalDRF"" --authenticate=""false"" --authenticate_slaves=""false"" --authenticators=""crammd5"" --authorizers=""local"" --credentials=""/tmp/ExamplesTest_EventCallFramework_9TX8jH/credentials"" --framework_sorter=""drf"" --help=""true"" --hostname_lookup=""true"" --initialize_driver_logging=""true"" --log_auto_initialize=""true"" --logbufsecs=""0"" --logging_level=""INFO"" --max_slave_ping_timeouts=""5"" --quiet=""false"" --recovery_slave_removal_limit=""100%"" --registry=""replicated_log"" --registry_fetch_timeout=""1mins"" --registry_store_timeout=""5secs"" --registry_strict=""false"" --root_submissions=""true"" --slave_ping_timeout=""15secs"" --slave_reregister_timeout=""10mins"" --user_sorter=""drf"" --version=""false"" --webui_dir=""/mesos/mesos-0.27.0/src/webui"" --work_dir=""/tmp/mesos-WEDoSa"" --zk_session_timeout=""10secs""
I1221 17:49:05.839956   405 master.cpp:414] Master allowing unauthenticated frameworks to register
I1221 17:49:05.839974   405 master.cpp:419] Master allowing unauthenticated slaves to register
I1221 17:49:05.839990   405 credentials.hpp:35] Loading credentials for authentication from '/tmp/ExamplesTest_EventCallFramework_9TX8jH/credentials'
W1221 17:49:05.840143   405 credentials.hpp:50] Permissions on credentials file '/tmp/ExamplesTest_EventCallFramework_9TX8jH/credentials' are too open. It is recommended that your credentials file is NOT accessible by others.
I1221 17:49:05.840260   418 recover.cpp:193] Received a recover response from a replica in STARTING status
I1221 17:49:05.840479   405 master.cpp:456] Using default 'crammd5' authenticator
I1221 17:49:05.840912   405 authenticator.cpp:518] Initializing server SASL
I1221 17:49:05.841112   412 recover.cpp:564] Updating replica status to VOTING
I1221 17:49:05.841882   413 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 641489ns
I1221 17:49:05.841920   413 replica.cpp:320] Persisted replica status to VOTING
I1221 17:49:05.842102   406 recover.cpp:578] Successfully joined the Paxos group
I1221 17:49:05.842471   406 recover.cpp:462] Recover process terminated
I1221 17:49:05.843750   405 auxprop.cpp:71] Initialized in-memory auxiliary property plugin
I1221 17:49:05.843941   405 master.cpp:493] Authorization enabled
I1221 17:49:05.844133   381 containerizer.cpp:141] Using isolation: filesystem/posix,posix/cpu,posix/mem
I1221 17:49:05.844813   410 hierarchical.cpp:147] Initialized hierarchical allocator process
W1221 17:49:05.845093   381 backend.cpp:48] Failed to create 'bind' backend: BindBackend requires root privileges
I1221 17:49:05.845433   416 whitelist_watcher.cpp:77] No whitelist given
I1221 17:49:05.850373   411 slave.cpp:191] Slave started on 1)@172.17.0.2:60254
I1221 17:49:05.850433   411 slave.cpp:192] Flags at startup: --appc_store_dir=""/tmp/mesos/store/appc"" --authenticatee=""crammd5"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_auth_server=""auth.docker.io"" --docker_auth_server_port=""443"" --docker_kill_orphans=""true"" --docker_local_archives_dir=""/tmp/mesos/images/docker"" --docker_puller=""local"" --docker_puller_timeout=""60"" --docker_registry=""registry-1.docker.io"" --docker_registry_port=""443"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/tmp/mesos/store/docker"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/tmp/mesos/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --hostname_lookup=""true"" --image_provisioner_backend=""copy"" --initialize_driver_logging=""true"" --isolation=""filesystem/posix,posix/cpu,posix/mem"" --launcher=""posix"" --launcher_dir=""/mesos/mesos-0.27.0/_build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""1secs"" --resources=""cpus:2;mem:10240"" --revocable_cpu_low_priority=""true"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --systemd_runtime_directory=""/run/systemd/system"" --version=""false"" --work_dir=""/tmp/mesos-WEDoSa/0""
I1221 17:49:05.851650   411 resources.cpp:478] Parsing resources as JSON failed: cpus:2;mem:10240
Trying semicolon-delimited string format instead
I1221 17:49:05.854473   411 slave.cpp:392] Slave resources: cpus(*):2; mem(*):10240; disk(*):3.70122e+06; ports(*):[31000-32000]
I1221 17:49:05.854701   411 slave.cpp:400] Slave attributes: [  ]
I1221 17:49:05.854821   411 slave.cpp:405] Slave hostname: be9bd3a71092
I1221 17:49:05.854907   411 slave.cpp:410] Slave checkpoint: true
I1221 17:49:05.855715   381 containerizer.cpp:141] Using isolation: filesystem/posix,posix/cpu,posix/mem
W1221 17:49:05.856274   381 backend.cpp:48] Failed to create 'bind' backend: BindBackend requires root privileges
I1221 17:49:05.861232   407 slave.cpp:191] Slave started on 2)@172.17.0.2:60254
I1221 17:49:05.861279   407 slave.cpp:192] Flags at startup: --appc_store_dir=""/tmp/mesos/store/appc"" --authenticatee=""crammd5"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_auth_server=""auth.docker.io"" --docker_auth_server_port=""443"" --docker_kill_orphans=""true"" --docker_local_archives_dir=""/tmp/mesos/images/docker"" --docker_puller=""local"" --docker_puller_timeout=""60"" --docker_registry=""registry-1.docker.io"" --docker_registry_port=""443"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/tmp/mesos/store/docker"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/tmp/mesos/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --hostname_lookup=""true"" --image_provisioner_backend=""copy"" --initialize_driver_logging=""true"" --isolation=""filesystem/posix,posix/cpu,posix/mem"" --launcher=""posix"" --launcher_dir=""/mesos/mesos-0.27.0/_build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""1secs"" --resources=""cpus:2;mem:10240"" --revocable_cpu_low_priority=""true"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --systemd_runtime_directory=""/run/systemd/system"" --version=""false"" --work_dir=""/tmp/mesos-WEDoSa/1""
I1221 17:49:05.862290   407 resources.cpp:478] Parsing resources as JSON failed: cpus:2;mem:10240
Trying semicolon-delimited string format instead
I1221 17:49:05.862937   407 slave.cpp:392] Slave resources: cpus(*):2; mem(*):10240; disk(*):3.70122e+06; ports(*):[31000-32000]
I1221 17:49:05.863024   407 slave.cpp:400] Slave attributes: [  ]
I1221 17:49:05.863040   407 slave.cpp:405] Slave hostname: be9bd3a71092
I1221 17:49:05.863049   407 slave.cpp:410] Slave checkpoint: true
I1221 17:49:05.863625   418 state.cpp:58] Recovering state from '/tmp/mesos-WEDoSa/0/meta'
I1221 17:49:05.863787   381 containerizer.cpp:141] Using isolation: filesystem/posix,posix/cpu,posix/mem
I1221 17:49:05.864229   420 state.cpp:58] Recovering state from '/tmp/mesos-WEDoSa/1/meta'
W1221 17:49:05.864358   381 backend.cpp:48] Failed to create 'bind' backend: BindBackend requires root privileges
I1221 17:49:05.864397   418 status_update_manager.cpp:200] Recovering status update manager
I1221 17:49:05.865824   411 status_update_manager.cpp:200] Recovering status update manager
I1221 17:49:05.866319   411 containerizer.cpp:383] Recovering containerizer
I1221 17:49:05.866439   410 containerizer.cpp:383] Recovering containerizer
I1221 17:49:05.867146   416 master.cpp:1629] The newly elected leader is master@172.17.0.2:60254 with id 19baa4c9-78aa-4ffb-abb3-e4daf634ec63
I1221 17:49:05.867189   416 master.cpp:1642] Elected as the leading master!
I1221 17:49:05.867229   416 master.cpp:1387] Recovering from registrar
I1221 17:49:05.867583   409 registrar.cpp:307] Recovering registrar
I1221 17:49:05.868595   420 slave.cpp:4427] Finished recovery
I1221 17:49:05.869122   406 slave.cpp:4427] Finished recovery
I1221 17:49:05.869529   420 slave.cpp:4599] Querying resource estimator for oversubscribable resources
I1221 17:49:05.869848   411 log.cpp:659] Attempting to start the writer
I1221 17:49:05.870846   413 status_update_manager.cpp:174] Pausing sending status updates
I1221 17:49:05.870885   420 slave.cpp:729] New master detected at master@172.17.0.2:60254
I1221 17:49:05.870996   420 slave.cpp:754] No credentials provided. Attempting to register without authentication
I1221 17:49:05.871129   420 slave.cpp:765] Detecting new master
I1221 17:49:05.871261   420 slave.cpp:4613] Received oversubscribable resources  from the resource estimator
I1221 17:49:05.872555   406 slave.cpp:4599] Querying resource estimator for oversubscribable resources
I1221 17:49:05.872836   406 status_update_manager.cpp:174] Pausing sending status updates
I1221 17:49:05.872849   420 slave.cpp:729] New master detected at master@172.17.0.2:60254
I1221 17:49:05.872915   420 slave.cpp:754] No credentials provided. Attempting to register without authentication
I1221 17:49:05.872956   420 slave.cpp:765] Detecting new master
I1221 17:49:05.873045   420 slave.cpp:4613] Received oversubscribable resources  from the resource estimator
I1221 17:49:05.873503   408 replica.cpp:493] Replica received implicit promise request from (37)@172.17.0.2:60254 with proposal 1
I1221 17:49:05.874140   408 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 594758ns
I1221 17:49:05.874171   408 replica.cpp:342] Persisted promised to 1
I1221 17:49:05.875378   408 slave.cpp:191] Slave started on 3)@172.17.0.2:60254
I1221 17:49:05.875962   412 coordinator.cpp:238] Coordinator attempting to fill missing positions
I1221 17:49:05.875965   414 scheduler.cpp:236] New master detected at master@172.17.0.2:60254
I1221 17:49:05.875458   408 slave.cpp:192] Flags at startup: --appc_store_dir=""/tmp/mesos/store/appc"" --authenticatee=""crammd5"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_auth_server=""auth.docker.io"" --docker_auth_server_port=""443"" --docker_kill_orphans=""true"" --docker_local_archives_dir=""/tmp/mesos/images/docker"" --docker_puller=""local"" --docker_puller_timeout=""60"" --docker_registry=""registry-1.docker.io"" --docker_registry_port=""443"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/tmp/mesos/store/docker"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/tmp/mesos/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""true"" --hostname_lookup=""true"" --image_provisioner_backend=""copy"" --initialize_driver_logging=""true"" --isolation=""filesystem/posix,posix/cpu,posix/mem"" --launcher=""posix"" --launcher_dir=""/mesos/mesos-0.27.0/_build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""1secs"" --resources=""cpus:2;mem:10240"" --revocable_cpu_low_priority=""true"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --systemd_runtime_directory=""/run/systemd/system"" --version=""false"" --work_dir=""/tmp/mesos-WEDoSa/2""
I1221 17:49:05.877544   408 resources.cpp:478] Parsing resources as JSON failed: cpus:2;mem:10240
Trying semicolon-delimited string format instead
I1221 17:49:05.878088   410 replica.cpp:388] Replica received explicit promise request from (40)@172.17.0.2:60254 for position 0 with proposal 2
I1221 17:49:05.878116   408 slave.cpp:392] Slave resources: cpus(*):2; mem(*):10240; disk(*):3.70122e+06; ports(*):[31000-32000]
I1221 17:49:05.878209   408 slave.cpp:400] Slave attributes: [  ]
I1221 17:49:05.878226   408 slave.cpp:405] Slave hostname: be9bd3a71092
I1221 17:49:05.878237   408 slave.cpp:410] Slave checkpoint: true
I1221 17:49:05.878959   410 leveldb.cpp:341] Persisting action (8 bytes) to leveldb took 769060ns
I1221 17:49:05.878995   410 replica.cpp:712] Persisted action at 0
I1221 17:49:05.880889   419 state.cpp:58] Recovering state from '/tmp/mesos-WEDoSa/2/meta'
I1221 17:49:05.881033   418 replica.cpp:537] Replica received write request for position 0 from (41)@172.17.0.2:60254
I1221 17:49:05.881433   418 leveldb.cpp:436] Reading position from leveldb took 163395ns
I1221 17:49:05.881222   416 status_update_manager.cpp:200] Recovering status update manager
I1221 17:49:05.881892   419 containerizer.cpp:383] Recovering containerizer
I1221 17:49:05.882032   418 leveldb.cpp:341] Persisting action (14 bytes) to leveldb took 486145ns
I1221 17:49:05.882069   418 replica.cpp:712] Persisted action at 0
I1221 17:49:05.882983   419 replica.cpp:691] Replica received learned notice for position 0 from @0.0.0.0:0
I1221 17:49:05.883849   419 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 829258ns
I1221 17:49:05.883955   413 slave.cpp:4427] Finished recovery
I1221 17:49:05.883960   419 replica.cpp:712] Persisted action at 0
I1221 17:49:05.884121   419 replica.cpp:697] Replica learned NOP action at position 0
I1221 17:49:05.884806   413 slave.cpp:4599] Querying resource estimator for oversubscribable resources
I1221 17:49:05.884945   409 log.cpp:675] Writer started with ending position 0
I1221 17:49:05.885113   419 status_update_manager.cpp:174] Pausing sending status updates
I1221 17:49:05.885149   413 slave.cpp:729] New master detected at master@172.17.0.2:60254
I1221 17:49:05.885241   413 slave.cpp:754] No credentials provided. Attempting to register without authentication
I1221 17:49:05.885354   413 slave.cpp:765] Detecting new master
I1221 17:49:05.885524   413 slave.cpp:4613] Received oversubscribable resources  from the resource estimator
I1221 17:49:05.888422   409 leveldb.cpp:436] Reading position from leveldb took 76121ns
I1221 17:49:05.893468   420 registrar.cpp:340] Successfully fetched the registry (0B) in 25.776896ms
I1221 17:49:05.893851   420 registrar.cpp:439] Applied 1 operations in 117027ns; attempting to update the 'registry'
I1221 17:49:05.896961   412 log.cpp:683] Attempting to append 170 bytes to the log
I1221 17:49:05.897126   418 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 1
I1221 17:49:05.898660   412 replica.cpp:537] Replica received write request for position 1 from (43)@172.17.0.2:60254
I1221 17:49:05.899444   412 leveldb.cpp:341] Persisting action (189 bytes) to leveldb took 711265ns
I1221 17:49:05.899479   412 replica.cpp:712] Persisted action at 1
I1221 17:49:05.900216   410 replica.cpp:691] Replica received learned notice for position 1 from @0.0.0.0:0
I1221 17:49:05.900804   410 leveldb.cpp:341] Persisting action (191 bytes) to leveldb took 550441ns
I1221 17:49:05.900837   410 replica.cpp:712] Persisted action at 1
I1221 17:49:05.900876   410 replica.cpp:697] Replica learned APPEND action at position 1
I1221 17:49:05.903574   408 registrar.cpp:484] Successfully updated the 'registry' in 9.573888ms
I1221 17:49:05.903987   408 registrar.cpp:370] Successfully recovered registrar
I1221 17:49:05.906033   408 master.cpp:1439] Recovered 0 slaves from the Registry (131B) ; allowing 10mins for slaves to re-register
I1221 17:49:05.906111   407 hierarchical.cpp:165] Skipping recovery of hierarchical allocator: nothing to recover
I1221 17:49:05.907145   410 log.cpp:702] Attempting to truncate the log to 1
I1221 17:49:05.907457   419 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 2
I1221 17:49:05.908817   405 replica.cpp:537] Replica received write request for position 2 from (44)@172.17.0.2:60254
I1221 17:49:05.909519   405 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 635938ns
I1221 17:49:05.909554   405 replica.cpp:712] Persisted action at 2
I1221 17:49:05.910749   417 replica.cpp:691] Replica received learned notice for position 2 from @0.0.0.0:0
I1221 17:49:05.911264   417 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 472130ns
I1221 17:49:05.911339   417 leveldb.cpp:399] Deleting ~1 keys from leveldb took 42612ns
I1221 17:49:05.911365   417 replica.cpp:712] Persisted action at 2
I1221 17:49:05.911422   417 replica.cpp:697] Replica learned TRUNCATE action at position 2
I1221 17:49:05.936772   417 slave.cpp:1254] Will retry registration in 1.091568855secs if necessary
I1221 17:49:05.937356   417 master.cpp:4132] Registering slave at slave(3)@172.17.0.2:60254 (be9bd3a71092) with id 19baa4c9-78aa-4ffb-abb3-e4daf634ec63-S0
I1221 17:49:05.938593   417 registrar.cpp:439] Applied 1 operations in 224884ns; attempting to update the 'registry'
I1221 17:49:05.942648   416 log.cpp:683] Attempting to append 339 bytes to the log
I1221 17:49:05.942790   413 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 3
I1221 17:49:05.943749   419 replica.cpp:537] Replica received write request for position 3 from (45)@172.17.0.2:60254
I1221 17:49:05.944048   419 leveldb.cpp:341] Persisting action (358 bytes) to leveldb took 247879ns
I1221 17:49:05.944082   419 replica.cpp:712] Persisted action at 3
I1221 17:49:05.944911   416 replica.cpp:691] Replica received learned notice for position 3 from @0.0.0.0:0
I1221 17:49:05.945507   416 leveldb.cpp:341] Persisting action (360 bytes) to leveldb took 562713ns
I1221 17:49:05.945528   416 replica.cpp:712] Persisted action at 3
I1221 17:49:05.945552   416 replica.cpp:697] Replica learned APPEND action at position 3
I1221 17:49:05.947252   409 registrar.cpp:484] Successfully updated the 'registry' in 8.532992ms
I1221 17:49:05.947650   406 log.cpp:702] Attempting to truncate the log to 3
I1221 17:49:05.947787   407 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 4
I1221 17:49:05.949035   412 replica.cpp:537] Replica received write request for position 4 from (46)@172.17.0.2:60254
I1221 17:49:05.949101   406 slave.cpp:3371] Received ping from slave-observer(1)@172.17.0.2:60254
I1221 17:49:05.949318   416 master.cpp:4200] Registered slave 19baa4c9-78aa-4ffb-abb3-e4daf634ec63-S0 at slave(3)@172.17.0.2:60254 (be9bd3a71092) with cpus(*):2; mem(*):10240; disk(*):3.70122e+06; ports(*):[31000-32000]
I1221 17:49:05.949522   419 slave.cpp:904] Registered with master master@172.17.0.2:60254; given slave ID 19baa4c9-78aa-4ffb-abb3-e4daf634ec63-S0
I1221 17:49:05.949615   419 fetcher.cpp:81] Clearing fetcher cache
I1221 17:49:05.949668   412 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 577710ns
I1221 17:49:05.949704   412 replica.cpp:712] Persisted action at 4
I1221 17:49:05.949803   414 status_update_manager.cpp:181] Resuming sending status updates
I1221 17:49:05.949890   406 hierarchical.cpp:465] Added slave 19baa4c9-78aa-4ffb-abb3-e4daf634ec63-S0 (be9bd3a71092) with cpus(*):2; mem(*):10240; disk(*):3.70122e+06; ports(*):[31000-32000] (allocated: )
I1221 17:49:05.950103   419 slave.cpp:927] Checkpointing SlaveInfo to '/tmp/mesos-WEDoSa/2/meta/slaves/19baa4c9-78aa-4ffb-abb3-e4daf634ec63-S0/slave.info'
I1221 17:49:05.950390   406 hierarchical.cpp:1329] No resources available to allocate!
I1221 17:49:05.950456   413 replica.cpp:691] Replica received learned notice for position 4 from @0.0.0.0:0
I1221 17:49:05.950479   406 hierarchical.cpp:1101] Performed allocation for slave 19baa4c9-78aa-4ffb-abb3-e4daf634ec63-S0 in 539393ns
I1221 17:49:05.950675   419 slave.cpp:963] Forwarding total oversubscribed resources 
I1221 17:49:05.950897   419 master.cpp:4542] Received update of slave 19baa4c9-78aa-4ffb-abb3-e4daf634ec63-S0 at slave(3)@172.17.0.2:60254 (be9bd3a71092) with total oversubscribed resources 
I1221 17:49:05.951098   413 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 605919ns
I1221 17:49:05.951166   413 leveldb.cpp:399] Deleting ~2 keys from leveldb took 41299ns
I1221 17:49:05.951192   413 replica.cpp:712] Persisted action at 4
I1221 17:49:05.951226   413 replica.cpp:697] Replica learned TRUNCATE action at position 4
I1221 17:49:05.951581   416 hierarchical.cpp:521] Slave 19baa4c9-78aa-4ffb-abb3-e4daf634ec63-S0 (be9bd3a71092) updated with oversubscribed resources  (total: cpus(*):2; mem(*):10240; disk(*):3.70122e+06; ports(*):[31000-32000], allocated: )
I1221 17:49:05.951942   416 hierarchical.cpp:1329] No resources available to allocate!
I1221 17:49:05.951968   416 hierarchical.cpp:1101] Performed allocation for slave 19baa4c9-78aa-4ffb-abb3-e4daf634ec63-S0 in 342801ns
I1221 17:49:05.986484   417 slave.cpp:1254] Will retry registration in 1.181298785secs if necessary
I1221 17:49:05.986712   418 master.cpp:4132] Registering slave at slave(1)@172.17.0.2:60254 (be9bd3a71092) with id 19baa4c9-78aa-4ffb-abb3-e4daf634ec63-S1
I1221 17:49:05.987385   417 registrar.cpp:439] Applied 1 operations in 147902ns; attempting to update the 'registry'
I1221 17:49:05.988533   418 log.cpp:683] Attempting to append 505 bytes to the log
I1221 17:49:05.988661   409 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 5
I1221 17:49:05.989522   418 replica.cpp:537] Replica received write request for position 5 from (47)@172.17.0.2:60254
I1221 17:49:05.989840   418 leveldb.cpp:341] Persisting action (524 bytes) to leveldb took 269405ns
I1221 17:49:05.989867   418 replica.cpp:712] Persisted action at 5
I1221 17:49:05.990456   412 replica.cpp:691] Replica received learned notice for position 5 from @0.0.0.0:0
I1221 17:49:05.991312   412 leveldb.cpp:341] Persisting action (526 bytes) to leveldb took 824395ns
I1221 17:49:05.991338   412 replica.cpp:712] Persisted action at 5
I1221 17:49:05.991366   412 replica.cpp:697] Replica learned APPEND action at position 5
I1221 17:49:05.993288   415 registrar.cpp:484] Successfully updated the 'registry' in 5.81504ms
I1221 17:49:05.993649   415 log.cpp:702] Attempting to truncate the log to 5
I1221 17:49:05.993738   412 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 6
I1221 17:49:05.994153   413 slave.cpp:3371] Received ping from slave-observer(2)@172.17.0.2:60254
I1221 17:49:05.994281   414 master.cpp:4200] Registered slave 19baa4c9-78aa-4ffb-abb3-e4daf634ec63-S1 at slave(1)@172.17.0.2:60254 (be9bd3a71092) with cpus(*):2; mem(*):10240; disk(*):3.70122e+06; ports(*):[31000-32000]
I1221 17:49:05.994374   415 slave.cpp:904] Registered with master master@172.17.0.2:60254; given slave ID 19baa4c9-78aa-4ffb-abb3-e4daf634ec63-S1
I1221 17:49:05.994410   415 fetcher.cpp:81] Clearing fetcher cache
I1221 17:49:05.994499   413 replica.cpp:537] Replica received write request for position 6 from (48)@172.17.0.2:60254
I1221 17:49:05.994529   418 status_update_manager.cpp:181] Resuming sending status updates
I1221 17:49:05.994810   412 hierarchical.cpp:465] Added slave 19baa4c9-78aa-4ffb-abb3-e4daf634ec63-S1 (be9bd3a71092) with cpus(*):2; mem(*):10240; disk(*):3.70122e+06; ports(*):[31000-32000] (allocated: )
I1221 17:49:05.994884   413 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 345750ns
I1221 17:49:05.994915   413 replica.cpp:712] Persisted action at 6
I1221 17:49:05.995004   412 hierarchical.cpp:1329] No resources available to allocate!
I1221 17:49:05.994916   415 slave.cpp:927] Checkpointing SlaveInfo to '/tmp/mesos-WEDoSa/0/meta/slaves/19baa4c9-78aa-4ffb-abb3-e4daf634ec63-S1/slave.info'
I1221 17:49:05.995038   412 hierarchical.cpp:1101] Performed allocation for slave 19baa4c9-78aa-4ffb-abb3-e4daf634ec63-S1 in 184084ns
I1221 17:49:05.995410   407 replica.cpp:691] Replica received learned notice for position 6 from @0.0.0.0:0
I1221 17:49:05.995467   415 slave.cpp:963] Forwarding total oversubscribed resources 
I1221 17:49:05.995690   415 master.cpp:4542] Received update of slave 19baa4c9-78aa-4ffb-abb3-e4daf634ec63-S1 at slave(1)@172.17.0.2:60254 (be9bd3a71092) with total oversubscribed resources 
I1221 17:49:05.995858   407 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 413134ns
I1221 17:49:05.995918   407 leveldb.cpp:399] Deleting ~2 keys from leveldb took 34130ns
I1221 17:49:05.995941   407 replica.cpp:712] Persisted action at 6
I1221 17:49:05.995970   407 replica.cpp:697] Replica learned TRUNCATE action at position 6
I1221 17:49:05.996312   415 hierarchical.cpp:521] Slave 19baa4c9-78aa-4ffb-abb3-e4daf634ec63-S1 (be9bd3a71092) updated with oversubscribed resources  (total: cpus(*):2; mem(*):10240; disk(*):3.70122e+06; ports(*):[31000-32000], allocated: )
I1221 17:49:05.996492   415 hierarchical.cpp:1329] No resources available to allocate!
I1221 17:49:05.996608   415 hierarchical.cpp:1101] Performed allocation for slave 19baa4c9-78aa-4ffb-abb3-e4daf634ec63-S1 in 255224ns
I1221 17:49:06.594876   407 slave.cpp:1254] Will retry registration in 627.120173ms if necessary
I1221 17:49:06.595316   407 master.cpp:4132] Registering slave at slave(2)@172.17.0.2:60254 (be9bd3a71092) with id 19baa4c9-78aa-4ffb-abb3-e4daf634ec63-S2
I1221 17:49:06.596058   407 registrar.cpp:439] Applied 1 operations in 129882ns; attempting to update the 'registry'
I1221 17:49:06.599275   413 log.cpp:683] Attempting to append 671 bytes to the log
I1221 17:49:06.599606   413 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 7
I1221 17:49:06.600661   419 replica.cpp:537] Replica received write request for position 7 from (49)@172.17.0.2:60254
I1221 17:49:06.601197   419 leveldb.cpp:341] Persisting action (690 bytes) to leveldb took 470540ns
I1221 17:49:06.601229   419 replica.cpp:712] Persisted action at 7
I1221 17:49:06.602531   419 replica.cpp:691] Replica received learned notice for position 7 from @0.0.0.0:0
I1221 17:49:06.603359   419 leveldb.cpp:341] Persisting action (692 bytes) to leveldb took 786612ns
I1221 17:49:06.603394   419 replica.cpp:712] Persisted action at 7
I1221 17:49:06.603446   419 replica.cpp:697] Replica learned APPEND action at position 7
I1221 17:49:06.605727   406 registrar.cpp:484] Successfully updated the 'registry' in 9.575936ms
I1221 17:49:06.606062   420 log.cpp:702] Attempting to truncate the log to 7
I1221 17:49:06.606164   408 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 8
I1221 17:49:06.606712   414 slave.cpp:3371] Received ping from slave-observer(3)@172.17.0.2:60254
I1221 17:49:06.606941   418 slave.cpp:904] Registered with master master@172.17.0.2:60254; given slave ID 19baa4c9-78aa-4ffb-abb3-e4daf634ec63-S2
I1221 17:49:06.606894   410 master.cpp:4200] Registered slave 19baa4c9-78aa-4ffb-abb3-e4daf634ec63-S2 at slave(2)@172.17.0.2:60254 (be9bd3a71092) with cpus(*):2; mem(*):10240; disk(*):3.70122e+06; ports(*):[31000-32000]
I1221 17:49:06.606974   418 fetcher.cpp:81] Clearing fetcher cache
I1221 17:49:06.607029   407 replica.cpp:537] Replica received write request for position 8 from (50)@172.17.0.2:60254
I1221 17:49:06.607094   414 status_update_manager.cpp:181] Resuming sending status updates
I1221 17:49:06.607131   411 hierarchical.cpp:465] Added slave 19baa4c9-78aa-4ffb-abb3-e4daf634ec63-S2 (be9bd3a71092) with cpus(*):2; mem(*):10240; disk(*):3.70122e+06; ports(*):[31000-32000] (allocated: )
I1221 17:49:06.607316   411 hierarchical.cpp:1329] No resources available to allocate!
I1221 17:49:06.607342   411 hierarchical.cpp:1101] Performed allocation for slave 19baa4c9-78aa-4ffb-abb3-e4daf634ec63-S2 in 166605ns
I1221 17:49:06.607601   407 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 530912ns
I1221 17:49:06.607619   418 slave.cpp:927] Checkpointing SlaveInfo to '/tmp/mesos-WEDoSa/1/meta/slaves/19baa4c9-78aa-4ffb-abb3-e4daf634ec63-S2/slave.info'
I1221 17:49:06.607630   407 replica.cpp:712] Persisted action at 8
I1221 17:49:06.608086   418 slave.cpp:963] Forwarding total oversubscribed resources 
I1221 17:49:06.608234   410 replica.cpp:691] Replica received learned notice for position 8 from @0.0.0.0:0
I1221 17:49:06.608328   418 master.cpp:4542] Received update of slave 19baa4c9-78aa-4ffb-abb3-e4daf634ec63-S2 at slave(2)@172.17.0.2:60254 (be9bd3a71092) with total oversubscribed resources 
I1221 17:49:06.608750   410 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 485499ns
I1221 17:49:06.608825   410 leveldb.cpp:399] Deleting ~2 keys from leveldb took 48342ns
I1221 17:49:06.608852   410 replica.cpp:712] Persisted action at 8
I1221 17:49:06.608891   410 replica.cpp:697] Replica learned TRUNCATE action at position 8
I1221 17:49:06.608986   414 hierarchical.cpp:521] Slave 19baa4c9-78aa-4ffb-abb3-e4daf634ec63-S2 (be9bd3a71092) updated with oversubscribed resources  (total: cpus(*):2; mem(*):10240; disk(*):3.70122e+06; ports(*):[31000-32000], allocated: )
I1221 17:49:06.609119   414 hierarchical.cpp:1329] No resources available to allocate!
I1221 17:49:06.609149   414 hierarchical.cpp:1101] Performed allocation for slave 19baa4c9-78aa-4ffb-abb3-e4daf634ec63-S2 in 122353ns
I1221 17:49:06.847012   414 hierarchical.cpp:1329] No resources available to allocate!
I1221 17:49:06.847092   414 hierarchical.cpp:1079] Performed allocation for 3 slaves in 796116ns
I1221 17:49:07.848945   414 hierarchical.cpp:1329] No resources available to allocate!
I1221 17:49:07.849019   414 hierarchical.cpp:1079] Performed allocation for 3 slaves in 608917ns
I1221 17:49:08.850955   412 hierarchical.cpp:1329] No resources available to allocate!
I1221 17:49:08.851022   412 hierarchical.cpp:1079] Performed allocation for 3 slaves in 554275ns
I1221 17:49:09.853204   409 hierarchical.cpp:1329] No resources available to allocate!
I1221 17:49:09.853287   409 hierarchical.cpp:1079] Performed allocation for 3 slaves in 518583ns
I1221 17:49:10.855202   406 hierarchical.cpp:1329] No resources available to allocate!
I1221 17:49:10.855280   406 hierarchical.cpp:1079] Performed allocation for 3 slaves in 572773ns
I1221 17:49:11.857287   416 hierarchical.cpp:1329] No resources available to allocate!
I1221 17:49:11.857367   416 hierarchical.cpp:1079] Performed allocation for 3 slaves in 632156ns
I1221 17:49:12.859216   417 hierarchical.cpp:1329] No resources available to allocate!
I1221 17:49:12.859290   417 hierarchical.cpp:1079] Performed allocation for 3 slaves in 628905ns
I1221 17:49:13.861121   417 hierarchical.cpp:1329] No resources available to allocate!
I1221 17:49:13.861186   417 hierarchical.cpp:1079] Performed allocation for 3 slaves in 478235ns
I1221 17:49:14.862176   411 hierarchical.cpp:1329] No resources available to allocate!
I1221 17:49:14.862248   411 hierarchical.cpp:1079] Performed allocation for 3 slaves in 571251ns
I1221 17:49:15.863394   414 hierarchical.cpp:1329] No resources available to allocate!
I1221 17:49:15.863471   414 hierarchical.cpp:1079] Performed allocation for 3 slaves in 563911ns
I1221 17:49:16.866031   414 hierarchical.cpp:1329] No resources available to allocate!
I1221 17:49:16.866108   414 hierarchical.cpp:1079] Performed allocation for 3 slaves in 640235ns
I1221 17:49:17.868214   414 hierarchical.cpp:1329] No resources available to allocate!
I1221 17:49:17.868293   414 hierarchical.cpp:1079] Performed allocation for 3 slaves in 665485ns
I1221 17:49:18.870254   420 hierarchical.cpp:1329] No resources available to allocate!
I1221 17:49:18.870331   420 hierarchical.cpp:1079] Performed allocation for 3 slaves in 637136ns
I1221 17:49:19.872316   413 hierarchical.cpp:1329] No resources available to allocate!
I1221 17:49:19.872409   413 hierarchical.cpp:1079] Performed allocation for 3 slaves in 682662ns
I1221 17:49:20.872805   413 slave.cpp:4599] Querying resource estimator for oversubscribable resources
I1221 17:49:20.873576   413 hierarchical.cpp:1329] No resources available to allocate!
I1221 17:49:20.873625   413 hierarchical.cpp:1079] Performed allocation for 3 slaves in 641465ns
I1221 17:49:20.873903   413 slave.cpp:4613] Received oversubscribable resources  from the resource estimator
I1221 17:49:20.874142   419 slave.cpp:4599] Querying resource estimator for oversubscribable resources
I1221 17:49:20.874352   416 slave.cpp:4613] Received oversubscribable resources  from the resource estimator
I1221 17:49:20.886467   416 slave.cpp:4599] Querying resource estimator for oversubscribable resources
I1221 17:49:20.886788   416 slave.cpp:4613] Received oversubscribable resources  from the resource estimator
I1221 17:49:20.949981   420 slave.cpp:3371] Received ping from slave-observer(1)@172.17.0.2:60254
I1221 17:49:20.995293   406 slave.cpp:3371] Received ping from slave-observer(2)@172.17.0.2:60254
I1221 17:49:21.608170   415 slave.cpp:3371] Received ping from slave-observer(3)@172.17.0.2:60254
I1221 17:49:21.874987   415 hierarchical.cpp:1329] No resources available to allocate!
I1221 17:49:21.875057   415 hierarchical.cpp:1079] Performed allocation for 3 slaves in 478657ns
I1221 17:49:22.876283   408 hierarchical.cpp:1329] No resources available to allocate!
I1221 17:49:22.876358   408 hierarchical.cpp:1079] Performed allocation for 3 slaves in 579184ns
I1221 17:49:23.878239   412 hierarchical.cpp:1329] No resources available to allocate!
I1221 17:49:23.878310   412 hierarchical.cpp:1079] Performed allocation for 3 slaves in 560657ns
I1221 17:49:24.880295   412 hierarchical.cpp:1329] No resources available to allocate!
I1221 17:49:24.880372   412 hierarchical.cpp:1079] Performed allocation for 3 slaves in 660835ns
I1221 17:49:25.882365   412 hierarchical.cpp:1329] No resources available to allocate!
I1221 17:49:25.882438   412 hierarchical.cpp:1079] Performed allocation for 3 slaves in 514098ns
I1221 17:49:26.884279   420 hierarchical.cpp:1329] No resources available to allocate!
I1221 17:49:26.884357   420 hierarchical.cpp:1079] Performed allocation for 3 slaves in 646670ns
I1221 17:49:27.886270   420 hierarchical.cpp:1329] No resources available to allocate!
I1221 17:49:27.886353   420 hierarchical.cpp:1079] Performed allocation for 3 slaves in 682783ns
I1221 17:49:28.888252   414 hierarchical.cpp:1329] No resources available to allocate!
I1221 17:49:28.888330   414 hierarchical.cpp:1079] Performed allocation for 3 slaves in 613888ns
I1221 17:49:29.890243   414 hierarchical.cpp:1329] No resources available to allocate!
I1221 17:49:29.890316   414 hierarchical.cpp:1079] Performed allocation for 3 slaves in 624179ns
I1221 17:49:30.892174   414 hierarchical.cpp:1329] No resources available to allocate!
I1221 17:49:30.892246   414 hierarchical.cpp:1079] Performed allocation for 3 slaves in 564425ns
I1221 17:49:31.894242   414 hierarchical.cpp:1329] No resources available to allocate!
I1221 17:49:31.894316   414 hierarchical.cpp:1079] Performed allocation for 3 slaves in 625358ns
I1221 17:49:32.896178   414 hierarchical.cpp:1329] No resources available to allocate!
I1221 17:49:32.896248   414 hierarchical.cpp:1079] Performed allocation for 3 slaves in 564486ns
I1221 17:49:33.898360   414 hierarchical.cpp:1329] No resources available to allocate!
I1221 17:49:33.898444   414 hierarchical.cpp:1079] Performed allocation for 3 slaves in 664324ns
I1221 17:49:34.900513   409 hierarchical.cpp:1329] No resources available to allocate!
I1221 17:49:34.900584   409 hierarchical.cpp:1079] Performed allocation for 3 slaves in 584647ns
I1221 17:49:35.875272   416 slave.cpp:4599] Querying resource estimator for oversubscribable resources
I1221 17:49:35.875267   418 slave.cpp:4599] Querying resource estimator for oversubscribable resources
I1221 17:49:35.875730   409 slave.cpp:4613] Received oversubscribable resources  from the resource estimator
I1221 17:49:35.875766   412 slave.cpp:4613] Received oversubscribable resources  from the resource estimator
I1221 17:49:35.887620   419 slave.cpp:4599] Querying resource estimator for oversubscribable resources
I1221 17:49:35.887943   413 slave.cpp:4613] Received oversubscribable resources  from the resource estimator
I1221 17:49:35.901360   412 hierarchical.cpp:1329] No resources available to allocate!
I1221 17:49:35.901435   412 hierarchical.cpp:1079] Performed allocation for 3 slaves in 552395ns
I1221 17:49:35.950346   416 slave.cpp:3371] Received ping from slave-observer(1)@172.17.0.2:60254
I1221 17:49:35.995688   416 slave.cpp:3371] Received ping from slave-observer(2)@172.17.0.2:60254
I1221 17:49:36.609561   418 slave.cpp:3371] Received ping from slave-observer(3)@172.17.0.2:60254
{noformat}",Bug,Major,bmahler,2016-02-10T09:03:24.000+0000,5,Resolved,Complete,ExamplesTest.NoExecutorFramework runs forever.,2016-02-10T09:03:24.000+0000,MESOS-4257,3.0,mesos,Mesosphere Sprint 27
anandmazumdar,2015-12-29T03:51:14.000+0000,anandmazumdar,"Currently, the slave process generates a process ID every time it is initialized via {{process::ID::generate}} function call. This is a problem for testing HTTP executors as it can't retry if there is a disconnection after an agent restart since the prefix is incremented. 

{code}
Agent PID before:
slave(1)@127.0.0.1:43915

Agent PID after restart:
slave(2)@127.0.0.1:43915
{code}

There are a couple of ways to fix this:
- Add a constructor to {{Slave}} exclusively for testing that passes on a fixed {{ID}} instead of relying on {{ID::generate}}.
- Currently we delegate to slave(1)@ i.e. (1) when nothing is specified as the URL in libprocess i.e. {{127.0.0.1:43915/api/v1/executor}} would delegate to {{slave(1)@127.0.0.1:43915/api/v1/executor}}. Instead of defaulting to (1), we can default to the last known active ID.",Bug,Major,anandmazumdar,2016-02-04T22:44:15.000+0000,5,Resolved,Complete,Add mechanism for testing recovery of HTTP based executors,2016-02-27T00:05:07.000+0000,MESOS-4255,5.0,mesos,Mesosphere Sprint 28
tnachen,2015-12-22T21:20:48.000+0000,tnachen,"Currently there are too many slave flags for configuring the docker store/puller.
We can remove the following flags:

docker_auth_server_port
docker_local_archives_dir
docker_registry_port
docker_puller

And consolidate them into the existing flags.",Improvement,Major,tnachen,2016-01-07T17:00:42.000+0000,5,Resolved,Complete,Consolidate docker store slave flags,2016-01-07T17:00:42.000+0000,MESOS-4241,3.0,mesos,Mesosphere Sprint 25
gilbert,2015-12-22T21:01:38.000+0000,jieyu,"The rationale behind this change is that many of the image specifications (e.g., Docker/Appc) are not just for filesystems. They also specify runtime configurations (e.g., environment variables, volumes, etc) for the container.

Provisioner should return those runtime configurations to the Mesos containerizer and Mesos containerizer will delegate the isolation of those runtime configurations to the relevant isolator.

Here is what it will be look like eventually. We could do those changes in phases:
1) Provisioner will return a ProvisionInfo which includes a 'rootfs' and image specific runtime configurations (could be the Docker/Appc manifest).
2) Then, the Mesos containerizer will generate a ContainerConfig (a protobuf which includes rootfs, sandbox, docker/appc manifest, similar to OCI's host independent config.json) and pass that to each isolator in 'prepare'. Imaging in the future, a DockerRuntimeIsolator takes the docker manifest from ContainerConfig and prepare the container.
3) The isolator's prepare function will return a ContainerLaunchInfo (contains environment variables, namespaces, etc.) which will be used by Mesos containerize to launch containers. Imaging that information will be passed to the launcher in the future.

We can do the renaming (ContainerPrepareInfo -> ContainerLaunchInfo) later.

",Task,Major,jieyu,2016-01-14T22:48:59.000+0000,5,Resolved,Complete,Pull provisioner from linux filesystem isolator to Mesos containerizer.,2016-01-14T22:48:59.000+0000,MESOS-4240,5.0,mesos,Mesosphere Sprint 26
gilbert,2015-12-21T19:41:17.000+0000,gilbert,"Cmd is the command to run when starting a container. We should be able to collect Cmd config information from a docker image, and pass it back to provisioner.",Improvement,Major,gilbert,2016-01-02T07:33:52.000+0000,5,Resolved,Complete,Enable passing docker image cmd runtime config to provisioner,2016-01-02T07:33:52.000+0000,MESOS-4227,1.0,mesos,Mesosphere Sprint 25
gilbert,2015-12-21T19:38:35.000+0000,gilbert,"Collect environment variables runtime config information from a docker image, and save as a map. Pass it back to provisioner, and handling environment variables merge issue.",Improvement,Major,gilbert,2016-01-02T07:33:35.000+0000,5,Resolved,Complete,Enable passing docker image environment variables runtime config to provisioner,2016-01-02T07:33:35.000+0000,MESOS-4226,1.0,mesos,Mesosphere Sprint 25
gilbert,2015-12-21T19:34:59.000+0000,gilbert,"Collect docker image manifest from disk(which contains all runtime configurations), and pass it back to provisioner, so that mesos containerizer can grab all necessary info from provisioner.",Improvement,Major,gilbert,2016-01-28T21:48:46.000+0000,5,Resolved,Complete,Exposed docker/appc image manifest to mesos containerizer.,2016-01-28T21:48:46.000+0000,MESOS-4225,2.0,mesos,Mesosphere Sprint 25
jojy,2015-12-21T19:30:37.000+0000,jojy,"Document isolators from developer perspective, possibly covering:

* linux isolators
* posix isolators
* filesystem, network isolators",Documentation,Major,jojy,,1,Open,New,Document isolator internals.,2016-01-27T00:53:14.000+0000,MESOS-4224,4.0,mesos,
jojy,2015-12-21T19:27:37.000+0000,jojy,"The documentation should cover:

* Purpose of isolators (business/user perspective).
* What is the criteria for choosing/picking between available isolators.
* Behavior of each individual isolator, constraints on which platforms/versions are supported, etc.",Documentation,Major,jojy,,10020,Accepted,In Progress,Document isolators from user perspective.,2016-02-15T18:49:17.000+0000,MESOS-4223,4.0,mesos,
jojy,2015-12-21T19:22:41.000+0000,jojy,"Add documentation that covers:

* Purpose of containerizers from a use case perspective.
* What purpose does each containerizer (mesos. docker, compose) serve.
* What criteria could be used to choose a containerizer.",Documentation,Major,jojy,2016-01-07T16:54:27.000+0000,5,Resolved,Complete,Document containerizer from user perspective.,2016-01-07T16:57:10.000+0000,MESOS-4222,3.0,mesos,Mesosphere Sprint 25
neilc,2015-12-19T18:48:31.000+0000,neilc,"Specifically, some of the gotchas around:

* Retrying reservation attempts after a timeout
* Fuzzy-matching resources to determine whether a reservation/PV is successful
* Represent client state as a state machine and repeatedly move ""toward"" successful terminate stats

Should also point to persistent volume example framework. We should also ask Gabriel and others (Arango?) who have built frameworks with PVs/DRs for feedback.",Documentation,Major,neilc,2016-01-18T22:31:38.000+0000,5,Resolved,Complete,"Document ""how to program with dynamic reservations and persistent volumes""",2016-01-18T22:31:38.000+0000,MESOS-4209,3.0,mesos,Mesosphere Sprint 26
greggomann,2015-12-19T18:18:39.000+0000,jieyu,"{noformat}
[ RUN      ] PersistentVolumeTest.BadACLDropCreateAndDestroy
I1219 09:51:32.623245 31878 leveldb.cpp:174] Opened db in 4.393596ms
I1219 09:51:32.624084 31878 leveldb.cpp:181] Compacted db in 709447ns
I1219 09:51:32.624186 31878 leveldb.cpp:196] Created db iterator in 21252ns
I1219 09:51:32.624290 31878 leveldb.cpp:202] Seeked to beginning of db in 11391ns
I1219 09:51:32.624378 31878 leveldb.cpp:271] Iterated through 0 keys in the db in 611ns
I1219 09:51:32.624505 31878 replica.cpp:779] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned
I1219 09:51:32.625195 31904 recover.cpp:447] Starting replica recovery
I1219 09:51:32.625641 31904 recover.cpp:473] Replica is in EMPTY status
I1219 09:51:32.627305 31904 replica.cpp:673] Replica in EMPTY status received a broadcasted recover request from (6740)@172.17.0.3:36408
I1219 09:51:32.627749 31904 recover.cpp:193] Received a recover response from a replica in EMPTY status
I1219 09:51:32.628330 31904 recover.cpp:564] Updating replica status to STARTING
I1219 09:51:32.629068 31906 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 410494ns
I1219 09:51:32.629169 31906 replica.cpp:320] Persisted replica status to STARTING
I1219 09:51:32.629598 31906 recover.cpp:473] Replica is in STARTING status
I1219 09:51:32.630782 31912 replica.cpp:673] Replica in STARTING status received a broadcasted recover request from (6741)@172.17.0.3:36408
I1219 09:51:32.631166 31901 recover.cpp:193] Received a recover response from a replica in STARTING status
I1219 09:51:32.632467 31902 recover.cpp:564] Updating replica status to VOTING
I1219 09:51:32.633600 31907 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 311370ns
I1219 09:51:32.633627 31907 replica.cpp:320] Persisted replica status to VOTING
I1219 09:51:32.633719 31907 recover.cpp:578] Successfully joined the Paxos group
I1219 09:51:32.633874 31907 recover.cpp:462] Recover process terminated
I1219 09:51:32.636409 31909 master.cpp:365] Master bded856d-1c7f-4fad-a8bc-3629ba8c59d3 (60ab6e727501) started on 172.17.0.3:36408
I1219 09:51:32.636593 31909 master.cpp:367] Flags at startup: --acls=""create_volumes {
  principals {
    values: ""creator-principal""
  }
  volume_types {
    type: ANY
  }
}
create_volumes {
  principals {
    type: ANY
  }
  volume_types {
    type: NONE
  }
}
"" --allocation_interval=""1secs"" --allocator=""HierarchicalDRF"" --authenticate=""false"" --authenticate_slaves=""true"" --authenticators=""crammd5"" --authorizers=""local"" --credentials=""/tmp/SpPF7B/credentials"" --framework_sorter=""drf"" --help=""false"" --hostname_lookup=""true"" --initialize_driver_logging=""true"" --log_auto_initialize=""true"" --logbufsecs=""0"" --logging_level=""INFO"" --max_slave_ping_timeouts=""5"" --quiet=""false"" --recovery_slave_removal_limit=""100%"" --registry=""replicated_log"" --registry_fetch_timeout=""1mins"" --registry_store_timeout=""25secs"" --registry_strict=""true"" --roles=""role1"" --root_submissions=""true"" --slave_ping_timeout=""15secs"" --slave_reregister_timeout=""10mins"" --user_sorter=""drf"" --version=""false"" --webui_dir=""/mesos/mesos-0.27.0/_inst/share/mesos/webui"" --work_dir=""/tmp/SpPF7B/master"" --zk_session_timeout=""10secs""
I1219 09:51:32.637055 31909 master.cpp:414] Master allowing unauthenticated frameworks to register
I1219 09:51:32.637068 31909 master.cpp:417] Master only allowing authenticated slaves to register
I1219 09:51:32.637094 31909 credentials.hpp:35] Loading credentials for authentication from '/tmp/SpPF7B/credentials'
I1219 09:51:32.637403 31909 master.cpp:456] Using default 'crammd5' authenticator
I1219 09:51:32.637555 31909 master.cpp:493] Authorization enabled
W1219 09:51:32.637575 31909 master.cpp:553] The '--roles' flag is deprecated. This flag will be removed in the future. See the Mesos 0.27 upgrade notes for more information
I1219 09:51:32.637806 31897 whitelist_watcher.cpp:77] No whitelist given
I1219 09:51:32.637820 31910 hierarchical.cpp:147] Initialized hierarchical allocator process
I1219 09:51:32.639677 31909 master.cpp:1629] The newly elected leader is master@172.17.0.3:36408 with id bded856d-1c7f-4fad-a8bc-3629ba8c59d3
I1219 09:51:32.639768 31909 master.cpp:1642] Elected as the leading master!
I1219 09:51:32.639892 31909 master.cpp:1387] Recovering from registrar
I1219 09:51:32.640136 31907 registrar.cpp:307] Recovering registrar
I1219 09:51:32.640929 31901 log.cpp:659] Attempting to start the writer
I1219 09:51:32.642199 31912 replica.cpp:493] Replica received implicit promise request from (6742)@172.17.0.3:36408 with proposal 1
I1219 09:51:32.642719 31912 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 445876ns
I1219 09:51:32.642755 31912 replica.cpp:342] Persisted promised to 1
I1219 09:51:32.643478 31904 coordinator.cpp:238] Coordinator attempting to fill missing positions
I1219 09:51:32.645009 31909 replica.cpp:388] Replica received explicit promise request from (6743)@172.17.0.3:36408 for position 0 with proposal 2
I1219 09:51:32.645356 31909 leveldb.cpp:341] Persisting action (8 bytes) to leveldb took 310064ns
I1219 09:51:32.645382 31909 replica.cpp:712] Persisted action at 0
I1219 09:51:32.646662 31909 replica.cpp:537] Replica received write request for position 0 from (6744)@172.17.0.3:36408
I1219 09:51:32.646721 31909 leveldb.cpp:436] Reading position from leveldb took 29298ns
I1219 09:51:32.647047 31909 leveldb.cpp:341] Persisting action (14 bytes) to leveldb took 283424ns
I1219 09:51:32.647073 31909 replica.cpp:712] Persisted action at 0
I1219 09:51:32.647722 31909 replica.cpp:691] Replica received learned notice for position 0 from @0.0.0.0:0
I1219 09:51:32.648052 31909 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 300825ns
I1219 09:51:32.648077 31909 replica.cpp:712] Persisted action at 0
I1219 09:51:32.648095 31909 replica.cpp:697] Replica learned NOP action at position 0
I1219 09:51:32.655295 31899 log.cpp:675] Writer started with ending position 0
I1219 09:51:32.656543 31905 leveldb.cpp:436] Reading position from leveldb took 32788ns
I1219 09:51:32.658164 31905 registrar.cpp:340] Successfully fetched the registry (0B) in 0ns
I1219 09:51:32.658604 31905 registrar.cpp:439] Applied 1 operations in 38183ns; attempting to update the 'registry'
I1219 09:51:32.660102 31905 log.cpp:683] Attempting to append 170 bytes to the log
I1219 09:51:32.660538 31906 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 1
I1219 09:51:32.661872 31906 replica.cpp:537] Replica received write request for position 1 from (6745)@172.17.0.3:36408
I1219 09:51:32.662719 31906 leveldb.cpp:341] Persisting action (189 bytes) to leveldb took 483018ns
I1219 09:51:32.663054 31906 replica.cpp:712] Persisted action at 1
I1219 09:51:32.664008 31902 replica.cpp:691] Replica received learned notice for position 1 from @0.0.0.0:0
I1219 09:51:32.664330 31902 leveldb.cpp:341] Persisting action (191 bytes) to leveldb took 287310ns
I1219 09:51:32.664355 31902 replica.cpp:712] Persisted action at 1
I1219 09:51:32.664376 31902 replica.cpp:697] Replica learned APPEND action at position 1
I1219 09:51:32.665365 31902 registrar.cpp:484] Successfully updated the 'registry' in 0ns
I1219 09:51:32.665493 31902 registrar.cpp:370] Successfully recovered registrar
I1219 09:51:32.665894 31902 master.cpp:1439] Recovered 0 slaves from the Registry (131B) ; allowing 10mins for slaves to re-register
I1219 09:51:32.665990 31902 hierarchical.cpp:165] Skipping recovery of hierarchical allocator: nothing to recover
I1219 09:51:32.666266 31902 log.cpp:702] Attempting to truncate the log to 1
I1219 09:51:32.666424 31902 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 2
I1219 09:51:32.667181 31907 replica.cpp:537] Replica received write request for position 2 from (6746)@172.17.0.3:36408
I1219 09:51:32.667768 31907 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 335947ns
I1219 09:51:32.668067 31907 replica.cpp:712] Persisted action at 2
I1219 09:51:32.668942 31906 replica.cpp:691] Replica received learned notice for position 2 from @0.0.0.0:0
I1219 09:51:32.669240 31906 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 266566ns
I1219 09:51:32.669292 31906 leveldb.cpp:399] Deleting ~1 keys from leveldb took 27852ns
I1219 09:51:32.669314 31906 replica.cpp:712] Persisted action at 2
I1219 09:51:32.669334 31906 replica.cpp:697] Replica learned TRUNCATE action at position 2
I1219 09:51:32.691251 31878 containerizer.cpp:141] Using isolation: posix/cpu,posix/mem,filesystem/posix
W1219 09:51:32.691759 31878 backend.cpp:48] Failed to create 'bind' backend: BindBackend requires root privileges
I1219 09:51:32.697428 31901 slave.cpp:191] Slave started on 228)@172.17.0.3:36408
I1219 09:51:32.697459 31901 slave.cpp:192] Flags at startup: --appc_store_dir=""/tmp/mesos/store/appc"" --authenticatee=""crammd5"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --credential=""/tmp/PersistentVolumeTest_BadACLDropCreateAndDestroy_gWLtnc/credential"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_auth_server=""auth.docker.io"" --docker_auth_server_port=""443"" --docker_kill_orphans=""true"" --docker_local_archives_dir=""/tmp/mesos/images/docker"" --docker_puller=""local"" --docker_puller_timeout=""60"" --docker_registry=""registry-1.docker.io"" --docker_registry_port=""443"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/tmp/mesos/store/docker"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/tmp/PersistentVolumeTest_BadACLDropCreateAndDestroy_gWLtnc/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --hostname_lookup=""true"" --image_provisioner_backend=""copy"" --initialize_driver_logging=""true"" --isolation=""posix/cpu,posix/mem"" --launcher_dir=""/mesos/mesos-0.27.0/_build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""10ms"" --resources=""cpus:2;mem:1024;disk(role1):2048"" --revocable_cpu_low_priority=""true"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --systemd_runtime_directory=""/run/systemd/system"" --version=""false"" --work_dir=""/tmp/PersistentVolumeTest_BadACLDropCreateAndDestroy_gWLtnc""
I1219 09:51:32.697963 31901 credentials.hpp:83] Loading credential for authentication from '/tmp/PersistentVolumeTest_BadACLDropCreateAndDestroy_gWLtnc/credential'
I1219 09:51:32.698210 31901 slave.cpp:322] Slave using credential for: test-principal
I1219 09:51:32.698449 31901 resources.cpp:478] Parsing resources as JSON failed: cpus:2;mem:1024;disk(role1):2048
Trying semicolon-delimited string format instead
I1219 09:51:32.699065 31901 slave.cpp:392] Slave resources: cpus(*):2; mem(*):1024; disk(role1):2048; ports(*):[31000-32000]
I1219 09:51:32.699137 31901 slave.cpp:400] Slave attributes: [  ]
I1219 09:51:32.699151 31901 slave.cpp:405] Slave hostname: 60ab6e727501
I1219 09:51:32.699161 31901 slave.cpp:410] Slave checkpoint: true
I1219 09:51:32.699364 31878 sched.cpp:164] Version: 0.27.0
I1219 09:51:32.700614 31911 sched.cpp:262] New master detected at master@172.17.0.3:36408
I1219 09:51:32.700703 31911 sched.cpp:272] No credentials provided. Attempting to register without authentication
I1219 09:51:32.700724 31911 sched.cpp:714] Sending SUBSCRIBE call to master@172.17.0.3:36408
I1219 09:51:32.700839 31911 sched.cpp:747] Will retry registration in 620.399428ms if necessary
I1219 09:51:32.701244 31903 master.cpp:2197] Received SUBSCRIBE call for framework 'default' at scheduler-0333dddc-4b41-40ed-8853-a1aadf1f1879@172.17.0.3:36408
I1219 09:51:32.701313 31903 master.cpp:1668] Authorizing framework principal 'test-principal' to receive offers for role 'role1'
I1219 09:51:32.701625 31903 master.cpp:2268] Subscribing framework default with checkpointing disabled and capabilities [  ]
I1219 09:51:32.702308 31903 hierarchical.cpp:260] Added framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0000
I1219 09:51:32.702386 31903 hierarchical.cpp:1329] No resources available to allocate!
I1219 09:51:32.702422 31903 hierarchical.cpp:1423] No inverse offers to send out!
I1219 09:51:32.702448 31903 hierarchical.cpp:1079] Performed allocation for 0 slaves in 114358ns
I1219 09:51:32.702638 31903 sched.cpp:641] Framework registered with bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0000
I1219 09:51:32.702688 31903 sched.cpp:655] Scheduler::registered took 25558ns
I1219 09:51:32.703553 31901 state.cpp:58] Recovering state from '/tmp/PersistentVolumeTest_BadACLDropCreateAndDestroy_gWLtnc/meta'
I1219 09:51:32.704118 31897 status_update_manager.cpp:200] Recovering status update manager
I1219 09:51:32.704407 31907 containerizer.cpp:383] Recovering containerizer
I1219 09:51:32.705373 31907 slave.cpp:4427] Finished recovery
I1219 09:51:32.705991 31907 slave.cpp:4599] Querying resource estimator for oversubscribable resources
I1219 09:51:32.706277 31907 slave.cpp:4613] Received oversubscribable resources  from the resource estimator
I1219 09:51:32.706666 31907 slave.cpp:729] New master detected at master@172.17.0.3:36408
I1219 09:51:32.706738 31907 slave.cpp:792] Authenticating with master master@172.17.0.3:36408
I1219 09:51:32.706760 31907 slave.cpp:797] Using default CRAM-MD5 authenticatee
I1219 09:51:32.706886 31899 status_update_manager.cpp:174] Pausing sending status updates
I1219 09:51:32.706941 31907 slave.cpp:765] Detecting new master
I1219 09:51:32.707036 31899 authenticatee.cpp:121] Creating new client SASL connection
I1219 09:51:32.707291 31910 master.cpp:5423] Authenticating slave(228)@172.17.0.3:36408
I1219 09:51:32.707479 31910 authenticator.cpp:413] Starting authentication session for crammd5_authenticatee(510)@172.17.0.3:36408
I1219 09:51:32.707849 31910 authenticator.cpp:98] Creating new server SASL connection
I1219 09:51:32.708082 31910 authenticatee.cpp:212] Received SASL authentication mechanisms: CRAM-MD5
I1219 09:51:32.708112 31910 authenticatee.cpp:238] Attempting to authenticate with mechanism 'CRAM-MD5'
I1219 09:51:32.708196 31910 authenticator.cpp:203] Received SASL authentication start
I1219 09:51:32.708395 31910 authenticator.cpp:325] Authentication requires more steps
I1219 09:51:32.708611 31902 authenticatee.cpp:258] Received SASL authentication step
I1219 09:51:32.708773 31910 authenticator.cpp:231] Received SASL authentication step
I1219 09:51:32.708889 31910 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: '60ab6e727501' server FQDN: '60ab6e727501' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I1219 09:51:32.708976 31910 auxprop.cpp:179] Looking up auxiliary property '*userPassword'
I1219 09:51:32.709096 31910 auxprop.cpp:179] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I1219 09:51:32.709200 31910 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: '60ab6e727501' server FQDN: '60ab6e727501' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I1219 09:51:32.709285 31910 auxprop.cpp:129] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I1219 09:51:32.709363 31910 auxprop.cpp:129] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I1219 09:51:32.709452 31910 authenticator.cpp:317] Authentication success
I1219 09:51:32.709707 31910 authenticatee.cpp:298] Authentication success
I1219 09:51:32.710252 31910 slave.cpp:860] Successfully authenticated with master master@172.17.0.3:36408
I1219 09:51:32.710525 31910 slave.cpp:1254] Will retry registration in 17.44437ms if necessary
I1219 09:51:32.709839 31908 master.cpp:5453] Successfully authenticated principal 'test-principal' at slave(228)@172.17.0.3:36408
I1219 09:51:32.710985 31908 master.cpp:4132] Registering slave at slave(228)@172.17.0.3:36408 (60ab6e727501) with id bded856d-1c7f-4fad-a8bc-3629ba8c59d3-S0
I1219 09:51:32.711645 31908 registrar.cpp:439] Applied 1 operations in 83191ns; attempting to update the 'registry'
I1219 09:51:32.709908 31912 authenticator.cpp:431] Authentication session cleanup for crammd5_authenticatee(510)@172.17.0.3:36408
I1219 09:51:32.713407 31908 log.cpp:683] Attempting to append 343 bytes to the log
I1219 09:51:32.713646 31912 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 3
I1219 09:51:32.714884 31911 replica.cpp:537] Replica received write request for position 3 from (6758)@172.17.0.3:36408
I1219 09:51:32.715221 31911 leveldb.cpp:341] Persisting action (362 bytes) to leveldb took 288909ns
I1219 09:51:32.715250 31911 replica.cpp:712] Persisted action at 3
I1219 09:51:32.716145 31912 replica.cpp:691] Replica received learned notice for position 3 from @0.0.0.0:0
I1219 09:51:32.716689 31912 leveldb.cpp:341] Persisting action (364 bytes) to leveldb took 512217ns
I1219 09:51:32.716716 31912 replica.cpp:712] Persisted action at 3
I1219 09:51:32.716737 31912 replica.cpp:697] Replica learned APPEND action at position 3
I1219 09:51:32.718426 31911 registrar.cpp:484] Successfully updated the 'registry' in 0ns
I1219 09:51:32.719441 31902 slave.cpp:3371] Received ping from slave-observer(228)@172.17.0.3:36408
I1219 09:51:32.719843 31909 log.cpp:702] Attempting to truncate the log to 3
I1219 09:51:32.719908 31911 master.cpp:4200] Registered slave bded856d-1c7f-4fad-a8bc-3629ba8c59d3-S0 at slave(228)@172.17.0.3:36408 (60ab6e727501) with cpus(*):2; mem(*):1024; disk(role1):2048; ports(*):[31000-32000]
I1219 09:51:32.720064 31911 slave.cpp:904] Registered with master master@172.17.0.3:36408; given slave ID bded856d-1c7f-4fad-a8bc-3629ba8c59d3-S0
I1219 09:51:32.720088 31911 fetcher.cpp:81] Clearing fetcher cache
I1219 09:51:32.720491 31911 slave.cpp:927] Checkpointing SlaveInfo to '/tmp/PersistentVolumeTest_BadACLDropCreateAndDestroy_gWLtnc/meta/slaves/bded856d-1c7f-4fad-a8bc-3629ba8c59d3-S0/slave.info'
I1219 09:51:32.720844 31909 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 4
I1219 09:51:32.720929 31911 slave.cpp:963] Forwarding total oversubscribed resources 
I1219 09:51:32.721017 31903 status_update_manager.cpp:181] Resuming sending status updates
I1219 09:51:32.721099 31911 master.cpp:4542] Received update of slave bded856d-1c7f-4fad-a8bc-3629ba8c59d3-S0 at slave(228)@172.17.0.3:36408 (60ab6e727501) with total oversubscribed resources 
I1219 09:51:32.721141 31905 hierarchical.cpp:465] Added slave bded856d-1c7f-4fad-a8bc-3629ba8c59d3-S0 (60ab6e727501) with cpus(*):2; mem(*):1024; disk(role1):2048; ports(*):[31000-32000] (allocated: )
I1219 09:51:32.721879 31911 replica.cpp:537] Replica received write request for position 4 from (6759)@172.17.0.3:36408
I1219 09:51:32.722293 31905 hierarchical.cpp:1423] No inverse offers to send out!
I1219 09:51:32.722337 31905 hierarchical.cpp:1101] Performed allocation for slave bded856d-1c7f-4fad-a8bc-3629ba8c59d3-S0 in 1.155563ms
I1219 09:51:32.722681 31905 hierarchical.cpp:521] Slave bded856d-1c7f-4fad-a8bc-3629ba8c59d3-S0 (60ab6e727501) updated with oversubscribed resources  (total: cpus(*):2; mem(*):1024; disk(role1):2048; ports(*):[31000-32000], allocated: cpus(*):2; mem(*):1024; ports(*):[31000-32000]; disk(role1):2048)
I1219 09:51:32.722713 31909 master.cpp:5252] Sending 1 offers to framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0000 (default) at scheduler-0333dddc-4b41-40ed-8853-a1aadf1f1879@172.17.0.3:36408
I1219 09:51:32.723031 31905 hierarchical.cpp:1329] No resources available to allocate!
I1219 09:51:32.723073 31905 hierarchical.cpp:1423] No inverse offers to send out!
I1219 09:51:32.723095 31905 hierarchical.cpp:1101] Performed allocation for slave bded856d-1c7f-4fad-a8bc-3629ba8c59d3-S0 in 368889ns
I1219 09:51:32.723191 31909 sched.cpp:811] Scheduler::resourceOffers took 113921ns
I1219 09:51:32.723410 31911 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 1.418243ms
I1219 09:51:32.723497 31911 replica.cpp:712] Persisted action at 4
I1219 09:51:32.724326 31907 replica.cpp:691] Replica received learned notice for position 4 from @0.0.0.0:0
I1219 09:51:32.724758 31907 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 329678ns
I1219 09:51:32.724917 31907 leveldb.cpp:399] Deleting ~2 keys from leveldb took 58317ns
I1219 09:51:32.725025 31907 replica.cpp:712] Persisted action at 4
I1219 09:51:32.725127 31907 replica.cpp:697] Replica learned TRUNCATE action at position 4
I1219 09:51:32.731515 31910 hierarchical.cpp:1329] No resources available to allocate!
I1219 09:51:32.731564 31910 hierarchical.cpp:1423] No inverse offers to send out!
I1219 09:51:32.731591 31910 hierarchical.cpp:1079] Performed allocation for 1 slaves in 239271ns
I1219 09:51:32.741710 31910 master.cpp:3055] Processing ACCEPT call for offers: [ bded856d-1c7f-4fad-a8bc-3629ba8c59d3-O0 ] on slave bded856d-1c7f-4fad-a8bc-3629ba8c59d3-S0 at slave(228)@172.17.0.3:36408 (60ab6e727501) for framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0000 (default) at scheduler-0333dddc-4b41-40ed-8853-a1aadf1f1879@172.17.0.3:36408
I1219 09:51:32.741770 31910 master.cpp:2843] Authorizing principal 'test-principal' to create volumes
E1219 09:51:32.742707 31910 master.cpp:1737] Dropping CREATE offer operation from framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0000 (default) at scheduler-0333dddc-4b41-40ed-8853-a1aadf1f1879@172.17.0.3:36408: Not authorized to create persistent volumes as 'test-principal'
I1219 09:51:32.743219 31910 hierarchical.cpp:880] Recovered cpus(*):2; mem(*):1024; ports(*):[31000-32000]; disk(role1):2048 (total: cpus(*):2; mem(*):1024; disk(role1):2048; ports(*):[31000-32000], allocated: ) on slave bded856d-1c7f-4fad-a8bc-3629ba8c59d3-S0 from framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0000
I1219 09:51:32.752542 31908 hierarchical.cpp:1423] No inverse offers to send out!
I1219 09:51:32.752590 31908 hierarchical.cpp:1079] Performed allocation for 1 slaves in 888401ns
I1219 09:51:32.753018 31908 master.cpp:5252] Sending 1 offers to framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0000 (default) at scheduler-0333dddc-4b41-40ed-8853-a1aadf1f1879@172.17.0.3:36408
I1219 09:51:32.753435 31908 sched.cpp:811] Scheduler::resourceOffers took 92252ns
I1219 09:51:32.761533 31878 sched.cpp:164] Version: 0.27.0
I1219 09:51:32.761931 31897 master.cpp:3570] Processing DECLINE call for offers: [ bded856d-1c7f-4fad-a8bc-3629ba8c59d3-O1 ] for framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0000 (default) at scheduler-0333dddc-4b41-40ed-8853-a1aadf1f1879@172.17.0.3:36408
I1219 09:51:32.762373 31897 sched.cpp:262] New master detected at master@172.17.0.3:36408
I1219 09:51:32.762451 31897 sched.cpp:272] No credentials provided. Attempting to register without authentication
I1219 09:51:32.762470 31897 sched.cpp:714] Sending SUBSCRIBE call to master@172.17.0.3:36408
I1219 09:51:32.762543 31897 sched.cpp:747] Will retry registration in 465.481193ms if necessary
I1219 09:51:32.762572 31898 hierarchical.cpp:880] Recovered cpus(*):2; mem(*):1024; ports(*):[31000-32000]; disk(role1):2048 (total: cpus(*):2; mem(*):1024; disk(role1):2048; ports(*):[31000-32000], allocated: ) on slave bded856d-1c7f-4fad-a8bc-3629ba8c59d3-S0 from framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0000
I1219 09:51:32.762722 31898 master.cpp:2197] Received SUBSCRIBE call for framework 'creator-framework' at scheduler-d2c3fe43-4283-4153-8ee5-533ebc1ac5ce@172.17.0.3:36408
I1219 09:51:32.762785 31898 master.cpp:1668] Authorizing framework principal 'creator-principal' to receive offers for role 'role1'
I1219 09:51:32.763036 31897 master.cpp:2268] Subscribing framework creator-framework with checkpointing disabled and capabilities [  ]
I1219 09:51:32.763464 31898 hierarchical.cpp:260] Added framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0001
I1219 09:51:32.763562 31897 sched.cpp:641] Framework registered with bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0001
I1219 09:51:32.763605 31897 sched.cpp:655] Scheduler::registered took 20669ns
I1219 09:51:32.763804 31908 master.cpp:2650] Processing SUPPRESS call for framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0000 (default) at scheduler-0333dddc-4b41-40ed-8853-a1aadf1f1879@172.17.0.3:36408
I1219 09:51:32.764343 31898 hierarchical.cpp:1423] No inverse offers to send out!
I1219 09:51:32.764382 31898 hierarchical.cpp:1079] Performed allocation for 1 slaves in 893765ns
I1219 09:51:32.764428 31898 hierarchical.cpp:953] Suppressed offers for framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0000
I1219 09:51:32.764746 31898 master.cpp:5252] Sending 1 offers to framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0001 (creator-framework) at scheduler-d2c3fe43-4283-4153-8ee5-533ebc1ac5ce@172.17.0.3:36408
I1219 09:51:32.765127 31898 sched.cpp:811] Scheduler::resourceOffers took 83608ns
I1219 09:51:32.773298 31900 hierarchical.cpp:1329] No resources available to allocate!
I1219 09:51:32.773339 31900 hierarchical.cpp:1423] No inverse offers to send out!
I1219 09:51:32.773365 31900 hierarchical.cpp:1079] Performed allocation for 1 slaves in 201759ns
I1219 09:51:32.782901 31898 master.cpp:3055] Processing ACCEPT call for offers: [ bded856d-1c7f-4fad-a8bc-3629ba8c59d3-O2 ] on slave bded856d-1c7f-4fad-a8bc-3629ba8c59d3-S0 at slave(228)@172.17.0.3:36408 (60ab6e727501) for framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0001 (creator-framework) at scheduler-d2c3fe43-4283-4153-8ee5-533ebc1ac5ce@172.17.0.3:36408
I1219 09:51:32.782961 31898 master.cpp:2843] Authorizing principal 'creator-principal' to create volumes
I1219 09:51:32.784190 31904 master.cpp:3362] Applying CREATE operation for volumes disk(role1)[id1:path1]:128 from framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0001 (creator-framework) at scheduler-d2c3fe43-4283-4153-8ee5-533ebc1ac5ce@172.17.0.3:36408 to slave bded856d-1c7f-4fad-a8bc-3629ba8c59d3-S0 at slave(228)@172.17.0.3:36408 (60ab6e727501)
I1219 09:51:32.784548 31904 master.cpp:6486] Sending checkpointed resources disk(role1)[id1:path1]:128 to slave bded856d-1c7f-4fad-a8bc-3629ba8c59d3-S0 at slave(228)@172.17.0.3:36408 (60ab6e727501)
I1219 09:51:32.786471 31904 hierarchical.cpp:642] Updated allocation of framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0001 on slave bded856d-1c7f-4fad-a8bc-3629ba8c59d3-S0 from cpus(*):2; mem(*):1024; ports(*):[31000-32000]; disk(role1):2048 to cpus(*):2; mem(*):1024; ports(*):[31000-32000]; disk(role1):1920; disk(role1)[id1:path1]:128
I1219 09:51:32.786929 31904 hierarchical.cpp:880] Recovered cpus(*):2; mem(*):1024; ports(*):[31000-32000]; disk(role1):1920; disk(role1)[id1:path1]:128 (total: cpus(*):2; mem(*):1024; disk(role1):1920; ports(*):[31000-32000]; disk(role1)[id1:path1]:128, allocated: ) on slave bded856d-1c7f-4fad-a8bc-3629ba8c59d3-S0 from framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0001
I1219 09:51:32.788035 31904 slave.cpp:2277] Updated checkpointed resources from  to disk(role1)[id1:path1]:128
I1219 09:51:32.795177 31902 hierarchical.cpp:1423] No inverse offers to send out!
I1219 09:51:32.795250 31902 hierarchical.cpp:1079] Performed allocation for 1 slaves in 1.357898ms
I1219 09:51:32.795897 31902 master.cpp:5252] Sending 1 offers to framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0001 (creator-framework) at scheduler-d2c3fe43-4283-4153-8ee5-533ebc1ac5ce@172.17.0.3:36408
I1219 09:51:32.796540 31897 sched.cpp:811] Scheduler::resourceOffers took 138880ns
I1219 09:51:32.803026 31902 master.cpp:3570] Processing DECLINE call for offers: [ bded856d-1c7f-4fad-a8bc-3629ba8c59d3-O3 ] for framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0001 (creator-framework) at scheduler-d2c3fe43-4283-4153-8ee5-533ebc1ac5ce@172.17.0.3:36408
I1219 09:51:32.804143 31902 hierarchical.cpp:880] Recovered cpus(*):2; mem(*):1024; ports(*):[31000-32000]; disk(role1):1920; disk(role1)[id1:path1]:128 (total: cpus(*):2; mem(*):1024; disk(role1):1920; ports(*):[31000-32000]; disk(role1)[id1:path1]:128, allocated: ) on slave bded856d-1c7f-4fad-a8bc-3629ba8c59d3-S0 from framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0001
I1219 09:51:32.804622 31907 master.cpp:2650] Processing SUPPRESS call for framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0001 (creator-framework) at scheduler-d2c3fe43-4283-4153-8ee5-533ebc1ac5ce@172.17.0.3:36408
I1219 09:51:32.804729 31907 hierarchical.cpp:953] Suppressed offers for framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0001
I1219 09:51:32.805140 31897 master.cpp:3649] Processing REVIVE call for framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0000 (default) at scheduler-0333dddc-4b41-40ed-8853-a1aadf1f1879@172.17.0.3:36408
I1219 09:51:32.805250 31897 hierarchical.cpp:973] Removed offer filters for framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0000
I1219 09:51:32.806507 31897 hierarchical.cpp:1423] No inverse offers to send out!
I1219 09:51:32.806562 31897 hierarchical.cpp:1079] Performed allocation for 1 slaves in 1.284779ms
I1219 09:51:32.807067 31897 master.cpp:5252] Sending 1 offers to framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0000 (default) at scheduler-0333dddc-4b41-40ed-8853-a1aadf1f1879@172.17.0.3:36408
../../src/tests/persistent_volume_tests.cpp:1336: Failure
Mock function called more times than expected - returning directly.
    Function call: resourceOffers(0x7ffff9edb3a0, @0x7f71079798f0 { 144-byte object <F0-1B 42-14 71-7F 00-00 00-00 00-00 00-00 00-00 D0-96 02-F0 70-7F 00-00 50-97 02-F0 70-7F 00-00 20-A1 02-F0 70-7F 00-00 50-E0 01-F0 70-7F 00-00 B0-9F 02-F0 70-7F 00-00 00-32 01-F0 70-7F 00-00 ... 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 70-7F 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 1F-00 00-00> })
         Expected: to be called once
           Actual: called twice - over-saturated and active
I1219 09:51:32.807899 31897 sched.cpp:811] Scheduler::resourceOffers took 406435ns
I1219 09:51:32.820523 31909 hierarchical.cpp:1329] No resources available to allocate!
I1219 09:51:32.820611 31909 hierarchical.cpp:1423] No inverse offers to send out!
I1219 09:51:32.820642 31909 hierarchical.cpp:1079] Performed allocation for 1 slaves in 448034ns
2015-12-19 09:51:33,146:31878(0x7f6ff6ffd700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:39991] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2015-12-19 09:51:36,482:31878(0x7f6ff6ffd700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:39991] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2015-12-19 09:51:39,818:31878(0x7f6ff6ffd700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:39991] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2015-12-19 09:51:43,155:31878(0x7f6ff6ffd700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:39991] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2015-12-19 09:51:46,490:31878(0x7f6ff6ffd700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:39991] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
../../src/tests/persistent_volume_tests.cpp:1411: Failure
Failed to wait 15secs for offers
I1219 09:51:47.829073 31909 master.cpp:1130] Framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0001 (creator-framework) at scheduler-d2c3fe43-4283-4153-8ee5-533ebc1ac5ce@172.17.0.3:36408 disconnected
I1219 09:51:47.829169 31909 master.cpp:2493] Disconnecting framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0001 (creator-framework) at scheduler-d2c3fe43-4283-4153-8ee5-533ebc1ac5ce@172.17.0.3:36408
I1219 09:51:47.829200 31909 master.cpp:2517] Deactivating framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0001 (creator-framework) at scheduler-d2c3fe43-4283-4153-8ee5-533ebc1ac5ce@172.17.0.3:36408
I1219 09:51:47.829366 31909 master.cpp:1154] Giving framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0001 (creator-framework) at scheduler-d2c3fe43-4283-4153-8ee5-533ebc1ac5ce@172.17.0.3:36408 0ns to failover
I1219 09:51:47.829720 31909 hierarchical.cpp:366] Deactivated framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0001
I1219 09:51:47.831614 31907 master.cpp:5100] Framework failover timeout, removing framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0001 (creator-framework) at scheduler-d2c3fe43-4283-4153-8ee5-533ebc1ac5ce@172.17.0.3:36408
I1219 09:51:47.831748 31907 master.cpp:5835] Removing framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0001 (creator-framework) at scheduler-d2c3fe43-4283-4153-8ee5-533ebc1ac5ce@172.17.0.3:36408
I1219 09:51:47.833314 31897 slave.cpp:2012] Asked to shut down framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0001 by master@172.17.0.3:36408
W1219 09:51:47.833421 31897 slave.cpp:2027] Cannot shut down unknown framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0001
I1219 09:51:47.834002 31897 hierarchical.cpp:321] Removed framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0001
I1219 09:51:47.843332 31908 master.cpp:1130] Framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0000 (default) at scheduler-0333dddc-4b41-40ed-8853-a1aadf1f1879@172.17.0.3:36408 disconnected
I1219 09:51:47.843521 31908 master.cpp:2493] Disconnecting framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0000 (default) at scheduler-0333dddc-4b41-40ed-8853-a1aadf1f1879@172.17.0.3:36408
I1219 09:51:47.843663 31908 master.cpp:2517] Deactivating framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0000 (default) at scheduler-0333dddc-4b41-40ed-8853-a1aadf1f1879@172.17.0.3:36408
W1219 09:51:47.844665 31908 master.hpp:1758] Master attempted to send message to disconnected framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0000 (default) at scheduler-0333dddc-4b41-40ed-8853-a1aadf1f1879@172.17.0.3:36408
I1219 09:51:47.845077 31908 master.cpp:1154] Giving framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0000 (default) at scheduler-0333dddc-4b41-40ed-8853-a1aadf1f1879@172.17.0.3:36408 0ns to failover
I1219 09:51:47.844887 31903 hierarchical.cpp:366] Deactivated framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0000
I1219 09:51:47.845728 31903 hierarchical.cpp:880] Recovered cpus(*):2; mem(*):1024; ports(*):[31000-32000]; disk(role1):1920; disk(role1)[id1:path1]:128 (total: cpus(*):2; mem(*):1024; disk(role1):1920; ports(*):[31000-32000]; disk(role1)[id1:path1]:128, allocated: ) on slave bded856d-1c7f-4fad-a8bc-3629ba8c59d3-S0 from framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0000
../../src/tests/persistent_volume_tests.cpp:1404: Failure
Actual function call count doesn't match EXPECT_CALL(sched1, resourceOffers(&driver1, _))...
         Expected: to be called once
           Actual: never called - unsatisfied and active
I1219 09:51:47.847968 31902 master.cpp:5100] Framework failover timeout, removing framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0000 (default) at scheduler-0333dddc-4b41-40ed-8853-a1aadf1f1879@172.17.0.3:36408
I1219 09:51:47.848068 31902 master.cpp:5835] Removing framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0000 (default) at scheduler-0333dddc-4b41-40ed-8853-a1aadf1f1879@172.17.0.3:36408
I1219 09:51:47.848553 31902 slave.cpp:2012] Asked to shut down framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0000 by master@172.17.0.3:36408
W1219 09:51:47.848644 31902 slave.cpp:2027] Cannot shut down unknown framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0000
I1219 09:51:47.848999 31902 hierarchical.cpp:321] Removed framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0000
I1219 09:51:47.849782 31912 master.cpp:930] Master terminating
I1219 09:51:47.851934 31899 hierarchical.cpp:496] Removed slave bded856d-1c7f-4fad-a8bc-3629ba8c59d3-S0
I1219 09:51:47.855919 31907 slave.cpp:3417] master@172.17.0.3:36408 exited
W1219 09:51:47.856021 31907 slave.cpp:3420] Master disconnected! Waiting for a new master to be elected
I1219 09:51:47.908278 31878 slave.cpp:601] Slave terminating
[  FAILED  ] PersistentVolumeTest.BadACLDropCreateAndDestroy (15298 ms)
{noformat}",Bug,Major,jieyu,2016-01-09T01:25:11.000+0000,5,Resolved,Complete,PersistentVolumeTest.BadACLDropCreateAndDestroy is flaky,2016-01-11T00:42:31.000+0000,MESOS-4208,1.0,mesos,Mesosphere Sprint 26
greggomann,2015-12-19T00:22:14.000+0000,greggomann,"In the past, some bugs have been introduced into the codebase due to a lack of {{defer()}} where it should have been used. It would be useful to add an example of this to the {{defer()}} documentation.",Documentation,Minor,greggomann,2016-01-15T01:17:12.000+0000,5,Resolved,Complete,Add an example bug due to a lack of defer() to the defer() documentation,2016-01-15T01:17:19.000+0000,MESOS-4207,2.0,mesos,Mesosphere Sprint 26
kaysoky,2015-12-18T23:59:21.000+0000,neilc,"This should include:
* Default logging behavior for master, agent, framework, executor, task.
* Master/agent:
** A summary of log-related flags.
** {{glog}} specific options.
* Separation of master/agent logs from container logs.
* The {{ContainerLogger}} module.",Documentation,Major,neilc,2016-01-25T04:17:42.000+0000,5,Resolved,Complete,Write new logging-related documentation,2016-01-27T00:49:30.000+0000,MESOS-4206,3.0,mesos,Mesosphere Sprint 26
jojy,2015-12-18T23:13:38.000+0000,jojy,"libprocess Socket shares the ownership of the file descriptor with libevent. In
the destructor of the libprocess libevent_ssl socket, we call ssl shutdown which
is executed asynchronously. This causes the libprocess socket file descriptor tobe closed (and possibly reused) when the same file descriptor could be used bylibevent/ssl. Since we set the shutdown options as SSL_RECEIVED_SHUTDOWN, we leave the any write operations to continue with possibly closed file descriptor.

This issue manifests as junk characters written to the file that has been handled the closed socket file descriptor (by OS) that has the above issue.",Bug,Major,jojy,2015-12-19T00:00:41.000+0000,5,Resolved,Complete,Race in SSL socket shutdown ,2015-12-19T00:00:41.000+0000,MESOS-4202,5.0,mesos,Mesosphere Sprint 24
gradywang,2015-12-18T19:30:34.000+0000,neilc,"As far as I can see, we currently have NO test cases for behavior when weights are defined.",Task,Major,neilc,2016-02-19T09:19:19.000+0000,5,Resolved,Complete,Test case(s) for weights + allocation behavior,2016-03-09T08:56:39.000+0000,MESOS-4200,2.0,mesos,Mesosphere Sprint 27
hartem,2015-12-18T19:12:37.000+0000,gabriel.hartmann@gmail.com,"If I create a persistent volume on a reserved disk resource, I am able to write data in excess of my reserved size.

Disk resource reservation should be enforced just as ""cpus"" and ""mem"" reservations are enforced.",Bug,Major,gabriel.hartmann@gmail.com,2016-01-04T18:33:28.000+0000,5,Resolved,Complete,Disk Resource Reservation is NOT Enforced for Persistent Volumes,2016-01-04T19:31:36.000+0000,MESOS-4198,3.0,mesos,Mesosphere Sprint 25
,2015-12-18T14:49:52.000+0000,alexr,"We do not support creating {{Master}} instance without an {{Authorizer}} in tests: https://github.com/apache/mesos/blob/aa497e81c945677c570484a8aa1a8c8b2e979dfd/src/tests/cluster.cpp#L217. This leads to a segfault when {{masterFlags.acls = None();}} is used in a test, while it's a valid use case and should be allowed.

Alternatively, we use {{masterFlags.acls = ACLs();}}, which triggers creation of {{LocalAuthorizer}} with emtpy {{ACLs}}, which seems to be semantically equal to the absence of an authorizer, given {{permissive}} flag is {{true}}. This equivalence should be verified by a test.",Bug,Major,alexr,,10020,Accepted,In Progress,Enable running tests without authorizer.,2016-01-07T15:00:44.000+0000,MESOS-4196,3.0,mesos,
greggomann,2015-12-18T07:23:25.000+0000,greggomann,"Currently, there exist no dynamic reservation tests that include authorization of a framework that is registered with no principal. This should be added in order to more comprehensively test the dynamic reservation code.",Improvement,Major,greggomann,2016-01-23T19:45:01.000+0000,5,Resolved,Complete,Add dynamic reservation tests with no principal,2016-02-01T18:38:47.000+0000,MESOS-4195,1.0,mesos,Mesosphere Sprint 27
jojy,2015-12-18T01:32:51.000+0000,kaysoky,"If you run:
{{bin/mesos-tests.sh --gtest_filter=""*MesosContainerizer*"" --gtest_repeat=-1 --gtest_break_on_failure}}

And then check:
{{lsof | grep mesos}}

The number of open pipes will grow linearly with the number of test repetitions.",Bug,Major,kaysoky,,10020,Accepted,In Progress,MesosContainerizer* tests leak FDs (pipes),2015-12-18T17:47:51.000+0000,MESOS-4194,2.0,mesos,
anandmazumdar,2015-12-17T19:59:07.000+0000,anandmazumdar,"Currently, we don't have any documentation for:

- How Mesos implements API versioning ?
- How are protobufs versioned and how does mesos handle them internally ?
- What do contributors need to do when they make a change to a external user facing protobuf ?

The relevant design doc:
https://docs.google.com/document/d/1-iQjo6778H_fU_1Zi_Yk6szg8qj-wqYgVgnx7u3h6OU/edit#heading=h.2gkbjz6amn7b
",Bug,Major,anandmazumdar,2015-12-31T20:27:49.000+0000,5,Resolved,Complete,Add documentation for API Versioning,2015-12-31T20:27:49.000+0000,MESOS-4192,3.0,mesos,Mesosphere Sprint 25
gradywang,2015-12-17T11:19:24.000+0000,gradywang,"A short design doc for dynamic weights, it will focus on /weights API and the changes to the allocator API.",Documentation,Major,gradywang,2016-01-04T10:32:30.000+0000,5,Resolved,Complete,Create a Design Doc for dynamic weights.,2016-01-04T10:32:30.000+0000,MESOS-4190,3.0,mesos,Mesosphere Sprint 25
neilc,2015-12-17T03:22:03.000+0000,neilc,"Links from one documentation page to another should not use absolute URLs (e.g., {{http://mesos.apache.org/documentation/latest/...}}) for several good reasons. For instance, absolute URLs break when the docs are generated/previewed locally.",Documentation,Minor,neilc,2015-12-17T05:12:35.000+0000,5,Resolved,Complete,Avoid using absolute URLs in documentation pages,2015-12-17T05:12:35.000+0000,MESOS-4187,1.0,mesos,Mesosphere Sprint 24
gilbert,2015-12-17T02:31:01.000+0000,gilbert,"Currently we only support v2 docker manifest serialization method. When we read docker image spec locally from disk, we should be able to parse v1 docker manifest as protobuf, which will make it easier to gather runtime config and other necessary info.",Improvement,Major,gilbert,2015-12-18T23:05:29.000+0000,5,Resolved,Complete,Serialize docker v1 image spec as protobuf,2015-12-18T23:05:29.000+0000,MESOS-4186,2.0,mesos,Mesosphere Sprint 24
klueska,2015-12-16T21:58:54.000+0000,klueska,"Jenkins builds are now consistently failing for centos 7, withe the failure:

checking value of Java system property 'java.home'...
/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.65-3.b17.el7.x86_64/jre
configure: error: could not guess JAVA_HOME

They also fail early on during 'bootstrap' with a missing 'which' command.

The solution is to update support/docker_build.sh to install 'which' as well as make sure the proper versions of java are installed during the installation process.

The problem here is that we install maven BEFORE installing java-1.7.0-openjdk-devel, causing maven to pull in a dependency on java-1.8.0-openjdk. This causes problems with finding the proper java.home in our mesos/configure script because of the mismatch between the most up to date jre (1.8.0) and the most up to date development tools (1.7.0).  We can either update the script to pull in the 1.8 devel tools or move our dependence on maven until AFTER our installation of java-1.7.0-openjdk-devel.  Unclear what the best solution is.",Bug,Major,klueska,2015-12-17T23:11:33.000+0000,5,Resolved,Complete,Jenkins builds for Centos fail with missing 'which' utility and incorrect 'java.home',2015-12-17T23:11:33.000+0000,MESOS-4184,3.0,mesos,Mesosphere Sprint 24
alexr,2015-12-16T15:37:08.000+0000,alexr,"We often include complex headers like {{<ostream>}} in "".hpp"" files to define {{operator<<()}} inline (e.g. ""mesos/authorizer/authorizer.hpp""). Instead, we can move definitions to corresponding "".cpp"" files and replace stream headers with {{iosfwd}}, for example, this is partially done for {{URI}} in ""mesos/uri/uri.hpp"".",Improvement,Minor,alexr,2016-01-04T10:43:27.000+0000,5,Resolved,Complete,Move operator<< definitions to .cpp files and include <iosfwd> in .hpp where possible.,2016-01-04T11:29:00.000+0000,MESOS-4183,3.0,mesos,Mesosphere Sprint 25
greggomann,2015-12-15T21:08:26.000+0000,greggomann,"This ticket is the second in a series that adds authorization support for persistent volumes.

Methods {{Master::authorizeCreateVolume()}} and {{Master::authorizeDestroyVolume}} must be added to allow the Master to authorize these operations.",Bug,Major,greggomann,2015-12-19T01:31:09.000+0000,5,Resolved,Complete,Extend `Master` to authorize persistent volumes,2015-12-19T01:31:09.000+0000,MESOS-4179,1.0,mesos,Mesosphere Sprint 24
greggomann,2015-12-15T21:04:39.000+0000,greggomann,"This ticket is the first in a series that adds authorization support for persistent volume creation and destruction.

Persistent volumes should be authorized with the {{principal}} of the reserving entity (framework or master). The idea is to introduce {{Create}} and {{Destroy}} into the ACL.

{code}
  message Create {
    // Subjects.
    required Entity principals = 1;

    // Objects? Perhaps the kind of volume? allowed permissions?
  }

  message Destroy {
    // Subjects.
    required Entity principals = 1;

    // Objects.
    required Entity creator_principals = 2;
  }
{code}

ACLs for volume creation and destruction must be added to {{authorizer.proto}}, and the appropriate function overloads must be added to the Authorizer.",Bug,Major,greggomann,2015-12-19T01:34:44.000+0000,5,Resolved,Complete,Add persistent volume support to the Authorizer,2015-12-19T01:34:45.000+0000,MESOS-4178,1.0,mesos,Mesosphere Sprint 24
anandmazumdar,2015-12-15T19:18:17.000+0000,anandmazumdar,We need a user doc similar to the corresponding one for the Scheduler HTTP API.,Bug,Major,anandmazumdar,2016-01-06T01:10:06.000+0000,5,Resolved,Complete,Create a user doc for Executor HTTP API,2016-02-27T00:05:04.000+0000,MESOS-4177,3.0,mesos,Mesosphere Sprint 24
lins05,2015-12-15T17:52:31.000+0000,alexr,"The {{ContentType/SchedulerTest.Decline}} test takes more than {{1s}} to finish on my Mac OS 10.10.4:
{code}
ContentType/SchedulerTest.Decline/0 (1022 ms)
{code}",Improvement,Minor,alexr,2016-04-12T15:39:25.000+0000,5,Resolved,Complete,ContentType/SchedulerTest.Decline is slow.,2016-04-12T15:39:25.000+0000,MESOS-4175,1.0,mesos,Mesosphere Sprint 33
qiujian,2015-12-15T17:50:52.000+0000,alexr,"The {{HookTest.VerifySlaveLaunchExecutorHook}} test takes more than {{5s}} to finish on my Mac OS 10.10.4:
{code}
HookTest.VerifySlaveLaunchExecutorHook (5061 ms)
{code}",Improvement,Minor,alexr,2016-04-12T15:32:04.000+0000,5,Resolved,Complete,HookTest.VerifySlaveLaunchExecutorHook is slow.,2016-04-12T15:32:04.000+0000,MESOS-4174,1.0,mesos,Mesosphere Sprint 33
haosdent@gmail.com,2015-12-15T17:46:33.000+0000,alexr,"The {{OversubscriptionTest.RemoveCapabilitiesOnSchedulerFailover}} test takes more than {{1s}} to finish on my Mac OS 10.10.4:
{code}
OversubscriptionTest.RemoveCapabilitiesOnSchedulerFailover (1018 ms)
{code}",Improvement,Minor,alexr,2016-04-12T15:30:24.000+0000,5,Resolved,Complete,OversubscriptionTest.RemoveCapabilitiesOnSchedulerFailover is slow.,2016-04-12T15:30:24.000+0000,MESOS-4171,1.0,mesos,Mesosphere Sprint 33
haosdent@gmail.com,2015-12-15T17:45:31.000+0000,alexr,"The {{OversubscriptionTest.UpdateAllocatorOnSchedulerFailover}} test takes more than {{1s}} to finish on my Mac OS 10.10.4:
{code}
OversubscriptionTest.UpdateAllocatorOnSchedulerFailover (1018 ms)
{code}",Improvement,Minor,alexr,2016-04-12T15:29:00.000+0000,5,Resolved,Complete,OversubscriptionTest.UpdateAllocatorOnSchedulerFailover is slow.,2016-04-12T15:29:00.000+0000,MESOS-4170,1.0,mesos,Mesosphere Sprint 33
haosdent@gmail.com,2015-12-15T17:39:48.000+0000,alexr,"The {{MasterTest.OfferTimeout}} test takes more than {{1s}} to finish on my Mac OS 10.10.4:
{code}
MasterTest.OfferTimeout (1053 ms)
{code}",Improvement,Minor,alexr,2016-04-12T15:26:04.000+0000,5,Resolved,Complete,MasterTest.OfferTimeout is slow.,2016-04-12T15:26:04.000+0000,MESOS-4167,1.0,mesos,Mesosphere Sprint 33
haosdent@gmail.com,2015-12-15T17:38:26.000+0000,alexr,"The {{MasterTest.LaunchCombinedOfferTest}} test takes more than {{2s}} to finish on my Mac OS 10.10.4:
{code}
MasterTest.LaunchCombinedOfferTest (2023 ms)
{code}",Improvement,Minor,alexr,2016-04-12T15:24:04.000+0000,5,Resolved,Complete,MasterTest.LaunchCombinedOfferTest is slow.,2016-04-12T15:24:12.000+0000,MESOS-4166,1.0,mesos,Mesosphere Sprint 33
haosdent@gmail.com,2015-12-15T17:37:12.000+0000,alexr,"The {{MasterTest.MasterInfoOnReElection}} test takes more than {{1s}} to finish on my Mac OS 10.10.4:
{code}
MasterTest.MasterInfoOnReElection (1024 ms)
{code}",Improvement,Minor,alexr,2016-04-12T15:22:35.000+0000,5,Resolved,Complete,MasterTest.MasterInfoOnReElection is slow.,2016-04-12T15:22:35.000+0000,MESOS-4165,1.0,mesos,Mesosphere Sprint 33
haosdent@gmail.com,2015-12-15T17:35:58.000+0000,alexr,"The {{MasterTest.RecoverResources}} test takes more than {{1s}} to finish on my Mac OS 10.10.4:
{code}
MasterTest.RecoverResources (1018 ms)
{code}",Improvement,Minor,alexr,2016-04-12T15:17:11.000+0000,5,Resolved,Complete,MasterTest.RecoverResources is slow.,2016-04-12T15:17:11.000+0000,MESOS-4164,1.0,mesos,Mesosphere Sprint 33
lins05,2015-12-15T17:23:42.000+0000,alexr,"On Mac OS 10.10.4, some tests take longer than {{1s}} to finish:
{code}
RecoverTest.AutoInitialization (1003 ms)
RecoverTest.AutoInitializationRetry (1000 ms)
{code}",Improvement,Minor,alexr,2016-04-12T15:15:31.000+0000,5,Resolved,Complete,Log recover tests are slow.,2016-04-12T15:15:31.000+0000,MESOS-4160,1.0,mesos,Mesosphere Sprint 33
gyliu,2015-12-15T05:37:25.000+0000,gyliu,"The mesos is now using teardown framework to shutdown a framework but the acls are still using shutdown_framework, it is better to rename shutdown_framework to teardown_framework for acl to keep consistent.

This is a post review request for https://reviews.apache.org/r/40829/",Bug,Minor,gyliu,2016-01-21T12:34:43.000+0000,5,Resolved,Complete,Rename shutdown_frameworks to teardown_frameworks,2016-03-03T08:15:59.000+0000,MESOS-4154,2.0,mesos,Mesosphere Sprint 27
kaysoky,2015-12-14T20:32:54.000+0000,kaysoky,"The {{ContainerLoggers}} are intended to be isolated from agent failover, in the same way that executors do not crash when the agent process crashes.

For default {{ContainerLogger}} s, like the {{SandboxContainerLogger}} and the (tentatively named) {{TruncatingSandboxContainerLogger}}, the log files are exposed during agent recovery regardless.

For non-default {{ContainerLogger}} s, the recovery of executor metadata may be necessary to rebuild endpoints that expose the logs.  This can be implemented as part of {{Containerizer::recover}}.",Task,Major,kaysoky,2016-01-25T04:17:25.000+0000,5,Resolved,Complete,Implement container logger module metadata recovery,2016-01-25T04:17:25.000+0000,MESOS-4150,3.0,mesos,Mesosphere Sprint 26
arojas,2015-12-14T13:36:14.000+0000,alexr,"To authenticate quota requests we allowed {{QuotaHandler}} to call private {{Http::authenticate()}} function. Once MESOS-3231 lands we do not need neither this injection, nor {{authenticate()}} calls in the {{QuotaHandler}}.",Improvement,Major,alexr,2016-01-13T14:36:21.000+0000,5,Resolved,Complete,Clean up authentication implementation for quota,2016-01-15T00:32:51.000+0000,MESOS-4149,1.0,mesos,
gradywang,2015-12-13T18:27:31.000+0000,mandoskippy,"When working with Dynamic reservations via the /reserve and /unreserve endpoints, it is possible to reserve resources for roles that have not been specified via the --roles flag on the master.  However, these roles are not usable because the roles have not been defined, nor are they added to the list of roles available. 

Per the mailing list, changing roles after the fact is not possible at this time. (That may be another JIRA), more importantly, the /reserve and /unreserve end points should not allow reservation of roles not specified by --roles.  ",Bug,Minor,mandoskippy,,10006,Reviewable,New,Reserve/UnReserve Dynamic Reservation Endpoints allow reservations on non-existing roles,2016-03-03T07:10:07.000+0000,MESOS-4143,2.0,mesos,Mesosphere Sprint 24
kaysoky,2015-12-11T23:51:28.000+0000,kaysoky,"Adding a hook inside the Docker containerizer is slightly more involved than the Mesos containerizer.

Docker executors/tasks perform plain-file logging in different places depending on whether the agent is in a Docker container itself
|| Agent || Code ||
| Not in container | {{DockerContainerizerProcess::launchExecutorProcess}} |
| In container | {{Docker::run}} in a {{mesos-docker-executor}} process |

This means a {{ContainerLogger}} will need to be loaded or hooked into the {{mesos-docker-executor}}.  Or we will need to change how piping in done in {{mesos-docker-executor}}.",Task,Major,kaysoky,2015-12-29T23:55:01.000+0000,5,Resolved,Complete,Modularize plain-file logging for executor/task logs launched with the Docker Containerizer,2015-12-29T23:55:01.000+0000,MESOS-4137,3.0,mesos,Mesosphere Sprint 24
kaysoky,2015-12-11T23:40:22.000+0000,kaysoky,"One of the major problems this logger module aims to solve is overflowing executor/task log files.  Log files are simply written to disk, and are not managed other than via occasional garbage collection by the agent process (and this only deals with terminated executors).

We should add a {{ContainerLogger}} module that truncates logs as it reaches a configurable maximum size.  Additionally, we should determine if the web UI's {{pailer}} needs to be changed to deal with logs that are not append-only.

This will be a non-default module which will also serve as an example for how to implement the module.",Improvement,Major,kaysoky,2016-01-25T04:16:56.000+0000,5,Resolved,Complete,Add a ContainerLogger module that restrains log sizes,2016-01-25T04:16:56.000+0000,MESOS-4136,3.0,mesos,Mesosphere Sprint 24
lins05,2015-12-11T09:44:39.000+0000,bernd-mesos,"The fetcher uses libcurl for downloading content from HTTP, HTTPS, etc. There is no source code in the pertinent parts of ""net.hpp"" that deals with proxy settings. However, libcurl automatically picks up certain environment variables and adjusts its settings accordingly. See ""man libcurl-tutorial"" for details. See section ""Proxies"", subsection ""Environment Variables"". If you follow this recipe in your Mesos agent startup script, you can use a proxy. 

We should document this in the fetcher (cache) doc (http://mesos.apache.org/documentation/latest/fetcher/).
",Documentation,Major,bernd-mesos,2016-01-15T10:27:01.000+0000,5,Resolved,Complete,Document how the fetcher can reach across a proxy connection.,2016-01-15T10:28:40.000+0000,MESOS-4130,1.0,mesos,
alexr,2015-12-11T08:40:07.000+0000,alexr,For clarity we want to refactor the factory section in the allocator and explain the purpose (and necessity) of all sorters.,Improvement,Major,alexr,2016-01-16T23:14:48.000+0000,5,Resolved,Complete,Refactor sorter factories in allocator and improve comments around them.,2016-01-16T23:14:48.000+0000,MESOS-4128,3.0,mesos,Mesosphere Sprint 26
,2015-12-11T08:33:18.000+0000,alexr,"As pointed out by [~anandmazumdar] in https://reviews.apache.org/r/40905/, we should make sure we set the {{Content-Type}} files for some responses.",Improvement,Major,alexr,,10020,Accepted,In Progress,Ensure `Content-Type` field is set for some responses.,2016-04-25T22:09:37.000+0000,MESOS-4127,3.0,mesos,
jjanco,2015-12-11T08:30:04.000+0000,alexr,"Consider constructing the error string in {{MethodNotAllowed}} rather than at the invocation site. Currently we want all error messages follow the same pattern, so instead of writing
{code}
return MethodNotAllowed({""POST""}, ""Expecting 'POST', received '"" + request.method + ""'"");
{code}
we can write something like
{code}
MethodNotAllowed({""POST""}, request.method)`
{code}
",Improvement,Minor,alexr,,10006,Reviewable,New,Construct the error string in `MethodNotAllowed`.,2016-04-22T15:10:40.000+0000,MESOS-4126,1.0,mesos,
jojy,2015-12-10T17:50:12.000+0000,jojy,RegistryClient tests show flakiness which manifests as socket timeouts or unexpected buffer showing up in the blobs. Investigate them for possible race conditions.,Improvement,Major,jojy,2015-12-10T17:51:16.000+0000,5,Resolved,Complete,Fix possible race conditions in registry client tests.,2015-12-10T17:51:58.000+0000,MESOS-4115,5.0,mesos,Mesosphere Sprint 24
avinash@mesosphere.io,2015-12-10T15:58:28.000+0000,sargun,"We would like to extend the Mesos protocol buffer 'Port' to include an optional repeated string named ""VIP"" - to map it to a well known virtual IP, or virtual hostname for discovery purposes.

We also want this field exposed in DiscoveryInfo in state.json.",Wish,Trivial,sargun,2016-01-06T22:49:05.000+0000,5,Resolved,Complete,Add field VIP to message Port,2016-01-06T22:49:38.000+0000,MESOS-4114,2.0,mesos,Mesosphere Sprint 25
yongtang,2015-12-10T12:05:52.000+0000,mcypark,"This ticket is regarding the libprocess gtest helpers in {{3rdparty/libprocess/include/process/gtest.hpp}}.

The pattern in this file seems to be a set of macros:

* {{AWAIT_ASSERT_<STATE>_FOR}}
* {{AWAIT_ASSERT_<STATE>}} -- default of 15 seconds
* {{AWAIT_<STATE>\_FOR}} -- alias for {{AWAIT_ASSERT_<STATE>_FOR}}
* {{AWAIT_<STATE>}} -- alias for {{AWAIT_ASSERT_<STATE>}}
* {{AWAIT_EXPECT_<STATE>_FOR}}
* {{AWAIT_EXPECT_<STATE>}} -- default of 15 seconds

(1) {{AWAIT_EQ_FOR}} should be added for completeness.

(2) In {{gtest}}, we've got {{EXPECT_EQ}} as well as the {{bool}}-specific versions: {{EXPECT_TRUE}} and {{EXPECT_FALSE}}.

We should adopt this pattern in these helpers as well. Keeping the pattern above in mind, the following are missing:

* {{AWAIT_ASSERT_TRUE_FOR}}
* {{AWAIT_ASSERT_TRUE}}
* {{AWAIT_ASSERT_FALSE_FOR}}
* {{AWAIT_ASSERT_FALSE}}
* {{AWAIT_EXPECT_TRUE_FOR}}
* {{AWAIT_EXPECT_FALSE_FOR}}

(3) There are HTTP response related macros at the bottom of the file, e.g. {{AWAIT_EXPECT_RESPONSE_STATUS_EQ}}, however these are missing their {{ASSERT}} counterparts.

-(4) The reason for (3) presumably is because we reach for {{EXPECT}} over {{ASSERT}} in general due to the test suite crashing behavior of {{ASSERT}}. If this is the case, it would be worthwhile considering whether macros such as {{AWAIT_READY}} should alias {{AWAIT_EXPECT_READY}} rather than {{AWAIT_ASSERT_READY}}.-

(5) There are a few more missing macros, given {{AWAIT_EQ_FOR}} and {{AWAIT_EQ}} which aliases to {{AWAIT_ASSERT_EQ_FOR}} and {{AWAIT_ASSERT_EQ}} respectively, we should also add {{AWAIT_TRUE_FOR}}, {{AWAIT_TRUE}}, {{AWAIT_FALSE_FOR}}, and {{AWAIT_FALSE}} as well.",Task,Major,mcypark,2016-04-04T06:22:30.000+0000,5,Resolved,Complete,Clean up libprocess gtest macros,2016-04-04T06:22:30.000+0000,MESOS-4112,2.0,mesos,Mesosphere Sprint 32
hausdorff,2015-12-10T02:14:08.000+0000,hausdorff,"In the C standard library, `errno` records the last error on a thread. You can pretty-print it with `strerror`.

In Stout, we report these errors with `ErrnoError`.

The Windows API has something similar, called `GetLastError()`. The way to pretty-print this is hilariously unintuitive and terrible, so in this case it is actually very beneficial to wrap it with something similar to `ErrnoError`, maybe called `WindowsError`.",Bug,Major,hausdorff,2015-12-14T23:50:22.000+0000,5,Resolved,Complete,Implement `WindowsError` to correspond with `ErrnoError`.,2015-12-14T23:50:22.000+0000,MESOS-4110,5.0,mesos,Mesosphere Sprint 24
bmahler,2015-12-10T01:55:08.000+0000,kaysoky,"Output of the test:
{code}
[ RUN      ] HTTPConnectionTest.ClosingResponse
I1210 01:20:27.048532 26671 process.cpp:3077] Handling HTTP event for process '(22)' with path: '/(22)/get'
../../../3rdparty/libprocess/src/tests/http_tests.cpp:919: Failure
Actual function call count doesn't match EXPECT_CALL(*http.process, get(_))...
         Expected: to be called twice
           Actual: called once - unsatisfied and active
[  FAILED  ] HTTPConnectionTest.ClosingResponse (43 ms)
{code}",Bug,Minor,kaysoky,2015-12-11T19:45:57.000+0000,5,Resolved,Complete,HTTPConnectionTest.ClosingResponse is flaky,2015-12-17T23:39:13.000+0000,MESOS-4109,1.0,mesos,Mesosphere Sprint 24
hausdorff,2015-12-10T01:14:32.000+0000,hausdorff,"Used basically exclusively for testing, this insecure and otherwise-not-quite-suitable-for-prod function needs to work to run what will eventually become the FS tests.",Bug,Major,hausdorff,2015-12-11T21:38:13.000+0000,5,Resolved,Complete,Implement `os::mkdtemp` for Windows,2015-12-11T21:38:13.000+0000,MESOS-4108,5.0,mesos,Mesosphere Sprint 24
hausdorff,2015-12-10T00:58:36.000+0000,hausdorff,`os::strerror_r` does not exist on Windows.,Bug,Major,hausdorff,2015-12-10T01:09:17.000+0000,5,Resolved,Complete,`os::strerror_r` breaks the Windows build,2015-12-10T01:09:17.000+0000,MESOS-4107,1.0,mesos,Mesosphere Sprint 24
,2015-12-09T17:05:04.000+0000,jojy,"As a first step to address the use cases, propose a design document covering the requirement, design and implementation details.",Task,Major,jojy,,10020,Accepted,In Progress,Design document for interactive terminal for mesos containerizer,2015-12-18T07:04:59.000+0000,MESOS-4104,4.0,mesos,
klaus1982,2015-12-09T01:30:43.000+0000,neilc,"See attached patch. {{framework1}} is not allocated any resources, despite the fact that the resources on {{agent2}} can safely be allocated to it without risk of violating {{quota1}}. If I understand the intended quota behavior correctly, this doesn't seem intended.

Note that if the framework is added _after_ the slaves are added, the resources on {{agent2}} are allocated to {{framework1}}.",Bug,Blocker,neilc,2016-01-19T23:10:43.000+0000,5,Resolved,Complete,Quota doesn't allocate resources on slave joining.,2016-01-19T23:10:44.000+0000,MESOS-4102,5.0,mesos,Mesosphere Sprint 26
,2015-12-08T20:10:59.000+0000,jojy,Today mesos containerizer does not have a way to run tasks that require interactive sessions. An example use case is running a task that requires a manual password entry from an operator. Another use case could be debugging (gdb). ,Story,Major,jojy,,10020,Accepted,In Progress,Allow interactive terminal for mesos containerizer,2015-12-18T07:04:36.000+0000,MESOS-4098,10.0,mesos,
SteveNiemitz,2015-12-07T22:13:46.000+0000,SteveNiemitz,"Currently, when running tasks in docker containers, if the executor uses the mesos.native python library, the execution environment inside the container (OS, native libs, etc) must match the execution environment outside the container fairly closely in order to load the mesos.so library.

The solution here can be to introduce a much lighter weight python egg, mesos.executor, which only includes code (and dependencies) needed to create and run an MesosExecutorDriver.  Executors can then use this native library instead of mesos.native.",Improvement,Major,SteveNiemitz,2016-03-29T23:14:09.000+0000,5,Resolved,Complete,Create light-weight executor only and scheduler only mesos eggs,2016-04-21T20:25:04.000+0000,MESOS-4090,5.0,mesos,
kaysoky,2015-12-07T21:01:42.000+0000,kaysoky,"Once a module for executor/task output logging has been introduced, the default module will mirror the existing behavior.  Executor/task stdout/stderr is piped into files within the executor's sandbox directory.

The files are exposed in the web UI, via the {{/files}} endpoint.",Task,Major,kaysoky,2015-12-29T23:16:09.000+0000,5,Resolved,Complete,Modularize existing plain-file logging for executor/task logs launched with the Mesos Containerizer,2015-12-29T23:16:23.000+0000,MESOS-4088,2.0,mesos,Mesosphere Sprint 24
kaysoky,2015-12-07T20:53:36.000+0000,kaysoky,"Existing executor/task logs are logged to files in their sandbox directory, with some nuances based on which containerizer is used (see background section in linked document).

A logger for executor/task logs has the following requirements:
* The logger is given a command to run and must handle the stdout/stderr of the command.
* The handling of stdout/stderr must be resilient across agent failover.  Logging should not stop if the agent fails.
* Logs should be readable, presumably via the web UI, or via some other module-specific UI.",Task,Major,kaysoky,2015-12-29T19:22:33.000+0000,5,Resolved,Complete,Introduce a module for logging executor/task output,2015-12-29T19:24:41.000+0000,MESOS-4087,5.0,mesos,Mesosphere Sprint 24
neilc,2015-12-07T19:21:14.000+0000,neilc,See also design doc: MESOS-4000.,Improvement,Major,neilc,2015-12-19T09:29:09.000+0000,5,Resolved,Complete,Implement implicit roles,2015-12-19T09:29:09.000+0000,MESOS-4085,5.0,mesos,Mesosphere Sprint 24
,2015-12-07T14:50:32.000+0000,bernd-mesos,"Currently, mesos-tests.sh exits when a test crashes. This is inconvenient when trying to find out all tests that fail. 

mesos-tests.sh should rate a test that crashes as failed and continue the same way as if the test merely returned with a failure result and exited properly.",Improvement,Major,bernd-mesos,,10020,Accepted,In Progress,Continue test suite execution across crashing tests.,2016-02-11T01:30:20.000+0000,MESOS-4075,8.0,mesos,
alexr,2015-12-07T09:06:16.000+0000,alexr,"While implementing recovery in the hierarchical allocator, we introduced some internal constants that influence the recovery process: {{ALLOCATION_HOLD_OFF_RECOVERY_TIMEOUT}} and {{AGENT_RECOVERY_FACTOR}}. We should expose these parameters for operators to configure.

However, I am a bit reluctant to expose them as master flags, because they are implementation specific. It would be nice to combine all hierarchical allocator-related flags into one (maybe JSON) file, similar to how we do it for modules.",Improvement,Major,alexr,,10020,Accepted,In Progress,Expose recovery parameters from Hierarchical allocator,2015-12-08T00:03:34.000+0000,MESOS-4073,3.0,mesos,
jojy,2015-12-04T19:32:42.000+0000,jojy,"Have been seeing the following socket  receive error frequently:

{code}
F1204 11:12:47.301839 54104 libevent_ssl_socket.cpp:245] Check failed: length > 0 
*** Check failure stack trace: ***
    @     0x7f73227fe5a6  google::LogMessage::Fail()
    @     0x7f73227fe4f2  google::LogMessage::SendToLog()
    @     0x7f73227fdef4  google::LogMessage::Flush()
    @     0x7f7322800e08  google::LogMessageFatal::~LogMessageFatal()
    @     0x7f73227b93e2  process::network::LibeventSSLSocketImpl::recv_callback()
    @     0x7f73227b9182  process::network::LibeventSSLSocketImpl::recv_callback()
    @     0x7f731cbc75cc  bufferevent_run_deferred_callbacks_locked
    @     0x7f731cbbdc5d  event_base_loop
    @     0x7f73227d9ded  process::EventLoop::run()
    @     0x7f73227a3101  _ZNSt12_Bind_simpleIFPFvvEvEE9_M_invokeIJEEEvSt12_Index_tupleIJXspT_EEE
    @     0x7f73227a305b  std::_Bind_simple<>::operator()()
    @     0x7f73227a2ff4  std::thread::_Impl<>::_M_run()
    @     0x7f731e0d1a40  (unknown)
    @     0x7f731de0a182  start_thread
    @     0x7f731db3730d  (unknown)
    @              (nil)  (unknown)

{code}

In this case this was a HTTP get over SSL. The url being:

https://dseasb33srnrn.cloudfront.net:443/registry-v2/docker/registry/v2/blobs/sha256/44/44be94a95984bb47dc3a193f59bf8c04d5e877160b745b119278f38753a6f58f/data?Expires=1449259252&Signature=Q4CQdr1LbxsiYyVebmetrx~lqDgQfHVkGxpbMM3PoISn6r07DXIzBX6~tl1iZx9uXdfr~5awH8Kxwh-y8b0dTV3mLTZAVlneZlHbhBAX9qbYMd180-QvUvrFezwOlSmX4B3idvo-zK0CarUu3Ev1hbJz5y3olwe2ZC~RXHEwzkQ_&Key-Pair-Id=APKAJECH5M7VWIS5YZ6Q


*Steps to reproduce:*

1. Run master
2. Run slave from your build directory as  as:

{code}
 GLOG_v=1;SSL_ENABLED=1;SSL_KEY_FILE=<path_to_key>;SSL_CERT_FILE=<path_to_cert>;sudo -E ./bin/mesos-slave.sh \
      --master=127.0.0.1:5050 \                                                  
      --executor_registration_timeout=5mins \                                    
      --containerizers=mesos  \                                                  
      --isolation=filesystem/linux \                                             
      --image_providers=DOCKER  \                                                
      --docker_puller_timeout=600 \                                              
      --launcher_dir=$MESOS_BUILD_DIR/src/.libs \                                
      --switch_user=""false"" \                                                    
      --docker_puller=""registry""          
{code} 

3. Run mesos-execute from your build directory as :

{code}                                                        
    ./src/mesos-execute \                                                        
    --master=127.0.0.1:5050 \                                                    
    --command=""uname -a""  \                                                      
    --name=test \                                                                
    --docker_image=ubuntu 
{code}",Bug,Major,jojy,2015-12-22T20:55:28.000+0000,5,Resolved,Complete,libevent_ssl_socket assertion fails ,2016-02-27T00:15:54.000+0000,MESOS-4069,8.0,mesos,Mesosphere Sprint 25
greggomann,2015-12-04T08:53:05.000+0000,mcypark,"Observed from the CI: https://builds.apache.org/job/Mesos/COMPILER=gcc,CONFIGURATION=--verbose%20--enable-libevent%20--enable-ssl,OS=ubuntu%3A14.04,label_exp=docker%7C%7CHadoop/1319/changes",Bug,Major,mcypark,2015-12-09T06:10:43.000+0000,5,Resolved,Complete,ReservationTest.ACLMultipleOperations is flaky,2015-12-09T06:10:51.000+0000,MESOS-4067,2.0,mesos,Mesosphere Sprint 24
vinodkone,2015-12-03T22:43:31.000+0000,bmahler,"Currently when a user is hitting /state.json on the agent, it may return partial state if the agent has failed over and is recovering. There is currently no clear way to tell if this is the case when looking at a response, so the user may incorrectly interpret the agent as being empty of tasks.

We could consider exposing the 'state' enum of the agent in the endpoint:

{code}
  enum State
  {
    RECOVERING,   // Slave is doing recovery.
    DISCONNECTED, // Slave is not connected to the master.
    RUNNING,      // Slave has (re-)registered.
    TERMINATING,  // Slave is shutting down.
  } state;
{code}

This may be a bit tricky to maintain as far as backwards-compatibility of the endpoint, if we were to alter this enum.

Exposing this would allow users to be more informed about the state of the agent.",Task,Major,bmahler,2016-02-09T00:53:17.000+0000,5,Resolved,Complete,Agent should not return partial state when a request is made to /state endpoint during recovery.,2016-02-09T00:53:17.000+0000,MESOS-4066,3.0,mesos,Mesosphere Sprint 28
hartem,2015-12-03T21:34:53.000+0000,bmahler,"In what seems like an oversight, when ContainerInfo was added to TaskInfo, it was not added to our internal Task protobuf.

Also, unlike the agent, it appears that the master does not use protobuf::createTask. We should try remove the manual construction in the master in favor of construction through protobuf::createTask.

Partial contents of ContainerInfo should be exposed through state endpoints on the master and the agent.
",Task,Major,bmahler,2015-12-21T16:39:19.000+0000,5,Resolved,Complete,Add ContainerInfo to internal Task protobuf.,2015-12-21T18:22:16.000+0000,MESOS-4064,3.0,mesos,Mesosphere Sprint 24
kaysoky,2015-12-03T16:53:08.000+0000,neilc,"Per comments in MESOS-3916, the fix for that issue decreased the degree of flakiness, but it seems that some intermittent test failures do occur -- should be investigated.

*Flakiness in task acknowledgment*
{code}
I1203 18:25:04.609817 28732 status_update_manager.cpp:392] Received status update acknowledgement (UUID: 6afd012e-8e88-41b2-8239-a9b852d07ca1) for task 26305fdd-edb0-4764-8b8a-2558f2b2d81b of framework c7900911-cc7a-4dde-92e7-48fe82cddd9e-0000
W1203 18:25:04.610076 28732 status_update_manager.cpp:762] Unexpected status update acknowledgement (received 6afd012e-8e88-41b2-8239-a9b852d07ca1, expecting 82fc7a7b-e64a-4f4d-ab74-76abac42b4e6) for update TASK_RUNNING (UUID: 82fc7a7b-e64a-4f4d-ab74-76abac42b4e6) for task 26305fdd-edb0-4764-8b8a-2558f2b2d81b of framework c7900911-cc7a-4dde-92e7-48fe82cddd9e-0000
E1203 18:25:04.610339 28736 slave.cpp:2339] Failed to handle status update acknowledgement (UUID: 6afd012e-8e88-41b2-8239-a9b852d07ca1) for task 26305fdd-edb0-4764-8b8a-2558f2b2d81b of framework c7900911-cc7a-4dde-92e7-48fe82cddd9e-0000: Duplicate acknowledgemen
{code}

This is a race between [launching and acknowledging two tasks|https://github.com/apache/mesos/blob/75aaaacb89fa961b249c9ab7fa0f45dfa9d415a5/src/tests/master_maintenance_tests.cpp#L1486-L1517].  The status updates for each task are not necessarily received in the same order as launching the tasks.

*Flakiness in first inverse offer filter*
See [this comment in MESOS-3916|https://issues.apache.org/jira/browse/MESOS-3916?focusedCommentId=15027478&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-15027478] for the explanation.  The related logs are above the comment.",Bug,Minor,neilc,2016-01-05T23:13:26.000+0000,5,Resolved,Complete,Investigate remaining flakiness in MasterMaintenanceTest.InverseOffersFilters,2016-01-06T00:02:05.000+0000,MESOS-4059,1.0,mesos,Mesosphere Sprint 26
alexr,2015-12-03T16:39:16.000+0000,alexr,"To be consistent with other operator endpoints and to adhere to the principal of least surprise, move role from each {{Resource}} in quota set request to the request itself. 

{{Resource.role}} is used for reserved resources. Since quota is not a direct reservation request, to avoid confusion we shall not reuse this field for communicating the role for which quota should be reserved.

Food for thought: Shall we try to keep internal storage protobufs as close as possible to operator's JSON to provide some sort of a schema or decouple those two for the sake of flexibility?",Improvement,Major,alexr,2016-01-15T23:01:47.000+0000,5,Resolved,Complete,Do not use `Resource.role` for resources in quota request.,2016-01-15T23:01:47.000+0000,MESOS-4058,1.0,mesos,Mesosphere Sprint 26
alexr,2015-12-03T10:48:07.000+0000,alexr,"We are inconsistent right now in how we respond to endpoint requests with unsupported methods: both {{MethodNotAllowed}} and {{BadRequest}} are used. We are also not consistent in the error message we include in the body.

This ticket proposes use {{MethodNotAllowed}} with standardized message text.",Improvement,Minor,alexr,2015-12-10T17:00:47.000+0000,5,Resolved,Complete,Respond with `MethodNotAllowed` if a request uses an unsupported method.,2015-12-21T13:19:43.000+0000,MESOS-4056,1.0,mesos,Mesosphere Sprint 24
greggomann,2015-12-02T22:32:05.000+0000,greggomann,"{{MemoryPressureMesosTest.CGROUPS_ROOT_Statistics}} and {{MemoryPressureMesosTest.CGROUPS_ROOT_SlaveRecovery}} fail on CentOS 6.6. It seems that mounted cgroups are not properly cleaned up after previous tests, so multiple hierarchies are detected and thus an error is produced:

{code}
[ RUN      ] MemoryPressureMesosTest.CGROUPS_ROOT_Statistics
../../src/tests/mesos.cpp:849: Failure
Value of: _baseHierarchy.get()
  Actual: ""/cgroup""
Expected: baseHierarchy
Which is: ""/tmp/mesos_test_cgroup""
-------------------------------------------------------------
Multiple cgroups base hierarchies detected:
  '/tmp/mesos_test_cgroup'
  '/cgroup'
Mesos does not support multiple cgroups base hierarchies.
Please unmount the corresponding (or all) subsystems.
-------------------------------------------------------------
../../src/tests/mesos.cpp:932: Failure
(cgroups::destroy(hierarchy, cgroup)).failure(): Failed to remove cgroup '/tmp/mesos_test_cgroup/perf_event/mesos_test': Device or resource busy
[  FAILED  ] MemoryPressureMesosTest.CGROUPS_ROOT_Statistics (12 ms)
[ RUN      ] MemoryPressureMesosTest.CGROUPS_ROOT_SlaveRecovery
../../src/tests/mesos.cpp:849: Failure
Value of: _baseHierarchy.get()
  Actual: ""/cgroup""
Expected: baseHierarchy
Which is: ""/tmp/mesos_test_cgroup""
-------------------------------------------------------------
Multiple cgroups base hierarchies detected:
  '/tmp/mesos_test_cgroup'
  '/cgroup'
Mesos does not support multiple cgroups base hierarchies.
Please unmount the corresponding (or all) subsystems.
-------------------------------------------------------------
../../src/tests/mesos.cpp:932: Failure
(cgroups::destroy(hierarchy, cgroup)).failure(): Failed to remove cgroup '/tmp/mesos_test_cgroup/perf_event/mesos_test': Device or resource busy
[  FAILED  ] MemoryPressureMesosTest.CGROUPS_ROOT_SlaveRecovery (7 ms)
{code}",Bug,Major,greggomann,,10020,Accepted,In Progress,MemoryPressureMesosTest tests fail on CentOS 6.6,2016-04-12T16:44:59.000+0000,MESOS-4053,3.0,mesos,Mesosphere Sprint 26
arojas,2015-12-02T19:19:09.000+0000,kaysoky,"{code:title=Output from passed test}
[----------] 1 test from MemoryPressureMesosTest
1+0 records in
1+0 records out
1048576 bytes (1.0 MB) copied, 0.000430889 s, 2.4 GB/s
[ RUN      ] MemoryPressureMesosTest.CGROUPS_ROOT_SlaveRecovery
I1202 11:09:14.319327  5062 exec.cpp:134] Version: 0.27.0
I1202 11:09:14.333317  5079 exec.cpp:208] Executor registered on slave bea15b35-9aa1-4b57-96fb-29b5f70638ac-S0
Registered executor on ubuntu
Starting task 4e62294c-cfcf-4a13-b699-c6a4b7ac5162
sh -c 'while true; do dd count=512 bs=1M if=/dev/zero of=./temp; done'
Forked command at 5085
I1202 11:09:14.391739  5077 exec.cpp:254] Received reconnect request from slave bea15b35-9aa1-4b57-96fb-29b5f70638ac-S0
I1202 11:09:14.398598  5082 exec.cpp:231] Executor re-registered on slave bea15b35-9aa1-4b57-96fb-29b5f70638ac-S0
Re-registered executor on ubuntu
Shutting down
Sending SIGTERM to process tree at pid 5085
Killing the following process trees:
[ 
-+- 5085 sh -c while true; do dd count=512 bs=1M if=/dev/zero of=./temp; done 
 \--- 5086 dd count=512 bs=1M if=/dev/zero of=./temp 
]
[       OK ] MemoryPressureMesosTest.CGROUPS_ROOT_SlaveRecovery (1096 ms)
{code}

{code:title=Output from failed test}
[----------] 1 test from MemoryPressureMesosTest
1+0 records in
1+0 records out
1048576 bytes (1.0 MB) copied, 0.000404489 s, 2.6 GB/s
[ RUN      ] MemoryPressureMesosTest.CGROUPS_ROOT_SlaveRecovery
I1202 11:09:15.509950  5109 exec.cpp:134] Version: 0.27.0
I1202 11:09:15.568183  5123 exec.cpp:208] Executor registered on slave 88734acc-718e-45b0-95b9-d8f07cea8a9e-S0
Registered executor on ubuntu
Starting task 14b6bab9-9f60-4130-bdc4-44efba262bc6
Forked command at 5132
sh -c 'while true; do dd count=512 bs=1M if=/dev/zero of=./temp; done'
I1202 11:09:15.665498  5129 exec.cpp:254] Received reconnect request from slave 88734acc-718e-45b0-95b9-d8f07cea8a9e-S0
I1202 11:09:15.670995  5123 exec.cpp:381] Executor asked to shutdown
Shutting down
Sending SIGTERM to process tree at pid 5132
../../src/tests/containerizer/memory_pressure_tests.cpp:283: Failure
(usage).failure(): Unknown container: ebe90e15-72fa-4519-837b-62f43052c913
*** Aborted at 1449083355 (unix time) try ""date -d @1449083355"" if you are using GNU date ***
{code}

Notice that in the failed test, the executor is asked to shutdown when it tries to reconnect to the agent.",Bug,Major,kaysoky,2016-03-04T00:12:47.000+0000,5,Resolved,Complete,MemoryPressureMesosTest.CGROUPS_ROOT_SlaveRecovery is flaky,2016-03-14T20:01:15.000+0000,MESOS-4047,1.0,mesos,Mesosphere Sprint 23
gilbert,2015-12-02T17:54:32.000+0000,gilbert,"Currently docker pull only return an image structure, which only contains entrypoint info. We have docker inspect as a subprocess inside docker pull, which contains many other useful information of a docker image. We should be able to support returning environment variables information from the image.",Improvement,Major,gilbert,2015-12-21T17:29:18.000+0000,5,Resolved,Complete,Enable `Env` specified in docker image can be returned from docker pull,2015-12-21T17:29:18.000+0000,MESOS-4046,3.0,mesos,Mesosphere Sprint 23
,2015-12-02T01:59:53.000+0000,greggomann,"After using the current installation instructions in the getting started documentation, {{perf}} will not run on CentOS 6.6 because the version of elfutils included in devtoolset-2 is not compatible with the version of {{perf}} installed by {{yum}}. Installing and using devtoolset-3, however (http://linux.web.cern.ch/linux/scientific6/docs/softwarecollections.shtml) fixes this issue. This could be resolved by updating the getting started documentation to recommend installing devtoolset-3.",Bug,Major,greggomann,,10020,Accepted,In Progress,Install instructions for CentOS 6.6 lead to errors running `perf`,2015-12-04T16:42:59.000+0000,MESOS-4036,1.0,mesos,
anandmazumdar,2015-12-01T00:06:03.000+0000,tillt,"SSL build, [Ubuntu 14.04|https://github.com/tillt/mesos-vagrant-ci/blob/master/ubuntu14/setup.sh], non-root test run.

{noformat}
[----------] 22 tests from ContentType/SchedulerTest
[ RUN      ] ContentType/SchedulerTest.Subscribe/0
[       OK ] ContentType/SchedulerTest.Subscribe/0 (48 ms)
*** Aborted at 1448928007 (unix time) try ""date -d @1448928007"" if you are using GNU date ***
[ RUN      ] ContentType/SchedulerTest.Subscribe/1
PC: @          0x1451b8e testing::internal::UntypedFunctionMockerBase::UntypedInvokeWith()
*** SIGSEGV (@0x100000030) received by PID 21320 (TID 0x2b549e5d4700) from PID 48; stack trace: ***
    @     0x2b54c95940b7 os::Linux::chained_handler()
    @     0x2b54c9598219 JVM_handle_linux_signal
    @     0x2b5496300340 (unknown)
    @          0x1451b8e testing::internal::UntypedFunctionMockerBase::UntypedInvokeWith()
    @           0xe2ea6d _ZN7testing8internal18FunctionMockerBaseIFvRKSt5queueIN5mesos2v19scheduler5EventESt5dequeIS6_SaIS6_EEEEE10InvokeWithERKSt5tupleIJSC_EE
    @           0xe2b1bc testing::internal::FunctionMocker<>::Invoke()
    @          0x1118aed mesos::internal::tests::SchedulerTest::Callbacks::received()
    @          0x111c453 _ZNKSt7_Mem_fnIMN5mesos8internal5tests13SchedulerTest9CallbacksEFvRKSt5queueINS0_2v19scheduler5EventESt5dequeIS8_SaIS8_EEEEEclIJSE_EvEEvRS4_DpOT_
    @          0x111c001 _ZNSt5_BindIFSt7_Mem_fnIMN5mesos8internal5tests13SchedulerTest9CallbacksEFvRKSt5queueINS1_2v19scheduler5EventESt5dequeIS9_SaIS9_EEEEESt17reference_wrapperIS5_ESt12_PlaceholderILi1EEEE6__callIvJSF_EJLm0ELm1EEEET_OSt5tupleIJDpT0_EESt12_Index_tupleIJXspT1_EEE
    @          0x111b90d _ZNSt5_BindIFSt7_Mem_fnIMN5mesos8internal5tests13SchedulerTest9CallbacksEFvRKSt5queueINS1_2v19scheduler5EventESt5dequeIS9_SaIS9_EEEEESt17reference_wrapperIS5_ESt12_PlaceholderILi1EEEEclIJSF_EvEET0_DpOT_
    @          0x111ae09 std::_Function_handler<>::_M_invoke()
    @     0x2b5493c6da09 std::function<>::operator()()
    @     0x2b5493c688ee process::AsyncExecutorProcess::execute<>()
    @     0x2b5493c6db2a _ZZN7process8dispatchI7NothingNS_20AsyncExecutorProcessERKSt8functionIFvRKSt5queueIN5mesos2v19scheduler5EventESt5dequeIS8_SaIS8_EEEEESC_PvSG_SC_SJ_EENS_6FutureIT_EERKNS_3PIDIT0_EEMSO_FSL_T1_T2_T3_ET4_T5_T6_ENKUlPNS_11ProcessBaseEE_clES11_
    @     0x2b5493c765a4 _ZNSt17_Function_handlerIFvPN7process11ProcessBaseEEZNS0_8dispatchI7NothingNS0_20AsyncExecutorProcessERKSt8functionIFvRKSt5queueIN5mesos2v19scheduler5EventESt5dequeISC_SaISC_EEEEESG_PvSK_SG_SN_EENS0_6FutureIT_EERKNS0_3PIDIT0_EEMSS_FSP_T1_T2_T3_ET4_T5_T6_EUlS2_E_E9_M_invokeERKSt9_Any_dataS2_
    @     0x2b54946b1201 std::function<>::operator()()
    @     0x2b549469960f process::ProcessBase::visit()
    @     0x2b549469d480 process::DispatchEvent::visit()
    @           0x9dc0ba process::ProcessBase::serve()
    @     0x2b54946958cc process::ProcessManager::resume()
    @     0x2b5494692a9c _ZZN7process14ProcessManager12init_threadsEvENKUlRKSt11atomic_boolE_clES3_
    @     0x2b549469ccac _ZNSt5_BindIFZN7process14ProcessManager12init_threadsEvEUlRKSt11atomic_boolE_St17reference_wrapperIS3_EEE6__callIvIEILm0EEEET_OSt5tupleIIDpT0_EESt12_Index_tupleIIXspT1_EEE
    @     0x2b549469cc5c _ZNSt5_BindIFZN7process14ProcessManager12init_threadsEvEUlRKSt11atomic_boolE_St17reference_wrapperIS3_EEEclIIEvEET0_DpOT_
    @     0x2b549469cbee _ZNSt12_Bind_simpleIFSt5_BindIFZN7process14ProcessManager12init_threadsEvEUlRKSt11atomic_boolE_St17reference_wrapperIS4_EEEvEE9_M_invokeIIEEEvSt12_Index_tupleIIXspT_EEE
    @     0x2b549469cb45 _ZNSt12_Bind_simpleIFSt5_BindIFZN7process14ProcessManager12init_threadsEvEUlRKSt11atomic_boolE_St17reference_wrapperIS4_EEEvEEclEv
    @     0x2b549469cade _ZNSt6thread5_ImplISt12_Bind_simpleIFSt5_BindIFZN7process14ProcessManager12init_threadsEvEUlRKSt11atomic_boolE_St17reference_wrapperIS6_EEEvEEE6_M_runEv
    @     0x2b5495b81a40 (unknown)
    @     0x2b54962f8182 start_thread
    @     0x2b549660847d (unknown)
make[3]: *** [check-local] Segmentation fault
make[3]: Leaving directory `/home/vagrant/mesos/build/src'
make[2]: *** [check-am] Error 2
make[2]: Leaving directory `/home/vagrant/mesos/build/src'
make[1]: *** [check] Error 2
make[1]: Leaving directory `/home/vagrant/mesos/build/src'
make: *** [check-recursive] Error 1
{noformat}",Bug,Major,tillt,2016-03-04T00:11:49.000+0000,5,Resolved,Complete,ContentType/SchedulerTest is flaky.,2016-03-04T00:11:49.000+0000,MESOS-4029,3.0,mesos,Mesosphere Sprint 23
jojy,2015-11-30T18:50:32.000+0000,anandmazumdar,"From ASF CI:
https://builds.apache.org/job/Mesos/1289/COMPILER=gcc,CONFIGURATION=--verbose%20--enable-libevent%20--enable-ssl,OS=centos:7,label_exp=docker%7C%7CHadoop/console

{code}
[ RUN      ] RegistryClientTest.SimpleRegistryPuller
I1127 02:51:40.235900   362 registry_client.cpp:511] Response status for url 'https://localhost:57828/v2/library/busybox/manifests/latest': 401 Unauthorized
I1127 02:51:40.249766   360 registry_client.cpp:511] Response status for url 'https://localhost:57828/v2/library/busybox/manifests/latest': 200 OK
I1127 02:51:40.251137   361 registry_puller.cpp:195] Downloading layer '1ce2e90b0bc7224de3db1f0d646fe8e2c4dd37f1793928287f6074bc451a57ea' for image 'busybox:latest'
I1127 02:51:40.258514   354 registry_client.cpp:511] Response status for url 'https://localhost:57828/v2/library/busybox/blobs/sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4': 307 Temporary Redirect
I1127 02:51:40.264171   367 libevent_ssl_socket.cpp:1023] Socket error: Connection reset by peer
../../src/tests/containerizer/provisioner_docker_tests.cpp:1210: Failure
(socket).failure(): Failed accept: connection error: Connection reset by peer
[  FAILED  ] RegistryClientTest.SimpleRegistryPuller (349 ms)
{code}

Logs from a previous run that passed:
{code}
[ RUN      ] RegistryClientTest.SimpleRegistryPuller
I1126 18:49:05.306396   349 registry_client.cpp:511] Response status for url 'https://localhost:53492/v2/library/busybox/manifests/latest': 401 Unauthorized
I1126 18:49:05.321362   347 registry_client.cpp:511] Response status for url 'https://localhost:53492/v2/library/busybox/manifests/latest': 200 OK
I1126 18:49:05.322720   352 registry_puller.cpp:195] Downloading layer '1ce2e90b0bc7224de3db1f0d646fe8e2c4dd37f1793928287f6074bc451a57ea' for image 'busybox:latest'
I1126 18:49:05.331317   350 registry_client.cpp:511] Response status for url 'https://localhost:53492/v2/library/busybox/blobs/sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4': 307 Temporary Redirect
I1126 18:49:05.370625   352 registry_client.cpp:511] Response status for url 'https://127.0.0.1:53492/': 200 OK
I1126 18:49:05.372102   355 registry_puller.cpp:294] Untarring layer '1ce2e90b0bc7224de3db1f0d646fe8e2c4dd37f1793928287f6074bc451a57ea' downloaded from registry to directory 'output_dir'
[       OK ] RegistryClientTest.SimpleRegistryPuller (353 ms)
{code}",Bug,Major,anandmazumdar,2015-12-18T17:41:20.000+0000,5,Resolved,Complete,RegistryClientTest.SimpleRegistryPuller is flaky,2015-12-18T17:41:21.000+0000,MESOS-4026,4.0,mesos,Mesosphere Sprint 23
alexr,2015-11-26T12:09:34.000+0000,alexr,"When a remove quota requests hits the endpoint and passes validation, quota should be removed from the registry before the allocator is notified about the change.",Improvement,Major,alexr,2015-12-02T20:04:31.000+0000,5,Resolved,Complete,Remove quota from Registry for quota remove request,2015-12-02T20:04:31.000+0000,MESOS-4021,1.0,mesos,Mesosphere Sprint 23
alexr,2015-11-26T09:47:18.000+0000,alexr,"{{Resources}} class defines some handy filters, like {{revocable()}}, {{unreserved()}}, and so on. This ticket proposes to add one more: {{nonRevocable()}}.",Improvement,Minor,alexr,2015-12-01T11:57:51.000+0000,5,Resolved,Complete,Introduce filter for non-revocable resources in `Resources`,2015-12-01T11:57:51.000+0000,MESOS-4020,1.0,mesos,Mesosphere Sprint 23
js84,2015-11-25T14:38:32.000+0000,alexr,This endpoint is for removing quotas via the DELETE method.,Task,Major,alexr,2015-11-25T22:24:57.000+0000,5,Resolved,Complete,Introduce remove endpoint for quota,2015-11-25T22:24:57.000+0000,MESOS-4014,3.0,mesos,Mesosphere Sprint 23
js84,2015-11-25T14:35:56.000+0000,alexr,This endpoint is for querying quota status via the GET method.,Task,Major,alexr,2016-01-06T01:52:10.000+0000,5,Resolved,Complete,Introduce status endpoint for quota,2016-01-06T01:52:10.000+0000,MESOS-4013,5.0,mesos,Mesosphere Sprint 26
nfnt,2015-11-25T10:48:13.000+0000,nfnt,GCC 5.1.1 has {{-Werror=sign-compare}} in {{-Wall}} and stumbles over a comparison between signed and unsigned int in {{provisioner_docker_tests.cpp}}.,Bug,Trivial,nfnt,2015-11-25T11:33:31.000+0000,5,Resolved,Complete,RegistryClientTest.SimpleRegistryPuller doesn't compile with GCC 5.1.1,2015-11-25T15:03:20.000+0000,MESOS-4009,1.0,mesos,Mesosphere Sprint 23
gilbert,2015-11-24T23:25:53.000+0000,tnachen,We need to support workdir runtime configuration returned from image such as Dockerfile.,Improvement,Major,tnachen,2016-02-05T21:45:36.000+0000,5,Resolved,Complete,Support workdir runtime configuration from image ,2016-02-05T21:45:36.000+0000,MESOS-4005,2.0,mesos,Mesosphere Sprint 27
gilbert,2015-11-24T23:24:52.000+0000,tnachen,We need to use the entrypoint and command runtime configuration returned from image to be used in Mesos containerizer.,Improvement,Major,tnachen,2016-02-05T21:43:29.000+0000,5,Resolved,Complete,Support default entrypoint and command runtime config in Mesos containerizer,2016-02-05T21:43:29.000+0000,MESOS-4004,3.0,mesos,Mesosphere Sprint 27
greggomann,2015-11-24T21:12:37.000+0000,greggomann,"Some isolator modules can benefit from access to the agent's {{work_dir}}. For example, the DVD isolator (https://github.com/emccode/mesos-module-dvdi) is currently forced to mount external volumes in a hard-coded directory. Making the {{work_dir}} accessible to the isolator via {{Isolator::recover()}} would allow the isolator to mount volumes within the agent's {{work_dir}}. This can be accomplished by simply adding an overloaded signature for {{Isolator::recover()}} which includes the {{work_dir}} as a parameter.",Bug,Major,greggomann,2015-12-11T18:49:31.000+0000,5,Resolved,Complete,Pass agent work_dir to isolator modules,2015-12-11T18:49:31.000+0000,MESOS-4003,1.0,mesos,Mesosphere Sprint 23
anandmazumdar,2015-11-24T19:57:49.000+0000,anandmazumdar,"Showed up on ASF CI: ( test kept looping on and on and ultimately failing the build after 300 minutes )
https://builds.apache.org/job/Mesos/COMPILER=gcc,CONFIGURATION=--verbose,OS=ubuntu%3A14.04,label_exp=docker%7C%7CHadoop/1269/changes

{code}
[ RUN      ] ReservationEndpointsTest.UnreserveAvailableAndOfferedResources
I1124 01:07:20.050729 30260 leveldb.cpp:174] Opened db in 107.434842ms
I1124 01:07:20.099630 30260 leveldb.cpp:181] Compacted db in 48.82312ms
I1124 01:07:20.099722 30260 leveldb.cpp:196] Created db iterator in 29905ns
I1124 01:07:20.099738 30260 leveldb.cpp:202] Seeked to beginning of db in 3145ns
I1124 01:07:20.099750 30260 leveldb.cpp:271] Iterated through 0 keys in the db in 279ns
I1124 01:07:20.099804 30260 replica.cpp:778] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned
I1124 01:07:20.100637 30292 recover.cpp:447] Starting replica recovery
I1124 01:07:20.100934 30292 recover.cpp:473] Replica is in EMPTY status
I1124 01:07:20.103240 30288 replica.cpp:674] Replica in EMPTY status received a broadcasted recover request from (6305)@172.17.18.107:37993
I1124 01:07:20.103672 30292 recover.cpp:193] Received a recover response from a replica in EMPTY status
I1124 01:07:20.104142 30292 recover.cpp:564] Updating replica status to STARTING
I1124 01:07:20.114534 30284 master.cpp:365] Master ad27bc60-16d1-4239-9a65-235a991f9600 (9f2f81738d5e) started on 172.17.18.107:37993
I1124 01:07:20.114558 30284 master.cpp:367] Flags at startup: --acls="""" --allocation_interval=""1000secs"" --allocator=""HierarchicalDRF"" --authenticate=""true"" --authenticate_slaves=""true"" --authenticators=""crammd5"" --authorizers=""local"" --credentials=""/tmp/I60I5f/credentials"" --framework_sorter=""drf"" --help=""false"" --hostname_lookup=""true"" --initialize_driver_logging=""true"" --log_auto_initialize=""true"" --logbufsecs=""0"" --logging_level=""INFO"" --max_slave_ping_timeouts=""5"" --quiet=""false"" --recovery_slave_removal_limit=""100%"" --registry=""replicated_log"" --registry_fetch_timeout=""1mins"" --registry_store_timeout=""25secs"" --registry_strict=""true"" --roles=""role"" --root_submissions=""true"" --slave_ping_timeout=""15secs"" --slave_reregister_timeout=""10mins"" --user_sorter=""drf"" --version=""false"" --webui_dir=""/mesos/mesos-0.26.0/_inst/share/mesos/webui"" --work_dir=""/tmp/I60I5f/master"" --zk_session_timeout=""10secs""
I1124 01:07:20.114809 30284 master.cpp:412] Master only allowing authenticated frameworks to register
I1124 01:07:20.114820 30284 master.cpp:417] Master only allowing authenticated slaves to register
I1124 01:07:20.114825 30284 credentials.hpp:35] Loading credentials for authentication from '/tmp/I60I5f/credentials'
I1124 01:07:20.115067 30284 master.cpp:456] Using default 'crammd5' authenticator
I1124 01:07:20.115320 30284 master.cpp:493] Authorization enabled
I1124 01:07:20.115792 30285 hierarchical.cpp:162] Initialized hierarchical allocator process
I1124 01:07:20.115855 30285 whitelist_watcher.cpp:77] No whitelist given
I1124 01:07:20.118755 30285 master.cpp:1625] The newly elected leader is master@172.17.18.107:37993 with id ad27bc60-16d1-4239-9a65-235a991f9600
I1124 01:07:20.118788 30285 master.cpp:1638] Elected as the leading master!
I1124 01:07:20.118809 30285 master.cpp:1383] Recovering from registrar
I1124 01:07:20.119078 30285 registrar.cpp:307] Recovering registrar
I1124 01:07:20.143256 30292 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 38.787419ms
I1124 01:07:20.143347 30292 replica.cpp:321] Persisted replica status to STARTING
I1124 01:07:20.143717 30292 recover.cpp:473] Replica is in STARTING status
I1124 01:07:20.145454 30286 replica.cpp:674] Replica in STARTING status received a broadcasted recover request from (6307)@172.17.18.107:37993
I1124 01:07:20.145979 30292 recover.cpp:193] Received a recover response from a replica in STARTING status
I1124 01:07:20.146654 30292 recover.cpp:564] Updating replica status to VOTING
I1124 01:07:20.182672 30286 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 35.422256ms
I1124 01:07:20.182747 30286 replica.cpp:321] Persisted replica status to VOTING
I1124 01:07:20.182929 30286 recover.cpp:578] Successfully joined the Paxos group
I1124 01:07:20.183115 30286 recover.cpp:462] Recover process terminated
I1124 01:07:20.183831 30286 log.cpp:659] Attempting to start the writer
I1124 01:07:20.185907 30285 replica.cpp:494] Replica received implicit promise request from (6308)@172.17.18.107:37993 with proposal 1
I1124 01:07:20.225256 30285 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 39.291288ms
I1124 01:07:20.225344 30285 replica.cpp:343] Persisted promised to 1
I1124 01:07:20.226850 30286 coordinator.cpp:238] Coordinator attempting to fill missing positions
I1124 01:07:20.228394 30293 replica.cpp:389] Replica received explicit promise request from (6309)@172.17.18.107:37993 for position 0 with proposal 2
I1124 01:07:20.266371 30293 leveldb.cpp:341] Persisting action (8 bytes) to leveldb took 37.874181ms
I1124 01:07:20.266456 30293 replica.cpp:713] Persisted action at 0
I1124 01:07:20.267927 30293 replica.cpp:538] Replica received write request for position 0 from (6310)@172.17.18.107:37993
I1124 01:07:20.268002 30293 leveldb.cpp:436] Reading position from leveldb took 37139ns
I1124 01:07:20.308117 30293 leveldb.cpp:341] Persisting action (14 bytes) to leveldb took 39.961976ms
I1124 01:07:20.308205 30293 replica.cpp:713] Persisted action at 0
I1124 01:07:20.309033 30290 replica.cpp:692] Replica received learned notice for position 0 from @0.0.0.0:0
I1124 01:07:20.343257 30290 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 34.175337ms
I1124 01:07:20.343343 30290 replica.cpp:713] Persisted action at 0
I1124 01:07:20.343377 30290 replica.cpp:698] Replica learned NOP action at position 0
I1124 01:07:20.344446 30282 log.cpp:675] Writer started with ending position 0
I1124 01:07:20.346143 30291 leveldb.cpp:436] Reading position from leveldb took 56896ns
I1124 01:07:20.347618 30291 registrar.cpp:340] Successfully fetched the registry (0B) in 228.495104ms
I1124 01:07:20.347862 30291 registrar.cpp:439] Applied 1 operations in 41164ns; attempting to update the 'registry'
I1124 01:07:20.348794 30279 log.cpp:683] Attempting to append 178 bytes to the log
I1124 01:07:20.349081 30279 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 1
I1124 01:07:20.350244 30294 replica.cpp:538] Replica received write request for position 1 from (6311)@172.17.18.107:37993
I1124 01:07:20.385246 30294 leveldb.cpp:341] Persisting action (197 bytes) to leveldb took 34.872508ms
I1124 01:07:20.385323 30294 replica.cpp:713] Persisted action at 1
I1124 01:07:20.386814 30294 replica.cpp:692] Replica received learned notice for position 1 from @0.0.0.0:0
I1124 01:07:20.425163 30294 leveldb.cpp:341] Persisting action (199 bytes) to leveldb took 38.282493ms
I1124 01:07:20.425262 30294 replica.cpp:713] Persisted action at 1
I1124 01:07:20.425298 30294 replica.cpp:698] Replica learned APPEND action at position 1
I1124 01:07:20.427994 30287 registrar.cpp:484] Successfully updated the 'registry' in 79.949056ms
I1124 01:07:20.428141 30283 log.cpp:702] Attempting to truncate the log to 1
I1124 01:07:20.428738 30287 registrar.cpp:370] Successfully recovered registrar
I1124 01:07:20.429306 30290 master.cpp:1435] Recovered 0 slaves from the Registry (139B) ; allowing 10mins for slaves to re-register
I1124 01:07:20.429592 30290 hierarchical.cpp:174] Allocator recovery is not supported yet
I1124 01:07:20.430083 30294 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 2
I1124 01:07:20.431411 30294 replica.cpp:538] Replica received write request for position 2 from (6312)@172.17.18.107:37993
I1124 01:07:20.467258 30294 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 35.661978ms
I1124 01:07:20.467342 30294 replica.cpp:713] Persisted action at 2
I1124 01:07:20.468842 30290 replica.cpp:692] Replica received learned notice for position 2 from @0.0.0.0:0
I1124 01:07:20.502264 30290 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 33.367074ms
I1124 01:07:20.502426 30290 leveldb.cpp:399] Deleting ~1 keys from leveldb took 80765ns
I1124 01:07:20.502452 30290 replica.cpp:713] Persisted action at 2
I1124 01:07:20.502488 30290 replica.cpp:698] Replica learned TRUNCATE action at position 2
I1124 01:07:20.510509 30260 containerizer.cpp:141] Using isolation: posix/cpu,posix/mem,filesystem/posix
W1124 01:07:20.511119 30260 backend.cpp:48] Failed to create 'bind' backend: BindBackend requires root privileges
I1124 01:07:20.516801 30288 slave.cpp:189] Slave started on 219)@172.17.18.107:37993
I1124 01:07:20.516839 30288 slave.cpp:190] Flags at startup: --appc_store_dir=""/tmp/mesos/store/appc"" --authenticatee=""crammd5"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --credential=""/tmp/ReservationEndpointsTest_UnreserveAvailableAndOfferedResources_CSzecr/credential"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_auth_server=""auth.docker.io"" --docker_auth_server_port=""443"" --docker_kill_orphans=""true"" --docker_local_archives_dir=""/tmp/mesos/images/docker"" --docker_puller=""local"" --docker_puller_timeout=""60"" --docker_registry=""registry-1.docker.io"" --docker_registry_port=""443"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/tmp/mesos/store/docker"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/tmp/ReservationEndpointsTest_UnreserveAvailableAndOfferedResources_CSzecr/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --hostname_lookup=""true"" --image_provisioner_backend=""copy"" --initialize_driver_logging=""true"" --isolation=""posix/cpu,posix/mem"" --launcher_dir=""/mesos/mesos-0.26.0/_build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""10ms"" --resources=""cpus:2;mem:1024;disk:1024;ports:[31000-32000]"" --revocable_cpu_low_priority=""true"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --systemd_runtime_directory=""/run/systemd/system"" --version=""false"" --work_dir=""/tmp/ReservationEndpointsTest_UnreserveAvailableAndOfferedResources_CSzecr""
I1124 01:07:20.517670 30288 credentials.hpp:83] Loading credential for authentication from '/tmp/ReservationEndpointsTest_UnreserveAvailableAndOfferedResources_CSzecr/credential'
I1124 01:07:20.517982 30288 slave.cpp:320] Slave using credential for: test-principal
I1124 01:07:20.518334 30288 resources.cpp:472] Parsing resources as JSON failed: cpus:2;mem:1024;disk:1024;ports:[31000-32000]
Trying semicolon-delimited string format instead
I1124 01:07:20.518815 30260 resources.cpp:472] Parsing resources as JSON failed: cpus:1;mem:128
Trying semicolon-delimited string format instead
I1124 01:07:20.518975 30288 slave.cpp:390] Slave resources: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I1124 01:07:20.519104 30288 slave.cpp:398] Slave attributes: [  ]
I1124 01:07:20.519124 30288 slave.cpp:403] Slave hostname: 9f2f81738d5e
I1124 01:07:20.519136 30288 slave.cpp:408] Slave checkpoint: true
I1124 01:07:20.519407 30260 resources.cpp:472] Parsing resources as JSON failed: mem:384
Trying semicolon-delimited string format instead
I1124 01:07:20.522702 30288 state.cpp:52] Recovering state from '/tmp/ReservationEndpointsTest_UnreserveAvailableAndOfferedResources_CSzecr/meta'
I1124 01:07:20.523265 30288 status_update_manager.cpp:200] Recovering status update manager
I1124 01:07:20.523531 30288 containerizer.cpp:383] Recovering containerizer
I1124 01:07:20.524998 30288 slave.cpp:4258] Finished recovery
I1124 01:07:20.525802 30288 slave.cpp:4430] Querying resource estimator for oversubscribable resources
I1124 01:07:20.526753 30288 slave.cpp:727] New master detected at master@172.17.18.107:37993
I1124 01:07:20.527292 30288 slave.cpp:790] Authenticating with master master@172.17.18.107:37993
I1124 01:07:20.528240 30288 slave.cpp:795] Using default CRAM-MD5 authenticatee
I1124 01:07:20.527003 30286 status_update_manager.cpp:174] Pausing sending status updates
I1124 01:07:20.528955 30285 authenticatee.cpp:121] Creating new client SASL connection
I1124 01:07:20.529469 30285 master.cpp:5169] Authenticating slave(219)@172.17.18.107:37993
I1124 01:07:20.529729 30283 authenticator.cpp:413] Starting authentication session for crammd5_authenticatee(515)@172.17.18.107:37993
I1124 01:07:20.530287 30283 authenticator.cpp:98] Creating new server SASL connection
I1124 01:07:20.530764 30285 authenticatee.cpp:212] Received SASL authentication mechanisms: CRAM-MD5
I1124 01:07:20.530903 30285 authenticatee.cpp:238] Attempting to authenticate with mechanism 'CRAM-MD5'
I1124 01:07:20.531096 30285 authenticator.cpp:203] Received SASL authentication start
I1124 01:07:20.531241 30285 authenticator.cpp:325] Authentication requires more steps
I1124 01:07:20.531388 30285 authenticatee.cpp:258] Received SASL authentication step
I1124 01:07:20.531616 30285 authenticator.cpp:231] Received SASL authentication step
I1124 01:07:20.531668 30285 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: '9f2f81738d5e' server FQDN: '9f2f81738d5e' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I1124 01:07:20.531690 30285 auxprop.cpp:179] Looking up auxiliary property '*userPassword'
I1124 01:07:20.531774 30285 auxprop.cpp:179] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I1124 01:07:20.531834 30285 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: '9f2f81738d5e' server FQDN: '9f2f81738d5e' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I1124 01:07:20.531855 30285 auxprop.cpp:129] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I1124 01:07:20.531867 30285 auxprop.cpp:129] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I1124 01:07:20.531903 30285 authenticator.cpp:317] Authentication success
I1124 01:07:20.532016 30283 authenticatee.cpp:298] Authentication success
I1124 01:07:20.532331 30281 master.cpp:5199] Successfully authenticated principal 'test-principal' at slave(219)@172.17.18.107:37993
I1124 01:07:20.532652 30291 authenticator.cpp:431] Authentication session cleanup for crammd5_authenticatee(515)@172.17.18.107:37993
I1124 01:07:20.533113 30288 slave.cpp:763] Detecting new master
I1124 01:07:20.533628 30288 slave.cpp:4444] Received oversubscribable resources  from the resource estimator
I1124 01:07:20.546396 30288 slave.cpp:858] Successfully authenticated with master master@172.17.18.107:37993
I1124 01:07:20.547111 30287 master.cpp:3878] Registering slave at slave(219)@172.17.18.107:37993 (9f2f81738d5e) with id ad27bc60-16d1-4239-9a65-235a991f9600-S0
I1124 01:07:20.547886 30287 registrar.cpp:439] Applied 1 operations in 91121ns; attempting to update the 'registry'
I1124 01:07:20.550647 30287 log.cpp:683] Attempting to append 347 bytes to the log
I1124 01:07:20.550935 30279 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 3
I1124 01:07:20.551534 30288 slave.cpp:1252] Will retry registration in 3.399312ms if necessary
I1124 01:07:20.551868 30291 replica.cpp:538] Replica received write request for position 3 from (6324)@172.17.18.107:37993
I1124 01:07:20.557605 30281 slave.cpp:1252] Will retry registration in 16.296866ms if necessary
I1124 01:07:20.557891 30293 master.cpp:3866] Ignoring register slave message from slave(219)@172.17.18.107:37993 (9f2f81738d5e) as admission is already in progress
I1124 01:07:20.574681 30279 slave.cpp:1252] Will retry registration in 73.52632ms if necessary
I1124 01:07:20.575078 30293 master.cpp:3866] Ignoring register slave message from slave(219)@172.17.18.107:37993 (9f2f81738d5e) as admission is already in progress
I1124 01:07:20.586236 30291 leveldb.cpp:341] Persisting action (366 bytes) to leveldb took 34.301173ms
I1124 01:07:20.586287 30291 replica.cpp:713] Persisted action at 3
I1124 01:07:20.587509 30289 replica.cpp:692] Replica received learned notice for position 3 from @0.0.0.0:0
I1124 01:07:20.611263 30289 leveldb.cpp:341] Persisting action (368 bytes) to leveldb took 23.677211ms
I1124 01:07:20.611352 30289 replica.cpp:713] Persisted action at 3
I1124 01:07:20.611387 30289 replica.cpp:698] Replica learned APPEND action at position 3
I1124 01:07:20.613580 30279 registrar.cpp:484] Successfully updated the 'registry' in 65.490944ms
I1124 01:07:20.613802 30288 log.cpp:702] Attempting to truncate the log to 3
I1124 01:07:20.613993 30288 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 4
I1124 01:07:20.615281 30289 replica.cpp:538] Replica received write request for position 4 from (6325)@172.17.18.107:37993
I1124 01:07:20.615883 30279 master.cpp:3946] Registered slave ad27bc60-16d1-4239-9a65-235a991f9600-S0 at slave(219)@172.17.18.107:37993 (9f2f81738d5e) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I1124 01:07:20.616261 30282 slave.cpp:902] Registered with master master@172.17.18.107:37993; given slave ID ad27bc60-16d1-4239-9a65-235a991f9600-S0
I1124 01:07:20.616883 30282 fetcher.cpp:79] Clearing fetcher cache
I1124 01:07:20.617261 30280 status_update_manager.cpp:181] Resuming sending status updates
I1124 01:07:20.617766 30282 slave.cpp:925] Checkpointing SlaveInfo to '/tmp/ReservationEndpointsTest_UnreserveAvailableAndOfferedResources_CSzecr/meta/slaves/ad27bc60-16d1-4239-9a65-235a991f9600-S0/slave.info'
I1124 01:07:20.616550 30284 hierarchical.cpp:380] Added slave ad27bc60-16d1-4239-9a65-235a991f9600-S0 (9f2f81738d5e) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (allocated: )
I1124 01:07:20.618670 30282 slave.cpp:961] Forwarding total oversubscribed resources 
I1124 01:07:20.618932 30282 slave.cpp:3197] Received ping from slave-observer(216)@172.17.18.107:37993
I1124 01:07:20.619288 30285 master.cpp:4288] Received update of slave ad27bc60-16d1-4239-9a65-235a991f9600-S0 at slave(219)@172.17.18.107:37993 (9f2f81738d5e) with total oversubscribed resources 
I1124 01:07:20.619446 30284 hierarchical.cpp:1066] No resources available to allocate!
I1124 01:07:20.619526 30284 hierarchical.cpp:1159] No inverse offers to send out!
I1124 01:07:20.619568 30284 hierarchical.cpp:977] Performed allocation for slave ad27bc60-16d1-4239-9a65-235a991f9600-S0 in 1.108641ms
I1124 01:07:20.620057 30284 hierarchical.cpp:436] Slave ad27bc60-16d1-4239-9a65-235a991f9600-S0 (9f2f81738d5e) updated with oversubscribed resources  (total: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000], allocated: )
I1124 01:07:20.620393 30284 hierarchical.cpp:1066] No resources available to allocate!
I1124 01:07:20.620462 30284 hierarchical.cpp:1159] No inverse offers to send out!
I1124 01:07:20.620507 30284 hierarchical.cpp:977] Performed allocation for slave ad27bc60-16d1-4239-9a65-235a991f9600-S0 in 395959ns
I1124 01:07:20.624356 30285 process.cpp:3067] Handling HTTP event for process 'master' with path: '/master/reserve'
I1124 01:07:20.624418 30285 http.cpp:336] HTTP POST for /master/reserve from 172.17.18.107:48995
I1124 01:07:20.626936 30285 master.cpp:6224] Sending checkpointed resources cpus(role, test-principal):1; mem(role, test-principal):512 to slave ad27bc60-16d1-4239-9a65-235a991f9600-S0 at slave(219)@172.17.18.107:37993 (9f2f81738d5e)
I1124 01:07:20.631428 30260 sched.cpp:164] Version: 0.26.0
I1124 01:07:20.632068 30287 sched.cpp:262] New master detected at master@172.17.18.107:37993
I1124 01:07:20.632230 30287 sched.cpp:318] Authenticating with master master@172.17.18.107:37993
I1124 01:07:20.632307 30287 sched.cpp:325] Using default CRAM-MD5 authenticatee
I1124 01:07:20.632693 30287 authenticatee.cpp:121] Creating new client SASL connection
I1124 01:07:20.633275 30287 master.cpp:5169] Authenticating scheduler-2c19aed9-7470-4927-949b-fb23f1775ab4@172.17.18.107:37993
I1124 01:07:20.633519 30287 authenticator.cpp:413] Starting authentication session for crammd5_authenticatee(516)@172.17.18.107:37993
I1124 01:07:20.633965 30287 authenticator.cpp:98] Creating new server SASL connection
I1124 01:07:20.634316 30287 authenticatee.cpp:212] Received SASL authentication mechanisms: CRAM-MD5
I1124 01:07:20.634456 30287 authenticatee.cpp:238] Attempting to authenticate with mechanism 'CRAM-MD5'
I1124 01:07:20.634605 30287 authenticator.cpp:203] Received SASL authentication start
I1124 01:07:20.634771 30287 authenticator.cpp:325] Authentication requires more steps
I1124 01:07:20.634914 30287 authenticatee.cpp:258] Received SASL authentication step
I1124 01:07:20.635126 30287 authenticator.cpp:231] Received SASL authentication step
I1124 01:07:20.635270 30287 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: '9f2f81738d5e' server FQDN: '9f2f81738d5e' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I1124 01:07:20.635347 30287 auxprop.cpp:179] Looking up auxiliary property '*userPassword'
I1124 01:07:20.636262 30287 auxprop.cpp:179] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I1124 01:07:20.636349 30287 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: '9f2f81738d5e' server FQDN: '9f2f81738d5e' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I1124 01:07:20.636415 30287 auxprop.cpp:129] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I1124 01:07:20.636466 30287 auxprop.cpp:129] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I1124 01:07:20.636541 30287 authenticator.cpp:317] Authentication success
I1124 01:07:20.636754 30287 authenticatee.cpp:298] Authentication success
I1124 01:07:20.636831 30286 master.cpp:5199] Successfully authenticated principal 'test-principal' at scheduler-2c19aed9-7470-4927-949b-fb23f1775ab4@172.17.18.107:37993
I1124 01:07:20.636884 30281 authenticator.cpp:431] Authentication session cleanup for crammd5_authenticatee(516)@172.17.18.107:37993
I1124 01:07:20.637516 30286 sched.cpp:407] Successfully authenticated with master master@172.17.18.107:37993
I1124 01:07:20.637622 30286 sched.cpp:714] Sending SUBSCRIBE call to master@172.17.18.107:37993
I1124 01:07:20.637763 30286 sched.cpp:747] Will retry registration in 1.659715229secs if necessary
I1124 01:07:20.637928 30280 master.cpp:2195] Received SUBSCRIBE call for framework 'default' at scheduler-2c19aed9-7470-4927-949b-fb23f1775ab4@172.17.18.107:37993
I1124 01:07:20.638162 30280 master.cpp:1664] Authorizing framework principal 'test-principal' to receive offers for role 'role'
I1124 01:07:20.638510 30280 master.cpp:2266] Subscribing framework default with checkpointing disabled and capabilities [  ]
I1124 01:07:20.639348 30283 sched.cpp:641] Framework registered with ad27bc60-16d1-4239-9a65-235a991f9600-0000
I1124 01:07:20.639452 30283 sched.cpp:655] Scheduler::registered took 18594ns
I1124 01:07:20.639559 30280 hierarchical.cpp:220] Added framework ad27bc60-16d1-4239-9a65-235a991f9600-0000
I1124 01:07:20.640575 30280 hierarchical.cpp:1159] No inverse offers to send out!
I1124 01:07:20.641254 30280 hierarchical.cpp:961] Performed allocation for 1 slaves in 1.618341ms
I1124 01:07:20.641125 30283 master.cpp:4998] Sending 1 offers to framework ad27bc60-16d1-4239-9a65-235a991f9600-0000 (default) at scheduler-2c19aed9-7470-4927-949b-fb23f1775ab4@172.17.18.107:37993

GMOCK WARNING:
Uninteresting mock function call - returning directly.
    Function call: resourceOffers(0x7fffe27c3830, @0x2ae03c9ce9d0 { 144-byte object <D0-1C B2-33 E0-2A 00-00 00-00 00-00 00-00 00-00 70-CF 00-48 E0-2A 00-00 10-D0 00-48 E0-2A 00-00 B0-D0 00-48 E0-2A 00-00 50-D1 00-48 E0-2A 00-00 A0-D1 00-48 E0-2A 00-00 40-00 01-48 E0-2A 00-00 ... 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 35-61 39-39 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 63-36 30-2D 00-00 00-00 00-00 00-00 00-00 00-00 1F-00 00-00> })
Stack trace:
I1124 01:07:20.642010 30283 sched.cpp:811] Scheduler::resourceOffers took 257483ns
I1124 01:07:20.642578 30283 master.cpp:3395] Processing REVIVE call for framework ad27bc60-16d1-4239-9a65-235a991f9600-0000 (default) at scheduler-2c19aed9-7470-4927-949b-fb23f1775ab4@172.17.18.107:37993
I1124 01:07:20.642757 30283 hierarchical.cpp:886] Removed offer filters for framework ad27bc60-16d1-4239-9a65-235a991f9600-0000
I1124 01:07:20.644323 30283 hierarchical.cpp:1066] No resources available to allocate!
I1124 01:07:20.644377 30283 hierarchical.cpp:1159] No inverse offers to send out!
I1124 01:07:20.644404 30283 hierarchical.cpp:961] Performed allocation for 1 slaves in 1.616762ms
I1124 01:07:20.644626 30291 slave.cpp:2275] Updated checkpointed resources from  to cpus(role, test-principal):1; mem(role, test-principal):512
I1124 01:07:20.652545 30289 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 37.216951ms
I1124 01:07:20.652601 30289 replica.cpp:713] Persisted action at 4
I1124 01:07:20.667095 30279 replica.cpp:692] Replica received learned notice for position 4 from @0.0.0.0:0
I1124 01:07:20.695262 30279 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 28.08901ms
I1124 01:07:20.695446 30279 leveldb.cpp:399] Deleting ~2 keys from leveldb took 96033ns
I1124 01:07:20.695482 30279 replica.cpp:713] Persisted action at 4
I1124 01:07:20.695523 30279 replica.cpp:698] Replica learned TRUNCATE action at position 4
2015-11-24 01:07:21,415:30260(0x2ae0d6471700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:46930] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2015-11-24 01:07:24,751:30260(0x2ae0d6471700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:46930] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2015-11-24 01:07:28,087:30260(0x2ae0d6471700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:46930] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2015-11-24 01:07:31,424:30260(0x2ae0d6471700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:46930] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2015-11-24 01:07:34,760:30260(0x2ae0d6471700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:46930] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
I1124 01:07:35.534328 30283 slave.cpp:4430] Querying resource estimator for oversubscribable resources
I1124 01:07:35.534613 30283 slave.cpp:4444] Received oversubscribable resources  from the resource estimator
I1124 01:07:35.616957 30293 slave.cpp:3197] Received ping from slave-observer(216)@172.17.18.107:37993
2015-11-24 01:07:38,096:30260(0x2ae0d6471700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:46930] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2015-11-24 01:07:41,433:30260(0x2ae0d6471700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:46930] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2015-11-24 01:07:44,769:30260(0x2ae0d6471700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:46930] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2015-11-24 01:07:48,105:30260(0x2ae0d6471700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:46930] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
I1124 01:07:50.535526 30284 slave.cpp:4430] Querying resource estimator for oversubscribable resources
I1124 01:07:50.535809 30284 slave.cpp:4444] Received oversubscribable resources  from the resource estimator
I1124 01:07:50.618424 30284 slave.cpp:3197] Received ping from slave-observer(216)@172.17.18.107:37993
2015-11-24 01:07:51,441:30260(0x2ae0d6471700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:46930] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2015-11-24 01:07:54,778:30260(0x2ae0d6471700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:46930] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2015-11-24 01:07:58,114:30260(0x2ae0d6471700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:46930] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2015-11-24 01:08:01,450:30260(0x2ae0d6471700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:46930] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2015-11-24 01:08:04,785:30260(0x2ae0d6471700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:46930] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
I1124 01:08:05.536947 30293 slave.cpp:4430] Querying resource estimator for oversubscribable resources
I1124 01:08:05.537225 30293 slave.cpp:4444] Received oversubscribable resources  from the resource estimator
I1124 01:08:05.619575 30293 slave.cpp:3197] Received ping from slave-observer(216)@172.17.18.107:37993
2015-11-24 01:08:08,119:30260(0x2ae0d6471700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:46930] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2015-11-24 01:08:11,456:30260(0x2ae0d6471700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:46930] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2015-11-24 01:08:14,790:30260(0x2ae0d6471700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:46930] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2015-11-24 01:08:18,127:30260(0x2ae0d6471700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:46930] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
I1124 01:08:20.520412 30280 slave.cpp:4067] Current disk usage 6.86%. Max allowed age: 5.820034363201435days
I1124 01:08:20.539533 30289 slave.cpp:4430] Querying resource estimator for oversubscribable resources
I1124 01:08:20.539852 30289 slave.cpp:4444] Received oversubscribable resources  from the resource estimator
I1124 01:08:20.620409 30284 slave.cpp:3197] Received ping from slave-observer(216)@172.17.18.107:37993
2015-11-24 01:08:21,463:30260(0x2ae0d6471700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:46930] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2015-11-24 01:08:24,799:30260(0x2ae0d6471700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:46930] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2015-11-24 01:08:28,135:30260(0x2ae0d6471700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:46930] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2015-11-24 01:08:31,471:30260(0x2ae0d6471700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:46930] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2015-11-24 01:08:34,807:30260(0x2ae0d6471700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:46930] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
I1124 01:08:35.540747 30288 slave.cpp:4430] Querying resource estimator for oversubscribable resources
I1124 01:08:35.541132 30288 slave.cpp:4444] Received oversubscribable resources  from the resource estimator
I1124 01:08:35.621484 30287 slave.cpp:3197] Received ping from slave-observer(216)@172.17.18.107:37993
2015-11-24 01:08:38,143:30260(0x2ae0d6471700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:46930] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2015-11-24 01:08:41,479:30260(0x2ae0d6471700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:46930] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2015-11-24 01:08:44,815:30260(0x2ae0d6471700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:46930] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2015-11-24 01:08:48,152:30260(0x2ae0d6471700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:46930] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
I1124 01:08:50.543668 30280 slave.cpp:4430] Querying resource estimator for oversubscribable resources
I1124 01:08:50.544008 30280 slave.cpp:4444] Received oversubscribable resources  from the resource estimator
I1124 01:08:50.622236 30287 slave.cpp:3197] Received ping from slave-observer(216)@172.17.18.107:37993
2015-11-24 01:08:51,488:30260(0x2ae0d6471700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:46930] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2015-11-24 01:08:54,824:30260(0x2ae0d6471700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:46930] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2015-11-24 01:08:58,160:30260(0x2ae0d6471700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:46930] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2015-11-24
.... (more lines of similar logs)
{code}

Logs from a good run:

{code}
[ RUN      ] ReservationEndpointsTest.UnreserveAvailableAndOfferedResources
I1124 14:31:32.379406 30274 leveldb.cpp:174] Opened db in 3.209566ms
I1124 14:31:32.380513 30274 leveldb.cpp:181] Compacted db in 1.056579ms
I1124 14:31:32.380601 30274 leveldb.cpp:196] Created db iterator in 26511ns
I1124 14:31:32.380617 30274 leveldb.cpp:202] Seeked to beginning of db in 1855ns
I1124 14:31:32.380625 30274 leveldb.cpp:271] Iterated through 0 keys in the db in 390ns
I1124 14:31:32.380677 30274 replica.cpp:778] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned
I1124 14:31:32.381772 30304 recover.cpp:447] Starting replica recovery
I1124 14:31:32.382041 30304 recover.cpp:473] Replica is in EMPTY status
I1124 14:31:32.383296 30303 replica.cpp:674] Replica in EMPTY status received a broadcasted recover request from (6249)@172.17.6.149:52680
I1124 14:31:32.383708 30298 recover.cpp:193] Received a recover response from a replica in EMPTY status
I1124 14:31:32.384263 30305 recover.cpp:564] Updating replica status to STARTING
I1124 14:31:32.384837 30303 master.cpp:365] Master 540f518e-1ba4-4f89-8b15-7b99ef53c093 (c264cb162c79) started on 172.17.6.149:52680
I1124 14:31:32.384984 30304 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 591432ns
I1124 14:31:32.385017 30304 replica.cpp:321] Persisted replica status to STARTING
I1124 14:31:32.384865 30303 master.cpp:367] Flags at startup: --acls="""" --allocation_interval=""1000secs"" --allocator=""HierarchicalDRF"" --authenticate=""true"" --authenticate_slaves=""true"" --authenticators=""crammd5"" --authorizers=""local"" --credentials=""/tmp/L5EFJU/credentials"" --framework_sorter=""drf"" --help=""false"" --hostname_lookup=""true"" --initialize_driver_logging=""true"" --log_auto_initialize=""true"" --logbufsecs=""0"" --logging_level=""INFO"" --max_slave_ping_timeouts=""5"" --quiet=""false"" --recovery_slave_removal_limit=""100%"" --registry=""replicated_log"" --registry_fetch_timeout=""1mins"" --registry_store_timeout=""25secs"" --registry_strict=""true"" --roles=""role"" --root_submissions=""true"" --slave_ping_timeout=""15secs"" --slave_reregister_timeout=""10mins"" --user_sorter=""drf"" --version=""false"" --webui_dir=""/mesos/mesos-0.27.0/_inst/share/mesos/webui"" --work_dir=""/tmp/L5EFJU/master"" --zk_session_timeout=""10secs""
I1124 14:31:32.385262 30303 master.cpp:412] Master only allowing authenticated frameworks to register
I1124 14:31:32.385279 30303 master.cpp:417] Master only allowing authenticated slaves to register
I1124 14:31:32.385288 30303 credentials.hpp:35] Loading credentials for authentication from '/tmp/L5EFJU/credentials'
I1124 14:31:32.385319 30297 recover.cpp:473] Replica is in STARTING status
I1124 14:31:32.385628 30303 master.cpp:456] Using default 'crammd5' authenticator
I1124 14:31:32.385783 30303 master.cpp:493] Authorization enabled
I1124 14:31:32.386221 30299 whitelist_watcher.cpp:77] No whitelist given
I1124 14:31:32.386329 30302 hierarchical.cpp:162] Initialized hierarchical allocator process
I1124 14:31:32.386343 30299 replica.cpp:674] Replica in STARTING status received a broadcasted recover request from (6250)@172.17.6.149:52680
I1124 14:31:32.386756 30308 recover.cpp:193] Received a recover response from a replica in STARTING status
I1124 14:31:32.387285 30305 recover.cpp:564] Updating replica status to VOTING
I1124 14:31:32.388052 30293 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 444555ns
I1124 14:31:32.388090 30293 replica.cpp:321] Persisted replica status to VOTING
I1124 14:31:32.388299 30300 recover.cpp:578] Successfully joined the Paxos group
I1124 14:31:32.388371 30308 master.cpp:1625] The newly elected leader is master@172.17.6.149:52680 with id 540f518e-1ba4-4f89-8b15-7b99ef53c093
I1124 14:31:32.388417 30308 master.cpp:1638] Elected as the leading master!
I1124 14:31:32.388442 30308 master.cpp:1383] Recovering from registrar
I1124 14:31:32.388593 30305 registrar.cpp:307] Recovering registrar
I1124 14:31:32.388627 30300 recover.cpp:462] Recover process terminated
I1124 14:31:32.389482 30294 log.cpp:659] Attempting to start the writer
I1124 14:31:32.390638 30294 replica.cpp:494] Replica received implicit promise request from (6251)@172.17.6.149:52680 with proposal 1
I1124 14:31:32.391034 30294 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 365035ns
I1124 14:31:32.391055 30294 replica.cpp:343] Persisted promised to 1
I1124 14:31:32.391635 30297 coordinator.cpp:238] Coordinator attempting to fill missing positions
I1124 14:31:32.392724 30305 replica.cpp:389] Replica received explicit promise request from (6252)@172.17.6.149:52680 for position 0 with proposal 2
I1124 14:31:32.393254 30305 leveldb.cpp:341] Persisting action (8 bytes) to leveldb took 490425ns
I1124 14:31:32.393277 30305 replica.cpp:713] Persisted action at 0
I1124 14:31:32.394223 30305 replica.cpp:538] Replica received write request for position 0 from (6253)@172.17.6.149:52680
I1124 14:31:32.394282 30305 leveldb.cpp:436] Reading position from leveldb took 29241ns
I1124 14:31:32.394809 30305 leveldb.cpp:341] Persisting action (14 bytes) to leveldb took 477617ns
I1124 14:31:32.394830 30305 replica.cpp:713] Persisted action at 0
I1124 14:31:32.395393 30307 replica.cpp:692] Replica received learned notice for position 0 from @0.0.0.0:0
I1124 14:31:32.395923 30307 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 499295ns
I1124 14:31:32.395946 30307 replica.cpp:713] Persisted action at 0
I1124 14:31:32.395979 30307 replica.cpp:698] Replica learned NOP action at position 0
I1124 14:31:32.396602 30304 log.cpp:675] Writer started with ending position 0
I1124 14:31:32.397611 30298 leveldb.cpp:436] Reading position from leveldb took 29312ns
I1124 14:31:32.398551 30297 registrar.cpp:340] Successfully fetched the registry (0B) in 9.914112ms
I1124 14:31:32.398671 30297 registrar.cpp:439] Applied 1 operations in 29912ns; attempting to update the 'registry'
I1124 14:31:32.399397 30296 log.cpp:683] Attempting to append 176 bytes to the log
I1124 14:31:32.399523 30302 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 1
I1124 14:31:32.400318 30304 replica.cpp:538] Replica received write request for position 1 from (6254)@172.17.6.149:52680
I1124 14:31:32.400794 30304 leveldb.cpp:341] Persisting action (195 bytes) to leveldb took 416219ns
I1124 14:31:32.400825 30304 replica.cpp:713] Persisted action at 1
I1124 14:31:32.401752 30307 replica.cpp:692] Replica received learned notice for position 1 from @0.0.0.0:0
I1124 14:31:32.402163 30307 leveldb.cpp:341] Persisting action (197 bytes) to leveldb took 367685ns
I1124 14:31:32.402194 30307 replica.cpp:713] Persisted action at 1
I1124 14:31:32.402217 30307 replica.cpp:698] Replica learned APPEND action at position 1
I1124 14:31:32.403475 30301 registrar.cpp:484] Successfully updated the 'registry' in 4.722944ms
I1124 14:31:32.403673 30301 registrar.cpp:370] Successfully recovered registrar
I1124 14:31:32.403895 30297 log.cpp:702] Attempting to truncate the log to 1
I1124 14:31:32.404165 30293 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 2
I1124 14:31:32.404492 30307 master.cpp:1435] Recovered 0 slaves from the Registry (137B) ; allowing 10mins for slaves to re-register
I1124 14:31:32.404580 30306 hierarchical.cpp:174] Allocator recovery is not supported yet
I1124 14:31:32.405313 30307 replica.cpp:538] Replica received write request for position 2 from (6255)@172.17.6.149:52680
I1124 14:31:32.405725 30307 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 369717ns
I1124 14:31:32.405756 30307 replica.cpp:713] Persisted action at 2
I1124 14:31:32.406597 30303 replica.cpp:692] Replica received learned notice for position 2 from @0.0.0.0:0
I1124 14:31:32.407234 30303 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 590194ns
I1124 14:31:32.407291 30303 leveldb.cpp:399] Deleting ~1 keys from leveldb took 31709ns
I1124 14:31:32.407313 30303 replica.cpp:713] Persisted action at 2
I1124 14:31:32.407346 30303 replica.cpp:698] Replica learned TRUNCATE action at position 2
I1124 14:31:32.417404 30274 containerizer.cpp:141] Using isolation: posix/cpu,posix/mem,filesystem/posix
W1124 14:31:32.417768 30274 backend.cpp:48] Failed to create 'bind' backend: BindBackend requires root privileges
I1124 14:31:32.420657 30300 slave.cpp:189] Slave started on 219)@172.17.6.149:52680
I1124 14:31:32.420680 30300 slave.cpp:190] Flags at startup: --appc_store_dir=""/tmp/mesos/store/appc"" --authenticatee=""crammd5"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --credential=""/tmp/ReservationEndpointsTest_UnreserveAvailableAndOfferedResources_N4vdsa/credential"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_auth_server=""auth.docker.io"" --docker_auth_server_port=""443"" --docker_kill_orphans=""true"" --docker_local_archives_dir=""/tmp/mesos/images/docker"" --docker_puller=""local"" --docker_puller_timeout=""60"" --docker_registry=""registry-1.docker.io"" --docker_registry_port=""443"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/tmp/mesos/store/docker"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/tmp/ReservationEndpointsTest_UnreserveAvailableAndOfferedResources_N4vdsa/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --hostname_lookup=""true"" --image_provisioner_backend=""copy"" --initialize_driver_logging=""true"" --isolation=""posix/cpu,posix/mem"" --launcher_dir=""/mesos/mesos-0.27.0/_build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""10ms"" --resources=""cpus:2;mem:1024;disk:1024;ports:[31000-32000]"" --revocable_cpu_low_priority=""true"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --systemd_runtime_directory=""/run/systemd/system"" --version=""false"" --work_dir=""/tmp/ReservationEndpointsTest_UnreserveAvailableAndOfferedResources_N4vdsa""
I1124 14:31:32.421241 30300 credentials.hpp:83] Loading credential for authentication from '/tmp/ReservationEndpointsTest_UnreserveAvailableAndOfferedResources_N4vdsa/credential'
I1124 14:31:32.421465 30300 slave.cpp:320] Slave using credential for: test-principal
I1124 14:31:32.421685 30300 resources.cpp:472] Parsing resources as JSON failed: cpus:2;mem:1024;disk:1024;ports:[31000-32000]
Trying semicolon-delimited string format instead
I1124 14:31:32.422097 30300 slave.cpp:390] Slave resources: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I1124 14:31:32.422158 30300 slave.cpp:398] Slave attributes: [  ]
I1124 14:31:32.422168 30300 slave.cpp:403] Slave hostname: c264cb162c79
I1124 14:31:32.422163 30274 resources.cpp:472] Parsing resources as JSON failed: cpus:1;mem:128
Trying semicolon-delimited string format instead
I1124 14:31:32.422174 30300 slave.cpp:408] Slave checkpoint: true
I1124 14:31:32.422358 30274 resources.cpp:472] Parsing resources as JSON failed: mem:384
Trying semicolon-delimited string format instead
I1124 14:31:32.423035 30299 state.cpp:52] Recovering state from '/tmp/ReservationEndpointsTest_UnreserveAvailableAndOfferedResources_N4vdsa/meta'
I1124 14:31:32.423334 30294 status_update_manager.cpp:200] Recovering status update manager
I1124 14:31:32.423507 30300 containerizer.cpp:383] Recovering containerizer
I1124 14:31:32.424350 30296 slave.cpp:4258] Finished recovery
I1124 14:31:32.424875 30296 slave.cpp:4430] Querying resource estimator for oversubscribable resources
I1124 14:31:32.425091 30297 status_update_manager.cpp:174] Pausing sending status updates
I1124 14:31:32.425112 30296 slave.cpp:727] New master detected at master@172.17.6.149:52680
I1124 14:31:32.425194 30296 slave.cpp:790] Authenticating with master master@172.17.6.149:52680
I1124 14:31:32.425214 30296 slave.cpp:795] Using default CRAM-MD5 authenticatee
I1124 14:31:32.425395 30296 slave.cpp:763] Detecting new master
I1124 14:31:32.425431 30305 authenticatee.cpp:121] Creating new client SASL connection
I1124 14:31:32.425519 30296 slave.cpp:4444] Received oversubscribable resources  from the resource estimator
I1124 14:31:32.425684 30301 master.cpp:5169] Authenticating slave(219)@172.17.6.149:52680
I1124 14:31:32.425815 30293 authenticator.cpp:413] Starting authentication session for crammd5_authenticatee(513)@172.17.6.149:52680
I1124 14:31:32.426038 30296 authenticator.cpp:98] Creating new server SASL connection
I1124 14:31:32.426308 30305 authenticatee.cpp:212] Received SASL authentication mechanisms: CRAM-MD5
I1124 14:31:32.426344 30305 authenticatee.cpp:238] Attempting to authenticate with mechanism 'CRAM-MD5'
I1124 14:31:32.426435 30305 authenticator.cpp:203] Received SASL authentication start
I1124 14:31:32.426487 30305 authenticator.cpp:325] Authentication requires more steps
I1124 14:31:32.426573 30305 authenticatee.cpp:258] Received SASL authentication step
I1124 14:31:32.426687 30302 authenticator.cpp:231] Received SASL authentication step
I1124 14:31:32.426719 30302 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: 'c264cb162c79' server FQDN: 'c264cb162c79' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I1124 14:31:32.426733 30302 auxprop.cpp:179] Looking up auxiliary property '*userPassword'
I1124 14:31:32.426770 30302 auxprop.cpp:179] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I1124 14:31:32.426789 30302 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: 'c264cb162c79' server FQDN: 'c264cb162c79' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I1124 14:31:32.426797 30302 auxprop.cpp:129] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I1124 14:31:32.426803 30302 auxprop.cpp:129] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I1124 14:31:32.426815 30302 authenticator.cpp:317] Authentication success
I1124 14:31:32.426888 30299 authenticatee.cpp:298] Authentication success
I1124 14:31:32.426975 30307 master.cpp:5199] Successfully authenticated principal 'test-principal' at slave(219)@172.17.6.149:52680
I1124 14:31:32.427198 30308 authenticator.cpp:431] Authentication session cleanup for crammd5_authenticatee(513)@172.17.6.149:52680
I1124 14:31:32.427345 30299 slave.cpp:858] Successfully authenticated with master master@172.17.6.149:52680
I1124 14:31:32.427480 30299 slave.cpp:1252] Will retry registration in 14.843642ms if necessary
I1124 14:31:32.427793 30298 master.cpp:3878] Registering slave at slave(219)@172.17.6.149:52680 (c264cb162c79) with id 540f518e-1ba4-4f89-8b15-7b99ef53c093-S0
I1124 14:31:32.428304 30301 registrar.cpp:439] Applied 1 operations in 66689ns; attempting to update the 'registry'
I1124 14:31:32.429102 30296 log.cpp:683] Attempting to append 345 bytes to the log
I1124 14:31:32.429255 30301 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 3
I1124 14:31:32.429944 30305 replica.cpp:538] Replica received write request for position 3 from (6267)@172.17.6.149:52680
I1124 14:31:32.430567 30305 leveldb.cpp:341] Persisting action (364 bytes) to leveldb took 565035ns
I1124 14:31:32.430591 30305 replica.cpp:713] Persisted action at 3
I1124 14:31:32.431191 30301 replica.cpp:692] Replica received learned notice for position 3 from @0.0.0.0:0
I1124 14:31:32.431586 30301 leveldb.cpp:341] Persisting action (366 bytes) to leveldb took 363559ns
I1124 14:31:32.431609 30301 replica.cpp:713] Persisted action at 3
I1124 14:31:32.431627 30301 replica.cpp:698] Replica learned APPEND action at position 3
2015-11-24 14:31:32,432:30274(0x2b7afc200700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:34475] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
I1124 14:31:32.433154 30296 registrar.cpp:484] Successfully updated the 'registry' in 4.772864ms
I1124 14:31:32.433446 30300 log.cpp:702] Attempting to truncate the log to 3
I1124 14:31:32.433552 30294 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 4
I1124 14:31:32.434074 30295 slave.cpp:3197] Received ping from slave-observer(219)@172.17.6.149:52680
I1124 14:31:32.434813 30302 hierarchical.cpp:380] Added slave 540f518e-1ba4-4f89-8b15-7b99ef53c093-S0 (c264cb162c79) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (allocated: )
I1124 14:31:32.435241 30302 hierarchical.cpp:1066] No resources available to allocate!
I1124 14:31:32.435408 30302 hierarchical.cpp:1159] No inverse offers to send out!
I1124 14:31:32.435560 30302 hierarchical.cpp:977] Performed allocation for slave 540f518e-1ba4-4f89-8b15-7b99ef53c093-S0 in 687663ns
I1124 14:31:32.435897 30304 replica.cpp:538] Replica received write request for position 4 from (6268)@172.17.6.149:52680
I1124 14:31:32.435978 30306 master.cpp:3946] Registered slave 540f518e-1ba4-4f89-8b15-7b99ef53c093-S0 at slave(219)@172.17.6.149:52680 (c264cb162c79) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I1124 14:31:32.436049 30294 slave.cpp:902] Registered with master master@172.17.6.149:52680; given slave ID 540f518e-1ba4-4f89-8b15-7b99ef53c093-S0
I1124 14:31:32.436082 30294 fetcher.cpp:79] Clearing fetcher cache
I1124 14:31:32.436264 30300 status_update_manager.cpp:181] Resuming sending status updates
I1124 14:31:32.436611 30294 slave.cpp:925] Checkpointing SlaveInfo to '/tmp/ReservationEndpointsTest_UnreserveAvailableAndOfferedResources_N4vdsa/meta/slaves/540f518e-1ba4-4f89-8b15-7b99ef53c093-S0/slave.info'
I1124 14:31:32.437463 30294 slave.cpp:961] Forwarding total oversubscribed resources 
I1124 14:31:32.437489 30304 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 1.540024ms
I1124 14:31:32.437528 30304 replica.cpp:713] Persisted action at 4
I1124 14:31:32.437707 30293 process.cpp:3067] Handling HTTP event for process 'master' with path: '/master/reserve'
I1124 14:31:32.437942 30293 http.cpp:336] HTTP POST for /master/reserve from 172.17.6.149:46929
I1124 14:31:32.438607 30294 replica.cpp:692] Replica received learned notice for position 4 from @0.0.0.0:0
I1124 14:31:32.439095 30294 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 443410ns
I1124 14:31:32.439173 30294 leveldb.cpp:399] Deleting ~2 keys from leveldb took 47749ns
I1124 14:31:32.439280 30294 replica.cpp:713] Persisted action at 4
I1124 14:31:32.439323 30294 replica.cpp:698] Replica learned TRUNCATE action at position 4
I1124 14:31:32.439617 30293 master.cpp:4288] Received update of slave 540f518e-1ba4-4f89-8b15-7b99ef53c093-S0 at slave(219)@172.17.6.149:52680 (c264cb162c79) with total oversubscribed resources 
I1124 14:31:32.440897 30294 master.cpp:6224] Sending checkpointed resources cpus(role, test-principal):1; mem(role, test-principal):512 to slave 540f518e-1ba4-4f89-8b15-7b99ef53c093-S0 at slave(219)@172.17.6.149:52680 (c264cb162c79)
I1124 14:31:32.441396 30298 hierarchical.cpp:436] Slave 540f518e-1ba4-4f89-8b15-7b99ef53c093-S0 (c264cb162c79) updated with oversubscribed resources  (total: cpus(*):1; mem(*):512; disk(*):1024; ports(*):[31000-32000]; cpus(role, test-principal):1; mem(role, test-principal):512, allocated: )
I1124 14:31:32.441532 30294 slave.cpp:2275] Updated checkpointed resources from  to cpus(role, test-principal):1; mem(role, test-principal):512
I1124 14:31:32.441783 30298 hierarchical.cpp:1066] No resources available to allocate!
I1124 14:31:32.441862 30298 hierarchical.cpp:1159] No inverse offers to send out!
I1124 14:31:32.441889 30298 hierarchical.cpp:977] Performed allocation for slave 540f518e-1ba4-4f89-8b15-7b99ef53c093-S0 in 427498ns
I1124 14:31:32.443641 30274 sched.cpp:164] Version: 0.27.0
I1124 14:31:32.444103 30303 sched.cpp:1261] Ignoring revive offers message as master is disconnected
I1124 14:31:32.444250 30304 sched.cpp:262] New master detected at master@172.17.6.149:52680
I1124 14:31:32.444331 30304 sched.cpp:318] Authenticating with master master@172.17.6.149:52680
I1124 14:31:32.444356 30304 sched.cpp:325] Using default CRAM-MD5 authenticatee
I1124 14:31:32.444573 30298 authenticatee.cpp:121] Creating new client SASL connection
I1124 14:31:32.444828 30297 master.cpp:5169] Authenticating scheduler-0ff32e9c-a0bb-4f1c-a2e6-580485a123be@172.17.6.149:52680
I1124 14:31:32.444926 30307 authenticator.cpp:413] Starting authentication session for crammd5_authenticatee(514)@172.17.6.149:52680
I1124 14:31:32.445137 30302 authenticator.cpp:98] Creating new server SASL connection
I1124 14:31:32.445329 30304 authenticatee.cpp:212] Received SASL authentication mechanisms: CRAM-MD5
I1124 14:31:32.445359 30304 authenticatee.cpp:238] Attempting to authenticate with mechanism 'CRAM-MD5'
I1124 14:31:32.445464 30294 authenticator.cpp:203] Received SASL authentication start
I1124 14:31:32.445531 30294 authenticator.cpp:325] Authentication requires more steps
I1124 14:31:32.445641 30304 authenticatee.cpp:258] Received SASL authentication step
I1124 14:31:32.445760 30294 authenticator.cpp:231] Received SASL authentication step
I1124 14:31:32.445794 30294 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: 'c264cb162c79' server FQDN: 'c264cb162c79' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I1124 14:31:32.445811 30294 auxprop.cpp:179] Looking up auxiliary property '*userPassword'
I1124 14:31:32.445849 30294 auxprop.cpp:179] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I1124 14:31:32.445875 30294 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: 'c264cb162c79' server FQDN: 'c264cb162c79' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I1124 14:31:32.445885 30294 auxprop.cpp:129] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I1124 14:31:32.445890 30294 auxprop.cpp:129] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I1124 14:31:32.445904 30294 authenticator.cpp:317] Authentication success
I1124 14:31:32.446023 30303 authenticatee.cpp:298] Authentication success
I1124 14:31:32.446034 30308 master.cpp:5199] Successfully authenticated principal 'test-principal' at scheduler-0ff32e9c-a0bb-4f1c-a2e6-580485a123be@172.17.6.149:52680
I1124 14:31:32.446112 30302 authenticator.cpp:431] Authentication session cleanup for crammd5_authenticatee(514)@172.17.6.149:52680
I1124 14:31:32.446387 30299 sched.cpp:407] Successfully authenticated with master master@172.17.6.149:52680
I1124 14:31:32.446409 30299 sched.cpp:714] Sending SUBSCRIBE call to master@172.17.6.149:52680
I1124 14:31:32.446502 30299 sched.cpp:747] Will retry registration in 22.168105ms if necessary
I1124 14:31:32.446610 30308 master.cpp:2195] Received SUBSCRIBE call for framework 'default' at scheduler-0ff32e9c-a0bb-4f1c-a2e6-580485a123be@172.17.6.149:52680
I1124 14:31:32.446663 30308 master.cpp:1664] Authorizing framework principal 'test-principal' to receive offers for role 'role'
I1124 14:31:32.446889 30293 master.cpp:2266] Subscribing framework default with checkpointing disabled and capabilities [  ]
I1124 14:31:32.447237 30297 hierarchical.cpp:220] Added framework 540f518e-1ba4-4f89-8b15-7b99ef53c093-0000
I1124 14:31:32.447360 30304 sched.cpp:641] Framework registered with 540f518e-1ba4-4f89-8b15-7b99ef53c093-0000
I1124 14:31:32.447406 30304 sched.cpp:655] Scheduler::registered took 20613ns
I1124 14:31:32.448019 30297 hierarchical.cpp:1159] No inverse offers to send out!
I1124 14:31:32.448055 30297 hierarchical.cpp:961] Performed allocation for 1 slaves in 792842ns
I1124 14:31:32.448572 30294 master.cpp:4998] Sending 1 offers to framework 540f518e-1ba4-4f89-8b15-7b99ef53c093-0000 (default) at scheduler-0ff32e9c-a0bb-4f1c-a2e6-580485a123be@172.17.6.149:52680
I1124 14:31:32.449141 30293 sched.cpp:811] Scheduler::resourceOffers took 122685ns
I1124 14:31:32.451211 30305 master.cpp:2934] Processing ACCEPT call for offers: [ 540f518e-1ba4-4f89-8b15-7b99ef53c093-O0 ] on slave 540f518e-1ba4-4f89-8b15-7b99ef53c093-S0 at slave(219)@172.17.6.149:52680 (c264cb162c79) for framework 540f518e-1ba4-4f89-8b15-7b99ef53c093-0000 (default) at scheduler-0ff32e9c-a0bb-4f1c-a2e6-580485a123be@172.17.6.149:52680
I1124 14:31:32.451257 30305 master.cpp:2730] Authorizing framework principal 'test-principal' to launch task 981ca865-a5f9-42c6-8090-eaf909f78186 as user 'mesos'
I1124 14:31:32.452697 30296 master.hpp:176] Adding task 981ca865-a5f9-42c6-8090-eaf909f78186 with resources cpus(role, test-principal):1; mem(role, test-principal):128 on slave 540f518e-1ba4-4f89-8b15-7b99ef53c093-S0 (c264cb162c79)
I1124 14:31:32.452819 30296 master.cpp:3264] Launching task 981ca865-a5f9-42c6-8090-eaf909f78186 of framework 540f518e-1ba4-4f89-8b15-7b99ef53c093-0000 (default) at scheduler-0ff32e9c-a0bb-4f1c-a2e6-580485a123be@172.17.6.149:52680 with resources cpus(role, test-principal):1; mem(role, test-principal):128 on slave 540f518e-1ba4-4f89-8b15-7b99ef53c093-S0 at slave(219)@172.17.6.149:52680 (c264cb162c79)
I1124 14:31:32.453191 30305 slave.cpp:1292] Got assigned task 981ca865-a5f9-42c6-8090-eaf909f78186 for framework 540f518e-1ba4-4f89-8b15-7b99ef53c093-0000
I1124 14:31:32.453485 30305 resources.cpp:472] Parsing resources as JSON failed: cpus:0.1;mem:32
Trying semicolon-delimited string format instead
I1124 14:31:32.453652 30303 hierarchical.cpp:793] Recovered cpus(*):1; mem(*):512; disk(*):1024; ports(*):[31000-32000]; mem(role, test-principal):384 (total: cpus(*):1; mem(*):512; disk(*):1024; ports(*):[31000-32000]; cpus(role, test-principal):1; mem(role, test-principal):512, allocated: cpus(role, test-principal):1; mem(role, test-principal):128) on slave 540f518e-1ba4-4f89-8b15-7b99ef53c093-S0 from framework 540f518e-1ba4-4f89-8b15-7b99ef53c093-0000
I1124 14:31:32.453729 30303 hierarchical.cpp:830] Framework 540f518e-1ba4-4f89-8b15-7b99ef53c093-0000 filtered slave 540f518e-1ba4-4f89-8b15-7b99ef53c093-S0 for 5secs
I1124 14:31:32.454195 30305 slave.cpp:1411] Launching task 981ca865-a5f9-42c6-8090-eaf909f78186 for framework 540f518e-1ba4-4f89-8b15-7b99ef53c093-0000
I1124 14:31:32.454306 30305 resources.cpp:472] Parsing resources as JSON failed: cpus:0.1;mem:32
Trying semicolon-delimited string format instead
I1124 14:31:32.455086 30305 paths.cpp:434] Trying to chown '/tmp/ReservationEndpointsTest_UnreserveAvailableAndOfferedResources_N4vdsa/slaves/540f518e-1ba4-4f89-8b15-7b99ef53c093-S0/frameworks/540f518e-1ba4-4f89-8b15-7b99ef53c093-0000/executors/981ca865-a5f9-42c6-8090-eaf909f78186/runs/8544eb55-baa8-495a-98e7-565ec58da673' to user 'mesos'
I1124 14:31:32.462555 30305 slave.cpp:5043] Launching executor 981ca865-a5f9-42c6-8090-eaf909f78186 of framework 540f518e-1ba4-4f89-8b15-7b99ef53c093-0000 with resources cpus(*):0.1; mem(*):32 in work directory '/tmp/ReservationEndpointsTest_UnreserveAvailableAndOfferedResources_N4vdsa/slaves/540f518e-1ba4-4f89-8b15-7b99ef53c093-S0/frameworks/540f518e-1ba4-4f89-8b15-7b99ef53c093-0000/executors/981ca865-a5f9-42c6-8090-eaf909f78186/runs/8544eb55-baa8-495a-98e7-565ec58da673'
I1124 14:31:32.463389 30296 containerizer.cpp:617] Starting container '8544eb55-baa8-495a-98e7-565ec58da673' for executor '981ca865-a5f9-42c6-8090-eaf909f78186' of framework '540f518e-1ba4-4f89-8b15-7b99ef53c093-0000'
I1124 14:31:32.463459 30305 slave.cpp:1629] Queuing task '981ca865-a5f9-42c6-8090-eaf909f78186' for executor '981ca865-a5f9-42c6-8090-eaf909f78186' of framework 540f518e-1ba4-4f89-8b15-7b99ef53c093-0000
I1124 14:31:32.463846 30305 slave.cpp:680] Successfully attached file '/tmp/ReservationEndpointsTest_UnreserveAvailableAndOfferedResources_N4vdsa/slaves/540f518e-1ba4-4f89-8b15-7b99ef53c093-S0/frameworks/540f518e-1ba4-4f89-8b15-7b99ef53c093-0000/executors/981ca865-a5f9-42c6-8090-eaf909f78186/runs/8544eb55-baa8-495a-98e7-565ec58da673'
I1124 14:31:32.468849 30301 launcher.cpp:132] Forked child with pid '3901' for container '8544eb55-baa8-495a-98e7-565ec58da673'
WARNING: Logging before InitGoogleLogging() is written to STDERR
I1124 14:31:32.614969  3915 process.cpp:936] libprocess is initialized on 172.17.6.149:55307 for 16 cpus
I1124 14:31:32.615892  3915 logging.cpp:175] Logging to STDERR
I1124 14:31:32.617889  3915 exec.cpp:134] Version: 0.27.0
I1124 14:31:32.627719  3936 exec.cpp:184] Executor started at: executor(1)@172.17.6.149:55307 with pid 3915
I1124 14:31:32.630262 30298 slave.cpp:2406] Got registration for executor '981ca865-a5f9-42c6-8090-eaf909f78186' of framework 540f518e-1ba4-4f89-8b15-7b99ef53c093-0000 from executor(1)@172.17.6.149:55307
I1124 14:31:32.632133 30293 slave.cpp:1794] Sending queued task '981ca865-a5f9-42c6-8090-eaf909f78186' to executor '981ca865-a5f9-42c6-8090-eaf909f78186' of framework 540f518e-1ba4-4f89-8b15-7b99ef53c093-0000 at executor(1)@172.17.6.149:55307
I1124 14:31:32.632527  3933 exec.cpp:208] Executor registered on slave 540f518e-1ba4-4f89-8b15-7b99ef53c093-S0
Registered executor on c264cb162c79
I1124 14:31:32.634414  3933 exec.cpp:220] Executor::registered took 256013ns
I1124 14:31:32.634769  3933 exec.cpp:295] Executor asked to run task '981ca865-a5f9-42c6-8090-eaf909f78186'
I1124 14:31:32.634874  3933 exec.cpp:304] Executor::launchTask took 75212ns
Starting task 981ca865-a5f9-42c6-8090-eaf909f78186
Forked command at 3946
sh -c 'sleep 1000'
I1124 14:31:32.638731  3939 exec.cpp:517] Executor sending status update TASK_RUNNING (UUID: 574d2d42-c323-4733-9982-f4398c654776) for task 981ca865-a5f9-42c6-8090-eaf909f78186 of framework 540f518e-1ba4-4f89-8b15-7b99ef53c093-0000
I1124 14:31:32.639981 30298 slave.cpp:2763] Handling status update TASK_RUNNING (UUID: 574d2d42-c323-4733-9982-f4398c654776) for task 981ca865-a5f9-42c6-8090-eaf909f78186 of framework 540f518e-1ba4-4f89-8b15-7b99ef53c093-0000 from executor(1)@172.17.6.149:55307
I1124 14:31:32.640444 30296 status_update_manager.cpp:320] Received status update TASK_RUNNING (UUID: 574d2d42-c323-4733-9982-f4398c654776) for task 981ca865-a5f9-42c6-8090-eaf909f78186 of framework 540f518e-1ba4-4f89-8b15-7b99ef53c093-0000
I1124 14:31:32.640498 30296 status_update_manager.cpp:497] Creating StatusUpdate stream for task 981ca865-a5f9-42c6-8090-eaf909f78186 of framework 540f518e-1ba4-4f89-8b15-7b99ef53c093-0000
I1124 14:31:32.640938 30296 status_update_manager.cpp:374] Forwarding update TASK_RUNNING (UUID: 574d2d42-c323-4733-9982-f4398c654776) for task 981ca865-a5f9-42c6-8090-eaf909f78186 of framework 540f518e-1ba4-4f89-8b15-7b99ef53c093-0000 to the slave
I1124 14:31:32.641361 30301 slave.cpp:3115] Forwarding the update TASK_RUNNING (UUID: 574d2d42-c323-4733-9982-f4398c654776) for task 981ca865-a5f9-42c6-8090-eaf909f78186 of framework 540f518e-1ba4-4f89-8b15-7b99ef53c093-0000 to master@172.17.6.149:52680
I1124 14:31:32.641582 30301 slave.cpp:3009] Status update manager successfully handled status update TASK_RUNNING (UUID: 574d2d42-c323-4733-9982-f4398c654776) for task 981ca865-a5f9-42c6-8090-eaf909f78186 of framework 540f518e-1ba4-4f89-8b15-7b99ef53c093-0000
I1124 14:31:32.641636 30301 slave.cpp:3025] Sending acknowledgement for status update TASK_RUNNING (UUID: 574d2d42-c323-4733-9982-f4398c654776) for task 981ca865-a5f9-42c6-8090-eaf909f78186 of framework 540f518e-1ba4-4f89-8b15-7b99ef53c093-0000 to executor(1)@172.17.6.149:55307
I1124 14:31:32.641923 30306 master.cpp:4433] Status update TASK_RUNNING (UUID: 574d2d42-c323-4733-9982-f4398c654776) for task 981ca865-a5f9-42c6-8090-eaf909f78186 of framework 540f518e-1ba4-4f89-8b15-7b99ef53c093-0000 from slave 540f518e-1ba4-4f89-8b15-7b99ef53c093-S0 at slave(219)@172.17.6.149:52680 (c264cb162c79)
I1124 14:31:32.642014 30306 master.cpp:4481] Forwarding status update TASK_RUNNING (UUID: 574d2d42-c323-4733-9982-f4398c654776) for task 981ca865-a5f9-42c6-8090-eaf909f78186 of framework 540f518e-1ba4-4f89-8b15-7b99ef53c093-0000
I1124 14:31:32.642205 30306 master.cpp:6085] Updating the state of task 981ca865-a5f9-42c6-8090-eaf909f78186 of framework 540f518e-1ba4-4f89-8b15-7b99ef53c093-0000 (latest state: TASK_RUNNING, status update state: TASK_RUNNING)
I1124 14:31:32.642366 30298 sched.cpp:919] Scheduler::statusUpdate took 38074ns
I1124 14:31:32.642711 30307 master.cpp:3590] Processing ACKNOWLEDGE call 574d2d42-c323-4733-9982-f4398c654776 for task 981ca865-a5f9-42c6-8090-eaf909f78186 of framework 540f518e-1ba4-4f89-8b15-7b99ef53c093-0000 (default) at scheduler-0ff32e9c-a0bb-4f1c-a2e6-580485a123be@172.17.6.149:52680 on slave 540f518e-1ba4-4f89-8b15-7b99ef53c093-S0
I1124 14:31:32.642789  3932 exec.cpp:341] Executor received status update acknowledgement 574d2d42-c323-4733-9982-f4398c654776 for task 981ca865-a5f9-42c6-8090-eaf909f78186 of framework 540f518e-1ba4-4f89-8b15-7b99ef53c093-0000
I1124 14:31:32.643095 30307 status_update_manager.cpp:392] Received status update acknowledgement (UUID: 574d2d42-c323-4733-9982-f4398c654776) for task 981ca865-a5f9-42c6-8090-eaf909f78186 of framework 540f518e-1ba4-4f89-8b15-7b99ef53c093-0000
I1124 14:31:32.643564 30304 slave.cpp:2346] Status update manager successfully handled status update acknowledgement (UUID: 574d2d42-c323-4733-9982-f4398c654776) for task 981ca865-a5f9-42c6-8090-eaf909f78186 of framework 540f518e-1ba4-4f89-8b15-7b99ef53c093-0000
I1124 14:31:32.644278 30308 master.cpp:3395] Processing REVIVE call for framework 540f518e-1ba4-4f89-8b15-7b99ef53c093-0000 (default) at scheduler-0ff32e9c-a0bb-4f1c-a2e6-580485a123be@172.17.6.149:52680
I1124 14:31:32.644439 30308 hierarchical.cpp:886] Removed offer filters for framework 540f518e-1ba4-4f89-8b15-7b99ef53c093-0000
I1124 14:31:32.645293 30308 hierarchical.cpp:1159] No inverse offers to send out!
I1124 14:31:32.645335 30308 hierarchical.cpp:961] Performed allocation for 1 slaves in 872704ns
I1124 14:31:32.645678 30306 master.cpp:4998] Sending 1 offers to framework 540f518e-1ba4-4f89-8b15-7b99ef53c093-0000 (default) at scheduler-0ff32e9c-a0bb-4f1c-a2e6-580485a123be@172.17.6.149:52680
I1124 14:31:32.646204 30297 sched.cpp:811] Scheduler::resourceOffers took 109082ns
I1124 14:31:32.646776 30299 master.cpp:3498] Telling slave 540f518e-1ba4-4f89-8b15-7b99ef53c093-S0 at slave(219)@172.17.6.149:52680 (c264cb162c79) to kill task 981ca865-a5f9-42c6-8090-eaf909f78186 of framework 540f518e-1ba4-4f89-8b15-7b99ef53c093-0000 (default) at scheduler-0ff32e9c-a0bb-4f1c-a2e6-580485a123be@172.17.6.149:52680
I1124 14:31:32.646927 30294 slave.cpp:1822] Asked to kill task 981ca865-a5f9-42c6-8090-eaf909f78186 of framework 540f518e-1ba4-4f89-8b15-7b99ef53c093-0000
I1124 14:31:32.647976  3937 exec.cpp:315] Executor asked to kill task '981ca865-a5f9-42c6-8090-eaf909f78186'
I1124 14:31:32.648072  3937 exec.cpp:324] Executor::killTask took 61578ns
Shutting down
Sending SIGTERM to process tree at pid 3946
Killing the following process trees:
[ 
-+- 3946 sh -c sleep 1000 
 \--- 3947 sleep 1000 
]
Command terminated with signal Terminated (pid: 3946)
I1124 14:31:32.738593  3939 exec.cpp:517] Executor sending status update TASK_KILLED (UUID: 26f9ee98-2124-44d0-93f0-4537c828a375) for task 981ca865-a5f9-42c6-8090-eaf909f78186 of framework 540f518e-1ba4-4f89-8b15-7b99ef53c093-0000
I1124 14:31:32.739966 30306 slave.cpp:2763] Handling status update TASK_KILLED (UUID: 26f9ee98-2124-44d0-93f0-4537c828a375) for task 981ca865-a5f9-42c6-8090-eaf909f78186 of framework 540f518e-1ba4-4f89-8b15-7b99ef53c093-0000 from executor(1)@172.17.6.149:55307
I1124 14:31:32.740249 30306 slave.cpp:5342] Terminating task 981ca865-a5f9-42c6-8090-eaf909f78186
I1124 14:31:32.741641 30299 status_update_manager.cpp:320] Received status update TASK_KILLED (UUID: 26f9ee98-2124-44d0-93f0-4537c828a375) for task 981ca865-a5f9-42c6-8090-eaf909f78186 of framework 540f518e-1ba4-4f89-8b15-7b99ef53c093-0000
I1124 14:31:32.741881 30299 status_update_manager.cpp:374] Forwarding update TASK_KILLED (UUID: 26f9ee98-2124-44d0-93f0-4537c828a375) for task 981ca865-a5f9-42c6-8090-eaf909f78186 of framework 540f518e-1ba4-4f89-8b15-7b99ef53c093-0000 to the slave
I1124 14:31:32.742216 30297 slave.cpp:3115] Forwarding the update TASK_KILLED (UUID: 26f9ee98-2124-44d0-93f0-4537c828a375) for task 981ca865-a5f9-42c6-8090-eaf909f78186 of framework 540f518e-1ba4-4f89-8b15-7b99ef53c093-0000 to master@172.17.6.149:52680
I1124 14:31:32.742477 30297 slave.cpp:3009] Status update manager successfully handled status update TASK_KILLED (UUID: 26f9ee98-2124-44d0-93f0-4537c828a375) for task 981ca865-a5f9-42c6-8090-eaf909f78186 of framework 540f518e-1ba4-4f89-8b15-7b99ef53c093-0000
I1124 14:31:32.742528 30297 slave.cpp:3025] Sending acknowledgement for status update TASK_KILLED (UUID: 26f9ee98-2124-44d0-93f0-4537c828a375) for task 981ca865-a5f9-42c6-8090-eaf909f78186 of framework 540f518e-1ba4-4f89-8b15-7b99ef53c093-0000 to executor(1)@172.17.6.149:55307
I1124 14:31:32.742624 30299 master.cpp:4433] Status update TASK_KILLED (UUID: 26f9ee98-2124-44d0-93f0-4537c828a375) for task 981ca865-a5f9-42c6-8090-eaf909f78186 of framework 540f518e-1ba4-4f89-8b15-7b99ef53c093-0000 from slave 540f518e-1ba4-4f89-8b15-7b99ef53c093-S0 at slave(219)@172.17.6.149:52680 (c264cb162c79)
I1124 14:31:32.742676 30299 master.cpp:4481] Forwarding status update TASK_KILLED (UUID: 26f9ee98-2124-44d0-93f0-4537c828a375) for task 981ca865-a5f9-42c6-8090-eaf909f78186 of framework 540f518e-1ba4-4f89-8b15-7b99ef53c093-0000
I1124 14:31:32.742945 30299 master.cpp:6085] Updating the state of task 981ca865-a5f9-42c6-8090-eaf909f78186 of framework 540f518e-1ba4-4f89-8b15-7b99ef53c093-0000 (latest state: TASK_KILLED, status update state: TASK_KILLED)
I1124 14:31:32.743191 30297 sched.cpp:919] Scheduler::statusUpdate took 26231ns
I1124 14:31:32.743536 30297 hierarchical.cpp:793] Recovered cpus(role, test-principal):1; mem(role, test-principal):128 (total: cpus(*):1; mem(*):512; disk(*):1024; ports(*):[31000-32000]; cpus(role, test-principal):1; mem(role, test-principal):512, allocated: mem(role, test-principal):384; cpus(*):1; mem(*):512; disk(*):1024; ports(*):[31000-32000]) on slave 540f518e-1ba4-4f89-8b15-7b99ef53c093-S0 from framework 540f518e-1ba4-4f89-8b15-7b99ef53c093-0000
I1124 14:31:32.743746 30299 master.cpp:3590] Processing ACKNOWLEDGE call 26f9ee98-2124-44d0-93f0-4537c828a375 for task 981ca865-a5f9-42c6-8090-eaf909f78186 of framework 540f518e-1ba4-4f89-8b15-7b99ef53c093-0000 (default) at scheduler-0ff32e9c-a0bb-4f1c-a2e6-580485a123be@172.17.6.149:52680 on slave 540f518e-1ba4-4f89-8b15-7b99ef53c093-S0
I1124 14:31:32.743865 30299 master.cpp:6151] Removing task 981ca865-a5f9-42c6-8090-eaf909f78186 with resources cpus(role, test-principal):1; mem(role, test-principal):128 of framework 540f518e-1ba4-4f89-8b15-7b99ef53c093-0000 on slave 540f518e-1ba4-4f89-8b15-7b99ef53c093-S0 at slave(219)@172.17.6.149:52680 (c264cb162c79)
I1124 14:31:32.744144  3932 exec.cpp:341] Executor received status update acknowledgement 26f9ee98-2124-44d0-93f0-4537c828a375 for task 981ca865-a5f9-42c6-8090-eaf909f78186 of framework 540f518e-1ba4-4f89-8b15-7b99ef53c093-0000
I1124 14:31:32.744676 30299 status_update_manager.cpp:392] Received status update acknowledgement (UUID: 26f9ee98-2124-44d0-93f0-4537c828a375) for task 981ca865-a5f9-42c6-8090-eaf909f78186 of framework 540f518e-1ba4-4f89-8b15-7b99ef53c093-0000
I1124 14:31:32.745034 30299 status_update_manager.cpp:528] Cleaning up status update stream for task 981ca865-a5f9-42c6-8090-eaf909f78186 of framework 540f518e-1ba4-4f89-8b15-7b99ef53c093-0000
I1124 14:31:32.745612 30299 slave.cpp:2346] Status update manager successfully handled status update acknowledgement (UUID: 26f9ee98-2124-44d0-93f0-4537c828a375) for task 981ca865-a5f9-42c6-8090-eaf909f78186 of framework 540f518e-1ba4-4f89-8b15-7b99ef53c093-0000
I1124 14:31:32.745776 30299 slave.cpp:5383] Completing task 981ca865-a5f9-42c6-8090-eaf909f78186
I1124 14:31:32.746186 30295 process.cpp:3067] Handling HTTP event for process 'master' with path: '/master/unreserve'
I1124 14:31:32.746253 30295 http.cpp:336] HTTP POST for /master/unreserve from 172.17.6.149:46936
I1124 14:31:32.747879 30307 hierarchical.cpp:793] Recovered cpus(*):1; mem(*):512; disk(*):1024; ports(*):[31000-32000]; mem(role, test-principal):384 (total: cpus(*):1; mem(*):512; disk(*):1024; ports(*):[31000-32000]; cpus(role, test-principal):1; mem(role, test-principal):512, allocated: ) on slave 540f518e-1ba4-4f89-8b15-7b99ef53c093-S0 from framework 540f518e-1ba4-4f89-8b15-7b99ef53c093-0000
I1124 14:31:32.748006 30307 hierarchical.cpp:830] Framework 540f518e-1ba4-4f89-8b15-7b99ef53c093-0000 filtered slave 540f518e-1ba4-4f89-8b15-7b99ef53c093-S0 for 5secs
I1124 14:31:32.748070 30299 sched.cpp:837] Rescinded offer 540f518e-1ba4-4f89-8b15-7b99ef53c093-O1
I1124 14:31:32.748138 30299 sched.cpp:848] Scheduler::offerRescinded took 21855ns
I1124 14:31:32.749619 30305 master.cpp:6224] Sending checkpointed resources  to slave 540f518e-1ba4-4f89-8b15-7b99ef53c093-S0 at slave(219)@172.17.6.149:52680 (c264cb162c79)
I1124 14:31:32.750159 30305 slave.cpp:2275] Updated checkpointed resources from cpus(role, test-principal):1; mem(role, test-principal):512 to 
I1124 14:31:32.754276 30296 master.cpp:3395] Processing REVIVE call for framework 540f518e-1ba4-4f89-8b15-7b99ef53c093-0000 (default) at scheduler-0ff32e9c-a0bb-4f1c-a2e6-580485a123be@172.17.6.149:52680
I1124 14:31:32.754508 30305 hierarchical.cpp:886] Removed offer filters for framework 540f518e-1ba4-4f89-8b15-7b99ef53c093-0000
I1124 14:31:32.755517 30305 hierarchical.cpp:1159] No inverse offers to send out!
I1124 14:31:32.755565 30305 hierarchical.cpp:961] Performed allocation for 1 slaves in 1.0104ms
I1124 14:31:32.755949 30298 master.cpp:4998] Sending 1 offers to framework 540f518e-1ba4-4f89-8b15-7b99ef53c093-0000 (default) at scheduler-0ff32e9c-a0bb-4f1c-a2e6-580485a123be@172.17.6.149:52680
I1124 14:31:32.756631 30298 sched.cpp:811] Scheduler::resourceOffers took 128650ns
I1124 14:31:32.757311 30274 sched.cpp:1803] Asked to stop the driver
I1124 14:31:32.757488 30303 sched.cpp:1041] Stopping framework '540f518e-1ba4-4f89-8b15-7b99ef53c093-0000'
I1124 14:31:32.757752 30295 master.cpp:5566] Processing TEARDOWN call for framework 540f518e-1ba4-4f89-8b15-7b99ef53c093-0000 (default) at scheduler-0ff32e9c-a0bb-4f1c-a2e6-580485a123be@172.17.6.149:52680
I1124 14:31:32.757786 30295 master.cpp:5578] Removing framework 540f518e-1ba4-4f89-8b15-7b99ef53c093-0000 (default) at scheduler-0ff32e9c-a0bb-4f1c-a2e6-580485a123be@172.17.6.149:52680
I1124 14:31:32.758023 30294 hierarchical.cpp:303] Deactivated framework 540f518e-1ba4-4f89-8b15-7b99ef53c093-0000
I1124 14:31:32.758117 30296 slave.cpp:2010] Asked to shut down framework 540f518e-1ba4-4f89-8b15-7b99ef53c093-0000 by master@172.17.6.149:52680
I1124 14:31:32.758167 30296 slave.cpp:2035] Shutting down framework 540f518e-1ba4-4f89-8b15-7b99ef53c093-0000
I1124 14:31:32.758249 30296 slave.cpp:3891] Shutting down executor '981ca865-a5f9-42c6-8090-eaf909f78186' of framework 540f518e-1ba4-4f89-8b15-7b99ef53c093-0000 at executor(1)@172.17.6.149:55307
I1124 14:31:32.758635 30302 hierarchical.cpp:793] Recovered cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (total: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000], allocated: ) on slave 540f518e-1ba4-4f89-8b15-7b99ef53c093-S0 from framework 540f518e-1ba4-4f89-8b15-7b99ef53c093-0000
I1124 14:31:32.758935 30302 hierarchical.cpp:260] Removed framework 540f518e-1ba4-4f89-8b15-7b99ef53c093-0000
I1124 14:31:32.759083 30304 master.cpp:926] Master terminating
I1124 14:31:32.759553 30303 hierarchical.cpp:410] Removed slave 540f518e-1ba4-4f89-8b15-7b99ef53c093-S0
I1124 14:31:32.759668  3938 exec.cpp:381] Executor asked to shutdown
I1124 14:31:32.759775  3938 exec.cpp:396] Executor::shutdown took 14378ns
I1124 14:31:32.759929  3938 exec.cpp:80] Scheduling shutdown of the executor
I1124 14:31:32.760166 30304 slave.cpp:3243] master@172.17.6.149:52680 exited
W1124 14:31:32.760193 30304 slave.cpp:3246] Master disconnected! Waiting for a new master to be elected
I1124 14:31:32.764683 30297 containerizer.cpp:1073] Destroying container '8544eb55-baa8-495a-98e7-565ec58da673'
I1124 14:31:32.774528 30299 containerizer.cpp:1256] Executor for container '8544eb55-baa8-495a-98e7-565ec58da673' has exited
I1124 14:31:32.776396 30296 slave.cpp:3581] Executor '981ca865-a5f9-42c6-8090-eaf909f78186' of framework 540f518e-1ba4-4f89-8b15-7b99ef53c093-0000 terminated with signal Killed
I1124 14:31:32.776568 30296 slave.cpp:3685] Cleaning up executor '981ca865-a5f9-42c6-8090-eaf909f78186' of framework 540f518e-1ba4-4f89-8b15-7b99ef53c093-0000 at executor(1)@172.17.6.149:55307
I1124 14:31:32.776974 30298 gc.cpp:54] Scheduling '/tmp/ReservationEndpointsTest_UnreserveAvailableAndOfferedResources_N4vdsa/slaves/540f518e-1ba4-4f89-8b15-7b99ef53c093-S0/frameworks/540f518e-1ba4-4f89-8b15-7b99ef53c093-0000/executors/981ca865-a5f9-42c6-8090-eaf909f78186/runs/8544eb55-baa8-495a-98e7-565ec58da673' for gc 6.99999100884444days in the future
I1124 14:31:32.777122 30296 slave.cpp:3773] Cleaning up framework 540f518e-1ba4-4f89-8b15-7b99ef53c093-0000
I1124 14:31:32.777163 30298 gc.cpp:54] Scheduling '/tmp/ReservationEndpointsTest_UnreserveAvailableAndOfferedResources_N4vdsa/slaves/540f518e-1ba4-4f89-8b15-7b99ef53c093-S0/frameworks/540f518e-1ba4-4f89-8b15-7b99ef53c093-0000/executors/981ca865-a5f9-42c6-8090-eaf909f78186' for gc 6.99999100695704days in the future
I1124 14:31:32.777220 30301 status_update_manager.cpp:282] Closing status update streams for framework 540f518e-1ba4-4f89-8b15-7b99ef53c093-0000
I1124 14:31:32.777307 30296 slave.cpp:599] Slave terminating
I1124 14:31:32.777298 30298 gc.cpp:54] Scheduling '/tmp/ReservationEndpointsTest_UnreserveAvailableAndOfferedResources_N4vdsa/slaves/540f518e-1ba4-4f89-8b15-7b99ef53c093-S0/frameworks/540f518e-1ba4-4f89-8b15-7b99ef53c093-0000' for gc 6.99999100426074days in the future
[       OK ] ReservationEndpointsTest.UnreserveAvailableAndOfferedResources (409 ms)
{code}
",Bug,Major,anandmazumdar,2015-12-03T08:54:41.000+0000,5,Resolved,Complete,ReservationEndpointsTest.UnreserveAvailableAndOfferedResources is flaky,2016-03-03T17:05:42.000+0000,MESOS-4002,1.0,mesos,Mesosphere Sprint 23
greggomann,2015-11-23T19:50:53.000+0000,neilc,"Current rules around this are pretty confusing and undocumented, as evidenced by some recent bugs in this area.

Some example snippets in the mesos source code that were a result of this confusion and are indeed bugs:

1. https://github.com/apache/mesos/blob/master/src/slave/containerizer/mesos/provisioner/docker/registry_client.cpp#L754
{code}
return doHttpGet(blobURL, None(), true, true, None())
    .then([this, blobURLPath, digest, filePath](
        const http::Response& response) -> Future<size_t> {
      Try<int> fd = os::open(
          filePath.value,
          O_WRONLY | O_CREAT | O_TRUNC | O_CLOEXEC,
          S_IRUSR | S_IWUSR | S_IRGRP | S_IROTH);
{code}
",Documentation,Minor,neilc,2015-12-21T02:13:39.000+0000,5,Resolved,Complete,"libprocess: document when, why defer() is necessary",2015-12-21T02:13:40.000+0000,MESOS-3996,1.0,mesos,Mesosphere Sprint 24
gilbert,2015-11-23T19:09:52.000+0000,gilbert,"We should get rid of all JSON and struct for message passing as function returned type. By using the methods provided by spec.hpp to refactor all unnecessary JSON message and struct in registry client and registry puller. Also, remove all redundant check in registry client that are already checked by spec validation. ",Improvement,Major,gilbert,2015-11-25T00:02:24.000+0000,5,Resolved,Complete,Refactor registry client/puller to avoid JSON and struct.,2015-11-25T00:02:24.000+0000,MESOS-3994,3.0,mesos,Mesosphere Sprint 23
js84,2015-11-23T10:26:49.000+0000,alexr,The allocator recover() call was introduced for correct recovery in presence of quota. We should add test verifying the correct behavior.,Task,Major,alexr,,10006,Reviewable,New,Tests for allocator recovery.,2016-02-16T23:56:31.000+0000,MESOS-3986,5.0,mesos,Mesosphere Sprint 28
js84,2015-11-23T09:59:41.000+0000,alexr,"Tests should include:
* JSON validation;
* Absence of irrelevant fields;
* Semantic validation.",Task,Major,alexr,2015-11-25T20:52:42.000+0000,5,Resolved,Complete,Tests for quota request validation,2015-11-25T20:52:42.000+0000,MESOS-3983,3.0,mesos,Mesosphere Sprint 23
alexr,2015-11-23T09:48:47.000+0000,alexr,The built-in Hierarchical allocator should implement the recovery (in the presence of quota).,Task,Major,alexr,2015-12-04T02:02:59.000+0000,5,Resolved,Complete,Implement recovery in the Hierarchical allocator,2015-12-04T16:44:22.000+0000,MESOS-3981,3.0,mesos,Mesosphere Sprint 23
alexr,2015-11-23T00:24:28.000+0000,alexr,"After introduction of C++ wrapper `Quota` for `QuotaInfo`, all allocator methods using `QuotaInfo` should be updated.",Improvement,Major,alexr,2016-01-15T23:00:40.000+0000,5,Resolved,Complete,Replace `QuotaInfo` with `Quota` in allocator interface,2016-01-15T23:00:40.000+0000,MESOS-3979,3.0,mesos,Mesosphere Sprint 26
anandmazumdar,2015-11-20T20:01:15.000+0000,kaysoky,"The C++ HTTP scheduler library does not work against Mesos when SSL is enabled (without downgrade).

The fix should be simple:
* The library should detect if SSL is enabled.
* If SSL is enabled, connections should be made with HTTPS instead of HTTP.",Bug,Major,kaysoky,2016-03-30T18:14:53.000+0000,5,Resolved,Complete,C++ HTTP Scheduler Library does not work with SSL enabled,2016-03-30T18:14:53.000+0000,MESOS-3976,3.0,mesos,Mesosphere Sprint 32
kaysoky,2015-11-20T19:23:33.000+0000,tillt,"When running the tests of an SSL build of Mesos on CentOS 7.1, I see spurious test failures that are, so far, not reproducible.

The following tests did fail for me in complete runs but did seem fine when running them individually, in repetition.  

{noformat}
DockerTest.ROOT_DOCKER_CheckPortResource
{noformat}

{noformat}
ContainerizerTest.ROOT_CGROUPS_BalloonFramework
{noformat}

{noformat}
[ RUN      ] LinuxFilesystemIsolatorTest.ROOT_ChangeRootFilesystemCommandExecutor
2015-11-20 19:08:38,826:21380(0x7fa10d5f2700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:53444] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
+ /home/vagrant/mesos/build/src/mesos-containerizer mount --help=false --operation=make-rslave --path=/
+ grep -E /tmp/LinuxFilesystemIsolatorTest_ROOT_ChangeRootFilesystemCommandExecutor_Tz7P8c/.+ /proc/self/mountinfo
+ grep -v 2b98025c-74f1-41d2-b35a-ce2cdfae347e
+ cut '-d ' -f5
+ xargs --no-run-if-empty umount -l
+ mount -n --rbind /tmp/LinuxFilesystemIsolatorTest_ROOT_ChangeRootFilesystemCommandExecutor_Tz7P8c/provisioner/containers/2b98025c-74f1-41d2-b35a-ce2cdfae347e/backends/copy/rootfses/bed11080-474b-4c69-8e7f-0ab85e895b0d /tmp/LinuxFilesystemIsolatorTest_ROOT_ChangeRootFilesystemCommandExecutor_Tz7P8c/slaves/830e842e-c36a-4e4c-bff4-5b9568d7df12-S0/frameworks/830e842e-c36a-4e4c-bff4-5b9568d7df12-0000/executors/c735be54-c47f-4645-bfc1-2f4647e2cddb/runs/2b98025c-74f1-41d2-b35a-ce2cdfae347e/.rootfs
Could not load cert file
../../src/tests/containerizer/filesystem_isolator_tests.cpp:354: Failure
Value of: statusRunning.get().state()
  Actual: TASK_FAILED
Expected: TASK_RUNNING
2015-11-20 19:08:42,164:21380(0x7fa10d5f2700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:53444] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2015-11-20 19:08:45,501:21380(0x7fa10d5f2700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:53444] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2015-11-20 19:08:48,837:21380(0x7fa10d5f2700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:53444] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2015-11-20 19:08:52,174:21380(0x7fa10d5f2700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:53444] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
../../src/tests/containerizer/filesystem_isolator_tests.cpp:355: Failure
Failed to wait 15secs for statusFinished
../../src/tests/containerizer/filesystem_isolator_tests.cpp:349: Failure
Actual function call count doesn't match EXPECT_CALL(sched, statusUpdate(&driver, _))...
         Expected: to be called twice
           Actual: called once - unsatisfied and active
2015-11-20 19:08:55,511:21380(0x7fa10d5f2700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:53444] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
*** Aborted at 1448046536 (unix time) try ""date -d @1448046536"" if you are using GNU date ***
PC: @                0x0 (unknown)
*** SIGSEGV (@0x0) received by PID 21380 (TID 0x7fa1549e68c0) from PID 0; stack trace: ***
    @     0x7fa141796fbb (unknown)
    @     0x7fa14179b341 (unknown)
    @     0x7fa14f096130 (unknown)
{noformat}


Vagrantfile generator:
{noformat}
cat << EOF > Vagrantfile
# -*- mode: ruby -*-"" >
# vi: set ft=ruby :
Vagrant.configure(2) do |config|
  # Disable shared folder to prevent certain kernel module dependencies.
  config.vm.synced_folder ""."", ""/vagrant"", disabled: true

  config.vm.hostname = ""centos71""

  config.vm.box = ""bento/centos-7.1""

  config.vm.provider ""virtualbox"" do |vb|
    vb.memory = 16384
    vb.cpus = 8
  end

  config.vm.provider ""vmware_fusion"" do |vb|
    vb.memory = 9216
    vb.cpus = 4
  end

  config.vm.provision ""shell"", inline: <<-SHELL

     sudo yum -y update systemd

     sudo yum install -y tar wget
     sudo wget http://repos.fedorapeople.org/repos/dchen/apache-maven/epel-apache-maven.repo -O /etc/yum.repos.d/epel-apache-maven.repo

     sudo yum groupinstall -y ""Development Tools""
     sudo yum install -y apache-maven python-devel java-1.7.0-openjdk-devel zlib-devel libcurl-devel openssl-devel cyrus-sasl-devel cyrus-sasl-md5 apr-devel subversion-devel apr-util-devel

     sudo yum install libevent-devel

     sudo yum install -y git

     sudo yum install -y docker
     sudo service docker start
     sudo docker info

     #sudo wget -qO- https://get.docker.com/ | sh

  SHELL
end
EOF

vagrant up
vagrant reload

vagrant ssh -c ""
git clone  https://github.com/apache/mesos.git mesos
cd mesos
git checkout -b 0.26.0-rc1 0.26.0-rc1

./bootstrap
mkdir build
cd build

../configure --enable-libevent --enable-ssl
GTEST_FILTER="""" make check
sudo ./bin/mesos-tests.sh
""
{noformat}",Bug,Major,tillt,2015-11-30T20:30:41.000+0000,5,Resolved,Complete,SSL build of mesos causes flaky testsuite.,2015-11-30T20:31:03.000+0000,MESOS-3975,5.0,mesos,Mesosphere Sprint 23
gilbert,2015-11-20T16:56:12.000+0000,bernd-mesos,"Non-root 'make distcheck.

{noformat}
...
[----------] Global test environment tear-down
[==========] 826 tests from 113 test cases ran. (276624 ms total)
[  PASSED  ] 826 tests.

  YOU HAVE 6 DISABLED TESTS

Making install in .
make[3]: Nothing to be done for `install-exec-am'.
 ../install-sh -c -d '/Users/bernd/mesos/mesos/build/mesos-0.26.0/_inst/lib/pkgconfig'
 /usr/bin/install -c -m 644 mesos.pc '/Users/bernd/mesos/mesos/build/mesos-0.26.0/_inst/lib/pkgconfig'
Making install in 3rdparty
/Applications/Xcode.app/Contents/Developer/usr/bin/make  install-recursive
Making install in libprocess
Making install in 3rdparty
/Applications/Xcode.app/Contents/Developer/usr/bin/make  install-recursive
Making install in stout
Making install in .
make[9]: Nothing to be done for `install-exec-am'.
make[9]: Nothing to be done for `install-data-am'.
Making install in include
make[9]: Nothing to be done for `install-exec-am'.
 ../../../../../../3rdparty/libprocess/3rdparty/stout/install-sh -c -d '/Users/bernd/mesos/mesos/build/mesos-0.26.0/_inst/include'
 ../../../../../../3rdparty/libprocess/3rdparty/stout/install-sh -c -d '/Users/bernd/mesos/mesos/build/mesos-0.26.0/_inst/include/stout'
 /usr/bin/install -c -m 644  ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/abort.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/attributes.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/base64.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/bits.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/bytes.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/cache.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/check.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/duration.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/dynamiclibrary.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/error.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/exit.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/flags.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/foreach.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/format.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/fs.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/gtest.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/gzip.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/hashmap.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/hashset.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/interval.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/ip.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/json.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/lambda.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/linkedhashmap.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/list.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/mac.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/multihashmap.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/multimap.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/net.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/none.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/nothing.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/numify.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/option.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/os.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/path.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/preprocessor.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/proc.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/protobuf.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/recordio.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/result.hpp '/Users/bernd/mesos/mesos/build/mesos-0.26.0/_inst/include/stout'
 ../../../../../../3rdparty/libprocess/3rdparty/stout/install-sh -c -d '/Users/bernd/mesos/mesos/build/mesos-0.26.0/_inst/include/stout/os'
 /usr/bin/install -c -m 644  ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/os/bootid.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/os/chdir.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/os/close.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/os/constants.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/os/environment.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/os/exists.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/os/fcntl.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/os/fork.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/os/ftruncate.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/os/getcwd.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/os/killtree.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/os/linux.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/os/ls.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/os/mkdir.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/os/mktemp.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/os/open.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/os/os.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/os/osx.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/os/permissions.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/os/process.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/os/pstree.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/os/read.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/os/realpath.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/os/rename.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/os/rm.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/os/sendfile.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/os/shell.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/os/signals.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/os/stat.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/os/sysctl.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/os/touch.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/os/utime.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/os/write.hpp '/Users/bernd/mesos/mesos/build/mesos-0.26.0/_inst/include/stout/os'
 ../../../../../../3rdparty/libprocess/3rdparty/stout/install-sh -c -d '/Users/bernd/mesos/mesos/build/mesos-0.26.0/_inst/include/stout/posix'
 /usr/bin/install -c -m 644  ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/posix/gzip.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/posix/os.hpp '/Users/bernd/mesos/mesos/build/mesos-0.26.0/_inst/include/stout/posix'
 ../../../../../../3rdparty/libprocess/3rdparty/stout/install-sh -c -d '/Users/bernd/mesos/mesos/build/mesos-0.26.0/_inst/include/stout/flags'
 /usr/bin/install -c -m 644  ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/flags/fetch.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/flags/flag.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/flags/flags.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/flags/parse.hpp '/Users/bernd/mesos/mesos/build/mesos-0.26.0/_inst/include/stout/flags'
 ../../../../../../3rdparty/libprocess/3rdparty/stout/install-sh -c -d '/Users/bernd/mesos/mesos/build/mesos-0.26.0/_inst/include/stout/tests'
 /usr/bin/install -c -m 644  ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/tests/utils.hpp '/Users/bernd/mesos/mesos/build/mesos-0.26.0/_inst/include/stout/tests'
 ../../../../../../3rdparty/libprocess/3rdparty/stout/install-sh -c -d '/Users/bernd/mesos/mesos/build/mesos-0.26.0/_inst/include/stout/os/windows'
 /usr/bin/install -c -m 644  ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/os/windows/bootid.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/os/windows/exists.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/os/windows/fcntl.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/os/windows/fork.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/os/windows/ftruncate.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/os/windows/killtree.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/os/windows/ls.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/os/windows/process.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/os/windows/pstree.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/os/windows/sendfile.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/os/windows/shell.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/os/windows/signals.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/os/windows/stat.hpp '/Users/bernd/mesos/mesos/build/mesos-0.26.0/_inst/include/stout/os/windows'
 ../../../../../../3rdparty/libprocess/3rdparty/stout/install-sh -c -d '/Users/bernd/mesos/mesos/build/mesos-0.26.0/_inst/include/stout/os/posix'
 /usr/bin/install -c -m 644  ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/os/posix/bootid.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/os/posix/exists.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/os/posix/fcntl.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/os/posix/fork.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/os/posix/ftruncate.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/os/posix/killtree.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/os/posix/ls.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/os/posix/process.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/os/posix/pstree.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/os/posix/sendfile.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/os/posix/shell.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/os/posix/signals.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/os/posix/stat.hpp '/Users/bernd/mesos/mesos/build/mesos-0.26.0/_inst/include/stout/os/posix'
 ../../../../../../3rdparty/libprocess/3rdparty/stout/install-sh -c -d '/Users/bernd/mesos/mesos/build/mesos-0.26.0/_inst/include/stout'
 /usr/bin/install -c -m 644  ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/set.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/some.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/stopwatch.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/stringify.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/strings.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/subcommand.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/svn.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/synchronized.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/thread_local.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/try.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/unimplemented.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/unreachable.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/utils.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/uuid.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/version.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/windows.hpp '/Users/bernd/mesos/mesos/build/mesos-0.26.0/_inst/include/stout'
 ../../../../../../3rdparty/libprocess/3rdparty/stout/install-sh -c -d '/Users/bernd/mesos/mesos/build/mesos-0.26.0/_inst/include/stout/windows'
 /usr/bin/install -c -m 644  ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/windows/format.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/windows/gzip.hpp ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/windows/os.hpp '/Users/bernd/mesos/mesos/build/mesos-0.26.0/_inst/include/stout/windows'
 ../../../../../../3rdparty/libprocess/3rdparty/stout/install-sh -c -d '/Users/bernd/mesos/mesos/build/mesos-0.26.0/_inst/include/stout/os/raw'
 /usr/bin/install -c -m 644  ../../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/os/raw/environment.hpp '/Users/bernd/mesos/mesos/build/mesos-0.26.0/_inst/include/stout/os/raw'
make[8]: Nothing to be done for `install-exec-am'.
make[8]: Nothing to be done for `install-data-am'.
Making install in .
make[6]: Nothing to be done for `install-exec-am'.
make[6]: Nothing to be done for `install-data-am'.
Making install in include
make[6]: Nothing to be done for `install-exec-am'.
 ../../../../3rdparty/libprocess/install-sh -c -d '/Users/bernd/mesos/mesos/build/mesos-0.26.0/_inst/include'
 ../../../../3rdparty/libprocess/install-sh -c -d '/Users/bernd/mesos/mesos/build/mesos-0.26.0/_inst/include/process'
 /usr/bin/install -c -m 644  ../../../../3rdparty/libprocess/include/process/address.hpp ../../../../3rdparty/libprocess/include/process/async.hpp ../../../../3rdparty/libprocess/include/process/check.hpp ../../../../3rdparty/libprocess/include/process/clock.hpp ../../../../3rdparty/libprocess/include/process/collect.hpp ../../../../3rdparty/libprocess/include/process/defer.hpp ../../../../3rdparty/libprocess/include/process/deferred.hpp ../../../../3rdparty/libprocess/include/process/delay.hpp ../../../../3rdparty/libprocess/include/process/dispatch.hpp ../../../../3rdparty/libprocess/include/process/event.hpp ../../../../3rdparty/libprocess/include/process/executor.hpp ../../../../3rdparty/libprocess/include/process/filter.hpp ../../../../3rdparty/libprocess/include/process/firewall.hpp ../../../../3rdparty/libprocess/include/process/future.hpp ../../../../3rdparty/libprocess/include/process/gc.hpp ../../../../3rdparty/libprocess/include/process/gmock.hpp ../../../../3rdparty/libprocess/include/process/gtest.hpp ../../../../3rdparty/libprocess/include/process/help.hpp ../../../../3rdparty/libprocess/include/process/http.hpp ../../../../3rdparty/libprocess/include/process/id.hpp ../../../../3rdparty/libprocess/include/process/io.hpp ../../../../3rdparty/libprocess/include/process/latch.hpp ../../../../3rdparty/libprocess/include/process/limiter.hpp ../../../../3rdparty/libprocess/include/process/logging.hpp ../../../../3rdparty/libprocess/include/process/message.hpp ../../../../3rdparty/libprocess/include/process/mime.hpp ../../../../3rdparty/libprocess/include/process/mutex.hpp ../../../../3rdparty/libprocess/include/process/network.hpp ../../../../3rdparty/libprocess/include/process/once.hpp ../../../../3rdparty/libprocess/include/process/owned.hpp ../../../../3rdparty/libprocess/include/process/pid.hpp ../../../../3rdparty/libprocess/include/process/process.hpp ../../../../3rdparty/libprocess/include/process/profiler.hpp ../../../../3rdparty/libprocess/include/process/protobuf.hpp ../../../../3rdparty/libprocess/include/process/queue.hpp ../../../../3rdparty/libprocess/include/process/reap.hpp ../../../../3rdparty/libprocess/include/process/run.hpp ../../../../3rdparty/libprocess/include/process/sequence.hpp ../../../../3rdparty/libprocess/include/process/shared.hpp ../../../../3rdparty/libprocess/include/process/socket.hpp '/Users/bernd/mesos/mesos/build/mesos-0.26.0/_inst/include/process'
 ../../../../3rdparty/libprocess/install-sh -c -d '/Users/bernd/mesos/mesos/build/mesos-0.26.0/_inst/include/process'
 /usr/bin/install -c -m 644  ../../../../3rdparty/libprocess/include/process/statistics.hpp ../../../../3rdparty/libprocess/include/process/system.hpp ../../../../3rdparty/libprocess/include/process/subprocess.hpp ../../../../3rdparty/libprocess/include/process/time.hpp ../../../../3rdparty/libprocess/include/process/timeout.hpp ../../../../3rdparty/libprocess/include/process/timer.hpp ../../../../3rdparty/libprocess/include/process/timeseries.hpp '/Users/bernd/mesos/mesos/build/mesos-0.26.0/_inst/include/process'
 ../../../../3rdparty/libprocess/install-sh -c -d '/Users/bernd/mesos/mesos/build/mesos-0.26.0/_inst/include/process/ssl'
 /usr/bin/install -c -m 644  ../../../../3rdparty/libprocess/include/process/ssl/gtest.hpp ../../../../3rdparty/libprocess/include/process/ssl/utilities.hpp '/Users/bernd/mesos/mesos/build/mesos-0.26.0/_inst/include/process/ssl'
 ../../../../3rdparty/libprocess/install-sh -c -d '/Users/bernd/mesos/mesos/build/mesos-0.26.0/_inst/include/process/metrics'
 /usr/bin/install -c -m 644  ../../../../3rdparty/libprocess/include/process/metrics/counter.hpp ../../../../3rdparty/libprocess/include/process/metrics/gauge.hpp ../../../../3rdparty/libprocess/include/process/metrics/metric.hpp ../../../../3rdparty/libprocess/include/process/metrics/metrics.hpp ../../../../3rdparty/libprocess/include/process/metrics/timer.hpp '/Users/bernd/mesos/mesos/build/mesos-0.26.0/_inst/include/process/metrics'
make[5]: Nothing to be done for `install-exec-am'.
make[5]: Nothing to be done for `install-data-am'.
Making install in src
/Applications/Xcode.app/Contents/Developer/usr/bin/make  install-am
test ""../.."" = "".."" ||			\
		(../../install-sh -c -d python/cli/src/mesos && cp -pf ../../src/python/cli/src/mesos/__init__.py python/cli/src/mesos/__init__.py)
test ""../.."" = "".."" ||			\
		(../../install-sh -c -d python/cli/src/mesos && cp -pf ../../src/python/cli/src/mesos/cli.py python/cli/src/mesos/cli.py)
test ""../.."" = "".."" ||			\
		(../../install-sh -c -d python/cli/src/mesos && cp -pf ../../src/python/cli/src/mesos/futures.py python/cli/src/mesos/futures.py)
test ""../.."" = "".."" ||			\
		(../../install-sh -c -d python/cli/src/mesos && cp -pf ../../src/python/cli/src/mesos/http.py python/cli/src/mesos/http.py)
test ""../.."" = "".."" ||			\
		(../../install-sh -c -d python/interface/src/mesos && cp -pf ../../src/python/interface/src/mesos/__init__.py python/interface/src/mesos/__init__.py)
test ""../.."" = "".."" ||			\
		(../../install-sh -c -d python/interface/src/mesos/interface && cp -pf ../../src/python/interface/src/mesos/interface/__init__.py python/interface/src/mesos/interface/__init__.py)
test ""../.."" = "".."" ||			\
		(../../install-sh -c -d python/interface/src/mesos/v1 && cp -pf ../../src/python/interface/src/mesos/v1/__init__.py python/interface/src/mesos/v1/__init__.py)
test ""../.."" = "".."" ||			\
		(../../install-sh -c -d python/interface/src/mesos/v1/interface && cp -pf ../../src/python/interface/src/mesos/v1/interface/__init__.py python/interface/src/mesos/v1/interface/__init__.py)
test ""../.."" = "".."" ||			\
		(../../install-sh -c -d python/native/src/mesos && cp -pf ../../src/python/native/src/mesos/__init__.py python/native/src/mesos/__init__.py)
test ""../.."" = "".."" ||			\
		(../../install-sh -c -d python/native/src/mesos/native && cp -pf ../../src/python/native/src/mesos/native/__init__.py python/native/src/mesos/native/__init__.py)
test ""../.."" = "".."" ||			\
		(../../install-sh -c -d python/native/src/mesos/native && cp -pf ../../src/python/native/src/mesos/native/mesos_executor_driver_impl.cpp python/native/src/mesos/native/mesos_executor_driver_impl.cpp)
test ""../.."" = "".."" ||			\
		(../../install-sh -c -d python/native/src/mesos/native && cp -pf ../../src/python/native/src/mesos/native/mesos_executor_driver_impl.hpp python/native/src/mesos/native/mesos_executor_driver_impl.hpp)
test ""../.."" = "".."" ||			\
		(../../install-sh -c -d python/native/src/mesos/native && cp -pf ../../src/python/native/src/mesos/native/mesos_scheduler_driver_impl.cpp python/native/src/mesos/native/mesos_scheduler_driver_impl.cpp)
test ""../.."" = "".."" ||			\
		(../../install-sh -c -d python/native/src/mesos/native && cp -pf ../../src/python/native/src/mesos/native/mesos_scheduler_driver_impl.hpp python/native/src/mesos/native/mesos_scheduler_driver_impl.hpp)
test ""../.."" = "".."" ||			\
		(../../install-sh -c -d python/native/src/mesos/native && cp -pf ../../src/python/native/src/mesos/native/module.cpp python/native/src/mesos/native/module.cpp)
test ""../.."" = "".."" ||			\
		(../../install-sh -c -d python/native/src/mesos/native && cp -pf ../../src/python/native/src/mesos/native/module.hpp python/native/src/mesos/native/module.hpp)
test ""../.."" = "".."" ||			\
		(../../install-sh -c -d python/native/src/mesos/native && cp -pf ../../src/python/native/src/mesos/native/proxy_executor.cpp python/native/src/mesos/native/proxy_executor.cpp)
test ""../.."" = "".."" ||			\
		(../../install-sh -c -d python/native/src/mesos/native && cp -pf ../../src/python/native/src/mesos/native/proxy_executor.hpp python/native/src/mesos/native/proxy_executor.hpp)
test ""../.."" = "".."" ||			\
		(../../install-sh -c -d python/native/src/mesos/native && cp -pf ../../src/python/native/src/mesos/native/proxy_scheduler.cpp python/native/src/mesos/native/proxy_scheduler.cpp)
test ""../.."" = "".."" ||			\
		(../../install-sh -c -d python/native/src/mesos/native && cp -pf ../../src/python/native/src/mesos/native/proxy_scheduler.hpp python/native/src/mesos/native/proxy_scheduler.hpp)
test ""../.."" = "".."" ||			\
		(../../install-sh -c -d python/src/mesos && cp -pf ../../src/python/src/mesos/__init__.py python/src/mesos/__init__.py)
running bdist_egg
running egg_info
writing requirements to src/mesos.egg-info/requires.txt
writing src/mesos.egg-info/PKG-INFO
writing top-level names to src/mesos.egg-info/top_level.txt
writing dependency_links to src/mesos.egg-info/dependency_links.txt
writing requirements to src/mesos.egg-info/requires.txt
writing src/mesos.egg-info/PKG-INFO
writing top-level names to src/mesos.egg-info/top_level.txt
writing dependency_links to src/mesos.egg-info/dependency_links.txt
reading manifest file 'src/mesos.egg-info/SOURCES.txt'
writing manifest file 'src/mesos.egg-info/SOURCES.txt'
installing library code to build/bdist.macosx-10.10-intel/egg
running install_lib
running build_py
creating build/bdist.macosx-10.10-intel/egg
creating build/bdist.macosx-10.10-intel/egg/mesos
copying build/lib/mesos/__init__.py -> build/bdist.macosx-10.10-intel/egg/mesos
byte-compiling build/bdist.macosx-10.10-intel/egg/mesos/__init__.py to __init__.pyc
creating build/bdist.macosx-10.10-intel/egg/EGG-INFO
copying src/mesos.egg-info/PKG-INFO -> build/bdist.macosx-10.10-intel/egg/EGG-INFO
copying src/mesos.egg-info/SOURCES.txt -> build/bdist.macosx-10.10-intel/egg/EGG-INFO
copying src/mesos.egg-info/dependency_links.txt -> build/bdist.macosx-10.10-intel/egg/EGG-INFO
copying src/mesos.egg-info/requires.txt -> build/bdist.macosx-10.10-intel/egg/EGG-INFO
copying src/mesos.egg-info/top_level.txt -> build/bdist.macosx-10.10-intel/egg/EGG-INFO
zip_safe flag not set; analyzing archive contents...
mesos.__init__: module references __path__
creating '/Users/bernd/mesos/mesos/build/mesos-0.26.0/_build/src/python/dist/mesos-0.26.0-py2.7.egg' and adding 'build/bdist.macosx-10.10-intel/egg' to it
removing 'build/bdist.macosx-10.10-intel/egg' (and everything under it)
running bdist_wheel
running build
installing to build/bdist.macosx-10.10-intel/wheel
running install
running install_lib
creating build/bdist.macosx-10.10-intel/wheel
creating build/bdist.macosx-10.10-intel/wheel/mesos
copying build/lib/mesos/__init__.py -> build/bdist.macosx-10.10-intel/wheel/mesos
running install_egg_info
Copying src/mesos.egg-info to build/bdist.macosx-10.10-intel/wheel/mesos-0.26.0-py2.7.egg-info
running install_scripts
creating build/bdist.macosx-10.10-intel/wheel/mesos-0.26.0.dist-info/WHEEL
running bdist_egg
running egg_info
writing src/mesos.cli.egg-info/PKG-INFO
writing namespace_packages to src/mesos.cli.egg-info/namespace_packages.txt
writing top-level names to src/mesos.cli.egg-info/top_level.txt
writing dependency_links to src/mesos.cli.egg-info/dependency_links.txt
writing src/mesos.cli.egg-info/PKG-INFO
writing namespace_packages to src/mesos.cli.egg-info/namespace_packages.txt
writing top-level names to src/mesos.cli.egg-info/top_level.txt
writing dependency_links to src/mesos.cli.egg-info/dependency_links.txt
reading manifest file 'src/mesos.cli.egg-info/SOURCES.txt'
writing manifest file 'src/mesos.cli.egg-info/SOURCES.txt'
installing library code to build/bdist.macosx-10.10-intel/egg
running install_lib
running build_py
creating build/bdist.macosx-10.10-intel/egg
creating build/bdist.macosx-10.10-intel/egg/mesos
copying build/lib/mesos/__init__.py -> build/bdist.macosx-10.10-intel/egg/mesos
copying build/lib/mesos/cli.py -> build/bdist.macosx-10.10-intel/egg/mesos
copying build/lib/mesos/futures.py -> build/bdist.macosx-10.10-intel/egg/mesos
copying build/lib/mesos/http.py -> build/bdist.macosx-10.10-intel/egg/mesos
byte-compiling build/bdist.macosx-10.10-intel/egg/mesos/__init__.py to __init__.pyc
byte-compiling build/bdist.macosx-10.10-intel/egg/mesos/cli.py to cli.pyc
byte-compiling build/bdist.macosx-10.10-intel/egg/mesos/futures.py to futures.pyc
byte-compiling build/bdist.macosx-10.10-intel/egg/mesos/http.py to http.pyc
creating build/bdist.macosx-10.10-intel/egg/EGG-INFO
copying src/mesos.cli.egg-info/PKG-INFO -> build/bdist.macosx-10.10-intel/egg/EGG-INFO
copying src/mesos.cli.egg-info/SOURCES.txt -> build/bdist.macosx-10.10-intel/egg/EGG-INFO
copying src/mesos.cli.egg-info/dependency_links.txt -> build/bdist.macosx-10.10-intel/egg/EGG-INFO
copying src/mesos.cli.egg-info/namespace_packages.txt -> build/bdist.macosx-10.10-intel/egg/EGG-INFO
copying src/mesos.cli.egg-info/top_level.txt -> build/bdist.macosx-10.10-intel/egg/EGG-INFO
zip_safe flag not set; analyzing archive contents...
mesos.__init__: module references __path__
creating '/Users/bernd/mesos/mesos/build/mesos-0.26.0/_build/src/python/dist/mesos.cli-0.26.0-py2.7.egg' and adding 'build/bdist.macosx-10.10-intel/egg' to it
removing 'build/bdist.macosx-10.10-intel/egg' (and everything under it)
running bdist_wheel
running build
installing to build/bdist.macosx-10.10-intel/wheel
running install
running install_lib
Skipping installation of build/bdist.macosx-10.10-intel/wheel/mesos/__init__.py (namespace package)
copying mesos/cli.py -> build/bdist.macosx-10.10-intel/wheel/mesos
copying mesos/futures.py -> build/bdist.macosx-10.10-intel/wheel/mesos
copying mesos/http.py -> build/bdist.macosx-10.10-intel/wheel/mesos
running install_egg_info
Copying src/mesos.cli.egg-info to build/bdist.macosx-10.10-intel/wheel/mesos.cli-0.26.0-py2.7.egg-info
Installing build/bdist.macosx-10.10-intel/wheel/mesos.cli-0.26.0-py2.7-nspkg.pth
running install_scripts
creating build/bdist.macosx-10.10-intel/wheel/mesos.cli-0.26.0.dist-info/WHEEL
running bdist_egg
running egg_info
writing requirements to src/mesos.interface.egg-info/requires.txt
writing src/mesos.interface.egg-info/PKG-INFO
writing namespace_packages to src/mesos.interface.egg-info/namespace_packages.txt
writing top-level names to src/mesos.interface.egg-info/top_level.txt
writing dependency_links to src/mesos.interface.egg-info/dependency_links.txt
writing requirements to src/mesos.interface.egg-info/requires.txt
writing src/mesos.interface.egg-info/PKG-INFO
writing namespace_packages to src/mesos.interface.egg-info/namespace_packages.txt
writing top-level names to src/mesos.interface.egg-info/top_level.txt
writing dependency_links to src/mesos.interface.egg-info/dependency_links.txt
reading manifest file 'src/mesos.interface.egg-info/SOURCES.txt'
writing manifest file 'src/mesos.interface.egg-info/SOURCES.txt'
installing library code to build/bdist.macosx-10.10-intel/egg
running install_lib
running build_py
creating build/bdist.macosx-10.10-intel/egg
creating build/bdist.macosx-10.10-intel/egg/mesos
copying build/lib/mesos/__init__.py -> build/bdist.macosx-10.10-intel/egg/mesos
creating build/bdist.macosx-10.10-intel/egg/mesos/interface
copying build/lib/mesos/interface/__init__.py -> build/bdist.macosx-10.10-intel/egg/mesos/interface
copying build/lib/mesos/interface/containerizer_pb2.py -> build/bdist.macosx-10.10-intel/egg/mesos/interface
copying build/lib/mesos/interface/mesos_pb2.py -> build/bdist.macosx-10.10-intel/egg/mesos/interface
byte-compiling build/bdist.macosx-10.10-intel/egg/mesos/__init__.py to __init__.pyc
byte-compiling build/bdist.macosx-10.10-intel/egg/mesos/interface/__init__.py to __init__.pyc
byte-compiling build/bdist.macosx-10.10-intel/egg/mesos/interface/containerizer_pb2.py to containerizer_pb2.pyc
byte-compiling build/bdist.macosx-10.10-intel/egg/mesos/interface/mesos_pb2.py to mesos_pb2.pyc
creating build/bdist.macosx-10.10-intel/egg/EGG-INFO
copying src/mesos.interface.egg-info/PKG-INFO -> build/bdist.macosx-10.10-intel/egg/EGG-INFO
copying src/mesos.interface.egg-info/SOURCES.txt -> build/bdist.macosx-10.10-intel/egg/EGG-INFO
copying src/mesos.interface.egg-info/dependency_links.txt -> build/bdist.macosx-10.10-intel/egg/EGG-INFO
copying src/mesos.interface.egg-info/namespace_packages.txt -> build/bdist.macosx-10.10-intel/egg/EGG-INFO
copying src/mesos.interface.egg-info/requires.txt -> build/bdist.macosx-10.10-intel/egg/EGG-INFO
copying src/mesos.interface.egg-info/top_level.txt -> build/bdist.macosx-10.10-intel/egg/EGG-INFO
zip_safe flag not set; analyzing archive contents...
mesos.__init__: module references __path__
creating '/Users/bernd/mesos/mesos/build/mesos-0.26.0/_build/src/python/dist/mesos.interface-0.26.0-py2.7.egg' and adding 'build/bdist.macosx-10.10-intel/egg' to it
removing 'build/bdist.macosx-10.10-intel/egg' (and everything under it)
running bdist_wheel
running build
installing to build/bdist.macosx-10.10-intel/wheel
running install
running install_lib
Skipping installation of build/bdist.macosx-10.10-intel/wheel/mesos/__init__.py (namespace package)
copying mesos/interface/__init__.py -> build/bdist.macosx-10.10-intel/wheel/mesos/interface
copying mesos/interface/containerizer_pb2.py -> build/bdist.macosx-10.10-intel/wheel/mesos/interface
copying mesos/interface/mesos_pb2.py -> build/bdist.macosx-10.10-intel/wheel/mesos/interface
running install_egg_info
Copying src/mesos.interface.egg-info to build/bdist.macosx-10.10-intel/wheel/mesos.interface-0.26.0-py2.7.egg-info
Installing build/bdist.macosx-10.10-intel/wheel/mesos.interface-0.26.0-py2.7-nspkg.pth
running install_scripts
creating build/bdist.macosx-10.10-intel/wheel/mesos.interface-0.26.0.dist-info/WHEEL
running bdist_egg
running egg_info
writing requirements to src/mesos.native.egg-info/requires.txt
writing src/mesos.native.egg-info/PKG-INFO
writing namespace_packages to src/mesos.native.egg-info/namespace_packages.txt
writing top-level names to src/mesos.native.egg-info/top_level.txt
writing dependency_links to src/mesos.native.egg-info/dependency_links.txt
writing requirements to src/mesos.native.egg-info/requires.txt
writing src/mesos.native.egg-info/PKG-INFO
writing namespace_packages to src/mesos.native.egg-info/namespace_packages.txt
writing top-level names to src/mesos.native.egg-info/top_level.txt
writing dependency_links to src/mesos.native.egg-info/dependency_links.txt
reading manifest file 'src/mesos.native.egg-info/SOURCES.txt'
writing manifest file 'src/mesos.native.egg-info/SOURCES.txt'
installing library code to build/bdist.macosx-10.10-intel/egg
running install_lib
running build_py
running build_ext
creating build/bdist.macosx-10.10-intel/egg
creating build/bdist.macosx-10.10-intel/egg/mesos
copying build/lib.macosx-10.10-intel-2.7/mesos/__init__.py -> build/bdist.macosx-10.10-intel/egg/mesos
creating build/bdist.macosx-10.10-intel/egg/mesos/native
copying build/lib.macosx-10.10-intel-2.7/mesos/native/__init__.py -> build/bdist.macosx-10.10-intel/egg/mesos/native
copying build/lib.macosx-10.10-intel-2.7/mesos/native/_mesos.so -> build/bdist.macosx-10.10-intel/egg/mesos/native
byte-compiling build/bdist.macosx-10.10-intel/egg/mesos/__init__.py to __init__.pyc
byte-compiling build/bdist.macosx-10.10-intel/egg/mesos/native/__init__.py to __init__.pyc
creating stub loader for mesos/native/_mesos.so
byte-compiling build/bdist.macosx-10.10-intel/egg/mesos/native/_mesos.py to _mesos.pyc
creating build/bdist.macosx-10.10-intel/egg/EGG-INFO
copying src/mesos.native.egg-info/PKG-INFO -> build/bdist.macosx-10.10-intel/egg/EGG-INFO
copying src/mesos.native.egg-info/SOURCES.txt -> build/bdist.macosx-10.10-intel/egg/EGG-INFO
copying src/mesos.native.egg-info/dependency_links.txt -> build/bdist.macosx-10.10-intel/egg/EGG-INFO
copying src/mesos.native.egg-info/namespace_packages.txt -> build/bdist.macosx-10.10-intel/egg/EGG-INFO
copying src/mesos.native.egg-info/requires.txt -> build/bdist.macosx-10.10-intel/egg/EGG-INFO
copying src/mesos.native.egg-info/top_level.txt -> build/bdist.macosx-10.10-intel/egg/EGG-INFO
writing build/bdist.macosx-10.10-intel/egg/EGG-INFO/native_libs.txt
zip_safe flag not set; analyzing archive contents...
mesos.__init__: module references __path__
creating '/Users/bernd/mesos/mesos/build/mesos-0.26.0/_build/src/python/dist/mesos.native-0.26.0-py2.7-macosx-10.10-intel.egg' and adding 'build/bdist.macosx-10.10-intel/egg' to it
removing 'build/bdist.macosx-10.10-intel/egg' (and everything under it)
running bdist_wheel
running build
installing to build/bdist.macosx-10.10-intel/wheel
running install
running install_lib
Skipping installation of build/bdist.macosx-10.10-intel/wheel/mesos/__init__.py (namespace package)
copying mesos/native/__init__.py -> build/bdist.macosx-10.10-intel/wheel/mesos/native
copying mesos/native/_mesos.so -> build/bdist.macosx-10.10-intel/wheel/mesos/native
running install_egg_info
Copying src/mesos.native.egg-info to build/bdist.macosx-10.10-intel/wheel/mesos.native-0.26.0-py2.7.egg-info
Installing build/bdist.macosx-10.10-intel/wheel/mesos.native-0.26.0-py2.7-nspkg.pth
running install_scripts
creating build/bdist.macosx-10.10-intel/wheel/mesos.native-0.26.0.dist-info/WHEEL
 ../../install-sh -c -d '/Users/bernd/mesos/mesos/build/mesos-0.26.0/_inst/lib'
 /bin/sh ../libtool   --mode=install /usr/bin/install -c   libmesos.la libfixed_resource_estimator.la '/Users/bernd/mesos/mesos/build/mesos-0.26.0/_inst/lib'
libtool: install: /usr/bin/install -c .libs/libmesos-0.26.0.dylib /Users/bernd/mesos/mesos/build/mesos-0.26.0/_inst/lib/libmesos-0.26.0.dylib
libtool: install: (cd /Users/bernd/mesos/mesos/build/mesos-0.26.0/_inst/lib && { ln -s -f libmesos-0.26.0.dylib libmesos.dylib || { rm -f libmesos.dylib && ln -s libmesos-0.26.0.dylib libmesos.dylib; }; })
libtool: install: /usr/bin/install -c .libs/libmesos.lai /Users/bernd/mesos/mesos/build/mesos-0.26.0/_inst/lib/libmesos.la
libtool: install: /usr/bin/install -c .libs/libfixed_resource_estimator-0.26.0.dylib /Users/bernd/mesos/mesos/build/mesos-0.26.0/_inst/lib/libfixed_resource_estimator-0.26.0.dylib
libtool: install: (cd /Users/bernd/mesos/mesos/build/mesos-0.26.0/_inst/lib && { ln -s -f libfixed_resource_estimator-0.26.0.dylib libfixed_resource_estimator.dylib || { rm -f libfixed_resource_estimator.dylib && ln -s libfixed_resource_estimator-0.26.0.dylib libfixed_resource_estimator.dylib; }; })
libtool: install: /usr/bin/install -c .libs/libfixed_resource_estimator.lai /Users/bernd/mesos/mesos/build/mesos-0.26.0/_inst/lib/libfixed_resource_estimator.la
 ../../install-sh -c -d '/Users/bernd/mesos/mesos/build/mesos-0.26.0/_inst/bin'
  /bin/sh ../libtool   --mode=install /usr/bin/install -c mesos-local mesos-log mesos mesos-execute mesos-resolve '/Users/bernd/mesos/mesos/build/mesos-0.26.0/_inst/bin'
libtool: install: /usr/bin/install -c .libs/mesos-local /Users/bernd/mesos/mesos/build/mesos-0.26.0/_inst/bin/mesos-local
libtool: install: /usr/bin/install -c .libs/mesos-log /Users/bernd/mesos/mesos/build/mesos-0.26.0/_inst/bin/mesos-log
libtool: install: /usr/bin/install -c .libs/mesos /Users/bernd/mesos/mesos/build/mesos-0.26.0/_inst/bin/mesos
libtool: install: /usr/bin/install -c .libs/mesos-execute /Users/bernd/mesos/mesos/build/mesos-0.26.0/_inst/bin/mesos-execute
libtool: install: /usr/bin/install -c .libs/mesos-resolve /Users/bernd/mesos/mesos/build/mesos-0.26.0/_inst/bin/mesos-resolve
 ../../install-sh -c -d '/Users/bernd/mesos/mesos/build/mesos-0.26.0/_inst/bin'
 /usr/bin/install -c ../../src/cli/mesos-cat ../../src/cli/mesos-ps ../../src/cli/mesos-scp ../../src/cli/mesos-tail '/Users/bernd/mesos/mesos/build/mesos-0.26.0/_inst/bin'
cd python/dist &&							\
	for whl in python/dist/mesos-0.26.0-py2-none-any.whl python/dist/mesos.cli-0.26.0-py2-none-any.whl python/dist/mesos.interface-0.26.0-py2-none-any.whl python/dist/mesos.native-0.26.0-cp27-none-macosx_10_10_intel.whl; do						\
		PYTHONPATH=/Users/bernd/mesos/mesos/build/mesos-0.26.0/_inst/lib/python2.7/site-packages:/Users/bernd/mesos/mesos/build/mesos-0.26.0/_build/3rdparty/distribute-0.6.26:/Users/bernd/mesos/mesos/build/mesos-0.26.0/_build/3rdparty/pip-1.5.6:/Users/bernd/mesos/mesos/build/mesos-0.26.0/_build/3rdparty/wheel-0.24.0	\
		PYTHONUSERBASE=/Users/bernd/mesos/mesos/build/mesos-0.26.0/_inst				\
		/usr/bin/python -c ""import pip; pip.main()"" install			\
		--user --no-deps					\
		--find-links=file:///Users/bernd/mesos/mesos/build/mesos-0.26.0/_build/src/python/dist		\
		/Users/bernd/mesos/mesos/build/mesos-0.26.0/_build/src/$whl;					\
	done
Unpacking ./mesos-0.26.0-py2-none-any.whl
Installing collected packages: mesos
Successfully installed mesos
Cleaning up...
Unpacking ./mesos.cli-0.26.0-py2-none-any.whl
Installing collected packages: mesos.cli
Successfully installed mesos.cli
Cleaning up...
Unpacking ./mesos.interface-0.26.0-py2-none-any.whl
Installing collected packages: mesos.interface
Successfully installed mesos.interface
Cleaning up...
Unpacking ./mesos.native-0.26.0-cp27-none-macosx_10_10_intel.whl
Installing collected packages: mesos.native
Successfully installed mesos.native
Cleaning up...
 ../../install-sh -c -d '/Users/bernd/mesos/mesos/build/mesos-0.26.0/_inst/include/mesos/executor'
 /usr/bin/install -c -m 644 ../../include/mesos/executor/executor.hpp ../../include/mesos/executor/executor.proto '/Users/bernd/mesos/mesos/build/mesos-0.26.0/_inst/include/mesos/executor'
 ../../install-sh -c -d '/Users/bernd/mesos/mesos/build/mesos-0.26.0/_inst/include/mesos/executor'
 /usr/bin/install -c -m 644 ../include/mesos/executor/executor.pb.h '/Users/bernd/mesos/mesos/build/mesos-0.26.0/_inst/include/mesos/executor'
 ../../install-sh -c -d '/Users/bernd/mesos/mesos/build/mesos-0.26.0/_inst/sbin'
 /usr/bin/install -c deploy/mesos-daemon.sh deploy/mesos-start-cluster.sh deploy/mesos-start-masters.sh deploy/mesos-start-slaves.sh deploy/mesos-stop-cluster.sh deploy/mesos-stop-masters.sh deploy/mesos-stop-slaves.sh '/Users/bernd/mesos/mesos/build/mesos-0.26.0/_inst/sbin'
 ../../install-sh -c -d '/Users/bernd/mesos/mesos/build/mesos-0.26.0/_inst/include/mesos/v1/executor'
 /usr/bin/install -c -m 644 ../include/mesos/v1/executor/executor.pb.h '/Users/bernd/mesos/mesos/build/mesos-0.26.0/_inst/include/mesos/v1/executor'
 ../../install-sh -c -d '/Users/bernd/mesos/mesos/build/mesos-0.26.0/_inst/libexec/mesos'
  /bin/sh ../libtool   --mode=install /usr/bin/install -c mesos-fetcher mesos-executor mesos-containerizer mesos-health-check mesos-usage mesos-docker-executor '/Users/bernd/mesos/mesos/build/mesos-0.26.0/_inst/libexec/mesos'
libtool: install: /usr/bin/install -c .libs/mesos-fetcher /Users/bernd/mesos/mesos/build/mesos-0.26.0/_inst/libexec/mesos/mesos-fetcher
libtool: install: /usr/bin/install -c .libs/mesos-executor /Users/bernd/mesos/mesos/build/mesos-0.26.0/_inst/libexec/mesos/mesos-executor
libtool: install: /usr/bin/install -c .libs/mesos-containerizer /Users/bernd/mesos/mesos/build/mesos-0.26.0/_inst/libexec/mesos/mesos-containerizer
libtool: install: /usr/bin/install -c .libs/mesos-health-check /Users/bernd/mesos/mesos/build/mesos-0.26.0/_inst/libexec/mesos/mesos-health-check
libtool: install: /usr/bin/install -c .libs/mesos-usage /Users/bernd/mesos/mesos/build/mesos-0.26.0/_inst/libexec/mesos/mesos-usage
libtool: install: /usr/bin/install -c .libs/mesos-docker-executor /Users/bernd/mesos/mesos/build/mesos-0.26.0/_inst/libexec/mesos/mesos-docker-executor
 ../../install-sh -c -d '/Users/bernd/mesos/mesos/build/mesos-0.26.0/_inst/sbin'
  /bin/sh ../libtool   --mode=install /usr/bin/install -c mesos-master mesos-slave '/Users/bernd/mesos/mesos/build/mesos-0.26.0/_inst/sbin'
libtool: install: /usr/bin/install -c .libs/mesos-master /Users/bernd/mesos/mesos/build/mesos-0.26.0/_inst/sbin/mesos-master
libtool: install: /usr/bin/install -c .libs/mesos-slave /Users/bernd/mesos/mesos/build/mesos-0.26.0/_inst/sbin/mesos-slave
 ../../install-sh -c -d '/Users/bernd/mesos/mesos/build/mesos-0.26.0/_inst/include/mesos/v1/executor'
 /usr/bin/install -c -m 644 ../../include/mesos/v1/executor/executor.hpp ../../include/mesos/v1/executor/executor.proto '/Users/bernd/mesos/mesos/build/mesos-0.26.0/_inst/include/mesos/v1/executor'
 ../../install-sh -c -d '/Users/bernd/mesos/mesos/build/mesos-0.26.0/_inst/include/mesos/authentication'
 /usr/bin/install -c -m 644 ../../include/mesos/authentication/authenticatee.hpp ../../include/mesos/authentication/authentication.hpp ../../include/mesos/authentication/authentication.proto ../../include/mesos/authentication/authenticator.hpp '/Users/bernd/mesos/mesos/build/mesos-0.26.0/_inst/include/mesos/authentication'
 ../../install-sh -c -d '/Users/bernd/mesos/mesos/build/mesos-0.26.0/_inst/include/mesos/authorizer'
 /usr/bin/install -c -m 644 ../../include/mesos/authorizer/authorizer.hpp ../../include/mesos/authorizer/authorizer.proto '/Users/bernd/mesos/mesos/build/mesos-0.26.0/_inst/include/mesos/authorizer'
 ../../install-sh -c -d '/Users/bernd/mesos/mesos/build/mesos-0.26.0/_inst/include/mesos/containerizer'
 /usr/bin/install -c -m 644 ../../include/mesos/containerizer/containerizer.hpp ../../include/mesos/containerizer/containerizer.proto '/Users/bernd/mesos/mesos/build/mesos-0.26.0/_inst/include/mesos/containerizer'
 ../../install-sh -c -d '/Users/bernd/mesos/mesos/build/mesos-0.26.0/_inst/include/mesos/fetcher'
 /usr/bin/install -c -m 644 ../../include/mesos/fetcher/fetcher.hpp ../../include/mesos/fetcher/fetcher.proto '/Users/bernd/mesos/mesos/build/mesos-0.26.0/_inst/include/mesos/fetcher'
 ../../install-sh -c -d '/Users/bernd/mesos/mesos/build/mesos-0.26.0/_inst/include/mesos/maintenance'
 /usr/bin/install -c -m 644 ../../include/mesos/maintenance/maintenance.hpp ../../include/mesos/maintenance/maintenance.proto '/Users/bernd/mesos/mesos/build/mesos-0.26.0/_inst/include/mesos/maintenance'
 ../../install-sh -c -d '/Users/bernd/mesos/mesos/build/mesos-0.26.0/_inst/include/mesos/master'
 /usr/bin/install -c -m 644 ../../include/mesos/master/allocator.hpp ../../include/mesos/master/allocator.proto '/Users/bernd/mesos/mesos/build/mesos-0.26.0/_inst/include/mesos/master'
 ../../install-sh -c -d '/Users/bernd/mesos/mesos/build/mesos-0.26.0/_inst/include/mesos/module'
 /usr/bin/install -c -m 644 ../../include/mesos/module/allocator.hpp ../../include/mesos/module/anonymous.hpp ../../include/mesos/module/authenticatee.hpp ../../include/mesos/module/authenticator.hpp ../../include/mesos/module/authorizer.hpp ../../include/mesos/module/hook.hpp ../../include/mesos/module/isolator.hpp ../../include/mesos/module/module.hpp ../../include/mesos/module/module.proto ../../include/mesos/module/qos_controller.hpp ../../include/mesos/module/resource_estimator.hpp '/Users/bernd/mesos/mesos/build/mesos-0.26.0/_inst/include/mesos/module'
 ../../install-sh -c -d '/Users/bernd/mesos/mesos/build/mesos-0.26.0/_inst/share/mesos'
 ../../install-sh -c -d '/Users/bernd/mesos/mesos/build/mesos-0.26.0/_inst/share/mesos/webui/master/static'
 /usr/bin/install -c -m 644  ../../src/webui/master/static/browse.html ../../src/webui/master/static/framework.html ../../src/webui/master/static/frameworks.html ../../src/webui/master/static/home.html ../../src/webui/master/static/index.html ../../src/webui/master/static/offers.html ../../src/webui/master/static/pailer.html ../../src/webui/master/static/slave.html ../../src/webui/master/static/slave_executor.html ../../src/webui/master/static/slave_framework.html ../../src/webui/master/static/slaves.html '/Users/bernd/mesos/mesos/build/mesos-0.26.0/_inst/share/mesos/webui/master/static'
 ../../install-sh -c -d '/Users/bernd/mesos/mesos/build/mesos-0.26.0/_inst/share/mesos/webui/master/static/obj'
 /usr/bin/install -c -m 644  ../../src/webui/master/static/obj/zeroclipboard-1.1.7.swf '/Users/bernd/mesos/mesos/build/mesos-0.26.0/_inst/share/mesos/webui/master/static/obj'
 ../../install-sh -c -d '/Users/bernd/mesos/mesos/build/mesos-0.26.0/_inst/share/mesos/webui/master/static/directives'
 /usr/bin/install -c -m 644  ../../src/webui/master/static/directives/pagination.html ../../src/webui/master/static/directives/tableHeader.html ../../src/webui/master/static/directives/timestamp.html '/Users/bernd/mesos/mesos/build/mesos-0.26.0/_inst/share/mesos/webui/master/static/directives'
 ../../install-sh -c -d '/Users/bernd/mesos/mesos/build/mesos-0.26.0/_inst/share/mesos/webui/master/static/css'
 /usr/bin/install -c -m 644  ../../src/webui/master/static/css/bootstrap-3.0.3.min.css ../../src/webui/master/static/css/mesos.css '/Users/bernd/mesos/mesos/build/mesos-0.26.0/_inst/share/mesos/webui/master/static/css'
 ../../install-sh -c -d '/Users/bernd/mesos/mesos/build/mesos-0.26.0/_inst/share/mesos/webui/master/static/ico'
 /usr/bin/install -c -m 644  ../../src/webui/master/static/ico/favicon.ico '/Users/bernd/mesos/mesos/build/mesos-0.26.0/_inst/share/mesos/webui/master/static/ico'
 ../../install-sh -c -d '/Users/bernd/mesos/mesos/build/mesos-0.26.0/_inst/share/mesos/webui/master/static/img'
 /usr/bin/install -c -m 644  ../../src/webui/master/static/img/loading.gif '/Users/bernd/mesos/mesos/build/mesos-0.26.0/_inst/share/mesos/webui/master/static/img'
 ../../install-sh -c -d '/Users/bernd/mesos/mesos/build/mesos-0.26.0/_inst/share/mesos/webui/master/static/fonts'
 /usr/bin/install -c -m 644  ../../src/webui/master/static/fonts/glyphicons-halflings-regular.eot ../../src/webui/master/static/fonts/glyphicons-halflings-regular.svg ../../src/webui/master/static/fonts/glyphicons-halflings-regular.ttf ../../src/webui/master/static/fonts/glyphicons-halflings-regular.woff '/Users/bernd/mesos/mesos/build/mesos-0.26.0/_inst/share/mesos/webui/master/static/fonts'
 ../../install-sh -c -d '/Users/bernd/mesos/mesos/build/mesos-0.26.0/_inst/share/mesos/webui/master/static/js'
 /usr/bin/install -c -m 644  ../../src/webui/master/static/js/app.js ../../src/webui/master/static/js/controllers.js ../../src/webui/master/static/js/jquery.pailer.js ../../src/webui/master/static/js/services.js ../../src/webui/master/static/js/angular-1.2.3.js ../../src/webui/master/static/js/angular-1.2.3.min.js ../../src/webui/master/static/js/angular-route-1.2.3.js ../../src/webui/master/static/js/angular-route-1.2.3.min.js ../../src/webui/master/static/js/jquery-1.7.1.js ../../src/webui/master/static/js/jquery-1.7.1.min.js ../../src/webui/master/static/js/relative-date.js ../../src/webui/master/static/js/ui-bootstrap-tpls-0.9.0.js ../../src/webui/master/static/js/ui-bootstrap-tpls-0.9.0.min.js ../../src/webui/master/static/js/underscore-1.4.3.js ../../src/webui/master/static/js/underscore-1.4.3.min.js ../../src/webui/master/static/js/zeroclipboard-1.1.7.js ../../src/webui/master/static/js/zeroclipboard-1.1.7.min.js '/Users/bernd/mesos/mesos/build/mesos-0.26.0/_inst/share/mesos/webui/master/static/js'
 ../../install-sh -c -d '/Users/bernd/mesos/mesos/build/mesos-0.26.0/_inst/include/mesos/authentication'
 /usr/bin/install -c -m 644 ../include/mesos/authentication/authentication.pb.h '/Users/bernd/mesos/mesos/build/mesos-0.26.0/_inst/include/mesos/authentication'
 ../../install-sh -c -d '/Users/bernd/mesos/mesos/build/mesos-0.26.0/_inst/include/mesos/authorizer'
 /usr/bin/install -c -m 644 ../include/mesos/authorizer/authorizer.pb.h '/Users/bernd/mesos/mesos/build/mesos-0.26.0/_inst/include/mesos/authorizer'
 ../../install-sh -c -d '/Users/bernd/mesos/mesos/build/mesos-0.26.0/_inst/include/mesos/containerizer'
 /usr/bin/install -c -m 644 ../include/mesos/containerizer/containerizer.pb.h '/Users/bernd/mesos/mesos/build/mesos-0.26.0/_inst/include/mesos/containerizer'
 ../../install-sh -c -d '/Users/bernd/mesos/mesos/build/mesos-0.26.0/_inst/include/mesos/fetcher'
 /usr/bin/install -c -m 644 ../include/mesos/fetcher/fetcher.pb.h '/Users/bernd/mesos/mesos/build/mesos-0.26.0/_inst/include/mesos/fetcher'
 ../../install-sh -c -d '/Users/bernd/mesos/mesos/build/mesos-0.26.0/_inst/include/mesos/maintenance'
 /usr/bin/install -c -m 644 ../include/mesos/maintenance/maintenance.pb.h '/Users/bernd/mesos/mesos/build/mesos-0.26.0/_inst/include/mesos/maintenance'
 ../../install-sh -c -d '/Users/bernd/mesos/mesos/build/mesos-0.26.0/_inst/include/mesos/master'
 /usr/bin/install -c -m 644 ../include/mesos/master/allocator.pb.h '/Users/bernd/mesos/mesos/build/mesos-0.26.0/_inst/include/mesos/master'
 ../../install-sh -c -d '/Users/bernd/mesos/mesos/build/mesos-0.26.0/_inst/include/mesos/module'
 /usr/bin/install -c -m 644 ../include/mesos/module/module.pb.h '/Users/bernd/mesos/mesos/build/mesos-0.26.0/_inst/include/mesos/module'
 ../../install-sh -c -d '/Users/bernd/mesos/mesos/build/mesos-0.26.0/_inst/include/mesos'
 /usr/bin/install -c -m 644 ../include/mesos/version.hpp ../include/mesos/mesos.pb.h '/Users/bernd/mesos/mesos/build/mesos-0.26.0/_inst/include/mesos'
 ../../install-sh -c -d '/Users/bernd/mesos/mesos/build/mesos-0.26.0/_inst/include/mesos/quota'
 /usr/bin/install -c -m 644 ../include/mesos/quota/quota.pb.h '/Users/bernd/mesos/mesos/build/mesos-0.26.0/_inst/include/mesos/quota'
 ../../install-sh -c -d '/Users/bernd/mesos/mesos/build/mesos-0.26.0/_inst/include/mesos/scheduler'
 /usr/bin/install -c -m 644 ../include/mesos/scheduler/scheduler.pb.h '/Users/bernd/mesos/mesos/build/mesos-0.26.0/_inst/include/mesos/scheduler'
 ../../install-sh -c -d '/Users/bernd/mesos/mesos/build/mesos-0.26.0/_inst/include/mesos/slave'
 /usr/bin/install -c -m 644 ../include/mesos/slave/isolator.pb.h ../include/mesos/slave/oversubscription.pb.h '/Users/bernd/mesos/mesos/build/mesos-0.26.0/_inst/include/mesos/slave'
 ../../install-sh -c -d '/Users/bernd/mesos/mesos/build/mesos-0.26.0/_inst/include/mesos/v1'
 /usr/bin/install -c -m 644 ../include/mesos/v1/mesos.pb.h '/Users/bernd/mesos/mesos/build/mesos-0.26.0/_inst/include/mesos/v1'
 ../../install-sh -c -d '/Users/bernd/mesos/mesos/build/mesos-0.26.0/_inst/include/mesos/v1/scheduler'
 /usr/bin/install -c -m 644 ../include/mesos/v1/scheduler/scheduler.pb.h '/Users/bernd/mesos/mesos/build/mesos-0.26.0/_inst/include/mesos/v1/scheduler'
 ../../install-sh -c -d '/Users/bernd/mesos/mesos/build/mesos-0.26.0/_inst/include/mesos'
 /usr/bin/install -c -m 644 ../../include/mesos/attributes.hpp ../../include/mesos/executor.hpp ../../include/mesos/hook.hpp ../../include/mesos/http.hpp ../../include/mesos/mesos.hpp ../../include/mesos/mesos.proto ../../include/mesos/module.hpp ../../include/mesos/resources.hpp ../../include/mesos/scheduler.hpp ../../include/mesos/type_utils.hpp ../../include/mesos/values.hpp '/Users/bernd/mesos/mesos/build/mesos-0.26.0/_inst/include/mesos'
 ../../install-sh -c -d '/Users/bernd/mesos/mesos/build/mesos-0.26.0/_inst/etc/mesos'
 /usr/bin/install -c -m 644 ../../src/deploy/mesos-deploy-env.sh.template ../../src/deploy/mesos-master-env.sh.template ../../src/deploy/mesos-slave-env.sh.template '/Users/bernd/mesos/mesos/build/mesos-0.26.0/_inst/etc/mesos'
 ../../install-sh -c -d '/Users/bernd/mesos/mesos/build/mesos-0.26.0/_inst/include/mesos/quota'
 /usr/bin/install -c -m 644 ../../include/mesos/quota/quota.hpp ../../include/mesos/quota/quota.proto '/Users/bernd/mesos/mesos/build/mesos-0.26.0/_inst/include/mesos/quota'
 ../../install-sh -c -d '/Users/bernd/mesos/mesos/build/mesos-0.26.0/_inst/include/mesos/scheduler'
 /usr/bin/install -c -m 644 ../../include/mesos/scheduler/scheduler.hpp ../../include/mesos/scheduler/scheduler.proto '/Users/bernd/mesos/mesos/build/mesos-0.26.0/_inst/include/mesos/scheduler'
 ../../install-sh -c -d '/Users/bernd/mesos/mesos/build/mesos-0.26.0/_inst/include/mesos/slave'
 /usr/bin/install -c -m 644 ../../include/mesos/slave/isolator.hpp ../../include/mesos/slave/isolator.proto ../../include/mesos/slave/oversubscription.hpp ../../include/mesos/slave/oversubscription.proto ../../include/mesos/slave/qos_controller.hpp ../../include/mesos/slave/resource_estimator.hpp '/Users/bernd/mesos/mesos/build/mesos-0.26.0/_inst/include/mesos/slave'
 ../../install-sh -c -d '/Users/bernd/mesos/mesos/build/mesos-0.26.0/_inst/include/mesos/v1'
 /usr/bin/install -c -m 644 ../../include/mesos/v1/attributes.hpp ../../include/mesos/v1/mesos.hpp ../../include/mesos/v1/mesos.proto ../../include/mesos/v1/resources.hpp ../../include/mesos/v1/scheduler.hpp ../../include/mesos/v1/values.hpp '/Users/bernd/mesos/mesos/build/mesos-0.26.0/_inst/include/mesos/v1'
 ../../install-sh -c -d '/Users/bernd/mesos/mesos/build/mesos-0.26.0/_inst/include/mesos/v1/scheduler'
 /usr/bin/install -c -m 644 ../../include/mesos/v1/scheduler/scheduler.hpp ../../include/mesos/v1/scheduler/scheduler.proto '/Users/bernd/mesos/mesos/build/mesos-0.26.0/_inst/include/mesos/v1/scheduler'
Making installcheck in .
make[2]: Nothing to be done for `installcheck-am'.
Making installcheck in 3rdparty
Making installcheck in libprocess
Making installcheck in 3rdparty
Making installcheck in stout
Making installcheck in .
make[6]: Nothing to be done for `installcheck-am'.
Making installcheck in include
make[6]: Nothing to be done for `installcheck'.
make[5]: Nothing to be done for `installcheck-am'.
Making installcheck in .
make[4]: Nothing to be done for `installcheck-am'.
Making installcheck in include
make[4]: Nothing to be done for `installcheck'.
make[3]: Nothing to be done for `installcheck-am'.
Making installcheck in src
make[2]: Nothing to be done for `installcheck'.
Making uninstall in .
 ( cd '/Users/bernd/mesos/mesos/build/mesos-0.26.0/_inst/lib/pkgconfig' && rm -f mesos.pc )
Making uninstall in 3rdparty
Making uninstall in libprocess
Making uninstall in 3rdparty
Making uninstall in stout
Making uninstall in .
make[6]: Nothing to be done for `uninstall-am'.
Making uninstall in include
 ( cd '/Users/bernd/mesos/mesos/build/mesos-0.26.0/_inst/include' && rm -f stout/abort.hpp stout/attributes.hpp stout/base64.hpp stout/bits.hpp stout/bytes.hpp stout/cache.hpp stout/check.hpp stout/duration.hpp stout/dynamiclibrary.hpp stout/error.hpp stout/exit.hpp stout/flags.hpp stout/flags/fetch.hpp stout/flags/flag.hpp stout/flags/flags.hpp stout/flags/parse.hpp stout/foreach.hpp stout/format.hpp stout/fs.hpp stout/gtest.hpp stout/gzip.hpp stout/hashmap.hpp stout/hashset.hpp stout/interval.hpp stout/ip.hpp stout/json.hpp stout/lambda.hpp stout/linkedhashmap.hpp stout/list.hpp stout/mac.hpp stout/multihashmap.hpp stout/multimap.hpp stout/net.hpp stout/none.hpp stout/nothing.hpp stout/numify.hpp stout/option.hpp stout/os.hpp stout/os/bootid.hpp stout/os/chdir.hpp stout/os/close.hpp stout/os/constants.hpp stout/os/environment.hpp stout/os/exists.hpp stout/os/fcntl.hpp stout/os/fork.hpp stout/os/ftruncate.hpp stout/os/getcwd.hpp stout/os/killtree.hpp stout/os/linux.hpp stout/os/ls.hpp stout/os/mkdir.hpp stout/os/mktemp.hpp stout/os/open.hpp stout/os/os.hpp stout/os/osx.hpp stout/os/permissions.hpp stout/os/process.hpp stout/os/pstree.hpp stout/os/read.hpp stout/os/realpath.hpp stout/os/rename.hpp stout/os/rm.hpp stout/os/sendfile.hpp stout/os/shell.hpp stout/os/signals.hpp stout/os/stat.hpp stout/os/sysctl.hpp stout/os/touch.hpp stout/os/utime.hpp stout/os/write.hpp stout/os/posix/bootid.hpp stout/os/posix/exists.hpp stout/os/posix/fcntl.hpp stout/os/posix/fork.hpp stout/os/posix/ftruncate.hpp stout/os/posix/killtree.hpp stout/os/posix/ls.hpp stout/os/posix/process.hpp stout/os/posix/pstree.hpp stout/os/posix/sendfile.hpp stout/os/posix/shell.hpp stout/os/posix/signals.hpp stout/os/posix/stat.hpp stout/os/raw/environment.hpp stout/os/windows/bootid.hpp stout/os/windows/exists.hpp stout/os/windows/fcntl.hpp stout/os/windows/fork.hpp stout/os/windows/ftruncate.hpp stout/os/windows/killtree.hpp stout/os/windows/ls.hpp stout/os/windows/process.hpp stout/os/windows/pstree.hpp stout/os/windows/sendfile.hpp stout/os/windows/shell.hpp stout/os/windows/signals.hpp stout/os/windows/stat.hpp stout/path.hpp stout/preprocessor.hpp stout/proc.hpp stout/protobuf.hpp stout/posix/gzip.hpp stout/posix/os.hpp stout/recordio.hpp stout/result.hpp stout/set.hpp stout/some.hpp stout/stopwatch.hpp stout/stringify.hpp stout/strings.hpp stout/subcommand.hpp stout/svn.hpp stout/synchronized.hpp stout/thread_local.hpp stout/try.hpp stout/tests/utils.hpp stout/unimplemented.hpp stout/unreachable.hpp stout/utils.hpp stout/uuid.hpp stout/version.hpp stout/windows.hpp stout/windows/format.hpp stout/windows/gzip.hpp stout/windows/os.hpp )
make[5]: Nothing to be done for `uninstall-am'.
Making uninstall in .
make[4]: Nothing to be done for `uninstall-am'.
Making uninstall in include
 ( cd '/Users/bernd/mesos/mesos/build/mesos-0.26.0/_inst/include' && rm -f process/address.hpp process/async.hpp process/check.hpp process/clock.hpp process/collect.hpp process/defer.hpp process/deferred.hpp process/delay.hpp process/dispatch.hpp process/event.hpp process/executor.hpp process/filter.hpp process/firewall.hpp process/future.hpp process/gc.hpp process/gmock.hpp process/gtest.hpp process/help.hpp process/http.hpp process/id.hpp process/io.hpp process/latch.hpp process/limiter.hpp process/logging.hpp process/message.hpp process/mime.hpp process/mutex.hpp process/metrics/counter.hpp process/metrics/gauge.hpp process/metrics/metric.hpp process/metrics/metrics.hpp process/metrics/timer.hpp process/network.hpp process/once.hpp process/owned.hpp process/pid.hpp process/process.hpp process/profiler.hpp process/protobuf.hpp process/queue.hpp process/reap.hpp process/run.hpp process/sequence.hpp process/shared.hpp process/socket.hpp process/statistics.hpp process/system.hpp process/subprocess.hpp process/ssl/gtest.hpp process/ssl/utilities.hpp process/time.hpp process/timeout.hpp process/timer.hpp process/timeseries.hpp )
make[3]: Nothing to be done for `uninstall-am'.
Making uninstall in src
 ( cd '/Users/bernd/mesos/mesos/build/mesos-0.26.0/_inst/include/mesos/authentication' && rm -f authenticatee.hpp authentication.hpp authentication.proto authenticator.hpp )
 ( cd '/Users/bernd/mesos/mesos/build/mesos-0.26.0/_inst/include/mesos/authorizer' && rm -f authorizer.hpp authorizer.proto )
 ( cd '/Users/bernd/mesos/mesos/build/mesos-0.26.0/_inst/bin' && rm -f mesos-local mesos-log mesos mesos-execute mesos-resolve )
 ( cd '/Users/bernd/mesos/mesos/build/mesos-0.26.0/_inst/include/mesos/containerizer' && rm -f containerizer.hpp containerizer.proto )
 ( cd '/Users/bernd/mesos/mesos/build/mesos-0.26.0/_inst/bin' && rm -f mesos-cat mesos-ps mesos-scp mesos-tail )
 ( cd '/Users/bernd/mesos/mesos/build/mesos-0.26.0/_inst/include/mesos/executor' && rm -f executor.hpp executor.proto )
 ( cd '/Users/bernd/mesos/mesos/build/mesos-0.26.0/_inst/include/mesos/fetcher' && rm -f fetcher.hpp fetcher.proto )
 /bin/sh ../libtool   --mode=uninstall rm -f '/Users/bernd/mesos/mesos/build/mesos-0.26.0/_inst/lib/libmesos.la'
libtool: uninstall: rm -f /Users/bernd/mesos/mesos/build/mesos-0.26.0/_inst/lib/libmesos.la /Users/bernd/mesos/mesos/build/mesos-0.26.0/_inst/lib/libmesos-0.26.0.dylib /Users/bernd/mesos/mesos/build/mesos-0.26.0/_inst/lib/libmesos.dylib
 /bin/sh ../libtool   --mode=uninstall rm -f '/Users/bernd/mesos/mesos/build/mesos-0.26.0/_inst/lib/libfixed_resource_estimator.la'
libtool: uninstall: rm -f /Users/bernd/mesos/mesos/build/mesos-0.26.0/_inst/lib/libfixed_resource_estimator.la /Users/bernd/mesos/mesos/build/mesos-0.26.0/_inst/lib/libfixed_resource_estimator-0.26.0.dylib /Users/bernd/mesos/mesos/build/mesos-0.26.0/_inst/lib/libfixed_resource_estimator.dylib
for whl in python/dist/mesos-0.26.0-py2-none-any.whl python/dist/mesos.cli-0.26.0-py2-none-any.whl python/dist/mesos.interface-0.26.0-py2-none-any.whl python/dist/mesos.native-0.26.0-cp27-none-macosx_10_10_intel.whl; do\
		PYTHONPATH=/Users/bernd/mesos/mesos/build/mesos-0.26.0/_inst/lib/python2.7/site-packages:/Users/bernd/mesos/mesos/build/mesos-0.26.0/_inst/lib/python2.7/site-packages:/Users/bernd/mesos/mesos/build/mesos-0.26.0/_build/3rdparty/distribute-0.6.26:/Users/bernd/mesos/mesos/build/mesos-0.26.0/_build/3rdparty/pip-1.5.6:/Users/bernd/mesos/mesos/build/mesos-0.26.0/_build/3rdparty/wheel-0.24.0		\
		/usr/bin/python -c ""import pip; pip.main()"" uninstall			\
		--yes $(echo $whl | cut -d/ -f3 | cut -d- -f1);		\
	done
Cannot uninstall requirement mesos, not installed
Storing debug log for failure in /Users/bernd/Library/Logs/pip.log
Cannot uninstall requirement mesos.cli, not installed
Storing debug log for failure in /Users/bernd/Library/Logs/pip.log
Cannot uninstall requirement mesos.interface, not installed
Storing debug log for failure in /Users/bernd/Library/Logs/pip.log
Cannot uninstall requirement mesos.native, not installed
Storing debug log for failure in /Users/bernd/Library/Logs/pip.log
 ( cd '/Users/bernd/mesos/mesos/build/mesos-0.26.0/_inst/include/mesos/maintenance' && rm -f maintenance.hpp maintenance.proto )
 ( cd '/Users/bernd/mesos/mesos/build/mesos-0.26.0/_inst/include/mesos/master' && rm -f allocator.hpp allocator.proto )
 ( cd '/Users/bernd/mesos/mesos/build/mesos-0.26.0/_inst/include/mesos/module' && rm -f allocator.hpp anonymous.hpp authenticatee.hpp authenticator.hpp authorizer.hpp hook.hpp isolator.hpp module.hpp module.proto qos_controller.hpp resource_estimator.hpp )
 ( cd '/Users/bernd/mesos/mesos/build/mesos-0.26.0/_inst/share/mesos' && rm -f webui/master/static/js/app.js webui/master/static/js/controllers.js webui/master/static/js/jquery.pailer.js webui/master/static/js/services.js webui/master/static/css/bootstrap-3.0.3.min.css webui/master/static/css/mesos.css webui/master/static/browse.html webui/master/static/framework.html webui/master/static/frameworks.html webui/master/static/home.html webui/master/static/index.html webui/master/static/offers.html webui/master/static/pailer.html webui/master/static/slave.html webui/master/static/slave_executor.html webui/master/static/slave_framework.html webui/master/static/slaves.html webui/master/static/directives/pagination.html webui/master/static/directives/tableHeader.html webui/master/static/directives/timestamp.html webui/master/static/ico/favicon.ico webui/master/static/img/loading.gif webui/master/static/fonts/glyphicons-halflings-regular.eot webui/master/static/fonts/glyphicons-halflings-regular.svg webui/master/static/fonts/glyphicons-halflings-regular.ttf webui/master/static/fonts/glyphicons-halflings-regular.woff webui/master/static/js/angular-1.2.3.js webui/master/static/js/angular-1.2.3.min.js webui/master/static/js/angular-route-1.2.3.js webui/master/static/js/angular-route-1.2.3.min.js webui/master/static/js/jquery-1.7.1.js webui/master/static/js/jquery-1.7.1.min.js webui/master/static/js/relative-date.js webui/master/static/js/ui-bootstrap-tpls-0.9.0.js webui/master/static/js/ui-bootstrap-tpls-0.9.0.min.js webui/master/static/js/underscore-1.4.3.js webui/master/static/js/underscore-1.4.3.min.js webui/master/static/js/zeroclipboard-1.1.7.js webui/master/static/js/zeroclipboard-1.1.7.min.js webui/master/static/obj/zeroclipboard-1.1.7.swf )
 ( cd '/Users/bernd/mesos/mesos/build/mesos-0.26.0/_inst/include/mesos/authentication' && rm -f authentication.pb.h )
 ( cd '/Users/bernd/mesos/mesos/build/mesos-0.26.0/_inst/include/mesos/authorizer' && rm -f authorizer.pb.h )
 ( cd '/Users/bernd/mesos/mesos/build/mesos-0.26.0/_inst/include/mesos/containerizer' && rm -f containerizer.pb.h )
 ( cd '/Users/bernd/mesos/mesos/build/mesos-0.26.0/_inst/include/mesos/executor' && rm -f executor.pb.h )
 ( cd '/Users/bernd/mesos/mesos/build/mesos-0.26.0/_inst/include/mesos/fetcher' && rm -f fetcher.pb.h )
 ( cd '/Users/bernd/mesos/mesos/build/mesos-0.26.0/_inst/include/mesos/maintenance' && rm -f maintenance.pb.h )
 ( cd '/Users/bernd/mesos/mesos/build/mesos-0.26.0/_inst/include/mesos/master' && rm -f allocator.pb.h )
 ( cd '/Users/bernd/mesos/mesos/build/mesos-0.26.0/_inst/include/mesos/module' && rm -f module.pb.h )
 ( cd '/Users/bernd/mesos/mesos/build/mesos-0.26.0/_inst/include/mesos' && rm -f version.hpp mesos.pb.h )
 ( cd '/Users/bernd/mesos/mesos/build/mesos-0.26.0/_inst/include/mesos/quota' && rm -f quota.pb.h )
 ( cd '/Users/bernd/mesos/mesos/build/mesos-0.26.0/_inst/sbin' && rm -f mesos-daemon.sh mesos-start-cluster.sh mesos-start-masters.sh mesos-start-slaves.sh mesos-stop-cluster.sh mesos-stop-masters.sh mesos-stop-slaves.sh )
 ( cd '/Users/bernd/mesos/mesos/build/mesos-0.26.0/_inst/include/mesos/scheduler' && rm -f scheduler.pb.h )
 ( cd '/Users/bernd/mesos/mesos/build/mesos-0.26.0/_inst/include/mesos/slave' && rm -f isolator.pb.h oversubscription.pb.h )
 ( cd '/Users/bernd/mesos/mesos/build/mesos-0.26.0/_inst/include/mesos/v1' && rm -f mesos.pb.h )
 ( cd '/Users/bernd/mesos/mesos/build/mesos-0.26.0/_inst/include/mesos/v1/executor' && rm -f executor.pb.h )
 ( cd '/Users/bernd/mesos/mesos/build/mesos-0.26.0/_inst/include/mesos/v1/scheduler' && rm -f scheduler.pb.h )
 ( cd '/Users/bernd/mesos/mesos/build/mesos-0.26.0/_inst/include/mesos' && rm -f attributes.hpp executor.hpp hook.hpp http.hpp mesos.hpp mesos.proto module.hpp resources.hpp scheduler.hpp type_utils.hpp values.hpp )
 ( cd '/Users/bernd/mesos/mesos/build/mesos-0.26.0/_inst/libexec/mesos' && rm -f mesos-fetcher mesos-executor mesos-containerizer mesos-health-check mesos-usage mesos-docker-executor )
 ( cd '/Users/bernd/mesos/mesos/build/mesos-0.26.0/_inst/etc/mesos' && rm -f mesos-deploy-env.sh.template mesos-master-env.sh.template mesos-slave-env.sh.template )
 ( cd '/Users/bernd/mesos/mesos/build/mesos-0.26.0/_inst/include/mesos/quota' && rm -f quota.hpp quota.proto )
 ( cd '/Users/bernd/mesos/mesos/build/mesos-0.26.0/_inst/sbin' && rm -f mesos-master mesos-slave )
 ( cd '/Users/bernd/mesos/mesos/build/mesos-0.26.0/_inst/include/mesos/scheduler' && rm -f scheduler.hpp scheduler.proto )
 ( cd '/Users/bernd/mesos/mesos/build/mesos-0.26.0/_inst/include/mesos/slave' && rm -f isolator.hpp isolator.proto oversubscription.hpp oversubscription.proto qos_controller.hpp resource_estimator.hpp )
 ( cd '/Users/bernd/mesos/mesos/build/mesos-0.26.0/_inst/include/mesos/v1' && rm -f attributes.hpp mesos.hpp mesos.proto resources.hpp scheduler.hpp values.hpp )
 ( cd '/Users/bernd/mesos/mesos/build/mesos-0.26.0/_inst/include/mesos/v1/executor' && rm -f executor.hpp executor.proto )
 ( cd '/Users/bernd/mesos/mesos/build/mesos-0.26.0/_inst/include/mesos/v1/scheduler' && rm -f scheduler.hpp scheduler.proto )
ERROR: files left after uninstall:
./lib/python/site-packages/mesos/__init__.py
./lib/python/site-packages/mesos/__init__.pyc
./lib/python/site-packages/mesos/cli.py
./lib/python/site-packages/mesos/cli.pyc
./lib/python/site-packages/mesos/futures.py
./lib/python/site-packages/mesos/futures.pyc
./lib/python/site-packages/mesos/http.py
./lib/python/site-packages/mesos/http.pyc
./lib/python/site-packages/mesos/interface/__init__.py
./lib/python/site-packages/mesos/interface/__init__.pyc
./lib/python/site-packages/mesos/interface/containerizer_pb2.py
./lib/python/site-packages/mesos/interface/containerizer_pb2.pyc
./lib/python/site-packages/mesos/interface/mesos_pb2.py
./lib/python/site-packages/mesos/interface/mesos_pb2.pyc
./lib/python/site-packages/mesos/native/__init__.py
./lib/python/site-packages/mesos/native/__init__.pyc
./lib/python/site-packages/mesos/native/_mesos.so
./lib/python/site-packages/mesos-0.26.0.dist-info/DESCRIPTION.rst
./lib/python/site-packages/mesos-0.26.0.dist-info/METADATA
./lib/python/site-packages/mesos-0.26.0.dist-info/metadata.json
./lib/python/site-packages/mesos-0.26.0.dist-info/RECORD
./lib/python/site-packages/mesos-0.26.0.dist-info/top_level.txt
./lib/python/site-packages/mesos-0.26.0.dist-info/WHEEL
./lib/python/site-packages/mesos.cli-0.26.0-py2.7-nspkg.pth
./lib/python/site-packages/mesos.cli-0.26.0.dist-info/DESCRIPTION.rst
./lib/python/site-packages/mesos.cli-0.26.0.dist-info/METADATA
./lib/python/site-packages/mesos.cli-0.26.0.dist-info/metadata.json
./lib/python/site-packages/mesos.cli-0.26.0.dist-info/namespace_packages.txt
./lib/python/site-packages/mesos.cli-0.26.0.dist-info/RECORD
./lib/python/site-packages/mesos.cli-0.26.0.dist-info/top_level.txt
./lib/python/site-packages/mesos.cli-0.26.0.dist-info/WHEEL
./lib/python/site-packages/mesos.interface-0.26.0-py2.7-nspkg.pth
./lib/python/site-packages/mesos.interface-0.26.0.dist-info/DESCRIPTION.rst
./lib/python/site-packages/mesos.interface-0.26.0.dist-info/METADATA
./lib/python/site-packages/mesos.interface-0.26.0.dist-info/metadata.json
./lib/python/site-packages/mesos.interface-0.26.0.dist-info/namespace_packages.txt
./lib/python/site-packages/mesos.interface-0.26.0.dist-info/RECORD
./lib/python/site-packages/mesos.interface-0.26.0.dist-info/top_level.txt
./lib/python/site-packages/mesos.interface-0.26.0.dist-info/WHEEL
./lib/python/site-packages/mesos.native-0.26.0-py2.7-nspkg.pth
./lib/python/site-packages/mesos.native-0.26.0.dist-info/DESCRIPTION.rst
./lib/python/site-packages/mesos.native-0.26.0.dist-info/METADATA
./lib/python/site-packages/mesos.native-0.26.0.dist-info/metadata.json
./lib/python/site-packages/mesos.native-0.26.0.dist-info/namespace_packages.txt
./lib/python/site-packages/mesos.native-0.26.0.dist-info/RECORD
./lib/python/site-packages/mesos.native-0.26.0.dist-info/top_level.txt
./lib/python/site-packages/mesos.native-0.26.0.dist-info/WHEEL
make[1]: *** [distuninstallcheck] Error 1
make: *** [distcheck] Error 1
{noformat}",Bug,Major,bernd-mesos,,10020,Accepted,In Progress,"Failing 'make distcheck' on Mac OS X 10.10.5, also 10.11.",2016-01-13T19:24:51.000+0000,MESOS-3973,2.0,mesos,
haosdent@gmail.com,2015-11-20T15:17:35.000+0000,bernd-mesos,"As non-root: make distcheck.

{noformat}
/bin/mkdir -p '/home/vagrant/mesos/build/mesos-0.26.0/_inst/bin'
/bin/bash ../libtool --mode=install /usr/bin/install -c mesos-local mesos-log mesos mesos-execute mesos-resolve '/home/vagrant/mesos/build/mesos-0.26.0/_inst/bin'
libtool: install: /usr/bin/install -c .libs/mesos-local /home/vagrant/mesos/build/mesos-0.26.0/_inst/bin/mesos-local
libtool: install: /usr/bin/install -c .libs/mesos-log /home/vagrant/mesos/build/mesos-0.26.0/_inst/bin/mesos-log
libtool: install: /usr/bin/install -c .libs/mesos /home/vagrant/mesos/build/mesos-0.26.0/_inst/bin/mesos
libtool: install: /usr/bin/install -c .libs/mesos-execute /home/vagrant/mesos/build/mesos-0.26.0/_inst/bin/mesos-execute
libtool: install: /usr/bin/install -c .libs/mesos-resolve /home/vagrant/mesos/build/mesos-0.26.0/_inst/bin/mesos-resolve
Traceback (most recent call last):
File ""<string>"", line 1, in <module>
File ""/home/vagrant/mesos/build/mesos-0.26.0/build/3rdparty/pip-1.5.6/pip/__init_.py"", line 11, in <module>
from pip.vcs import git, mercurial, subversion, bazaar # noqa
File ""/home/vagrant/mesos/build/mesos-0.26.0/_build/3rdparty/pip-1.5.6/pip/vcs/mercurial.py"", line 9, in <module>
from pip.download import path_to_url
File ""/home/vagrant/mesos/build/mesos-0.26.0/_build/3rdparty/pip-1.5.6/pip/download.py"", line 22, in <module>
from pip._vendor import requests, six
File ""/home/vagrant/mesos/build/mesos-0.26.0/build/3rdparty/pip-1.5.6/pip/_vendor/requests/__init_.py"", line 53, in <module>
from .packages.urllib3.contrib import pyopenssl
File ""/home/vagrant/mesos/build/mesos-0.26.0/_build/3rdparty/pip-1.5.6/pip/_vendor/requests/packages/urllib3/contrib/pyopenssl.py"", line 70, in <module>
ssl.PROTOCOL_SSLv3: OpenSSL.SSL.SSLv3_METHOD,
AttributeError: 'module' object has no attribute 'PROTOCOL_SSLv3'
Traceback (most recent call last):
File ""<string>"", line 1, in <module>
File ""/home/vagrant/mesos/build/mesos-0.26.0/_build/3rd
{noformat}
",Bug,Major,bernd-mesos,2015-11-26T17:29:58.000+0000,5,Resolved,Complete,"Failing 'make distcheck' on Debian 8, somehow SSL-related.",2016-03-22T22:46:00.000+0000,MESOS-3969,3.0,mesos,Mesosphere Sprint 23
alexr,2015-11-20T11:50:48.000+0000,alexr,These tests should verify whether quota implements declared functionality. This will require the whole pipeline: master harness code and an allocator implementation (in contrast to to isolated master and allocator tests).,Task,Major,alexr,,10020,Accepted,In Progress,Add integration tests for quota,2015-11-23T10:21:16.000+0000,MESOS-3967,8.0,mesos,
alexr,2015-11-20T11:32:36.000+0000,alexr,"{{QuotaInfo}} protobuf currently stores per-role quotas, including {{Resource}} objects. These resources are neither statically nor dynamically reserved, hence they may not contain {{role}} field. We should ensure this field is unset, as well as update validation routine for {{QuotaInfo}}",Bug,Major,alexr,2015-12-04T20:28:20.000+0000,5,Resolved,Complete,Ensure resources in `QuotaInfo` protobuf do not contain `role`,2015-12-04T20:28:20.000+0000,MESOS-3965,3.0,mesos,Mesosphere Sprint 23
greggomann,2015-11-20T11:25:52.000+0000,bernd-mesos,"sudo ./bin/mesos-test.sh --gtest_filter=""LimitedCpuIsolatorTest.ROOT_CGROUPS_Cfs""

{noformat}
...
F1119 14:34:52.514742 30706 isolator_tests.cpp:455] CHECK_SOME(isolator): Failed to find 'cpu.cfs_quota_us'. Your kernel might be too old to use the CFS cgroups feature.
{noformat}
",Bug,Blocker,bernd-mesos,2015-11-26T16:08:33.000+0000,5,Resolved,Complete,LimitedCpuIsolatorTest.ROOT_CGROUPS_Cfs and LimitedCpuIsolatorTest.ROOT_CGROUPS_Cfs_Big_Quota fail on Debian 8.,2015-11-27T12:41:53.000+0000,MESOS-3964,2.0,mesos,Mesosphere Sprint 23
alexr,2015-11-19T18:23:26.000+0000,alexr,"To be consistent with other operator endpoints, require a single JSON object in the request as opposed to key-value pairs encoded in a string.",Improvement,Major,alexr,2015-12-22T20:08:10.000+0000,5,Resolved,Complete,Standardize quota endpoints,2015-12-22T20:08:10.000+0000,MESOS-3960,3.0,mesos,Mesosphere Sprint 25
jieyu,2015-11-18T22:17:44.000+0000,jieyu,"The existing HDFS tool wrappers (src/hdfs/hdfs.hpp) are synchronous. They use os::shell to shell out the 'hadoop' commands. This makes it very hard to be reused at other locations in the code base.

The URI fetcher HDFS plugin will try to re-use the existing HDFS tool wrappers. In order to do that, we need to make it asynchronous first.",Task,Major,jieyu,2015-12-15T18:58:01.000+0000,5,Resolved,Complete,Make HDFS tool wrappers asynchronous.,2015-12-15T18:58:15.000+0000,MESOS-3951,5.0,mesos,Mesosphere Sprint 24
arojas,2015-11-18T17:03:18.000+0000,bernd-mesos,"UserCgroupIsolatorTest/0.ROOT_CGROUPS_UserCgroup and UserCgroupIsolatorTest/1.ROOT_CGROUPS_UserCgroup fail on CentOS 6.6 with similar output when libevent and SSL are enabled.

{noformat}
sudo ./bin/mesos-tests.sh --gtest_filter=""UserCgroupIsolatorTest/0.ROOT_CGROUPS_UserCgroup"" --verbose
{noformat}
{noformat}
[==========] Running 1 test from 1 test case.
[----------] Global test environment set-up.
[----------] 1 test from UserCgroupIsolatorTest/0, where TypeParam = mesos::internal::slave::CgroupsMemIsolatorProcess
userdel: user 'mesos.test.unprivileged.user' does not exist
[ RUN      ] UserCgroupIsolatorTest/0.ROOT_CGROUPS_UserCgroup
I1118 16:53:35.273717 30249 mem.cpp:605] Started listening for OOM events for container 867a829e-4a26-43f5-86e0-938bf1f47688
I1118 16:53:35.274538 30249 mem.cpp:725] Started listening on low memory pressure events for container 867a829e-4a26-43f5-86e0-938bf1f47688
I1118 16:53:35.275164 30249 mem.cpp:725] Started listening on medium memory pressure events for container 867a829e-4a26-43f5-86e0-938bf1f47688
I1118 16:53:35.275784 30249 mem.cpp:725] Started listening on critical memory pressure events for container 867a829e-4a26-43f5-86e0-938bf1f47688
I1118 16:53:35.276448 30249 mem.cpp:356] Updated 'memory.soft_limit_in_bytes' to 1GB for container 867a829e-4a26-43f5-86e0-938bf1f47688
I1118 16:53:35.277331 30249 mem.cpp:391] Updated 'memory.limit_in_bytes' to 1GB for container 867a829e-4a26-43f5-86e0-938bf1f47688
-bash: /sys/fs/cgroup/memory/mesos/867a829e-4a26-43f5-86e0-938bf1f47688/cgroup.procs: No such file or directory
mkdir: cannot create directory `/sys/fs/cgroup/memory/mesos/867a829e-4a26-43f5-86e0-938bf1f47688/user': No such file or directory
../../src/tests/containerizer/isolator_tests.cpp:1307: Failure
Value of: os::system( ""su - "" + UNPRIVILEGED_USERNAME + "" -c 'mkdir "" + path::join(flags.cgroups_hierarchy, userCgroup) + ""'"")
  Actual: 256
Expected: 0
-bash: /sys/fs/cgroup/memory/mesos/867a829e-4a26-43f5-86e0-938bf1f47688/user/cgroup.procs: No such file or directory
../../src/tests/containerizer/isolator_tests.cpp:1316: Failure
Value of: os::system( ""su - "" + UNPRIVILEGED_USERNAME + "" -c 'echo $$ >"" + path::join(flags.cgroups_hierarchy, userCgroup, ""cgroup.procs"") + ""'"")
  Actual: 256
Expected: 0
[  FAILED  ] UserCgroupIsolatorTest/0.ROOT_CGROUPS_UserCgroup, where TypeParam = mesos::internal::slave::CgroupsMemIsolatorProcess (149 ms)
{noformat}

{noformat}
sudo ./bin/mesos-tests.sh --gtest_filter=""UserCgroupIsolatorTest/1.ROOT_CGROUPS_UserCgroup"" --verbose
{noformat}
{noformat}
[==========] Running 1 test from 1 test case.
[----------] Global test environment set-up.
[----------] 1 test from UserCgroupIsolatorTest/1, where TypeParam = mesos::internal::slave::CgroupsCpushareIsolatorProcess
userdel: user 'mesos.test.unprivileged.user' does not exist
[ RUN      ] UserCgroupIsolatorTest/1.ROOT_CGROUPS_UserCgroup
I1118 17:01:00.550706 30357 cpushare.cpp:392] Updated 'cpu.shares' to 1024 (cpus 1) for container e57f4343-1a97-4b44-b347-803be47ace80
-bash: /sys/fs/cgroup/cpuacct/mesos/e57f4343-1a97-4b44-b347-803be47ace80/cgroup.procs: No such file or directory
mkdir: cannot create directory `/sys/fs/cgroup/cpuacct/mesos/e57f4343-1a97-4b44-b347-803be47ace80/user': No such file or directory
../../src/tests/containerizer/isolator_tests.cpp:1307: Failure
Value of: os::system( ""su - "" + UNPRIVILEGED_USERNAME + "" -c 'mkdir "" + path::join(flags.cgroups_hierarchy, userCgroup) + ""'"")
  Actual: 256
Expected: 0
-bash: /sys/fs/cgroup/cpuacct/mesos/e57f4343-1a97-4b44-b347-803be47ace80/user/cgroup.procs: No such file or directory
../../src/tests/containerizer/isolator_tests.cpp:1316: Failure
Value of: os::system( ""su - "" + UNPRIVILEGED_USERNAME + "" -c 'echo $$ >"" + path::join(flags.cgroups_hierarchy, userCgroup, ""cgroup.procs"") + ""'"")
  Actual: 256
Expected: 0
-bash: /sys/fs/cgroup/cpu/mesos/e57f4343-1a97-4b44-b347-803be47ace80/cgroup.procs: No such file or directory
mkdir: cannot create directory `/sys/fs/cgroup/cpu/mesos/e57f4343-1a97-4b44-b347-803be47ace80/user': No such file or directory
../../src/tests/containerizer/isolator_tests.cpp:1307: Failure
Value of: os::system( ""su - "" + UNPRIVILEGED_USERNAME + "" -c 'mkdir "" + path::join(flags.cgroups_hierarchy, userCgroup) + ""'"")
  Actual: 256
Expected: 0
-bash: /sys/fs/cgroup/cpu/mesos/e57f4343-1a97-4b44-b347-803be47ace80/user/cgroup.procs: No such file or directory
../../src/tests/containerizer/isolator_tests.cpp:1316: Failure
Value of: os::system( ""su - "" + UNPRIVILEGED_USERNAME + "" -c 'echo $$ >"" + path::join(flags.cgroups_hierarchy, userCgroup, ""cgroup.procs"") + ""'"")
  Actual: 256
Expected: 0
[  FAILED  ] UserCgroupIsolatorTest/1.ROOT_CGROUPS_UserCgroup, where TypeParam = mesos::internal::slave::CgroupsCpushareIsolatorProcess (116 ms)
{noformat}",Bug,Major,bernd-mesos,2015-11-24T08:40:46.000+0000,5,Resolved,Complete,User CGroup Isolation tests fail on Centos 6.,2015-11-24T08:44:32.000+0000,MESOS-3949,3.0,mesos,Mesosphere Sprint 23
gradywang,2015-11-18T08:39:55.000+0000,JamesYongQiaoWang,"This JIRA ticket will update the related doc to apply to dynamic weights, and add an new operator guide for dynamic weights which describes basic usage of the /weights endpoint.",Task,Major,JamesYongQiaoWang,2016-03-09T08:58:36.000+0000,5,Resolved,Complete,Add operator documentation for /weight endpoint,2016-03-09T08:58:36.000+0000,MESOS-3945,2.0,mesos,Mesosphere Sprint 29
gradywang,2015-11-18T05:24:22.000+0000,JamesYongQiaoWang,This JIRA will focus on update the allocator API to support weight update of a role.,Task,Major,JamesYongQiaoWang,2016-02-19T09:18:44.000+0000,5,Resolved,Complete,Support dynamic weight in allocator,2016-02-27T13:02:36.000+0000,MESOS-3943,5.0,mesos,Mesosphere Sprint 26
greggomann,2015-11-18T00:46:38.000+0000,mcypark,"Currently, the {{/reserve}} and {{/unreserve}} endpoints do not work without authentication enabled on the master. When authentication is disabled on the master, these endpoints should just be permissive.",Bug,Major,mcypark,2016-02-23T03:07:43.000+0000,5,Resolved,Complete,/reserve and /unreserve should be permissive under a master without authentication.,2016-02-26T23:24:39.000+0000,MESOS-3940,1.0,mesos,Mesosphere Sprint 29
neilc,2015-11-17T17:15:30.000+0000,neilc,"Running ubsan from GCC 5.2 on the current Mesos unit tests yields this, among other problems:

{noformat}
/mesos/3rdparty/libprocess/3rdparty/stout/include/stout/ip.hpp:230:56: runtime error: reference binding to misaligned address 0x00000199629c for type 'const struct sockaddr_storage', which requires 8 byte alignment
0x00000199629c: note: pointer points here
  00 00 00 00 02 00 00 00  ff ff ff 00 00 00 00 00  00 00 00 00 00 00 00 00  00 00 00 00 00 00 00 00
              ^
    #0 0x5950cb in net::IP::create(sockaddr const&) (/home/vagrant/build-mesos-ubsan/3rdparty/libprocess/3rdparty/stout-tests+0x5950cb)
    #1 0x5970cd in net::IPNetwork::fromLinkDevice(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, int) (/home/vagrant/build-mesos-ubsan/3rdparty/libprocess/3rdparty/stout-tests+0x5970cd)
    #2 0x58e006 in NetTest_LinkDevice_Test::TestBody() (/home/vagrant/build-mesos-ubsan/3rdparty/libprocess/3rdparty/stout-tests+0x58e006)
    #3 0x85abd5 in void testing::internal::HandleSehExceptionsInMethodIfSupported<testing::Test, void>(testing::Test*, void (testing::Test::*)(), char const*) (/home/vagrant/build-mesos-ubsan/3rdparty/libprocess/3rdparty/stout-tests+0x85abd5)
    #4 0x848abc in void testing::internal::HandleExceptionsInMethodIfSupported<testing::Test, void>(testing::Test*, void (testing::Test::*)(), char const*) (/home/vagrant/build-mesos-ubsan/3rdparty/libprocess/3rdparty/stout-tests+0x848abc)
    #5 0x7e2755 in testing::Test::Run() (/home/vagrant/build-mesos-ubsan/3rdparty/libprocess/3rdparty/stout-tests+0x7e2755)
    #6 0x7e44a0 in testing::TestInfo::Run() (/home/vagrant/build-mesos-ubsan/3rdparty/libprocess/3rdparty/stout-tests+0x7e44a0)
    #7 0x7e5ffa in testing::TestCase::Run() (/home/vagrant/build-mesos-ubsan/3rdparty/libprocess/3rdparty/stout-tests+0x7e5ffa)
    #8 0x7ffe21 in testing::internal::UnitTestImpl::RunAllTests() (/home/vagrant/build-mesos-ubsan/3rdparty/libprocess/3rdparty/stout-tests+0x7ffe21)
    #9 0x85d7a5 in bool testing::internal::HandleSehExceptionsInMethodIfSupported<testing::internal::UnitTestImpl, bool>(testing::internal::UnitTestImpl*, bool (testing::internal::UnitTestImpl::*)(), char const*) (/home/vagrant/build-mesos-ubsan/3rdparty/libprocess/3rdparty/stout-tests+0x85d7a5)
    #10 0x84b37a in bool testing::internal::HandleExceptionsInMethodIfSupported<testing::internal::UnitTestImpl, bool>(testing::internal::UnitTestImpl*, bool (testing::internal::UnitTestImpl::*)(), char const*) (/home/vagrant/build-mesos-ubsan/3rdparty/libprocess/3rdparty/stout-tests+0x84b37a)
    #11 0x7f8a4a in testing::UnitTest::Run() (/home/vagrant/build-mesos-ubsan/3rdparty/libprocess/3rdparty/stout-tests+0x7f8a4a)
    #12 0x608a96 in RUN_ALL_TESTS() (/home/vagrant/build-mesos-ubsan/3rdparty/libprocess/3rdparty/stout-tests+0x608a96)
    #13 0x60896b in main (/home/vagrant/build-mesos-ubsan/3rdparty/libprocess/3rdparty/stout-tests+0x60896b)
    #14 0x7fd0f0c7fa3f in __libc_start_main (/lib/x86_64-linux-gnu/libc.so.6+0x20a3f)
    #15 0x4145c8 in _start (/home/vagrant/build-mesos-ubsan/3rdparty/libprocess/3rdparty/stout-tests+0x4145c8)
{noformat}",Bug,Minor,neilc,2015-12-03T08:39:45.000+0000,5,Resolved,Complete,ubsan error in net::IP::create(sockaddr const&): misaligned address,2015-12-03T08:40:50.000+0000,MESOS-3939,2.0,mesos,
alexr,2015-11-17T15:18:51.000+0000,alexr,"Investigate use cases and implications of the possibility to set quota for the '*' role. For example, having quota for '*' set can effectively reduce the scope of the quota capacity heuristic.",Task,Major,alexr,,10020,Accepted,In Progress,Allow setting quotas for the default '*' role,2015-11-20T16:06:49.000+0000,MESOS-3938,3.0,mesos,
nfnt,2015-11-17T15:11:55.000+0000,bernd-mesos,"{noformat}
../configure
make check
sudo ./bin/mesos-tests.sh --gtest_filter=""DockerContainerizerTest.ROOT_DOCKER_Launch_Executor"" --verbose
{noformat}

{noformat}
[==========] Running 1 test from 1 test case.
[----------] Global test environment set-up.
[----------] 1 test from DockerContainerizerTest
I1117 15:08:09.265943 26380 leveldb.cpp:176] Opened db in 3.199666ms
I1117 15:08:09.267761 26380 leveldb.cpp:183] Compacted db in 1.684873ms
I1117 15:08:09.267902 26380 leveldb.cpp:198] Created db iterator in 58313ns
I1117 15:08:09.267966 26380 leveldb.cpp:204] Seeked to beginning of db in 4927ns
I1117 15:08:09.267997 26380 leveldb.cpp:273] Iterated through 0 keys in the db in 1605ns
I1117 15:08:09.268156 26380 replica.cpp:780] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned
I1117 15:08:09.270148 26396 recover.cpp:449] Starting replica recovery
I1117 15:08:09.272105 26396 recover.cpp:475] Replica is in EMPTY status
I1117 15:08:09.275640 26396 replica.cpp:676] Replica in EMPTY status received a broadcasted recover request from (4)@10.0.2.15:50088
I1117 15:08:09.276578 26399 recover.cpp:195] Received a recover response from a replica in EMPTY status
I1117 15:08:09.277600 26397 recover.cpp:566] Updating replica status to STARTING
I1117 15:08:09.279613 26396 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 1.016098ms
I1117 15:08:09.279731 26396 replica.cpp:323] Persisted replica status to STARTING
I1117 15:08:09.280306 26399 recover.cpp:475] Replica is in STARTING status
I1117 15:08:09.282181 26400 replica.cpp:676] Replica in STARTING status received a broadcasted recover request from (5)@10.0.2.15:50088
I1117 15:08:09.282552 26400 master.cpp:367] Master 59c600f1-92ff-4926-9c84-073d9b81f68a (vagrant-ubuntu-trusty-64) started on 10.0.2.15:50088
I1117 15:08:09.283021 26400 master.cpp:369] Flags at startup: --acls="""" --allocation_interval=""1secs"" --allocator=""HierarchicalDRF"" --authenticate=""true"" --authenticate_slaves=""true"" --authenticators=""crammd5"" --authorizers=""local"" --credentials=""/tmp/40AlT8/credentials"" --framework_sorter=""drf"" --help=""false"" --hostname_lookup=""true"" --initialize_driver_logging=""true"" --log_auto_initialize=""true"" --logbufsecs=""0"" --logging_level=""INFO"" --max_slave_ping_timeouts=""5"" --quiet=""false"" --recovery_slave_removal_limit=""100%"" --registry=""replicated_log"" --registry_fetch_timeout=""1mins"" --registry_store_timeout=""25secs"" --registry_strict=""true"" --root_submissions=""true"" --slave_ping_timeout=""15secs"" --slave_reregister_timeout=""10mins"" --user_sorter=""drf"" --version=""false"" --webui_dir=""/usr/local/share/mesos/webui"" --work_dir=""/tmp/40AlT8/master"" --zk_session_timeout=""10secs""
I1117 15:08:09.283920 26400 master.cpp:414] Master only allowing authenticated frameworks to register
I1117 15:08:09.283972 26400 master.cpp:419] Master only allowing authenticated slaves to register
I1117 15:08:09.284032 26400 credentials.hpp:37] Loading credentials for authentication from '/tmp/40AlT8/credentials'
I1117 15:08:09.282944 26401 recover.cpp:195] Received a recover response from a replica in STARTING status
I1117 15:08:09.284639 26401 recover.cpp:566] Updating replica status to VOTING
I1117 15:08:09.285539 26400 master.cpp:458] Using default 'crammd5' authenticator
I1117 15:08:09.285995 26401 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 1.075466ms
I1117 15:08:09.286062 26401 replica.cpp:323] Persisted replica status to VOTING
I1117 15:08:09.286200 26401 recover.cpp:580] Successfully joined the Paxos group
I1117 15:08:09.286471 26401 recover.cpp:464] Recover process terminated
I1117 15:08:09.287303 26400 authenticator.cpp:520] Initializing server SASL
I1117 15:08:09.289371 26400 master.cpp:495] Authorization enabled
I1117 15:08:09.296018 26399 master.cpp:1606] The newly elected leader is master@10.0.2.15:50088 with id 59c600f1-92ff-4926-9c84-073d9b81f68a
I1117 15:08:09.296115 26399 master.cpp:1619] Elected as the leading master!
I1117 15:08:09.296187 26399 master.cpp:1379] Recovering from registrar
I1117 15:08:09.296717 26397 registrar.cpp:309] Recovering registrar
I1117 15:08:09.298842 26396 log.cpp:661] Attempting to start the writer
I1117 15:08:09.301563 26394 replica.cpp:496] Replica received implicit promise request from (6)@10.0.2.15:50088 with proposal 1
I1117 15:08:09.302561 26394 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 922719ns
I1117 15:08:09.302635 26394 replica.cpp:345] Persisted promised to 1
I1117 15:08:09.303755 26394 coordinator.cpp:240] Coordinator attempting to fill missing positions
I1117 15:08:09.306161 26394 replica.cpp:391] Replica received explicit promise request from (7)@10.0.2.15:50088 for position 0 with proposal 2
I1117 15:08:09.306972 26394 leveldb.cpp:343] Persisting action (8 bytes) to leveldb took 711749ns
I1117 15:08:09.307034 26394 replica.cpp:715] Persisted action at 0
I1117 15:08:09.308732 26401 replica.cpp:540] Replica received write request for position 0 from (8)@10.0.2.15:50088
I1117 15:08:09.308830 26401 leveldb.cpp:438] Reading position from leveldb took 46444ns
I1117 15:08:09.309710 26401 leveldb.cpp:343] Persisting action (14 bytes) to leveldb took 779098ns
I1117 15:08:09.309754 26401 replica.cpp:715] Persisted action at 0
I1117 15:08:09.311007 26397 replica.cpp:694] Replica received learned notice for position 0 from @0.0.0.0:0
I1117 15:08:09.311652 26397 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 567289ns
I1117 15:08:09.311731 26397 replica.cpp:715] Persisted action at 0
I1117 15:08:09.311771 26397 replica.cpp:700] Replica learned NOP action at position 0
I1117 15:08:09.313212 26397 log.cpp:677] Writer started with ending position 0
I1117 15:08:09.315682 26399 leveldb.cpp:438] Reading position from leveldb took 27974ns
I1117 15:08:09.318694 26395 registrar.cpp:342] Successfully fetched the registry (0B) in 21.862144ms
I1117 15:08:09.319007 26395 registrar.cpp:441] Applied 1 operations in 91867ns; attempting to update the 'registry'
I1117 15:08:09.321730 26395 log.cpp:685] Attempting to append 193 bytes to the log
I1117 15:08:09.321935 26397 coordinator.cpp:350] Coordinator attempting to write APPEND action at position 1
I1117 15:08:09.323103 26399 replica.cpp:540] Replica received write request for position 1 from (9)@10.0.2.15:50088
I1117 15:08:09.323917 26399 leveldb.cpp:343] Persisting action (212 bytes) to leveldb took 735223ns
I1117 15:08:09.323983 26399 replica.cpp:715] Persisted action at 1
I1117 15:08:09.324975 26398 replica.cpp:694] Replica received learned notice for position 1 from @0.0.0.0:0
I1117 15:08:09.325695 26398 leveldb.cpp:343] Persisting action (214 bytes) to leveldb took 668268ns
I1117 15:08:09.325741 26398 replica.cpp:715] Persisted action at 1
I1117 15:08:09.325778 26398 replica.cpp:700] Replica learned APPEND action at position 1
I1117 15:08:09.327258 26396 registrar.cpp:486] Successfully updated the 'registry' in 8.090112ms
I1117 15:08:09.327525 26396 registrar.cpp:372] Successfully recovered registrar
I1117 15:08:09.328083 26400 log.cpp:704] Attempting to truncate the log to 1
I1117 15:08:09.328251 26394 master.cpp:1416] Recovered 0 slaves from the Registry (154B) ; allowing 10mins for slaves to re-register
I1117 15:08:09.328814 26396 coordinator.cpp:350] Coordinator attempting to write TRUNCATE action at position 2
I1117 15:08:09.330158 26401 replica.cpp:540] Replica received write request for position 2 from (10)@10.0.2.15:50088
I1117 15:08:09.330994 26401 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 760471ns
I1117 15:08:09.331055 26401 replica.cpp:715] Persisted action at 2
I1117 15:08:09.331583 26401 replica.cpp:694] Replica received learned notice for position 2 from @0.0.0.0:0
I1117 15:08:09.332172 26401 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 497457ns
I1117 15:08:09.332500 26401 leveldb.cpp:401] Deleting ~1 keys from leveldb took 49327ns
I1117 15:08:09.332715 26401 replica.cpp:715] Persisted action at 2
I1117 15:08:09.332964 26401 replica.cpp:700] Replica learned TRUNCATE action at position 2
I1117 15:08:09.354073 26401 slave.cpp:191] Slave started on 1)@10.0.2.15:50088
I1117 15:08:09.354316 26401 slave.cpp:192] Flags at startup: --appc_store_dir=""/tmp/mesos/store/appc"" --authenticatee=""crammd5"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --credential=""/tmp/DockerContainerizerTest_ROOT_DOCKER_Launch_Executor_HaKhAQ/credential"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_auth_server=""auth.docker.io"" --docker_auth_server_port=""443"" --docker_kill_orphans=""true"" --docker_local_archives_dir=""/tmp/mesos/images/docker"" --docker_puller=""local"" --docker_puller_timeout=""60"" --docker_registry=""registry-1.docker.io"" --docker_registry_port=""443"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/tmp/mesos/store/docker"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/tmp/DockerContainerizerTest_ROOT_DOCKER_Launch_Executor_HaKhAQ/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --hostname_lookup=""true"" --image_provisioner_backend=""copy"" --initialize_driver_logging=""true"" --isolation=""posix/cpu,posix/mem"" --launcher_dir=""/home/vagrant/mesos/build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""10ms"" --resources=""cpus:2;mem:1024;disk:1024;ports:[31000-32000]"" --revocable_cpu_low_priority=""true"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --systemd_runtime_directory=""/run/systemd/system"" --version=""false"" --work_dir=""/tmp/DockerContainerizerTest_ROOT_DOCKER_Launch_Executor_HaKhAQ""
I1117 15:08:09.355077 26401 credentials.hpp:85] Loading credential for authentication from '/tmp/DockerContainerizerTest_ROOT_DOCKER_Launch_Executor_HaKhAQ/credential'
I1117 15:08:09.355587 26401 slave.cpp:322] Slave using credential for: test-principal
I1117 15:08:09.357144 26401 slave.cpp:392] Slave resources: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I1117 15:08:09.357477 26401 slave.cpp:400] Slave attributes: [  ]
I1117 15:08:09.357719 26401 slave.cpp:405] Slave hostname: vagrant-ubuntu-trusty-64
I1117 15:08:09.357936 26380 sched.cpp:166] Version: 0.26.0
I1117 15:08:09.358010 26401 slave.cpp:410] Slave checkpoint: true
I1117 15:08:09.359058 26400 sched.cpp:264] New master detected at master@10.0.2.15:50088
I1117 15:08:09.359216 26400 sched.cpp:320] Authenticating with master master@10.0.2.15:50088
I1117 15:08:09.359277 26400 sched.cpp:327] Using default CRAM-MD5 authenticatee
I1117 15:08:09.359856 26400 authenticatee.cpp:99] Initializing client SASL
I1117 15:08:09.360539 26400 authenticatee.cpp:123] Creating new client SASL connection
I1117 15:08:09.361399 26398 state.cpp:54] Recovering state from '/tmp/DockerContainerizerTest_ROOT_DOCKER_Launch_Executor_HaKhAQ/meta'
I1117 15:08:09.361994 26398 status_update_manager.cpp:202] Recovering status update manager
I1117 15:08:09.362191 26395 master.cpp:5150] Authenticating scheduler-38aa807a-672a-4e1e-b823-71f119980e86@10.0.2.15:50088
I1117 15:08:09.362565 26401 docker.cpp:536] Recovering Docker containers
I1117 15:08:09.362908 26395 authenticator.cpp:100] Creating new server SASL connection
I1117 15:08:09.363533 26401 slave.cpp:4230] Finished recovery
I1117 15:08:09.363675 26394 authenticatee.cpp:214] Received SASL authentication mechanisms: CRAM-MD5
I1117 15:08:09.363950 26394 authenticatee.cpp:240] Attempting to authenticate with mechanism 'CRAM-MD5'
I1117 15:08:09.364137 26394 authenticator.cpp:205] Received SASL authentication start
I1117 15:08:09.364241 26394 authenticator.cpp:327] Authentication requires more steps
I1117 15:08:09.364481 26394 authenticatee.cpp:260] Received SASL authentication step
I1117 15:08:09.364667 26394 authenticator.cpp:233] Received SASL authentication step
I1117 15:08:09.364828 26394 authenticator.cpp:319] Authentication success
I1117 15:08:09.365039 26398 authenticatee.cpp:300] Authentication success
I1117 15:08:09.365170 26398 master.cpp:5180] Successfully authenticated principal 'test-principal' at scheduler-38aa807a-672a-4e1e-b823-71f119980e86@10.0.2.15:50088
I1117 15:08:09.365656 26398 sched.cpp:409] Successfully authenticated with master master@10.0.2.15:50088
I1117 15:08:09.366044 26401 slave.cpp:729] New master detected at master@10.0.2.15:50088
I1117 15:08:09.366283 26398 master.cpp:2176] Received SUBSCRIBE call for framework 'default' at scheduler-38aa807a-672a-4e1e-b823-71f119980e86@10.0.2.15:50088
I1117 15:08:09.366317 26401 slave.cpp:792] Authenticating with master master@10.0.2.15:50088
I1117 15:08:09.366688 26401 slave.cpp:797] Using default CRAM-MD5 authenticatee
I1117 15:08:09.366525 26395 status_update_manager.cpp:176] Pausing sending status updates
I1117 15:08:09.366442 26398 master.cpp:1645] Authorizing framework principal 'test-principal' to receive offers for role '*'
I1117 15:08:09.367207 26401 slave.cpp:765] Detecting new master
I1117 15:08:09.367496 26395 master.cpp:2247] Subscribing framework default with checkpointing disabled and capabilities [  ]
I1117 15:08:09.368417 26396 hierarchical.cpp:195] Added framework 59c600f1-92ff-4926-9c84-073d9b81f68a-0000
I1117 15:08:09.367250 26398 authenticatee.cpp:123] Creating new client SASL connection
I1117 15:08:09.368506 26395 sched.cpp:643] Framework registered with 59c600f1-92ff-4926-9c84-073d9b81f68a-0000
I1117 15:08:09.369287 26398 master.cpp:5150] Authenticating slave(1)@10.0.2.15:50088
I1117 15:08:09.370213 26401 authenticator.cpp:100] Creating new server SASL connection
I1117 15:08:09.370846 26396 authenticatee.cpp:214] Received SASL authentication mechanisms: CRAM-MD5
I1117 15:08:09.370964 26396 authenticatee.cpp:240] Attempting to authenticate with mechanism 'CRAM-MD5'
I1117 15:08:09.371233 26396 authenticator.cpp:205] Received SASL authentication start
I1117 15:08:09.371387 26396 authenticator.cpp:327] Authentication requires more steps
I1117 15:08:09.371707 26398 authenticatee.cpp:260] Received SASL authentication step
I1117 15:08:09.371835 26398 authenticator.cpp:233] Received SASL authentication step
I1117 15:08:09.371944 26398 authenticator.cpp:319] Authentication success
I1117 15:08:09.372195 26396 authenticatee.cpp:300] Authentication success
I1117 15:08:09.372248 26398 master.cpp:5180] Successfully authenticated principal 'test-principal' at slave(1)@10.0.2.15:50088
I1117 15:08:09.373002 26396 slave.cpp:860] Successfully authenticated with master master@10.0.2.15:50088
I1117 15:08:09.373566 26398 master.cpp:3859] Registering slave at slave(1)@10.0.2.15:50088 (vagrant-ubuntu-trusty-64) with id 59c600f1-92ff-4926-9c84-073d9b81f68a-S0
I1117 15:08:09.374301 26401 registrar.cpp:441] Applied 1 operations in 65094ns; attempting to update the 'registry'
I1117 15:08:09.376809 26400 log.cpp:685] Attempting to append 374 bytes to the log
I1117 15:08:09.376994 26399 coordinator.cpp:350] Coordinator attempting to write APPEND action at position 3
I1117 15:08:09.377960 26397 replica.cpp:540] Replica received write request for position 3 from (16)@10.0.2.15:50088
I1117 15:08:09.378844 26397 leveldb.cpp:343] Persisting action (393 bytes) to leveldb took 805302ns
I1117 15:08:09.378904 26397 replica.cpp:715] Persisted action at 3
I1117 15:08:09.379823 26400 replica.cpp:694] Replica received learned notice for position 3 from @0.0.0.0:0
I1117 15:08:09.380592 26400 leveldb.cpp:343] Persisting action (395 bytes) to leveldb took 691729ns
I1117 15:08:09.380666 26400 replica.cpp:715] Persisted action at 3
I1117 15:08:09.380702 26400 replica.cpp:700] Replica learned APPEND action at position 3
I1117 15:08:09.382014 26398 registrar.cpp:486] Successfully updated the 'registry' in 7.384064ms
I1117 15:08:09.382184 26400 log.cpp:704] Attempting to truncate the log to 3
I1117 15:08:09.382380 26398 coordinator.cpp:350] Coordinator attempting to write TRUNCATE action at position 4
I1117 15:08:09.383361 26399 master.cpp:3927] Registered slave 59c600f1-92ff-4926-9c84-073d9b81f68a-S0 at slave(1)@10.0.2.15:50088 (vagrant-ubuntu-trusty-64) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I1117 15:08:09.383437 26396 slave.cpp:904] Registered with master master@10.0.2.15:50088; given slave ID 59c600f1-92ff-4926-9c84-073d9b81f68a-S0
I1117 15:08:09.383741 26400 status_update_manager.cpp:183] Resuming sending status updates
I1117 15:08:09.384004 26401 hierarchical.cpp:344] Added slave 59c600f1-92ff-4926-9c84-073d9b81f68a-S0 (vagrant-ubuntu-trusty-64) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (allocated: )
I1117 15:08:09.384101 26396 slave.cpp:963] Forwarding total oversubscribed resources 
I1117 15:08:09.384831 26396 master.cpp:4269] Received update of slave 59c600f1-92ff-4926-9c84-073d9b81f68a-S0 at slave(1)@10.0.2.15:50088 (vagrant-ubuntu-trusty-64) with total oversubscribed resources 
I1117 15:08:09.384466 26398 replica.cpp:540] Replica received write request for position 4 from (17)@10.0.2.15:50088
I1117 15:08:09.385957 26397 master.cpp:4979] Sending 1 offers to framework 59c600f1-92ff-4926-9c84-073d9b81f68a-0000 (default) at scheduler-38aa807a-672a-4e1e-b823-71f119980e86@10.0.2.15:50088
I1117 15:08:09.386066 26401 hierarchical.cpp:400] Slave 59c600f1-92ff-4926-9c84-073d9b81f68a-S0 (vagrant-ubuntu-trusty-64) updated with oversubscribed resources  (total: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000], allocated: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000])
I1117 15:08:09.386219 26398 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 605641ns
I1117 15:08:09.386445 26398 replica.cpp:715] Persisted action at 4
I1117 15:08:09.388450 26397 replica.cpp:694] Replica received learned notice for position 4 from @0.0.0.0:0
I1117 15:08:09.389235 26397 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 715846ns
I1117 15:08:09.389345 26397 leveldb.cpp:401] Deleting ~2 keys from leveldb took 40455ns
I1117 15:08:09.389402 26397 replica.cpp:715] Persisted action at 4
I1117 15:08:09.389464 26397 replica.cpp:700] Replica learned TRUNCATE action at position 4
I1117 15:08:09.390585 26394 master.cpp:2915] Processing ACCEPT call for offers: [ 59c600f1-92ff-4926-9c84-073d9b81f68a-O0 ] on slave 59c600f1-92ff-4926-9c84-073d9b81f68a-S0 at slave(1)@10.0.2.15:50088 (vagrant-ubuntu-trusty-64) for framework 59c600f1-92ff-4926-9c84-073d9b81f68a-0000 (default) at scheduler-38aa807a-672a-4e1e-b823-71f119980e86@10.0.2.15:50088
I1117 15:08:09.390805 26394 master.cpp:2711] Authorizing framework principal 'test-principal' to launch task 1 as user 'root'
W1117 15:08:09.393517 26396 validation.cpp:422] Executor e1 for task 1 uses less CPUs (None) than the minimum required (0.01). Please update your executor, as this will be mandatory in future releases.
W1117 15:08:09.393632 26396 validation.cpp:434] Executor e1 for task 1 uses less memory (None) than the minimum required (32MB). Please update your executor, as this will be mandatory in future releases.
I1117 15:08:09.394270 26396 master.hpp:176] Adding task 1 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave 59c600f1-92ff-4926-9c84-073d9b81f68a-S0 (vagrant-ubuntu-trusty-64)
I1117 15:08:09.394580 26396 master.cpp:3245] Launching task 1 of framework 59c600f1-92ff-4926-9c84-073d9b81f68a-0000 (default) at scheduler-38aa807a-672a-4e1e-b823-71f119980e86@10.0.2.15:50088 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave 59c600f1-92ff-4926-9c84-073d9b81f68a-S0 at slave(1)@10.0.2.15:50088 (vagrant-ubuntu-trusty-64)
I1117 15:08:09.395247 26396 slave.cpp:1294] Got assigned task 1 for framework 59c600f1-92ff-4926-9c84-073d9b81f68a-0000
I1117 15:08:09.396234 26396 slave.cpp:1410] Launching task 1 for framework 59c600f1-92ff-4926-9c84-073d9b81f68a-0000
I1117 15:08:09.398111 26396 paths.cpp:436] Trying to chown '/tmp/DockerContainerizerTest_ROOT_DOCKER_Launch_Executor_HaKhAQ/slaves/59c600f1-92ff-4926-9c84-073d9b81f68a-S0/frameworks/59c600f1-92ff-4926-9c84-073d9b81f68a-0000/executors/e1/runs/e00106a7-b249-41c7-bff1-739485fe2d15' to user 'root'
I1117 15:08:09.402022 26396 slave.cpp:4999] Launching executor e1 of framework 59c600f1-92ff-4926-9c84-073d9b81f68a-0000 with resources  in work directory '/tmp/DockerContainerizerTest_ROOT_DOCKER_Launch_Executor_HaKhAQ/slaves/59c600f1-92ff-4926-9c84-073d9b81f68a-S0/frameworks/59c600f1-92ff-4926-9c84-073d9b81f68a-0000/executors/e1/runs/e00106a7-b249-41c7-bff1-739485fe2d15'
I1117 15:08:09.405066 26396 slave.cpp:1628] Queuing task '1' for executor 'e1' of framework 59c600f1-92ff-4926-9c84-073d9b81f68a-0000
I1117 15:08:09.407642 26399 docker.cpp:767] Starting container 'e00106a7-b249-41c7-bff1-739485fe2d15' for executor 'e1' and framework '59c600f1-92ff-4926-9c84-073d9b81f68a-0000'
E1117 15:08:09.976763 26396 slave.cpp:3440] Container 'e00106a7-b249-41c7-bff1-739485fe2d15' for executor 'e1' of framework 59c600f1-92ff-4926-9c84-073d9b81f68a-0000 failed to start: Container exited on error: exited with status 255
I1117 15:08:09.977694 26400 slave.cpp:3546] Executor 'e1' of framework 59c600f1-92ff-4926-9c84-073d9b81f68a-0000 has terminated with unknown status
I1117 15:08:09.979370 26400 slave.cpp:2762] Handling status update TASK_FAILED (UUID: c414cb06-6240-4893-b6ae-c2adec5258c3) for task 1 of framework 59c600f1-92ff-4926-9c84-073d9b81f68a-0000 from @0.0.0.0:0
W1117 15:08:09.980294 26400 docker.cpp:998] Ignoring updating unknown container: e00106a7-b249-41c7-bff1-739485fe2d15
I1117 15:08:09.980996 26400 master.cpp:4516] Executor 'e1' of framework 59c600f1-92ff-4926-9c84-073d9b81f68a-0000 on slave 59c600f1-92ff-4926-9c84-073d9b81f68a-S0 at slave(1)@10.0.2.15:50088 (vagrant-ubuntu-trusty-64): terminated with signal Unknown signal 127
I1117 15:08:09.981009 26401 status_update_manager.cpp:322] Received status update TASK_FAILED (UUID: c414cb06-6240-4893-b6ae-c2adec5258c3) for task 1 of framework 59c600f1-92ff-4926-9c84-073d9b81f68a-0000
I1117 15:08:09.981204 26400 master.cpp:6161] Removing executor 'e1' with resources  of framework 59c600f1-92ff-4926-9c84-073d9b81f68a-0000 on slave 59c600f1-92ff-4926-9c84-073d9b81f68a-S0 at slave(1)@10.0.2.15:50088 (vagrant-ubuntu-trusty-64)
I1117 15:08:09.982161 26401 slave.cpp:3087] Forwarding the update TASK_FAILED (UUID: c414cb06-6240-4893-b6ae-c2adec5258c3) for task 1 of framework 59c600f1-92ff-4926-9c84-073d9b81f68a-0000 to master@10.0.2.15:50088
I1117 15:08:09.982632 26401 master.cpp:4414] Status update TASK_FAILED (UUID: c414cb06-6240-4893-b6ae-c2adec5258c3) for task 1 of framework 59c600f1-92ff-4926-9c84-073d9b81f68a-0000 from slave 59c600f1-92ff-4926-9c84-073d9b81f68a-S0 at slave(1)@10.0.2.15:50088 (vagrant-ubuntu-trusty-64)
I1117 15:08:09.982789 26401 master.cpp:4462] Forwarding status update TASK_FAILED (UUID: c414cb06-6240-4893-b6ae-c2adec5258c3) for task 1 of framework 59c600f1-92ff-4926-9c84-073d9b81f68a-0000
I1117 15:08:09.983026 26401 master.cpp:6066] Updating the state of task 1 of framework 59c600f1-92ff-4926-9c84-073d9b81f68a-0000 (latest state: TASK_FAILED, status update state: TASK_FAILED)
../../src/tests/containerizer/docker_containerizer_tests.cpp:255: Failure
Value of: statusRunning.get().state()
  Actual: TASK_FAILED
Expected: TASK_RUNNING
I1117 15:08:09.985121 26395 hierarchical.cpp:744] Recovered cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (total: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000], allocated: ) on slave 59c600f1-92ff-4926-9c84-073d9b81f68a-S0 from framework 59c600f1-92ff-4926-9c84-073d9b81f68a-0000
I1117 15:08:09.985277 26401 master.cpp:3571] Processing ACKNOWLEDGE call c414cb06-6240-4893-b6ae-c2adec5258c3 for task 1 of framework 59c600f1-92ff-4926-9c84-073d9b81f68a-0000 (default) at scheduler-38aa807a-672a-4e1e-b823-71f119980e86@10.0.2.15:50088 on slave 59c600f1-92ff-4926-9c84-073d9b81f68a-S0
I1117 15:08:09.985471 26401 master.cpp:6132] Removing task 1 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] of framework 59c600f1-92ff-4926-9c84-073d9b81f68a-0000 on slave 59c600f1-92ff-4926-9c84-073d9b81f68a-S0 at slave(1)@10.0.2.15:50088 (vagrant-ubuntu-trusty-64)
I1117 15:08:09.986150 26394 status_update_manager.cpp:394] Received status update acknowledgement (UUID: c414cb06-6240-4893-b6ae-c2adec5258c3) for task 1 of framework 59c600f1-92ff-4926-9c84-073d9b81f68a-0000
I1117 15:08:09.986804 26394 slave.cpp:3657] Cleaning up executor 'e1' of framework 59c600f1-92ff-4926-9c84-073d9b81f68a-0000
I1117 15:08:09.987223 26401 gc.cpp:56] Scheduling '/tmp/DockerContainerizerTest_ROOT_DOCKER_Launch_Executor_HaKhAQ/slaves/59c600f1-92ff-4926-9c84-073d9b81f68a-S0/frameworks/59c600f1-92ff-4926-9c84-073d9b81f68a-0000/executors/e1/runs/e00106a7-b249-41c7-bff1-739485fe2d15' for gc 6.99998857599407days in the future
I1117 15:08:09.987319 26394 slave.cpp:3745] Cleaning up framework 59c600f1-92ff-4926-9c84-073d9b81f68a-0000
I1117 15:08:09.987622 26397 status_update_manager.cpp:284] Closing status update streams for framework 59c600f1-92ff-4926-9c84-073d9b81f68a-0000
I1117 15:08:09.987833 26401 gc.cpp:56] Scheduling '/tmp/DockerContainerizerTest_ROOT_DOCKER_Launch_Executor_HaKhAQ/slaves/59c600f1-92ff-4926-9c84-073d9b81f68a-S0/frameworks/59c600f1-92ff-4926-9c84-073d9b81f68a-0000/executors/e1' for gc 6.99998857381037days in the future
I1117 15:08:09.987995 26401 gc.cpp:56] Scheduling '/tmp/DockerContainerizerTest_ROOT_DOCKER_Launch_Executor_HaKhAQ/slaves/59c600f1-92ff-4926-9c84-073d9b81f68a-S0/frameworks/59c600f1-92ff-4926-9c84-073d9b81f68a-0000' for gc 6.99998856968593days in the future
I1117 15:08:10.300747 26395 master.cpp:4979] Sending 1 offers to framework 59c600f1-92ff-4926-9c84-073d9b81f68a-0000 (default) at scheduler-38aa807a-672a-4e1e-b823-71f119980e86@10.0.2.15:50088
I1117 15:09:09.400656 26395 slave.cpp:4039] Current disk usage 14.14%. Max allowed age: 5.310351133117882days
I1117 15:09:09.848752 26396 slave.cpp:3947] Framework 59c600f1-92ff-4926-9c84-073d9b81f68a-0000 seems to have exited. Ignoring registration timeout for executor 'e1'
../../src/tests/containerizer/docker_containerizer_tests.cpp:256: Failure
Failed to wait 1mins for statusFinished
../../src/tests/containerizer/docker_containerizer_tests.cpp:247: Failure
Actual function call count doesn't match EXPECT_CALL(sched, statusUpdate(&driver, _))...
         Expected: to be called twice
           Actual: called once - unsatisfied and active
I1117 15:09:09.993629 26396 master.cpp:1122] Framework 59c600f1-92ff-4926-9c84-073d9b81f68a-0000 (default) at scheduler-38aa807a-672a-4e1e-b823-71f119980e86@10.0.2.15:50088 disconnected
I1117 15:09:09.993923 26396 master.cpp:2472] Disconnecting framework 59c600f1-92ff-4926-9c84-073d9b81f68a-0000 (default) at scheduler-38aa807a-672a-4e1e-b823-71f119980e86@10.0.2.15:50088
I1117 15:09:09.993984 26396 master.cpp:2496] Deactivating framework 59c600f1-92ff-4926-9c84-073d9b81f68a-0000 (default) at scheduler-38aa807a-672a-4e1e-b823-71f119980e86@10.0.2.15:50088
W1117 15:09:09.994297 26396 master.hpp:1540] Master attempted to send message to disconnected framework 59c600f1-92ff-4926-9c84-073d9b81f68a-0000 (default) at scheduler-38aa807a-672a-4e1e-b823-71f119980e86@10.0.2.15:50088
I1117 15:09:09.994554 26396 master.cpp:1146] Giving framework 59c600f1-92ff-4926-9c84-073d9b81f68a-0000 (default) at scheduler-38aa807a-672a-4e1e-b823-71f119980e86@10.0.2.15:50088 0ns to failover
I1117 15:09:09.994380 26400 hierarchical.cpp:273] Deactivated framework 59c600f1-92ff-4926-9c84-073d9b81f68a-0000
I1117 15:09:09.994999 26400 hierarchical.cpp:744] Recovered cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (total: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000], allocated: ) on slave 59c600f1-92ff-4926-9c84-073d9b81f68a-S0 from framework 59c600f1-92ff-4926-9c84-073d9b81f68a-0000
I1117 15:09:09.998314 26395 master.cpp:4827] Framework failover timeout, removing framework 59c600f1-92ff-4926-9c84-073d9b81f68a-0000 (default) at scheduler-38aa807a-672a-4e1e-b823-71f119980e86@10.0.2.15:50088
I1117 15:09:09.998366 26395 master.cpp:5559] Removing framework 59c600f1-92ff-4926-9c84-073d9b81f68a-0000 (default) at scheduler-38aa807a-672a-4e1e-b823-71f119980e86@10.0.2.15:50088
I1117 15:09:09.998642 26401 slave.cpp:2009] Asked to shut down framework 59c600f1-92ff-4926-9c84-073d9b81f68a-0000 by master@10.0.2.15:50088
W1117 15:09:09.998677 26401 slave.cpp:2024] Cannot shut down unknown framework 59c600f1-92ff-4926-9c84-073d9b81f68a-0000
I1117 15:09:09.999388 26400 hierarchical.cpp:230] Removed framework 59c600f1-92ff-4926-9c84-073d9b81f68a-0000
F1117 15:09:09.999388 26380 logging.cpp:57] RAW: Pure virtual method called
    @     0x7f62e980e918  google::LogMessage::Fail()
    @     0x7f62e981403e  google::RawLog__()
    @     0x7f62e8aa0adc  __cxa_pure_virtual
    @           0x9d8e17  mesos::internal::tests::Cluster::Slaves::shutdown()
    @           0x9d8c62  mesos::internal::tests::Cluster::Slaves::~Slaves()
    @           0x91a048  mesos::internal::tests::Cluster::~Cluster()
    @           0x91a0b9  mesos::internal::tests::MesosTest::~MesosTest()
    @          0x12dd9d1  mesos::internal::tests::DockerContainerizerTest::~DockerContainerizerTest()
    @          0x12ed7cd  mesos::internal::tests::DockerContainerizerTest_ROOT_DOCKER_Launch_Executor_Test::~DockerContainerizerTest_ROOT_DOCKER_Launch_Executor_Test()
    @          0x12ed7fc  mesos::internal::tests::DockerContainerizerTest_ROOT_DOCKER_Launch_Executor_Test::~DockerContainerizerTest_ROOT_DOCKER_Launch_Executor_Test()
    @          0x1457316  testing::Test::DeleteSelf_()
    @          0x14614e4  testing::internal::HandleSehExceptionsInMethodIfSupported<>()
    @          0x145c35a  testing::internal::HandleExceptionsInMethodIfSupported<>()
    @          0x143e233  testing::TestInfo::Run()
    @          0x143e82c  testing::TestCase::Run()
    @          0x1444f74  testing::internal::UnitTestImpl::RunAllTests()
    @          0x1462109  testing::internal::HandleSehExceptionsInMethodIfSupported<>()
    @          0x145ced0  testing::internal::HandleExceptionsInMethodIfSupported<>()
    @          0x1443d10  testing::UnitTest::Run()
    @           0xd1589a  RUN_ALL_TESTS()
    @           0xd15487  main
    @     0x7f62e4482ec5  (unknown)
    @           0x912f99  (unknown)
{noformat}
",Bug,Major,bernd-mesos,,3,In Progress,In Progress,Test DockerContainerizerTest.ROOT_DOCKER_Launch_Executor fails.,2016-04-05T12:50:47.000+0000,MESOS-3937,2.0,mesos,Mesosphere Sprint 23
neilc,2015-11-17T06:42:32.000+0000,neilc,"We should document the possible ways in which the state of a task can evolve over time; what happens when an agent is partitioned from the master; and more generally, how we recommend that framework authors develop fault-tolerant schedulers and do task state reconciliation.",Documentation,Major,neilc,2016-01-19T15:57:31.000+0000,5,Resolved,Complete,Document possible task state transitions for framework authors,2016-01-19T15:57:31.000+0000,MESOS-3936,5.0,mesos,Mesosphere Sprint 24
kaysoky,2015-11-16T21:12:06.000+0000,kaysoky,"Related to this [TODO|https://github.com/apache/mesos/blob/aa0cd7ed4edf1184cbc592b5caa2429a8373e813/3rdparty/libprocess/src/process.cpp#L949-L950].

The {{MetricsProcess}} and {{ReaperProcess}} are global processes (singletons) which are initialized upon first use.  The two processes could be initialized alongside the {{gc}}, {{help}}, {{logging}}, {{profiler}}, and {{system}} (statistics) processes inside {{process::initialize}}.

This is also necessary for libprocess re-initialization.",Task,Major,kaysoky,,10006,Reviewable,New,Libprocess: Unify the initialization of the MetricsProcess and ReaperProcess,2016-01-12T21:23:01.000+0000,MESOS-3934,3.0,mesos,
vinodkone,2015-11-15T21:55:23.000+0000,hartem,"This script should do the following things

1) Apply a chain of reviews to a local branch
2) Push the commits upstream
3) Mark the reviews as submitted
4) Optionally close any attached JIRA tickets

",Task,Major,hartem,2016-02-26T22:12:17.000+0000,5,Resolved,Complete,Automate the process of landing commits for committers,2016-03-02T01:04:53.000+0000,MESOS-3929,3.0,mesos,Mesosphere Sprint 29
,2015-11-15T04:21:37.000+0000,marco-mesos,"Running {{0.26.0-rc1}} on both CentOS 7.1 and Ubuntu 14.04 with {{sudo}} privileges, causes segfaults when running Docker tests.

Logs attached.",Bug,Major,marco-mesos,2016-01-13T23:09:32.000+0000,5,Resolved,Complete,ROOT tests fail on Mesos 0.26 on Ubuntu/CentOS,2016-01-13T23:09:32.000+0000,MESOS-3928,2.0,mesos,Mesosphere Sprint 23
lins05,2015-11-13T20:37:57.000+0000,jieyu,So that we can add custom URI fetcher plugins using modules.,Task,Major,jieyu,,10006,Reviewable,New,Modularize URI fetcher plugin interface.  ,2016-04-25T18:38:53.000+0000,MESOS-3926,3.0,mesos,Mesosphere Sprint 34
jieyu,2015-11-13T20:35:24.000+0000,jieyu,"This plugin uses HDFS client to fetch artifacts. It can support schemes like hdfs/hftp/s3/s3n

It'll shell out the hadoop command to do the actual fetching.",Task,Major,jieyu,2016-01-04T19:16:31.000+0000,5,Resolved,Complete,Add HDFS based URI fetcher plugin.,2016-01-14T17:01:55.000+0000,MESOS-3925,3.0,mesos,Mesosphere Sprint 24
anandmazumdar,2015-11-13T20:05:26.000+0000,BenWhitehead,"If authentication(AuthN) is enabled on a master, frameworks attempting to use the HTTP Scheduler API can't register.

{code}
$ cat /tmp/subscribe-943257503176798091.bin | http --print=HhBb --stream --pretty=colors --auth verification:password1 POST :5050/api/v1/scheduler Accept:application/x-protobuf Content-Type:application/x-protobuf
POST /api/v1/scheduler HTTP/1.1
Connection: keep-alive
Content-Type: application/x-protobuf
Accept-Encoding: gzip, deflate
Accept: application/x-protobuf
Content-Length: 126
User-Agent: HTTPie/0.9.0
Host: localhost:5050
Authorization: Basic dmVyaWZpY2F0aW9uOnBhc3N3b3JkMQ==



+-----------------------------------------+
| NOTE: binary data not shown in terminal |
+-----------------------------------------+

HTTP/1.1 401 Unauthorized
Date: Fri, 13 Nov 2015 20:00:45 GMT
WWW-authenticate: Basic realm=""Mesos master""
Content-Length: 65

HTTP schedulers are not supported when authentication is required
{code}

Authorization(AuthZ) is already supported for HTTP based frameworks.",Bug,Major,BenWhitehead,2016-04-15T21:02:19.000+0000,5,Resolved,Complete,Implement AuthN handling in Master for the Scheduler endpoint,2016-04-15T21:02:19.000+0000,MESOS-3923,5.0,mesos,Mesosphere Sprint 33
neilc,2015-11-13T16:49:00.000+0000,neilc,"Verbose Logs:
{code}
[ RUN      ] MasterMaintenanceTest.InverseOffersFilters
I1113 16:43:58.486469  8728 leveldb.cpp:176] Opened db in 2.360405ms
I1113 16:43:58.486935  8728 leveldb.cpp:183] Compacted db in 407105ns
I1113 16:43:58.486995  8728 leveldb.cpp:198] Created db iterator in 16221ns
I1113 16:43:58.487030  8728 leveldb.cpp:204] Seeked to beginning of db in 10935ns
I1113 16:43:58.487046  8728 leveldb.cpp:273] Iterated through 0 keys in the db in 999ns
I1113 16:43:58.487090  8728 replica.cpp:780] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned
I1113 16:43:58.487735  8747 recover.cpp:449] Starting replica recovery
I1113 16:43:58.488047  8747 recover.cpp:475] Replica is in EMPTY status
I1113 16:43:58.488977  8745 replica.cpp:676] Replica in EMPTY status received a broadcasted recover request from (58)@10.0.2.15:45384
I1113 16:43:58.489452  8746 recover.cpp:195] Received a recover response from a replica in EMPTY status
I1113 16:43:58.489712  8747 recover.cpp:566] Updating replica status to STARTING
I1113 16:43:58.490706  8742 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 745443ns
I1113 16:43:58.490739  8742 replica.cpp:323] Persisted replica status to STARTING
I1113 16:43:58.490859  8742 recover.cpp:475] Replica is in STARTING status
I1113 16:43:58.491786  8747 replica.cpp:676] Replica in STARTING status received a broadcasted recover request from (59)@10.0.2.15:45384
I1113 16:43:58.492542  8749 recover.cpp:195] Received a recover response from a replica in STARTING status
I1113 16:43:58.493221  8743 recover.cpp:566] Updating replica status to VOTING
I1113 16:43:58.493710  8743 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 331874ns
I1113 16:43:58.493767  8743 replica.cpp:323] Persisted replica status to VOTING
I1113 16:43:58.493868  8743 recover.cpp:580] Successfully joined the Paxos group
I1113 16:43:58.494119  8743 recover.cpp:464] Recover process terminated
I1113 16:43:58.504369  8749 master.cpp:367] Master d59449fc-5462-43c5-b935-e05563fdd4b6 (vagrant-ubuntu-wily-64) started on 10.0.2.15:45384
I1113 16:43:58.504438  8749 master.cpp:369] Flags at startup: --acls="""" --allocation_interval=""1secs"" --allocator=""HierarchicalDRF"" --authenticate=""false"" --authenticate_slaves=""true"" --authenticators=""crammd5"" --authorizers=""local"" --credentials=""/tmp/ZB7csS/credentials"" --framework_sorter=""drf"" --help=""false"" --hostname_lookup=""true"" --initialize_driver_logging=""true"" --log_auto_initialize=""true"" --logbufsecs=""0"" --logging_level=""INFO"" --max_slave_ping_timeouts=""5"" --quiet=""false"" --recovery_slave_removal_limit=""100%"" --registry=""replicated_log"" --registry_fetch_timeout=""1mins"" --registry_store_timeout=""25secs"" --registry_strict=""true"" --root_submissions=""true"" --slave_ping_timeout=""15secs"" --slave_reregister_timeout=""10mins"" --user_sorter=""drf"" --version=""false"" --webui_dir=""/usr/local/share/mesos/webui"" --work_dir=""/tmp/ZB7csS/master"" --zk_session_timeout=""10secs""
I1113 16:43:58.504717  8749 master.cpp:416] Master allowing unauthenticated frameworks to register
I1113 16:43:58.504889  8749 master.cpp:419] Master only allowing authenticated slaves to register
I1113 16:43:58.504922  8749 credentials.hpp:37] Loading credentials for authentication from '/tmp/ZB7csS/credentials'
I1113 16:43:58.505497  8749 master.cpp:458] Using default 'crammd5' authenticator
I1113 16:43:58.505759  8749 master.cpp:495] Authorization enabled
I1113 16:43:58.507638  8746 master.cpp:1606] The newly elected leader is master@10.0.2.15:45384 with id d59449fc-5462-43c5-b935-e05563fdd4b6
I1113 16:43:58.507693  8746 master.cpp:1619] Elected as the leading master!
I1113 16:43:58.507720  8746 master.cpp:1379] Recovering from registrar
I1113 16:43:58.507946  8749 registrar.cpp:309] Recovering registrar
I1113 16:43:58.508561  8749 log.cpp:661] Attempting to start the writer
I1113 16:43:58.510282  8747 replica.cpp:496] Replica received implicit promise request from (60)@10.0.2.15:45384 with proposal 1
I1113 16:43:58.510867  8747 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 475696ns
I1113 16:43:58.510946  8747 replica.cpp:345] Persisted promised to 1
I1113 16:43:58.511912  8745 coordinator.cpp:240] Coordinator attempting to fill missing positions
I1113 16:43:58.513030  8749 replica.cpp:391] Replica received explicit promise request from (61)@10.0.2.15:45384 for position 0 with proposal 2
I1113 16:43:58.513819  8749 leveldb.cpp:343] Persisting action (8 bytes) to leveldb took 739171ns
I1113 16:43:58.513867  8749 replica.cpp:715] Persisted action at 0
I1113 16:43:58.522002  8745 replica.cpp:540] Replica received write request for position 0 from (62)@10.0.2.15:45384
I1113 16:43:58.522114  8745 leveldb.cpp:438] Reading position from leveldb took 33549ns
I1113 16:43:58.522599  8745 leveldb.cpp:343] Persisting action (14 bytes) to leveldb took 435729ns
I1113 16:43:58.522652  8745 replica.cpp:715] Persisted action at 0
I1113 16:43:58.523291  8746 replica.cpp:694] Replica received learned notice for position 0 from @0.0.0.0:0
I1113 16:43:58.523901  8746 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 538894ns
I1113 16:43:58.523983  8746 replica.cpp:715] Persisted action at 0
I1113 16:43:58.524060  8746 replica.cpp:700] Replica learned NOP action at position 0
I1113 16:43:58.524775  8747 log.cpp:677] Writer started with ending position 0
I1113 16:43:58.525902  8745 leveldb.cpp:438] Reading position from leveldb took 39685ns
I1113 16:43:58.526852  8745 registrar.cpp:342] Successfully fetched the registry (0B) in 18.832896ms
I1113 16:43:58.527084  8745 registrar.cpp:441] Applied 1 operations in 24930ns; attempting to update the 'registry'
I1113 16:43:58.528020  8745 log.cpp:685] Attempting to append 189 bytes to the log
I1113 16:43:58.528323  8748 coordinator.cpp:350] Coordinator attempting to write APPEND action at position 1
I1113 16:43:58.529465  8744 replica.cpp:540] Replica received write request for position 1 from (63)@10.0.2.15:45384
I1113 16:43:58.530081  8744 leveldb.cpp:343] Persisting action (208 bytes) to leveldb took 552812ns
I1113 16:43:58.530128  8744 replica.cpp:715] Persisted action at 1
I1113 16:43:58.530781  8745 replica.cpp:694] Replica received learned notice for position 1 from @0.0.0.0:0
I1113 16:43:58.531121  8745 leveldb.cpp:343] Persisting action (210 bytes) to leveldb took 271774ns
I1113 16:43:58.531162  8745 replica.cpp:715] Persisted action at 1
I1113 16:43:58.531188  8745 replica.cpp:700] Replica learned APPEND action at position 1
I1113 16:43:58.532064  8743 registrar.cpp:486] Successfully updated the 'registry' in 4.9152ms
I1113 16:43:58.532402  8743 registrar.cpp:372] Successfully recovered registrar
I1113 16:43:58.532768  8742 log.cpp:704] Attempting to truncate the log to 1
I1113 16:43:58.532891  8743 master.cpp:1416] Recovered 0 slaves from the Registry (150B) ; allowing 10mins for slaves to re-register
I1113 16:43:58.532968  8742 coordinator.cpp:350] Coordinator attempting to write TRUNCATE action at position 2
I1113 16:43:58.534010  8742 replica.cpp:540] Replica received write request for position 2 from (64)@10.0.2.15:45384
I1113 16:43:58.534488  8742 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 420186ns
I1113 16:43:58.534533  8742 replica.cpp:715] Persisted action at 2
I1113 16:43:58.535081  8748 replica.cpp:694] Replica received learned notice for position 2 from @0.0.0.0:0
I1113 16:43:58.535482  8748 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 360618ns
I1113 16:43:58.535550  8748 leveldb.cpp:401] Deleting ~1 keys from leveldb took 23693ns
I1113 16:43:58.535575  8748 replica.cpp:715] Persisted action at 2
I1113 16:43:58.535611  8748 replica.cpp:700] Replica learned TRUNCATE action at position 2
I1113 16:43:58.550834  8746 slave.cpp:191] Slave started on 5)@10.0.2.15:45384
I1113 16:43:58.550834  8746 slave.cpp:192] Flags at startup: --appc_store_dir=""/tmp/mesos/store/appc"" --authenticatee=""crammd5"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --credential=""/tmp/MasterMaintenanceTest_InverseOffersFilters_2zc09g/credential"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_auth_server=""auth.docker.io"" --docker_auth_server_port=""443"" --docker_kill_orphans=""true"" --docker_local_archives_dir=""/tmp/mesos/images/docker"" --docker_puller=""local"" --docker_puller_timeout=""60"" --docker_registry=""registry-1.docker.io"" --docker_registry_port=""443"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/tmp/mesos/store/docker"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/tmp/MasterMaintenanceTest_InverseOffersFilters_2zc09g/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --hostname=""maintenance-host"" --hostname_lookup=""true"" --image_provisioner_backend=""copy"" --initialize_driver_logging=""true"" --isolation=""posix/cpu,posix/mem"" --launcher_dir=""/home/vagrant/build-mesos/src"" --logbufsecs=""0"" --logging_level=""INFO"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""10ms"" --resources=""cpus:2;mem:1024;disk:1024;ports:[31000-32000]"" --revocable_cpu_low_priority=""true"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --systemd_runtime_directory=""/run/systemd/system"" --version=""false"" --work_dir=""/tmp/MasterMaintenanceTest_InverseOffersFilters_2zc09g""
I1113 16:43:58.551501  8746 credentials.hpp:85] Loading credential for authentication from '/tmp/MasterMaintenanceTest_InverseOffersFilters_2zc09g/credential'
I1113 16:43:58.551703  8746 slave.cpp:322] Slave using credential for: test-principal
I1113 16:43:58.552422  8746 slave.cpp:392] Slave resources: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I1113 16:43:58.552510  8746 slave.cpp:400] Slave attributes: [  ]
I1113 16:43:58.552532  8746 slave.cpp:405] Slave hostname: maintenance-host
I1113 16:43:58.552547  8746 slave.cpp:410] Slave checkpoint: true
I1113 16:43:58.553520  8746 state.cpp:54] Recovering state from '/tmp/MasterMaintenanceTest_InverseOffersFilters_2zc09g/meta'
I1113 16:43:58.553938  8746 status_update_manager.cpp:202] Recovering status update manager
I1113 16:43:58.554251  8746 slave.cpp:4230] Finished recovery
I1113 16:43:58.555016  8746 slave.cpp:729] New master detected at master@10.0.2.15:45384
I1113 16:43:58.555166  8746 slave.cpp:792] Authenticating with master master@10.0.2.15:45384
I1113 16:43:58.555207  8746 slave.cpp:797] Using default CRAM-MD5 authenticatee
I1113 16:43:58.555589  8746 slave.cpp:765] Detecting new master
I1113 16:43:58.555076  8749 status_update_manager.cpp:176] Pausing sending status updates
I1113 16:43:58.555719  8742 authenticatee.cpp:123] Creating new client SASL connection
I1113 16:43:58.560645  8744 master.cpp:5150] Authenticating slave(5)@10.0.2.15:45384
I1113 16:43:58.561305  8744 authenticator.cpp:100] Creating new server SASL connection
I1113 16:43:58.566682  8744 authenticatee.cpp:214] Received SASL authentication mechanisms: CRAM-MD5
I1113 16:43:58.566779  8744 authenticatee.cpp:240] Attempting to authenticate with mechanism 'CRAM-MD5'
I1113 16:43:58.566872  8744 authenticator.cpp:205] Received SASL authentication start
I1113 16:43:58.566936  8744 authenticator.cpp:327] Authentication requires more steps
I1113 16:43:58.567602  8744 authenticatee.cpp:260] Received SASL authentication step
I1113 16:43:58.567775  8744 authenticator.cpp:233] Received SASL authentication step
I1113 16:43:58.568128  8744 authenticator.cpp:319] Authentication success
I1113 16:43:58.568282  8742 authenticatee.cpp:300] Authentication success
I1113 16:43:58.568320  8749 master.cpp:5180] Successfully authenticated principal 'test-principal' at slave(5)@10.0.2.15:45384
I1113 16:43:58.568701  8742 slave.cpp:860] Successfully authenticated with master master@10.0.2.15:45384
I1113 16:43:58.569272  8747 master.cpp:3859] Registering slave at slave(5)@10.0.2.15:45384 (maintenance-host) with id d59449fc-5462-43c5-b935-e05563fdd4b6-S0
I1113 16:43:58.570096  8747 registrar.cpp:441] Applied 1 operations in 59195ns; attempting to update the 'registry'
I1113 16:43:58.570772  8748 log.cpp:685] Attempting to append 362 bytes to the log
I1113 16:43:58.570772  8749 coordinator.cpp:350] Coordinator attempting to write APPEND action at position 3
I1113 16:43:58.572155  8745 replica.cpp:540] Replica received write request for position 3 from (69)@10.0.2.15:45384
I1113 16:43:58.572801  8745 leveldb.cpp:343] Persisting action (381 bytes) to leveldb took 563073ns
I1113 16:43:58.572854  8745 replica.cpp:715] Persisted action at 3
I1113 16:43:58.573707  8745 replica.cpp:694] Replica received learned notice for position 3 from @0.0.0.0:0
I1113 16:43:58.574255  8745 leveldb.cpp:343] Persisting action (383 bytes) to leveldb took 485234ns
I1113 16:43:58.574311  8745 replica.cpp:715] Persisted action at 3
I1113 16:43:58.574342  8745 replica.cpp:700] Replica learned APPEND action at position 3
I1113 16:43:58.575857  8747 master.cpp:3847] Ignoring register slave message from slave(5)@10.0.2.15:45384 (maintenance-host) as admission is already in progress
I1113 16:43:58.576217  8744 log.cpp:704] Attempting to truncate the log to 3
I1113 16:43:58.575887  8748 registrar.cpp:486] Successfully updated the 'registry' in 5.682176ms
I1113 16:43:58.576400  8744 coordinator.cpp:350] Coordinator attempting to write TRUNCATE action at position 4
I1113 16:43:58.577169  8746 master.cpp:3927] Registered slave d59449fc-5462-43c5-b935-e05563fdd4b6-S0 at slave(5)@10.0.2.15:45384 (maintenance-host) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I1113 16:43:58.577287  8745 hierarchical.cpp:344] Added slave d59449fc-5462-43c5-b935-e05563fdd4b6-S0 (maintenance-host) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (allocated: )
I1113 16:43:58.577472  8744 slave.cpp:904] Registered with master master@10.0.2.15:45384; given slave ID d59449fc-5462-43c5-b935-e05563fdd4b6-S0
I1113 16:43:58.577999  8745 status_update_manager.cpp:183] Resuming sending status updates
I1113 16:43:58.578279  8748 replica.cpp:540] Replica received write request for position 4 from (70)@10.0.2.15:45384
I1113 16:43:58.578346  8744 slave.cpp:963] Forwarding total oversubscribed resources
I1113 16:43:58.578734  8744 master.cpp:4269] Received update of slave d59449fc-5462-43c5-b935-e05563fdd4b6-S0 at slave(5)@10.0.2.15:45384 (maintenance-host) with total oversubscribed resources
I1113 16:43:58.578846  8748 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 304993ns
I1113 16:43:58.578889  8748 replica.cpp:715] Persisted action at 4
I1113 16:43:58.578897  8744 hierarchical.cpp:400] Slave d59449fc-5462-43c5-b935-e05563fdd4b6-S0 (maintenance-host) updated with oversubscribed resources  (total: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000], allocated: )
I1113 16:43:58.579463  8744 replica.cpp:694] Replica received learned notice for position 4 from @0.0.0.0:0
I1113 16:43:58.579888  8744 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 384596ns
I1113 16:43:58.579952  8744 leveldb.cpp:401] Deleting ~2 keys from leveldb took 27011ns
I1113 16:43:58.579977  8744 replica.cpp:715] Persisted action at 4
I1113 16:43:58.580001  8744 replica.cpp:700] Replica learned TRUNCATE action at position 4
I1113 16:43:58.584300  8743 slave.cpp:191] Slave started on 6)@10.0.2.15:45384
I1113 16:43:58.584398  8743 slave.cpp:192] Flags at startup: --appc_store_dir=""/tmp/mesos/store/appc"" --authenticatee=""crammd5"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --credential=""/tmp/MasterMaintenanceTest_InverseOffersFilters_CDFgvt/credential"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_auth_server=""auth.docker.io"" --docker_auth_server_port=""443"" --docker_kill_orphans=""true"" --docker_local_archives_dir=""/tmp/mesos/images/docker"" --docker_puller=""local"" --docker_puller_timeout=""60"" --docker_registry=""registry-1.docker.io"" --docker_registry_port=""443"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/tmp/mesos/store/docker"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/tmp/MasterMaintenanceTest_InverseOffersFilters_CDFgvt/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --hostname=""maintenance-host-2"" --hostname_lookup=""true"" --image_provisioner_backend=""copy"" --initialize_driver_logging=""true"" --isolation=""posix/cpu,posix/mem"" --launcher_dir=""/home/vagrant/build-mesos/src"" --logbufsecs=""0"" --logging_level=""INFO"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""10ms"" --resources=""cpus:2;mem:1024;disk:1024;ports:[31000-32000]"" --revocable_cpu_low_priority=""true"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --systemd_runtime_directory=""/run/systemd/system"" --version=""false"" --work_dir=""/tmp/MasterMaintenanceTest_InverseOffersFilters_CDFgvt""
I1113 16:43:58.584731  8743 credentials.hpp:85] Loading credential for authentication from '/tmp/MasterMaintenanceTest_InverseOffersFilters_CDFgvt/credential'
I1113 16:43:58.584915  8743 slave.cpp:322] Slave using credential for: test-principal
I1113 16:43:58.585309  8743 slave.cpp:392] Slave resources: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I1113 16:43:58.585482  8743 slave.cpp:400] Slave attributes: [  ]
I1113 16:43:58.585566  8743 slave.cpp:405] Slave hostname: maintenance-host-2
I1113 16:43:58.585619  8743 slave.cpp:410] Slave checkpoint: true
I1113 16:43:58.586431  8743 state.cpp:54] Recovering state from '/tmp/MasterMaintenanceTest_InverseOffersFilters_CDFgvt/meta'
I1113 16:43:58.586890  8745 status_update_manager.cpp:202] Recovering status update manager
I1113 16:43:58.587136  8745 slave.cpp:4230] Finished recovery
I1113 16:43:58.587817  8745 slave.cpp:729] New master detected at master@10.0.2.15:45384
I1113 16:43:58.587836  8747 status_update_manager.cpp:176] Pausing sending status updates
I1113 16:43:58.587908  8745 slave.cpp:792] Authenticating with master master@10.0.2.15:45384
I1113 16:43:58.587934  8745 slave.cpp:797] Using default CRAM-MD5 authenticatee
I1113 16:43:58.588043  8745 slave.cpp:765] Detecting new master
I1113 16:43:58.588170  8745 authenticatee.cpp:123] Creating new client SASL connection
I1113 16:43:58.592891  8745 master.cpp:5150] Authenticating slave(6)@10.0.2.15:45384
I1113 16:43:58.594146  8745 authenticator.cpp:100] Creating new server SASL connection
I1113 16:43:58.599606  8749 authenticatee.cpp:214] Received SASL authentication mechanisms: CRAM-MD5
I1113 16:43:58.599684  8749 authenticatee.cpp:240] Attempting to authenticate with mechanism 'CRAM-MD5'
I1113 16:43:58.599774  8749 authenticator.cpp:205] Received SASL authentication start
I1113 16:43:58.599830  8749 authenticator.cpp:327] Authentication requires more steps
I1113 16:43:58.599895  8749 authenticatee.cpp:260] Received SASL authentication step
I1113 16:43:58.599966  8749 authenticator.cpp:233] Received SASL authentication step
I1113 16:43:58.600042  8749 authenticator.cpp:319] Authentication success
I1113 16:43:58.600183  8749 authenticatee.cpp:300] Authentication success
I1113 16:43:58.600304  8749 master.cpp:5180] Successfully authenticated principal 'test-principal' at slave(6)@10.0.2.15:45384
I1113 16:43:58.600749  8745 slave.cpp:860] Successfully authenticated with master master@10.0.2.15:45384
I1113 16:43:58.601652  8745 master.cpp:3859] Registering slave at slave(6)@10.0.2.15:45384 (maintenance-host-2) with id d59449fc-5462-43c5-b935-e05563fdd4b6-S1
I1113 16:43:58.602469  8745 registrar.cpp:441] Applied 1 operations in 62055ns; attempting to update the 'registry'
I1113 16:43:58.603483  8742 log.cpp:685] Attempting to append 534 bytes to the log
I1113 16:43:58.603664  8747 coordinator.cpp:350] Coordinator attempting to write APPEND action at position 5
I1113 16:43:58.604473  8748 replica.cpp:540] Replica received write request for position 5 from (75)@10.0.2.15:45384
I1113 16:43:58.605144  8748 leveldb.cpp:343] Persisting action (553 bytes) to leveldb took 512473ns
I1113 16:43:58.605190  8748 replica.cpp:715] Persisted action at 5
I1113 16:43:58.606076  8742 replica.cpp:694] Replica received learned notice for position 5 from @0.0.0.0:0
I1113 16:43:58.606385  8742 leveldb.cpp:343] Persisting action (555 bytes) to leveldb took 264699ns
I1113 16:43:58.606427  8742 replica.cpp:715] Persisted action at 5
I1113 16:43:58.606456  8742 replica.cpp:700] Replica learned APPEND action at position 5
I1113 16:43:58.607722  8749 registrar.cpp:486] Successfully updated the 'registry' in 4.815104ms
I1113 16:43:58.607897  8748 log.cpp:704] Attempting to truncate the log to 5
I1113 16:43:58.608088  8748 coordinator.cpp:350] Coordinator attempting to write TRUNCATE action at position 6
I1113 16:43:58.608280  8749 master.cpp:3927] Registered slave d59449fc-5462-43c5-b935-e05563fdd4b6-S1 at slave(6)@10.0.2.15:45384 (maintenance-host-2) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I1113 16:43:58.608378  8742 hierarchical.cpp:344] Added slave d59449fc-5462-43c5-b935-e05563fdd4b6-S1 (maintenance-host-2) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (allocated: )
I1113 16:43:58.608783  8747 slave.cpp:904] Registered with master master@10.0.2.15:45384; given slave ID d59449fc-5462-43c5-b935-e05563fdd4b6-S1
I1113 16:43:58.609323  8746 status_update_manager.cpp:183] Resuming sending status updates
I1113 16:43:58.609797  8747 slave.cpp:963] Forwarding total oversubscribed resources
I1113 16:43:58.610152  8749 master.cpp:4269] Received update of slave d59449fc-5462-43c5-b935-e05563fdd4b6-S1 at slave(6)@10.0.2.15:45384 (maintenance-host-2) with total oversubscribed resources
I1113 16:43:58.610436  8749 hierarchical.cpp:400] Slave d59449fc-5462-43c5-b935-e05563fdd4b6-S1 (maintenance-host-2) updated with oversubscribed resources  (total: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000], allocated: )
I1113 16:43:58.610265  8744 replica.cpp:540] Replica received write request for position 6 from (76)@10.0.2.15:45384
I1113 16:43:58.611052  8744 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 346656ns
I1113 16:43:58.611089  8744 replica.cpp:715] Persisted action at 6
I1113 16:43:58.611934  8744 http.cpp:338] HTTP POST for /master/maintenance/schedule from 10.0.2.15:60984
I1113 16:43:58.612453  8744 replica.cpp:694] Replica received learned notice for position 6 from @0.0.0.0:0
I1113 16:43:58.612859  8744 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 353053ns
I1113 16:43:58.612920  8744 leveldb.cpp:401] Deleting ~2 keys from leveldb took 25904ns
I1113 16:43:58.612957  8744 replica.cpp:715] Persisted action at 6
I1113 16:43:58.613082  8744 replica.cpp:700] Replica learned TRUNCATE action at position 6
I1113 16:43:58.612925  8742 registrar.cpp:441] Applied 1 operations in 184286ns; attempting to update the 'registry'
I1113 16:43:58.614495  8742 log.cpp:685] Attempting to append 749 bytes to the log
I1113 16:43:58.614680  8744 coordinator.cpp:350] Coordinator attempting to write APPEND action at position 7
I1113 16:43:58.615454  8745 replica.cpp:540] Replica received write request for position 7 from (77)@10.0.2.15:45384
I1113 16:43:58.615761  8745 leveldb.cpp:343] Persisting action (768 bytes) to leveldb took 261919ns
I1113 16:43:58.615790  8745 replica.cpp:715] Persisted action at 7
I1113 16:43:58.616392  8745 replica.cpp:694] Replica received learned notice for position 7 from @0.0.0.0:0
I1113 16:43:58.617075  8745 leveldb.cpp:343] Persisting action (770 bytes) to leveldb took 639439ns
I1113 16:43:58.617111  8745 replica.cpp:715] Persisted action at 7
I1113 16:43:58.617137  8745 replica.cpp:700] Replica learned APPEND action at position 7
I1113 16:43:58.618403  8745 registrar.cpp:486] Successfully updated the 'registry' in 4.761344ms
I1113 16:43:58.618563  8742 log.cpp:704] Attempting to truncate the log to 7
I1113 16:43:58.618996  8745 coordinator.cpp:350] Coordinator attempting to write TRUNCATE action at position 8
I1113 16:43:58.618693  8746 master.cpp:4325] Updating unavailability of slave d59449fc-5462-43c5-b935-e05563fdd4b6-S1 at slave(6)@10.0.2.15:45384 (maintenance-host-2), starting at 2393.24256053228weeks
I1113 16:43:58.619581  8746 master.cpp:4325] Updating unavailability of slave d59449fc-5462-43c5-b935-e05563fdd4b6-S0 at slave(5)@10.0.2.15:45384 (maintenance-host), starting at 2393.24256053228weeks
I1113 16:43:58.619680  8746 master.cpp:4325] Updating unavailability of slave d59449fc-5462-43c5-b935-e05563fdd4b6-S0 at slave(5)@10.0.2.15:45384 (maintenance-host), starting at 2393.24256053228weeks
I1113 16:43:58.619757  8746 master.cpp:4325] Updating unavailability of slave d59449fc-5462-43c5-b935-e05563fdd4b6-S1 at slave(6)@10.0.2.15:45384 (maintenance-host-2), starting at 2393.24256053228weeks
I1113 16:43:58.619964  8746 replica.cpp:540] Replica received write request for position 8 from (78)@10.0.2.15:45384
I1113 16:43:58.620584  8746 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 574141ns
I1113 16:43:58.620633  8746 replica.cpp:715] Persisted action at 8
I1113 16:43:58.621672  8746 replica.cpp:694] Replica received learned notice for position 8 from @0.0.0.0:0
I1113 16:43:58.622351  8728 scheduler.cpp:156] Version: 0.26.0
I1113 16:43:58.622827  8746 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 1.097466ms
I1113 16:43:58.622943  8746 leveldb.cpp:401] Deleting ~2 keys from leveldb took 47195ns
I1113 16:43:58.623087  8746 replica.cpp:715] Persisted action at 8
I1113 16:43:58.623191  8746 replica.cpp:700] Replica learned TRUNCATE action at position 8
I1113 16:43:58.639466  8747 http.cpp:338] HTTP POST for /master/api/v1/scheduler from 10.0.2.15:60986
I1113 16:43:58.639690  8747 master.cpp:1868] Received subscription request for HTTP framework 'default'
I1113 16:43:58.639804  8747 master.cpp:1645] Authorizing framework principal 'test-principal' to receive offers for role '*'
I1113 16:43:58.641152  8745 master.cpp:1960] Subscribing framework 'default' with checkpointing disabled and capabilities [  ]
I1113 16:43:58.642168  8745 hierarchical.cpp:195] Added framework d59449fc-5462-43c5-b935-e05563fdd4b6-0000
I1113 16:43:58.644707  8749 master.cpp:4979] Sending 2 offers to framework d59449fc-5462-43c5-b935-e05563fdd4b6-0000 (default)
I1113 16:43:58.645970  8749 master.cpp:5069] Sending 2 inverse offers to framework d59449fc-5462-43c5-b935-e05563fdd4b6-0000 (default)
I1113 16:44:00.693930  8742 http.cpp:338] HTTP POST for /master/api/v1/scheduler from 10.0.2.15:60988
I1113 16:44:00.694870  8742 master.cpp:2915] Processing ACCEPT call for offers: [ d59449fc-5462-43c5-b935-e05563fdd4b6-O0 ] on slave d59449fc-5462-43c5-b935-e05563fdd4b6-S1 at slave(6)@10.0.2.15:45384 (maintenance-host-2) for framework d59449fc-5462-43c5-b935-e05563fdd4b6-0000 (default)
I1113 16:44:00.695387  8742 master.cpp:2711] Authorizing framework principal 'test-principal' to launch task a2f7263f-17aa-4849-a0dc-7c8453b3b81f as user 'vagrant'
W1113 16:44:00.698230  8745 validation.cpp:422] Executor executor-1 for task a2f7263f-17aa-4849-a0dc-7c8453b3b81f uses less CPUs (None) than the minimum required (0.01). Please update your executor, as this will be mandatory in future releases.
W1113 16:44:00.698603  8745 validation.cpp:434] Executor executor-1 for task a2f7263f-17aa-4849-a0dc-7c8453b3b81f uses less memory (None) than the minimum required (32MB). Please update your executor, as this will be mandatory in future releases.
I1113 16:44:00.699178  8745 master.hpp:176] Adding task a2f7263f-17aa-4849-a0dc-7c8453b3b81f with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave d59449fc-5462-43c5-b935-e05563fdd4b6-S1 (maintenance-host-2)
I1113 16:44:00.699599  8745 master.cpp:3245] Launching task a2f7263f-17aa-4849-a0dc-7c8453b3b81f of framework d59449fc-5462-43c5-b935-e05563fdd4b6-0000 (default) with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave d59449fc-5462-43c5-b935-e05563fdd4b6-S1 at slave(6)@10.0.2.15:45384 (maintenance-host-2)
I1113 16:44:00.703222  8745 http.cpp:338] HTTP POST for /master/api/v1/scheduler from 10.0.2.15:60990
I1113 16:44:00.704217  8745 master.cpp:2915] Processing ACCEPT call for offers: [ d59449fc-5462-43c5-b935-e05563fdd4b6-O1 ] on slave d59449fc-5462-43c5-b935-e05563fdd4b6-S0 at slave(5)@10.0.2.15:45384 (maintenance-host) for framework d59449fc-5462-43c5-b935-e05563fdd4b6-0000 (default)
I1113 16:44:00.704463  8745 master.cpp:2711] Authorizing framework principal 'test-principal' to launch task a6ef5d55-4c84-4bc6-bd4a-a1f07de3fcc8 as user 'vagrant'
I1113 16:44:00.703451  8744 slave.cpp:1294] Got assigned task a2f7263f-17aa-4849-a0dc-7c8453b3b81f for framework d59449fc-5462-43c5-b935-e05563fdd4b6-0000
I1113 16:44:00.705693  8744 slave.cpp:1410] Launching task a2f7263f-17aa-4849-a0dc-7c8453b3b81f for framework d59449fc-5462-43c5-b935-e05563fdd4b6-0000
I1113 16:44:00.706001  8744 paths.cpp:436] Trying to chown '/tmp/MasterMaintenanceTest_InverseOffersFilters_CDFgvt/slaves/d59449fc-5462-43c5-b935-e05563fdd4b6-S1/frameworks/d59449fc-5462-43c5-b935-e05563fdd4b6-0000/executors/executor-1/runs/49d1654a-180d-406c-82f4-ab1ebd168451' to user 'vagrant'
W1113 16:44:00.707725  8745 validation.cpp:422] Executor executor-2 for task a6ef5d55-4c84-4bc6-bd4a-a1f07de3fcc8 uses less CPUs (None) than the minimum required (0.01). Please update your executor, as this will be mandatory in future releases.
W1113 16:44:00.707792  8745 validation.cpp:434] Executor executor-2 for task a6ef5d55-4c84-4bc6-bd4a-a1f07de3fcc8 uses less memory (None) than the minimum required (32MB). Please update your executor, as this will be mandatory in future releases.
I1113 16:44:00.708001  8745 master.hpp:176] Adding task a6ef5d55-4c84-4bc6-bd4a-a1f07de3fcc8 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave d59449fc-5462-43c5-b935-e05563fdd4b6-S0 (maintenance-host)
I1113 16:44:00.708151  8745 master.cpp:3245] Launching task a6ef5d55-4c84-4bc6-bd4a-a1f07de3fcc8 of framework d59449fc-5462-43c5-b935-e05563fdd4b6-0000 (default) with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave d59449fc-5462-43c5-b935-e05563fdd4b6-S0 at slave(5)@10.0.2.15:45384 (maintenance-host)
I1113 16:44:00.708411  8745 slave.cpp:1294] Got assigned task a6ef5d55-4c84-4bc6-bd4a-a1f07de3fcc8 for framework d59449fc-5462-43c5-b935-e05563fdd4b6-0000
I1113 16:44:00.708694  8745 slave.cpp:1410] Launching task a6ef5d55-4c84-4bc6-bd4a-a1f07de3fcc8 for framework d59449fc-5462-43c5-b935-e05563fdd4b6-0000
I1113 16:44:00.708943  8745 paths.cpp:436] Trying to chown '/tmp/MasterMaintenanceTest_InverseOffersFilters_2zc09g/slaves/d59449fc-5462-43c5-b935-e05563fdd4b6-S0/frameworks/d59449fc-5462-43c5-b935-e05563fdd4b6-0000/executors/executor-2/runs/e3cf520d-b4ef-47bb-ab6a-bd1d7fd0d0d9' to user 'vagrant'
I1113 16:44:00.709643  8744 slave.cpp:4999] Launching executor executor-1 of framework d59449fc-5462-43c5-b935-e05563fdd4b6-0000 with resources  in work directory '/tmp/MasterMaintenanceTest_InverseOffersFilters_CDFgvt/slaves/d59449fc-5462-43c5-b935-e05563fdd4b6-S1/frameworks/d59449fc-5462-43c5-b935-e05563fdd4b6-0000/executors/executor-1/runs/49d1654a-180d-406c-82f4-ab1ebd168451'
I1113 16:44:00.710903  8744 exec.cpp:136] Version: 0.26.0
I1113 16:44:00.711484  8744 slave.cpp:1628] Queuing task 'a2f7263f-17aa-4849-a0dc-7c8453b3b81f' for executor 'executor-1' of framework d59449fc-5462-43c5-b935-e05563fdd4b6-0000
I1113 16:44:00.711753  8745 slave.cpp:4999] Launching executor executor-2 of framework d59449fc-5462-43c5-b935-e05563fdd4b6-0000 with resources  in work directory '/tmp/MasterMaintenanceTest_InverseOffersFilters_2zc09g/slaves/d59449fc-5462-43c5-b935-e05563fdd4b6-S0/frameworks/d59449fc-5462-43c5-b935-e05563fdd4b6-0000/executors/executor-2/runs/e3cf520d-b4ef-47bb-ab6a-bd1d7fd0d0d9'
I1113 16:44:00.711833  8744 slave.cpp:2405] Got registration for executor 'executor-1' of framework d59449fc-5462-43c5-b935-e05563fdd4b6-0000 from executor(5)@10.0.2.15:45384
I1113 16:44:00.712290  8747 exec.cpp:210] Executor registered on slave d59449fc-5462-43c5-b935-e05563fdd4b6-S1
I1113 16:44:00.712692  8744 slave.cpp:1793] Sending queued task 'a2f7263f-17aa-4849-a0dc-7c8453b3b81f' to executor 'executor-1' of framework d59449fc-5462-43c5-b935-e05563fdd4b6-0000 at executor(5)@10.0.2.15:45384
I1113 16:44:00.713408  8745 exec.cpp:136] Version: 0.26.0
I1113 16:44:00.713841  8745 slave.cpp:1628] Queuing task 'a6ef5d55-4c84-4bc6-bd4a-a1f07de3fcc8' for executor 'executor-2' of framework d59449fc-5462-43c5-b935-e05563fdd4b6-0000
I1113 16:44:00.713989  8745 slave.cpp:2405] Got registration for executor 'executor-2' of framework d59449fc-5462-43c5-b935-e05563fdd4b6-0000 from executor(6)@10.0.2.15:45384
I1113 16:44:00.714262  8744 exec.cpp:210] Executor registered on slave d59449fc-5462-43c5-b935-e05563fdd4b6-S0
I1113 16:44:00.714367  8746 slave.cpp:2762] Handling status update TASK_RUNNING (UUID: 23650bfa-b3ac-470e-a901-b4171542cc32) for task a2f7263f-17aa-4849-a0dc-7c8453b3b81f of framework d59449fc-5462-43c5-b935-e05563fdd4b6-0000 from executor(5)@10.0.2.15:45384
I1113 16:44:00.714448  8745 slave.cpp:1793] Sending queued task 'a6ef5d55-4c84-4bc6-bd4a-a1f07de3fcc8' to executor 'executor-2' of framework d59449fc-5462-43c5-b935-e05563fdd4b6-0000 at executor(6)@10.0.2.15:45384
I1113 16:44:00.714660  8746 status_update_manager.cpp:322] Received status update TASK_RUNNING (UUID: 23650bfa-b3ac-470e-a901-b4171542cc32) for task a2f7263f-17aa-4849-a0dc-7c8453b3b81f of framework d59449fc-5462-43c5-b935-e05563fdd4b6-0000
I1113 16:44:00.714860  8747 slave.cpp:2762] Handling status update TASK_RUNNING (UUID: 6111319b-d579-4cd2-af67-ace8f47fda5b) for task a6ef5d55-4c84-4bc6-bd4a-a1f07de3fcc8 of framework d59449fc-5462-43c5-b935-e05563fdd4b6-0000 from executor(6)@10.0.2.15:45384
I1113 16:44:00.715095  8747 slave.cpp:3087] Forwarding the update TASK_RUNNING (UUID: 23650bfa-b3ac-470e-a901-b4171542cc32) for task a2f7263f-17aa-4849-a0dc-7c8453b3b81f of framework d59449fc-5462-43c5-b935-e05563fdd4b6-0000 to master@10.0.2.15:45384
I1113 16:44:00.715107  8745 status_update_manager.cpp:322] Received status update TASK_RUNNING (UUID: 6111319b-d579-4cd2-af67-ace8f47fda5b) for task a6ef5d55-4c84-4bc6-bd4a-a1f07de3fcc8 of framework d59449fc-5462-43c5-b935-e05563fdd4b6-0000
I1113 16:44:00.715265  8747 slave.cpp:3011] Sending acknowledgement for status update TASK_RUNNING (UUID: 23650bfa-b3ac-470e-a901-b4171542cc32) for task a2f7263f-17aa-4849-a0dc-7c8453b3b81f of framework d59449fc-5462-43c5-b935-e05563fdd4b6-0000 to executor(5)@10.0.2.15:45384
I1113 16:44:00.715437  8746 master.cpp:4414] Status update TASK_RUNNING (UUID: 23650bfa-b3ac-470e-a901-b4171542cc32) for task a2f7263f-17aa-4849-a0dc-7c8453b3b81f of framework d59449fc-5462-43c5-b935-e05563fdd4b6-0000 from slave d59449fc-5462-43c5-b935-e05563fdd4b6-S1 at slave(6)@10.0.2.15:45384 (maintenance-host-2)
I1113 16:44:00.715646  8746 master.cpp:4462] Forwarding status update TASK_RUNNING (UUID: 23650bfa-b3ac-470e-a901-b4171542cc32) for task a2f7263f-17aa-4849-a0dc-7c8453b3b81f of framework d59449fc-5462-43c5-b935-e05563fdd4b6-0000
I1113 16:44:00.715909  8746 master.cpp:6066] Updating the state of task a2f7263f-17aa-4849-a0dc-7c8453b3b81f of framework d59449fc-5462-43c5-b935-e05563fdd4b6-0000 (latest state: TASK_RUNNING, status update state: TASK_RUNNING)
I1113 16:44:00.715570  8742 slave.cpp:3087] Forwarding the update TASK_RUNNING (UUID: 6111319b-d579-4cd2-af67-ace8f47fda5b) for task a6ef5d55-4c84-4bc6-bd4a-a1f07de3fcc8 of framework d59449fc-5462-43c5-b935-e05563fdd4b6-0000 to master@10.0.2.15:45384
I1113 16:44:00.716055  8742 slave.cpp:3011] Sending acknowledgement for status update TASK_RUNNING (UUID: 6111319b-d579-4cd2-af67-ace8f47fda5b) for task a6ef5d55-4c84-4bc6-bd4a-a1f07de3fcc8 of framework d59449fc-5462-43c5-b935-e05563fdd4b6-0000 to executor(6)@10.0.2.15:45384
I1113 16:44:00.716188  8742 master.cpp:4414] Status update TASK_RUNNING (UUID: 6111319b-d579-4cd2-af67-ace8f47fda5b) for task a6ef5d55-4c84-4bc6-bd4a-a1f07de3fcc8 of framework d59449fc-5462-43c5-b935-e05563fdd4b6-0000 from slave d59449fc-5462-43c5-b935-e05563fdd4b6-S0 at slave(5)@10.0.2.15:45384 (maintenance-host)
I1113 16:44:00.716226  8742 master.cpp:4462] Forwarding status update TASK_RUNNING (UUID: 6111319b-d579-4cd2-af67-ace8f47fda5b) for task a6ef5d55-4c84-4bc6-bd4a-a1f07de3fcc8 of framework d59449fc-5462-43c5-b935-e05563fdd4b6-0000
I1113 16:44:00.716395  8742 master.cpp:6066] Updating the state of task a6ef5d55-4c84-4bc6-bd4a-a1f07de3fcc8 of framework d59449fc-5462-43c5-b935-e05563fdd4b6-0000 (latest state: TASK_RUNNING, status update state: TASK_RUNNING)
I1113 16:44:00.719805  8748 http.cpp:338] HTTP POST for /master/api/v1/scheduler from 10.0.2.15:60992
I1113 16:44:00.719944  8748 master.cpp:3571] Processing ACKNOWLEDGE call 23650bfa-b3ac-470e-a901-b4171542cc32 for task a2f7263f-17aa-4849-a0dc-7c8453b3b81f of framework d59449fc-5462-43c5-b935-e05563fdd4b6-0000 (default) on slave d59449fc-5462-43c5-b935-e05563fdd4b6-S1
I1113 16:44:00.720374  8748 status_update_manager.cpp:394] Received status update acknowledgement (UUID: 23650bfa-b3ac-470e-a901-b4171542cc32) for task a2f7263f-17aa-4849-a0dc-7c8453b3b81f of framework d59449fc-5462-43c5-b935-e05563fdd4b6-0000
I1113 16:44:00.723330  8743 http.cpp:338] HTTP POST for /master/api/v1/scheduler from 10.0.2.15:60994
I1113 16:44:00.723467  8743 master.cpp:3571] Processing ACKNOWLEDGE call 6111319b-d579-4cd2-af67-ace8f47fda5b for task a6ef5d55-4c84-4bc6-bd4a-a1f07de3fcc8 of framework d59449fc-5462-43c5-b935-e05563fdd4b6-0000 (default) on slave d59449fc-5462-43c5-b935-e05563fdd4b6-S0
I1113 16:44:00.724144  8743 status_update_manager.cpp:394] Received status update acknowledgement (UUID: 6111319b-d579-4cd2-af67-ace8f47fda5b) for task a6ef5d55-4c84-4bc6-bd4a-a1f07de3fcc8 of framework d59449fc-5462-43c5-b935-e05563fdd4b6-0000
I1113 16:44:00.726418  8742 http.cpp:338] HTTP POST for /master/api/v1/scheduler from 10.0.2.15:60996
I1113 16:44:00.726517  8742 master.cpp:3297] Processing DECLINE call for offers: [ d59449fc-5462-43c5-b935-e05563fdd4b6-O3 ] for framework d59449fc-5462-43c5-b935-e05563fdd4b6-0000 (default)
I1113 16:44:00.729523  8745 http.cpp:338] HTTP POST for /master/api/v1/scheduler from 10.0.2.15:60998
W1113 16:44:00.729823  8745 master.cpp:2878] ACCEPT call used invalid offers '[ d59449fc-5462-43c5-b935-e05563fdd4b6-O2 ]': Offer d59449fc-5462-43c5-b935-e05563fdd4b6-O2 is no longer valid
I1113 16:44:01.695679  8744 master.cpp:5069] Sending 1 inverse offers to framework d59449fc-5462-43c5-b935-e05563fdd4b6-0000 (default)
I1113 16:44:01.701465  8743 http.cpp:338] HTTP POST for /master/api/v1/scheduler from 10.0.2.15:32768
I1113 16:44:01.701598  8743 master.cpp:3297] Processing DECLINE call for offers: [ d59449fc-5462-43c5-b935-e05563fdd4b6-O4 ] for framework d59449fc-5462-43c5-b935-e05563fdd4b6-0000 (default)
I1113 16:44:02.767982  8742 master.cpp:5069] Sending 2 inverse offers to framework d59449fc-5462-43c5-b935-e05563fdd4b6-0000 (default)
/mesos/src/tests/master_maintenance_tests.cpp:1594: Failure
Value of: event.get().offers().inverse_offers().size()
  Actual: 2
Expected: 1
I1113 16:44:02.772536  8728 master.cpp:922] Master terminating
W1113 16:44:02.773030  8728 master.cpp:6118] Removing task a2f7263f-17aa-4849-a0dc-7c8453b3b81f with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] of framework d59449fc-5462-43c5-b935-e05563fdd4b6-0000 on slave d59449fc-5462-43c5-b935-e05563fdd4b6-S1 at slave(6)@10.0.2.15:45384 (maintenance-host-2) in non-terminal state TASK_RUNNING
I1113 16:44:02.774067  8728 master.cpp:6161] Removing executor 'executor-1' with resources  of framework d59449fc-5462-43c5-b935-e05563fdd4b6-0000 on slave d59449fc-5462-43c5-b935-e05563fdd4b6-S1 at slave(6)@10.0.2.15:45384 (maintenance-host-2)
W1113 16:44:02.774865  8728 master.cpp:6118] Removing task a6ef5d55-4c84-4bc6-bd4a-a1f07de3fcc8 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] of framework d59449fc-5462-43c5-b935-e05563fdd4b6-0000 on slave d59449fc-5462-43c5-b935-e05563fdd4b6-S0 at slave(5)@10.0.2.15:45384 (maintenance-host) in non-terminal state TASK_RUNNING
I1113 16:44:02.775600  8728 master.cpp:6161] Removing executor 'executor-2' with resources  of framework d59449fc-5462-43c5-b935-e05563fdd4b6-0000 on slave d59449fc-5462-43c5-b935-e05563fdd4b6-S0 at slave(5)@10.0.2.15:45384 (maintenance-host)
I1113 16:44:02.777118  8743 slave.cpp:3215] master@10.0.2.15:45384 exited
W1113 16:44:02.777174  8743 slave.cpp:3218] Master disconnected! Waiting for a new master to be elected
I1113 16:44:02.777202  8743 slave.cpp:3215] master@10.0.2.15:45384 exited
W1113 16:44:02.777222  8743 slave.cpp:3218] Master disconnected! Waiting for a new master to be elected
I1113 16:44:02.773278  8749 hierarchical.cpp:373] Removed slave d59449fc-5462-43c5-b935-e05563fdd4b6-S1
E1113 16:44:02.778477  8749 scheduler.cpp:433] End-Of-File received from master. The master closed the event stream
I1113 16:44:02.786072  8728 slave.cpp:601] Slave terminating
I1113 16:44:02.787068  8728 slave.cpp:2009] Asked to shut down framework d59449fc-5462-43c5-b935-e05563fdd4b6-0000 by @0.0.0.0:0
I1113 16:44:02.787595  8728 slave.cpp:2034] Shutting down framework d59449fc-5462-43c5-b935-e05563fdd4b6-0000
I1113 16:44:02.788187  8728 slave.cpp:3863] Shutting down executor 'executor-2' of framework d59449fc-5462-43c5-b935-e05563fdd4b6-0000 at executor(6)@10.0.2.15:45384
I1113 16:44:02.786433  8748 slave.cpp:3553] Executor 'executor-1' of framework d59449fc-5462-43c5-b935-e05563fdd4b6-0000 exited with status 0
I1113 16:44:02.790580  8748 slave.cpp:2762] Handling status update TASK_FAILED (UUID: a506266e-6515-4cf2-9f4d-7f75698fa90f) for task a2f7263f-17aa-4849-a0dc-7c8453b3b81f of framework d59449fc-5462-43c5-b935-e05563fdd4b6-0000 from @0.0.0.0:0
I1113 16:44:02.791623  8749 status_update_manager.cpp:322] Received status update TASK_FAILED (UUID: a506266e-6515-4cf2-9f4d-7f75698fa90f) for task a2f7263f-17aa-4849-a0dc-7c8453b3b81f of framework d59449fc-5462-43c5-b935-e05563fdd4b6-0000
I1113 16:44:02.792326  8749 slave.cpp:3087] Forwarding the update TASK_FAILED (UUID: a506266e-6515-4cf2-9f4d-7f75698fa90f) for task a2f7263f-17aa-4849-a0dc-7c8453b3b81f of framework d59449fc-5462-43c5-b935-e05563fdd4b6-0000 to master@10.0.2.15:45384
I1113 16:44:02.793318  8749 slave.cpp:601] Slave terminating
I1113 16:44:02.793442  8749 slave.cpp:2009] Asked to shut down framework d59449fc-5462-43c5-b935-e05563fdd4b6-0000 by @0.0.0.0:0
I1113 16:44:02.793474  8749 slave.cpp:2034] Shutting down framework d59449fc-5462-43c5-b935-e05563fdd4b6-0000
I1113 16:44:02.793625  8749 slave.cpp:3657] Cleaning up executor 'executor-1' of framework d59449fc-5462-43c5-b935-e05563fdd4b6-0000 at executor(5)@10.0.2.15:45384
I1113 16:44:02.794205  8746 gc.cpp:56] Scheduling '/tmp/MasterMaintenanceTest_InverseOffersFilters_CDFgvt/slaves/d59449fc-5462-43c5-b935-e05563fdd4b6-S1/frameworks/d59449fc-5462-43c5-b935-e05563fdd4b6-0000/executors/executor-1/runs/49d1654a-180d-406c-82f4-ab1ebd168451' for gc 6.99999080924741days in the future
I1113 16:44:02.794234  8749 slave.cpp:3745] Cleaning up framework d59449fc-5462-43c5-b935-e05563fdd4b6-0000
I1113 16:44:02.794458  8746 gc.cpp:56] Scheduling '/tmp/MasterMaintenanceTest_InverseOffersFilters_CDFgvt/slaves/d59449fc-5462-43c5-b935-e05563fdd4b6-S1/frameworks/d59449fc-5462-43c5-b935-e05563fdd4b6-0000/executors/executor-1' for gc 6.99999080794074days in the future
I1113 16:44:02.794710  8746 gc.cpp:56] Scheduling '/tmp/MasterMaintenanceTest_InverseOffersFilters_CDFgvt/slaves/d59449fc-5462-43c5-b935-e05563fdd4b6-S1/frameworks/d59449fc-5462-43c5-b935-e05563fdd4b6-0000' for gc 6.9999908036days in the future
I1113 16:44:02.794536  8747 status_update_manager.cpp:284] Closing status update streams for framework d59449fc-5462-43c5-b935-e05563fdd4b6-0000
[  FAILED  ] MasterMaintenanceTest.InverseOffersFilters (4316 ms)
[----------] 1 test from MasterMaintenanceTest (4316 ms total)

[----------] Global test environment tear-down
[==========] 1 test from 1 test case ran. (4320 ms total)
[  PASSED  ] 0 tests.
[  FAILED  ] 1 test, listed below:
[  FAILED  ] MasterMaintenanceTest.InverseOffersFilters

 1 FAILED TEST
{code}",Bug,Major,neilc,2015-12-03T16:38:49.000+0000,5,Resolved,Complete,MasterMaintenanceTest.InverseOffersFilters is flaky,2015-12-03T16:53:52.000+0000,MESOS-3916,3.0,mesos,Mesosphere Sprint 23
,2015-11-13T12:24:53.000+0000,alexr,"Having an empty role (empty string) looks like a terrible idea, but we do not prohibit it. I think we should add corresponding checks and update the docs to officially disallow empty roles.",Improvement,Minor,alexr,,10020,Accepted,In Progress,Disallow empty string roles,2015-12-18T23:37:29.000+0000,MESOS-3913,3.0,mesos,
alexr,2015-11-13T09:56:40.000+0000,alexr,"When a quota request comes in, we may need to rescind a certain amount of outstanding offers in order to satisfy it. Because resources are allocated in the allocator, there can be a race between rescinding and allocating. This race makes it hard to determine the exact amount of offers that should be rescinded in the master.",Task,Major,alexr,2015-11-25T23:37:16.000+0000,5,Resolved,Complete,Rescind offers in order to satisfy quota,2015-11-25T23:37:16.000+0000,MESOS-3912,3.0,mesos,Mesosphere Sprint 22
js84,2015-11-13T09:04:10.000+0000,alexr,"There are use cases when an operator may want to disable the sanity check for quota endpoints (MESOS-3074), even if this renders the cluster under quota. For example, an operator sets quota before adding more agents in order to make sure that no non-quota allocations from new agents are made. ",Task,Major,alexr,2015-11-25T21:19:36.000+0000,5,Resolved,Complete,Add a `--force` flag to disable sanity check in quota,2015-11-25T21:19:36.000+0000,MESOS-3911,1.0,mesos,Mesosphere Sprint 22
kaysoky,2015-11-13T00:33:16.000+0000,kaysoky,"The {{socket_manager}} and {{process_manager}} are intricately tied together.  Currently, only the {{process_manager}} is cleaned up by {{process::finalize}}.

To clean up the {{socket_manager}}, we must close all sockets and deallocate any existing {{HttpProxy}} or {{Encoder}} objects.  And we should prevent further objects from being created/tracked by the {{socket_manager}}.

*Proposal*
# Clean up all processes other than {{gc}}.  This will clear all links and delete all {{HttpProxy}} s while {{socket_manager}} still exists.
# Close all sockets via {{SocketManager::close}}.  All of {{socket_manager}} 's state is cleaned up via {{SocketManager::close}}, including termination of {{HttpProxy}} (termination is idempotent, meaning that killing {{HttpProxy}} s via {{process_manager}} is safe).
# At this point, {{socket_manager}} should be empty and only the {{gc}} process should be running.  (Since we're finalizing, assume there are no threads trying to spawn processes.)  {{socket_manager}} can be deleted.
# {{gc}} can be deleted.  This is currently a leaked pointer, so we'll also need to track and delete that.
# {{process_manager}} should be devoid of processes, so we can proceed with cleanup (join threads, stop the {{EventLoop}}, etc).",Task,Major,kaysoky,,10006,Reviewable,New,Libprocess: Implement cleanup of the SocketManager in process::finalize,2016-04-07T21:23:15.000+0000,MESOS-3910,5.0,mesos,
jamespeach,2015-11-13T00:11:31.000+0000,jamespeach,"When trying to build an isolator module, stout headers end up depending on {{picojson.hpp}} which is not installed.

{code}
In file included from /opt/mesos/include/mesos/module/isolator.hpp:25:
In file included from /opt/mesos/include/mesos/slave/isolator.hpp:30:
In file included from /opt/mesos/include/process/dispatch.hpp:22:
In file included from /opt/mesos/include/process/process.hpp:26:
In file included from /opt/mesos/include/process/event.hpp:21:
In file included from /opt/mesos/include/process/http.hpp:39:
/opt/mesos/include/stout/json.hpp:23:10: fatal error: 'picojson.h' file not found
#include <picojson.h>
         ^
8 warnings and 1 error generated.
{code}",Bug,Major,jamespeach,2016-01-04T17:47:25.000+0000,5,Resolved,Complete,isolator module headers depend on picojson headers,2016-01-19T22:02:25.000+0000,MESOS-3909,3.0,mesos,Mesosphere Sprint 25
bernd-mesos,2015-11-12T15:43:28.000+0000,bernd-mesos,"These flags were added to ""slave/flags.cpp"", but are not mentioned in ""docs/configuration.md"":

  add(&Flags::docker_auth_server,
      ""docker_auth_server"",
      ""Docker authentication server"",
      ""auth.docker.io"");

  add(&Flags::docker_auth_server_port,
      ""docker_auth_server_port"",
      ""Docker authentication server port"",
      ""443"");
  add(&Flags::docker_puller_timeout_secs,
      ""docker_puller_timeout"",
      ""Timeout value in seconds for pulling images from Docker registry"",
      ""60"");

  add(&Flags::docker_registry,
      ""docker_registry"",
      ""Default Docker image registry server host"",
      ""registry-1.docker.io"");
  add(&Flags::docker_registry_port,
      ""docker_registry_port"",
      ""Default Docker registry server port"",
      ""443"");
",Documentation,Minor,bernd-mesos,2015-11-12T16:32:36.000+0000,5,Resolved,Complete,Five new docker-related slave flags are not covered by the configuration documentation.,2015-11-12T16:32:36.000+0000,MESOS-3905,1.0,mesos,Mesosphere Sprint 22
greggomann,2015-11-12T04:45:58.000+0000,greggomann,"This is the fourth in a series of tickets that adds authorization support for persistent volumes.

We need to add ACL authorization for the '/create-volume' and '/destroy-volume' HTTP endpoints. In other complementary work, authorization for frameworks performing {{CREATE}} and {{DESTROY}} operations is being added by MESOS-3065.

This will consist of adding authorization calls into the HTTP endpoint code in {{src/master/http.cpp}}, as well as tests for both failed & successful calls to '/create-volumes' and '/destroy-volumes' with authorization. We also must ensure that the {{principal}} field of {{Resource.DiskInfo.Persistence}} is being populated correctly.",Improvement,Major,greggomann,2016-01-09T01:24:27.000+0000,5,Resolved,Complete,Add authorization for '/create-volume' and '/destroy-volume' HTTP endpoints,2016-01-09T01:24:27.000+0000,MESOS-3903,2.0,mesos,Mesosphere Sprint 26
jojy,2015-11-11T18:54:54.000+0000,jojy,"As a first step to adding capability for building multiple configurations on reviewbot, we need to change the build scripts to use docker. ",Task,Major,jojy,2015-11-11T18:56:49.000+0000,5,Resolved,Complete,Enable mesos-reviewbot project on jenkins to use docker,2015-11-11T18:56:49.000+0000,MESOS-3900,3.0,mesos,Mesosphere Sprint 22
nfnt,2015-11-11T14:11:38.000+0000,nfnt,"The JSON examples in the documentation of the commandline flags ({{mesos-master.sh --help}} and {{mesos-slave.sh --help}}) don't have a consistent formatting. Furthermore, some examples aren't even compliant JSON because they have trailing commas were they shouldn't.",Bug,Minor,nfnt,2015-11-25T08:54:14.000+0000,5,Resolved,Complete,Wrong syntax and inconsistent formatting of JSON examples in flag documentation,2015-11-25T08:54:14.000+0000,MESOS-3899,1.0,mesos,Mesosphere Sprint 22
klaus1982,2015-11-11T13:59:36.000+0000,hartem,An example is the when lender launches the task on an agent followed by a  borrower launching a task on the same agent before the optimistic offer is rescinded. ,Bug,Major,hartem,,10020,Accepted,In Progress,Identify and implement test cases for handling a race between optimistic lender and tenant offers.,2016-03-17T13:07:01.000+0000,MESOS-3898,13.0,mesos,
klaus1982,2015-11-11T13:54:41.000+0000,hartem,"MESOS-XXX: Optimsistic accounter

{code}
    class HierarchicalAllocatorProcess 
    {
      struct Slave
      {
        ...
        struct Optimistic 
        {
          Resources total; // The total allocation slack resources
          Resources allocated; // The allocated allocation slack resources
        };
    
        Optimistic optimistic;
      };
    }
{code}

MESOS-4146: flatten & allocationSlack for Optimistic Offer

{code}
    class Resources
    {
        // Returns a Resources object with the same amount of each resource
        // type as these Resources, but with all Resource objects marked as
        // the specified `RevocableInfo::Type`; the other attribute is not
        // affected.
        Resources flatten(Resource::RevocableInfo::Type type);

        // Return a Resources object that:
        //   - if role is given, the resources did not include role's reserved
        //     resources.
        //   - the resources's revocable type is `ALLOCATION_SLACK`
        //   - the role of resources is set to ""*""
        Resources allocationSlack(Option<string> role = None());
    }
{code}

MESOS-XXX: Allocate the allocation_slack resources to framework

{code}
    void HierarchicalAllocatorProcess::allocate(
        const hashset<SlaveID>& slaveIds_)
    {
      foreach slave; foreach role; foreach framework
      {
        Resource optimistic;

        if (framework.revocable) {
          Resources total = slaves[slaveId].optimistic.total.allocationSlack(role);
          optimistic = total - slaves[slaveId].optimistic.allocated;
        }

        ...
        offerable[frameworkId][slaveId] += resources + optimistic;

        ...
        slaves[slaveId].optimistic.allocated += optimistic;
      }
    }
{code}
  
Here's some consideration about `ALLOCATION_SLACK`:

1. 'Old' resources (available/total) did not include ALLOCATION_SLACK
2. After `Quota`, `remainingClusterResources.contains` should not check ALLOCATION_SLACK; if there no enough resources,  master can still offer ALLOCATION_SALCK resources.
3. In sorter, it'll not include ALLOCATION_SLACK; as those resources are borrowed from other role/framework
4. If either normal resources or ALLOCATION_SLACK resources are allocable/!filtered, it can be offered to framework
5. Currently, allocator will assign all ALLOCATION_SALCK resources in slave to one framework

MESOS-XXX: Update ALLOCATION_SLACK for dynamic reservation (updateAllocation)

{code}
    void HierarchicalAllocatorProcess::updateAllocation(
        const FrameworkID& frameworkId,
        const SlaveID& slaveId,
        const vector<Offer::Operation>& operations)
    {
        ...
        Try<Resources> updatedOptimistic =
            slaves[slaveId].optimistic.total.apply(operations);
        CHECK_SOME(updatedTotal);

        slaves[slaveId].optimistic.total =
            updatedOptimistic.get().stateless().reserved().flatten(ALLOCATION_SLACK);
        ...
    }
{code}
    
MESOS-XXX: Add ALLOCATION_SLACK when slaver register/re-register (addSlave)

{code}
    void HierarchicalAllocatorProcess::addSlave(
        const SlaveID& slaveId,
        const SlaveInfo& slaveInfo,
        const Option<Unavailability>& unavailability,
        const Resources& total,
        const hashmap<FrameworkID, Resources>& used)
    {
      ...
      slaves[slaveId].optimistic.total =
          total.stateless().reserved().flatten(ALLOCATION_SLACK);
      ...
    }
{code}
  
No need to handle `removeSlave`, it'll all related info from `slaves` including `optimistic`.

MESOS-XXX: return resources to allocator (recoverResources)

{code}
    void HierarchicalAllocatorProcess::recoverResources(
        const FrameworkID& frameworkId,
        const SlaveID& slaveId,
        const Resources& resources,
        const Option<Filters>& filters)
    {
      if (slaves.contains(slaveId))
      {
        ...
        slaves[slaveId].optimistic.allocated -= resources.allocationSlack();
        ...
      }
    }
{code}",Bug,Major,hartem,,10020,Accepted,In Progress,Add accounting for reservation slack in the allocator.,2016-03-17T13:05:52.000+0000,MESOS-3896,13.0,mesos,
gyliu,2015-11-11T13:38:18.000+0000,hartem,Write a test to ensure that the allocator performs the reservation slack calculations correctly.,Bug,Major,hartem,,10020,Accepted,In Progress,Implement tests for verifying allocator resource math.,2016-03-17T13:06:20.000+0000,MESOS-3893,8.0,mesos,
klaus1982,2015-11-11T13:27:08.000+0000,hartem,"In the agent, add a helper function to get the list of the exeuctor using ALLOCATION_SLACK.

It's short term solution which is different the design document, because master did not have executor for command line executor. Send evicatble executors from master to slave will addess in post-MVP after MESOS-1718.

{noformat}
class Slave {
...
  // If the executor used revocable resources, add it into `evictableExecutors`
  // list.
  void addEvictableExecutor(Executor* executor);

  // If the executor used revocable resources, remove it from
  // `evictableExecutors` list.
  void removeEvictableExecutor(Executor* executor);

  // Get evictable executor ID list by `request resources`. The return value is `Result<list<Executor*>>`:
  //  - if `isError()`, there's not enough resources to launch tasks
  //  - if `isNone()`, no evictable exectuors need to be terminated
  //  - if !`isNone()`, the list of executors that need to be evicted for resources
  Result<std::list<Executor*>> getEvictableExecutors(const Resources& request);

...

  // The map of evictable executor list. If there's not enough resources,
  // the evictable executor will be terminated by slave to release resources.
  hashmap<FrameworkID, std::set<ExecutorID>> evictableExecutors;
...
}
{noformat}
",Bug,Major,hartem,,10006,Reviewable,New,"Add a helper function to the Agent to retrieve the list of executors that are using optimistically offered, revocable resources.",2016-03-17T13:06:30.000+0000,MESOS-3892,5.0,mesos,
gyliu,2015-11-11T13:23:53.000+0000,hartem,"Launching a task using revocable resources should be funnelled through an accounting system:

* If a task is launched using revocable resources, the resources must not be in use when launching the task.  If they are in use, then the task should fail to start.
* If a task is launched using reserved resources, the resources must be made available.  This means potentially evicting tasks which are using revocable resources.

Both cases could be implemented by adding a check in Slave::runTask, like a new helper method:

{noformat}
class Slave {
  ...
  // Checks if the given resources are available (i.e. not utilized)
  // for starting a task.  If not, the task should either fail to
  // start or result in the eviction of revocable resources.
  virtual process::Future<bool> checkAvailableResources(
      const Resources& resources);
  ...
}
{noformat}",Bug,Major,hartem,,10020,Accepted,In Progress,Add a helper function to the Agent to check available resources before launching a task. ,2016-03-17T13:06:43.000+0000,MESOS-3891,5.0,mesos,
gyliu,2015-11-11T13:17:40.000+0000,hartem,"{code}
// Evict Resources to launch tasks.
  message Revocation {
    optional FrameworkID framework_id = 1;
    required string role = 2;
    repeated Resource revocable_resources = 3;
  }
  repeated Revocation revocations = 5;
{code}",Bug,Major,hartem,,10006,Reviewable,New,Add notion of evictable task to RunTaskMessage,2016-03-17T13:07:36.000+0000,MESOS-3890,2.0,mesos,
klaus1982,2015-11-11T13:16:40.000+0000,hartem,"The oversubcription documentation currently assumes that oversubscribed resources ({{USAGE_SLACK}}) are the only type of revocable resources.  Optimistic offers will add a second type of revocable resource ({{ALLOCATION_SLACK}}) that should not be acted upon by oversubscription components.

For example, the [oversubscription doc|http://mesos.apache.org/documentation/latest/oversubscription/] says the following:
{quote}
NOTE: If any resource used by a task or executor is revocable, the whole container is treated as a revocable container and can therefore be killed or throttled by the QoS Controller.
{quote}
which we may amend to something like:
{quote}
NOTE: If any resource used by a task or executor is revocable usage slack, the whole container is treated as an oversubscribed container and can therefore be killed or throttled by the QoS Controller.
{quote}",Bug,Major,hartem,,10020,Accepted,In Progress,Modify Oversubscription documentation to explicitly forbid the QoS Controller from killing executors running on optimistically offered resources.,2016-03-17T13:07:52.000+0000,MESOS-3889,2.0,mesos,
klaus1982,2015-11-11T13:15:06.000+0000,hartem,"Add enum type into RevocableInfo: 

* Framework need to assign RevocableInfo when launching task; if it’s not assign, use reserved resources. Framework need to identify which resources it’s using
* Oversubscription resources need to assign the type by Agent (MESOS-3930)
* Update Oversubscription document that OO has over-subscribe the Allocation Slack and recommend QoS to handle the usage slack only. (MESOS-3889)

{code}
message Resource {
  ...
  message RevocableInfo {
   enum Type {
     // Under-utilized, allocated resources.  Controlled by
     // oversubscription (QoSController & ResourceEstimator).
     USAGE_SLACK = 1;

     // Unallocated, reserved resources.
     // Controlled by optimistic offers (Allocator).
     ALLOCATION_SLACK = 2; 
   }

   optional Type type = 1;
  }
 ...
  optional RevocableInfo revocable = 9;
 }
{code}
",Bug,Major,hartem,,10006,Reviewable,New,Support distinguishing revocable resources in the Resource protobuf.,2016-03-17T13:08:04.000+0000,MESOS-3888,2.0,mesos,
alexr,2015-11-11T09:57:47.000+0000,alexr,"The built-in allocator code has some style issues (namespaces in the .cpp file, unfortunate formatting) which should be corrected for readability.",Improvement,Minor,alexr,2015-11-11T14:27:49.000+0000,5,Resolved,Complete,Corrected style in hierarchical allocator,2015-11-11T14:27:49.000+0000,MESOS-3884,1.0,mesos,Mesosphere Sprint 22
,2015-11-11T06:52:34.000+0000,hartem,"{quote}
That said, this can be automated as a step in apply-reviews script. For
example, the script can check if something in site/ (or docs/ ?) is being
committed and if yes, also do an svn update. @artem do you want to take
this on as you revamp the apply-reviews script?

On Tue, Nov 10, 2015 at 1:23 AM, Adam Bordelon <adam@mesosphere.io> wrote:

> Since it's still a manual process, the website is usually only updated a)
> when we have a new release to announce, or b) when some other blog-worthy
> content arises (e.g. MesosCon).
{quote}

https://mail-archives.apache.org/mod_mbox/mesos-dev/201511.mbox/%3CCAAkWvAzqJQ9kmdpcAQ_F%2Bh1bNnzBrRkNQZXkwjWzTRiHUf66fg%40mail.gmail.com%3E",Bug,Major,hartem,,1,Open,New,Add support to apply-reviews.py to update SVN when necessary. ,2015-11-11T06:52:34.000+0000,MESOS-3883,3.0,mesos,
kaysoky,2015-11-10T21:54:56.000+0000,kaysoky,"Tracks this [TODO|https://github.com/apache/mesos/blob/aa0cd7ed4edf1184cbc592b5caa2429a8373e813/3rdparty/libprocess/src/process.cpp#L974-L975].

The {{Clock}} is initialized with a callback that, among other things, will dereference the global {{process_manager}} object.

When libprocess is shutting down, the {{process_manager}} is cleaned up.  Between cleanup and termination of libprocess, there is some chance that a {{Timer}} will time out and result in dereferencing {{process_manager}}.

*Proposal* 
* Implement {{Clock::finalize}}.  This would clear:
** existing timers
** process-specific clocks
** ticks
* Change {{process::finalize}}.
*# Resume the clock.  (The clock is only paused during some tests.)  When the clock is not paused, the callback does not dereference {{process_manager}}.
*# Clean up {{process_manager}}.  This terminates all the processes that would potentially interact with {{Clock}}.
*# Call {{Clock::finalize}}.",Task,Major,kaysoky,2015-11-23T20:14:02.000+0000,5,Resolved,Complete,Libprocess: Implement process::Clock::finalize,2016-01-12T21:22:21.000+0000,MESOS-3882,3.0,mesos,Mesosphere Sprint 22
,2015-11-10T18:38:20.000+0000,alexr,"We are rather inconsistent in the way we write log messages. It would be helpful to come up with a style and document various aspects of logs, including but not limited to:
* Usage of backticks and/or single quotes to quote interpolated variables;
* Usage of backticks and/or single quotes to quote types and other names;
* Usage of tenses and other grammatical forms;
* Proper way of nesting [error] messages;",Documentation,Major,alexr,,10020,Accepted,In Progress,Propose a guideline for log messages,2015-11-13T16:20:27.000+0000,MESOS-3880,5.0,mesos,
js84,2015-11-10T18:28:43.000+0000,js84,"We currently have an inconsistent (and mostly incorrect) include order for <gmock/gmock.h> and <gtest/gtest.h> (see below). Some files include them (incorrectly)  between the c and cpp standard header, while other correclt include them afterwards. According to the [Google Styleguide| https://google.github.io/styleguide/cppguide.html#Names_and_Order_of_Includes] the second include order is correct.


{code:title=external_containerizer_test.cpp}
#include <unistd.h>

#include <gmock/gmock.h>

#include <string>
{code}

{code:title=launcher.hpp}
#include <vector>

#include <gmock/gmock.h>
{code}",Bug,Minor,js84,2015-11-11T01:04:26.000+0000,5,Resolved,Complete,Incorrect and inconsistent include order for <gmock/gmock.h> and <gtest/gtest.h>.,2015-11-11T01:04:26.000+0000,MESOS-3879,1.0,mesos,Mesosphere Sprint 22
js84,2015-11-10T17:25:06.000+0000,alexr,Draft an operator guide for quota which describes basic usage of the endpoints and few basic and advanced usage cases.,Task,Major,alexr,2016-01-08T13:10:45.000+0000,5,Resolved,Complete,Draft operator documentation for quota,2016-01-16T23:31:01.000+0000,MESOS-3877,5.0,mesos,Mesosphere Sprint 25
alexr,2015-11-10T16:45:42.000+0000,alexr,"Dynamic reservations—whether allocated or not—should be accounted towards role's quota. This requires update in at least two places:
* The built-in allocator, which actually satisfies quota;
* The sanity check in the master.",Task,Major,alexr,,10020,Accepted,In Progress,Account dynamic reservations towards quota,2015-11-10T16:45:59.000+0000,MESOS-3875,3.0,mesos,
alexr,2015-11-10T16:36:42.000+0000,alexr,The built-in Hierarchical allocator should implement the recovery (in the presence of quota).,Task,Major,alexr,2015-11-23T09:39:38.000+0000,5,Resolved,Complete,Investigate recovery for the Hierarchical allocator,2015-11-23T09:39:38.000+0000,MESOS-3874,3.0,mesos,Mesosphere Sprint 22
alexr,2015-11-10T16:33:26.000+0000,alexr,There are some scenarios (e.g. quota is set for some roles) when it makes sense to notify an allocator about the recovery. Introduce a method into the allocator interface that allows for this.,Task,Major,alexr,2015-11-22T19:14:36.000+0000,5,Resolved,Complete,Enhance allocator interface with the recovery() method,2015-11-22T19:14:36.000+0000,MESOS-3873,3.0,mesos,Mesosphere Sprint 22
greggomann,2015-11-10T00:50:57.000+0000,greggomann,"A `principal` field is being added to the `Resource.DiskInfo.Persistence` message to facilitate authorization of persistent volume creation/deletion. In the long-run it should be a required field, but it's being initially introduced as optional to avoid breaking existing frameworks. The field should be changed to required at the end of a deprecation cycle.",Improvement,Major,greggomann,2015-11-12T22:44:29.000+0000,5,Resolved,Complete,Make `Resource.DiskInfo.Persistence.principal` a required field,2015-11-12T22:44:29.000+0000,MESOS-3867,1.0,mesos,
kaysoky,2015-11-09T22:21:13.000+0000,kaysoky,"Tracks this [TODO|https://github.com/apache/mesos/blob/3bda55da1d0b580a1b7de43babfdc0d30fbc87ea/3rdparty/libprocess/src/process.cpp#L749].

The [synchronization logic of libprocess|https://github.com/apache/mesos/commit/cd757cf75637c92c438bf4cd22f21ba1b5be702f#diff-128d3b56fc8c9ec0176fdbadcfd11fc2] [predates abstractions|https://github.com/apache/mesos/commit/6c3b107e4e02d5ba0673eb3145d71ec9d256a639#diff-0eebc8689450916990abe080d86c2acb] like {{process::Once}}, which is used in almost all other one-time initialization blocks.  

The logic should be documented.  It can also be simplified (see the [review description|https://reviews.apache.org/r/39949/]).  Or it can be replaced with {{process::Once}}.",Task,Minor,kaysoky,2015-11-23T19:40:04.000+0000,5,Resolved,Complete,Simplify and/or document the libprocess initialization synchronization logic,2015-11-23T19:40:04.000+0000,MESOS-3864,1.0,mesos,Mesosphere Sprint 22
kaysoky,2015-11-09T22:13:04.000+0000,kaysoky,"This issue is for investigating what needs to be added/changed in {{process::finalize}} such that {{process::initialize}} will start on a clean slate.  Additional issues will be created once done.  Also see [the parent issue|MESOS-3820].

{{process::finalize}} should cover the following components:
* {{__s__}} (the server socket)
** {{delete}} should be sufficient.  This closes the socket and thereby prevents any further interaction from it.
* {{process_manager}}
** Related prior work: [MESOS-3158]
** Cleans up the garbage collector, help, logging, profiler, statistics, route processes (including [this one|https://github.com/apache/mesos/blob/3bda55da1d0b580a1b7de43babfdc0d30fbc87ea/3rdparty/libprocess/src/process.cpp#L963], which currently leaks a pointer).
** Cleans up any other {{spawn}} 'd process.
** Manages the {{EventLoop}}.
* {{Clock}}
** The goal here is to clear any timers so that nothing can deference {{process_manager}} while we're finalizing/finalized.  It's probably not important to execute any remaining timers, since we're ""shutting down"" libprocess.  This means:
*** The clock should be {{paused}} and {{settled}} before the clean up of {{process_manager}}.
*** Processes, which might interact with the {{Clock}}, should be cleaned up next.
*** A new {{Clock::finalize}} method would then clear timers, process-specific clocks, and {{tick}} s; and then {{resume}} the clock.
* {{__address__}} (the advertised IP and port)
** Needs to be cleared after {{process_manager}} has been cleaned up.  Processes use this to communicate events.  If cleared prematurely, {{TerminateEvents}} will not be sent correctly, leading to infinite waits.
* {{socket_manager}}
** The idea here is to close all sockets and deallocate any existing {{HttpProxy}} or {{Encoder}} objects.
** All sockets are created via {{__s__}}, so cleaning up the server socket prior will prevent any new activity.
* {{mime}}
** This is effectively a static map.
** It should be possible to statically initialize it.
* Synchronization atomics {{initialized}} & {{initializing}}.
** Once cleanup is done, these should be reset.

*Summary*:
* Implement {{Clock::finalize}}.  [MESOS-3882]
* Implement {{~SocketManager}}.  [MESOS-3910]
* Make sure the {{MetricsProcess}} and {{ReaperProcess}} are reinitialized.  [MESOS-3934]
* (Optional) Clean up {{mime}}.
* Wrap everything up in {{process::finalize}}.",Task,Major,kaysoky,2015-11-20T19:10:08.000+0000,5,Resolved,Complete,Investigate the requirements of programmatically re-initializing libprocess,2016-01-12T21:22:00.000+0000,MESOS-3863,2.0,mesos,Mesosphere Sprint 22
nfnt,2015-11-09T18:56:24.000+0000,nfnt,"When quotas are requested they should authorize their roles.
This ticket will authorize quota requests with ACLs. The existing authorization support that has been implemented in MESOS-1342 will be extended to add a `request_quotas` ACL.",Task,Major,nfnt,2015-12-18T20:09:36.000+0000,5,Resolved,Complete,Authorize set quota requests.,2016-01-14T10:06:53.000+0000,MESOS-3862,5.0,mesos,Mesosphere Sprint 22
nfnt,2015-11-09T18:52:55.000+0000,nfnt,"Quota requests need to be authenticated.
This ticket will authenticate quota requests using credentials provided by the {{Authorization}} field of the HTTP request. This is similar to how authentication is implemented in {{Master::Http}}.",Task,Major,nfnt,2015-12-10T19:53:56.000+0000,5,Resolved,Complete,Authenticate quota requests,2015-12-10T19:53:56.000+0000,MESOS-3861,3.0,mesos,Mesosphere Sprint 22
nfnt,2015-11-09T14:21:30.000+0000,nfnt,In the design documents for Quota (https://docs.google.com/document/d/16iRNmziasEjVOblYp5bbkeBZ7pnjNlaIzPQqMTHQ-9I/edit#) the proposed MVP does not include quota limits. Quota limits represent an upper bound of resources that a role is allowed to use. The task of this ticket is to outline a design document on how to implement quota limits when the quota MVP is implemented.,Task,Major,nfnt,,10020,Accepted,In Progress,Draft quota limits design document,2015-12-07T13:12:47.000+0000,MESOS-3858,5.0,mesos,Mesosphere Sprint 22
js84,2015-11-09T12:55:29.000+0000,js84,"As part of the overall design doc for global resources we would like to introduce improvements for Docker Volume Driver isolator module (https://github.com/emccode/mesos-module-dvdi).
Currently the isolator module is controlled by setting environment variables as follows: {code} ""env"": {
  ""DVDI_VOLUME_NAME"": ""testing"",
  ""DVDI_VOLUME_DRIVER"": ""platform1"",
  ""DVDI_VOLUME_OPTS"": ""size=5,iops=150,volumetype=io1,newfstype=ext4,overwritefs=false"",
  ""DVDI_VOLUME_NAME1"": ""testing2"",
  ""DVDI_VOLUME_DRIVER1"": ""platform2"",
  ""DVDI_VOLUME_OPTS1"": ""size=6,volumetype=gp2,newfstype=xfs,overwritefs=true""
} {code} We should develop a more structured way for passing these settings to the isolator module which is in line with the overall goal of global resources.",Task,Major,js84,2015-12-07T13:20:50.000+0000,5,Resolved,Complete,Draft Design Doc for first Step External Volume MVP,2015-12-07T13:20:50.000+0000,MESOS-3857,3.0,mesos,Mesosphere Sprint 22
arojas,2015-11-09T09:39:16.000+0000,bernd-mesos,"Finalize the structure the interface and achieve consensus on the design doc proposed in MESOS-2949.

https://docs.google.com/document/d/1-XARWJFUq0r_TgRHz_472NvLZNjbqE4G8c2JL44OSMQ/edit",Task,Major,bernd-mesos,2016-03-04T20:09:03.000+0000,5,Resolved,Complete,Finalize design for generalized Authorizer interface,2016-03-04T20:09:04.000+0000,MESOS-3854,5.0,mesos,Mesosphere Sprint 27
anandmazumdar,2015-11-08T07:30:53.000+0000,anandmazumdar,"Post https://reviews.apache.org/r/38900 i.e. updating CommandExecutor to support rootfs. There seem to be some tests showing frequent crashes due to assert violations.

{{FetcherCacheTest.SimpleEviction}} failed due to the following log:

{code}
I1107 19:36:46.360908 30657 slave.cpp:1793] Sending queued task '3' to executor ''3' of framework 7d94c7fb-8950-4bcf-80c1-46112292dcd6-0000 at executor(1)@172.17.5.200:33871'
I1107 19:36:46.363682  1236 exec.cpp:297] 

I1107 19:36:46.373569  1245 exec.cpp:210] Executor registered on slave 7d94c7fb-8950-4bcf-80c1-46112292dcd6-S0
    @     0x7f9f5a7db3fa  google::LogMessage::Fail()
I1107 19:36:46.394081  1245 exec.cpp:222] Executor::registered took 395411ns
    @     0x7f9f5a7db359  google::LogMessage::SendToLog()
    @     0x7f9f5a7dad6a  google::LogMessage::Flush()
    @     0x7f9f5a7dda9e  google::LogMessageFatal::~LogMessageFatal()
    @           0x48d00a  _CheckFatal::~_CheckFatal()
    @           0x49c99d  mesos::internal::CommandExecutorProcess::launchTask()
    @           0x4b3dd7  _ZZN7process8dispatchIN5mesos8internal22CommandExecutorProcessEPNS1_14ExecutorDriverERKNS1_8TaskInfoES5_S6_EEvRKNS_3PIDIT_EEMSA_FvT0_T1_ET2_T3_ENKUlPNS_11ProcessBaseEE_clESL_
    @           0x4c470c  _ZNSt17_Function_handlerIFvPN7process11ProcessBaseEEZNS0_8dispatchIN5mesos8internal22CommandExecutorProcessEPNS5_14ExecutorDriverERKNS5_8TaskInfoES9_SA_EEvRKNS0_3PIDIT_EEMSE_FvT0_T1_ET2_T3_EUlS2_E_E9_M_invokeERKSt9_Any_dataS2_
    @     0x7f9f5a761b1b  std::function<>::operator()()
    @     0x7f9f5a749935  process::ProcessBase::visit()
    @     0x7f9f5a74d700  process::DispatchEvent::visit()
    @           0x48e004  process::ProcessBase::serve()
    @     0x7f9f5a745d21  process::ProcessManager::resume()
    @     0x7f9f5a742f52  _ZZN7process14ProcessManager12init_threadsEvENKUlRKSt11atomic_boolE_clES3_
    @     0x7f9f5a74cf2c  _ZNSt5_BindIFZN7process14ProcessManager12init_threadsEvEUlRKSt11atomic_boolE_St17reference_wrapperIS3_EEE6__callIvIEILm0EEEET_OSt5tupleIIDpT0_EESt12_Index_tupleIIXspT1_EEE
    @     0x7f9f5a74cedc  _ZNSt5_BindIFZN7process14ProcessManager12init_threadsEvEUlRKSt11atomic_boolE_St17reference_wrapperIS3_EEEclIIEvEET0_DpOT_
    @     0x7f9f5a74ce6e  _ZNSt12_Bind_simpleIFSt5_BindIFZN7process14ProcessManager12init_threadsEvEUlRKSt11atomic_boolE_St17reference_wrapperIS4_EEEvEE9_M_invokeIIEEEvSt12_Index_tupleIIXspT_EEE
    @     0x7f9f5a74cdc5  _ZNSt12_Bind_simpleIFSt5_BindIFZN7process14ProcessManager12init_threadsEvEUlRKSt11atomic_boolE_St17reference_wrapperIS4_EEEvEEclEv
    @     0x7f9f5a74cd5e  _ZNSt6thread5_ImplISt12_Bind_simpleIFSt5_BindIFZN7process14ProcessManager12init_threadsEvEUlRKSt11atomic_boolE_St17reference_wrapperIS6_EEEvEEE6_M_runEv
    @     0x7f9f5624f1e0  (unknown)
    @     0x7f9f564a8df5  start_thread
    @     0x7f9f559b71ad  __clone
I1107 19:36:46.551370 30656 containerizer.cpp:1257] Executor for container '6553a617-6b4a-418d-9759-5681f45ff854' has exited
I1107 19:36:46.551429 30656 containerizer.cpp:1074] Destroying container '6553a617-6b4a-418d-9759-5681f45ff854'
I1107 19:36:46.553869 30656 containerizer.cpp:1257] Executor for container 'd2c1f924-c92a-453e-82b1-c294d09c4873' has exited
{code}

The reason seems to be a race between the executor receiving a {{RunTaskMessage}} before {{ExecutorRegisteredMessage}} leading to the {{CHECK_SOME(executorInfo)}} failure.

Link to complete log: https://issues.apache.org/jira/browse/MESOS-2831?focusedCommentId=14995535&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-14995535

Another related failure from {{ExamplesTest.PersistentVolumeFramework}}

{code}
    @     0x7f4f71529cbd  google::LogMessage::SendToLog()
I1107 13:15:09.949987 31573 slave.cpp:2337] Status update manager successfully handled status update acknowledgement (UUID: 721c7316-5580-4636-a83a-098e3bd4ed1f) for task ad90531f-d3d8-43f6-96f2-c81c4548a12d of framework ac4ea54a-7d19-4e41-9ee3-1a761f8e5b0f-0000
    @     0x7f4f715296ce  google::LogMessage::Flush()
    @     0x7f4f7152c402  google::LogMessageFatal::~LogMessageFatal()
    @           0x48d00a  _CheckFatal::~_CheckFatal()
    @           0x49c99d  mesos::internal::CommandExecutorProcess::launchTask()
    @           0x4b3dd7  _ZZN7process8dispatchIN5mesos8internal22CommandExecutorProcessEPNS1_14ExecutorDriverERKNS1_8TaskInfoES5_S6_EEvRKNS_3PIDIT_EEMSA_FvT0_T1_ET2_T3_ENKUlPNS_11ProcessBaseEE_clESL_
    @           0x4c470c  _ZNSt17_Function_handlerIFvPN7process11ProcessBaseEEZNS0_8dispatchIN5mesos8internal22CommandExecutorProcessEPNS5_14ExecutorDriverERKNS5_8TaskInfoES9_SA_EEvRKNS0_3PIDIT_EEMSE_FvT0_T1_ET2_T3_EUlS2_E_E9_M_invokeERKSt9_Any_dataS2_
    @     0x7f4f714b047f  std::function<>::operator()()
    @     0x7f4f71498299  process::ProcessBase::visit()
    @     0x7f4f7149c064  process::DispatchEvent::visit()
    @           0x48e004  process::ProcessBase::serve()
    @     0x7f4f71494685  process::ProcessManager::resume()
{code}

Full logs at:
https://builds.apache.org/job/Mesos/1191/COMPILER=gcc,CONFIGURATION=--verbose,OS=centos:7,label_exp=docker%7C%7CHadoop/consoleFull",Bug,Major,anandmazumdar,2015-12-30T22:28:09.000+0000,5,Resolved,Complete,Investigate recent crashes in Command Executor,2015-12-30T22:28:09.000+0000,MESOS-3851,2.0,mesos,Mesosphere Sprint 23
alexr,2015-11-07T12:41:47.000+0000,alexr,Order of files in Makefiles is not strictly alphabetic,Bug,Major,alexr,2015-11-07T12:53:36.000+0000,5,Resolved,Complete,Corrected style in Makefiles,2015-11-08T12:51:44.000+0000,MESOS-3849,1.0,mesos,Mesosphere Sprint 21
kaysoky,2015-11-07T00:00:42.000+0000,kaysoky,"As part of [MESOS-3762], many tests were changed from one {{TemporaryDirectoryTest}} to another {{TemporaryDirectoryTest}}.  One subtle difference is that the name of the temporary directory no longer contains the name of the test.  In [MESOS-3847], the duplicate {{TemporaryDirectoryTest}} was removed.

The original {{TemporaryDirectoryTest}} called [{{environment->mkdtemp}}|https://github.com/apache/mesos/blob/master/src/tests/environment.cpp#L494].  We would like the naming, which is valuable for debugging, to be available for a majority of tests.  (A majority of tests inherit from {{TemporaryDirectoryTest}} in some way.)

Note:
* Any additional directories created via {{environment->mkdtemp}} are cleaned up after the test.
* We don't want mesos-specific logic in Stout, like the {{umount}} shell command in {{Environment::TearDown}}.

*Proposed change:*
Move the temporary directory logic from {{Environment::mkdtemp}} to {{TemporaryDirectoryTest}}.

*Tests that need to change*
| {{log_tests.cpp}} | {{LogZooKeeperTest}} | We can change {{ZooKeeperTest}} to inherit from {{TemporaryDirectoryTest}} to get rid of code duplication |
| {{tests/mesos.cpp}} | {{MesosTest::CreateSlaveFlags}} | {{MesosTest}} already inherits from {{TemporaryDirectoryTest}}. |
| {{tests/script.hpp}} | {{TEST_SCRIPT}} | This is used for the {{ExampleTests}}.  We can define a test class that inherits appropriately. |
| {{docker_tests.cpp}} | {{*}} | Already inherits from {{MesosTest}}. |",Task,Minor,kaysoky,,10020,Accepted,In Progress,Refactor Environment::mkdtemp into TemporaryDirectoryTest.,2016-03-28T19:26:33.000+0000,MESOS-3848,3.0,mesos,
kaysoky,2015-11-06T22:08:00.000+0000,kaysoky,"The refactor in [MESOS-3762] ended up exposing some differences in the {{TemporaryDirectoryTest}} classes (one in Stout, one in Mesos-proper).

The tests that broke (during tear down):
{code}
LinuxFilesystemIsolatorTest.ROOT_PersistentVolumeWithRootFilesystem
LinuxFilesystemIsolatorTest.ROOT_PersistentVolumeWithoutRootFilesystem
LinuxFilesystemIsolatorTest.ROOT_MultipleContainers
{code}

As per an offline discussion between [~jvanremoortere] and [~jieyu], the solution is to merge the two {{TemporaryDirectoryTest}} classes and to fix the tear down of {{LinuxFilesystemIsolatorTest}}.",Bug,Minor,kaysoky,2015-11-07T00:47:31.000+0000,5,Resolved,Complete,Root tests for LinuxFilesystemIsolatorTest are broken,2015-11-07T00:47:31.000+0000,MESOS-3847,2.0,mesos,Mesosphere Sprint 21
tnachen,2015-11-06T02:29:06.000+0000,tnachen,"Currently Rootfs doesn't fully copy the directory structure over, and also doesn't create the symlinks in the new rootfs and will cause shell and other binaries that rely on the symlinks to no longer function.",Bug,Major,tnachen,2015-11-06T23:49:38.000+0000,5,Resolved,Complete,Rootfs in provisioner test doesn't handle symlink directories properly,2015-11-06T23:49:38.000+0000,MESOS-3837,4.0,mesos,Mesosphere Sprint 21
gyliu,2015-11-05T00:29:42.000+0000,anandmazumdar,"Mesos displays the list of all supported endpoints starting at a given path prefix using the {{/help}} suffix, e.g. {{master:5050/help}}.

It seems that the {{help}} functionality is broken for URL's having nested paths e.g. {{master:5050/help/master/machine/down}}. The response returned is:
{quote}
Malformed URL, expecting '/help/id/name/'
{quote}",Bug,Minor,anandmazumdar,2016-02-12T10:20:23.000+0000,5,Resolved,Complete,/help endpoints do not work for nested paths,2016-02-27T01:03:09.000+0000,MESOS-3833,2.0,mesos,
klueska,2015-11-04T22:54:02.000+0000,neilc,"These are not exhaustively documented; they probably should be.

Some endpoints have docs: e.g., {{/reserve}} and {{/unreserve}} are described in the reservation doc page. But it would be good to have a single page that lists all the endpoints and their semantics.",Documentation,Minor,neilc,2016-02-08T17:08:26.000+0000,5,Resolved,Complete,Document operator HTTP endpoints,2016-02-09T21:57:34.000+0000,MESOS-3831,3.0,mesos,Mesosphere Sprint 26
,2015-11-03T22:37:02.000+0000,jojy,"Currently mesos-reviewbot project does  not support parameterized configuration. This limits the project from building using --enable-ssl (and others) configuration arguments for building mesos. 
",Improvement,Major,jojy,,10020,Accepted,In Progress,Enable mesos-reviewbot project on jenkins to use SSL,2015-12-18T07:04:11.000+0000,MESOS-3825,3.0,mesos,
kaysoky,2015-11-02T23:40:15.000+0000,kaysoky,"*Background*
Libprocess initialization includes the spawning of a variety of global processes and the creation of the server socket which listens for incoming requests.  Some properties of the server socket are configured via environment variables, such as the IP and port or the SSL configuration.

In the case of tests, libprocess is initialized once per test binary.  This means that testing different configurations (SSL in particular) is cumbersome as a separate process would be needed for every test case.

*Proposal*
# Add some optional code between some tests like:
{code}
// Cleanup all of libprocess's state, as if we're starting anew.
process::finalize(); 

// For tests that need to test SSL connections with the Master:
openssl::reinitialize();

process::initialize();
{code}
See [MESOS-3863] for more on {{process::finalize}}.",Epic,Major,kaysoky,,10006,Reviewable,New,Test-only libprocess reinitialization,2016-01-12T21:23:36.000+0000,MESOS-3820,3.0,mesos,
neilc,2015-11-02T23:03:01.000+0000,neilc,"Docs currently talk about resources, static/dynamic reservations, but don't explain what a ""role"" concept is to begin with.",Improvement,Minor,neilc,2015-11-06T18:18:02.000+0000,5,Resolved,Complete,"Add documentation explaining ""roles""",2015-11-06T18:18:02.000+0000,MESOS-3819,2.0,mesos,Mesosphere Sprint 21
hartem,2015-10-30T23:47:37.000+0000,jieyu,"There's a recent change regarding the picking of which launcher (Linux or Posix) to use
https://reviews.apache.org/r/39604

In our environment, cgroups are not auto-mounted after reboot. We rely on Mesos itself to mount all relevant cgroups hierachies.

After the reboot, the above patch detects that 'freezer' hierarchy is not mounted, therefore, decided to use the Posix launcher (if --launcher is not specified explictly).

Port mapping isolator requires network namespace to be created for each container (thus requires Linux launcher). But we don't have a check to verify that launcher and isolators are compatible.

Slave thus starts fine and task failed with weird error like:
{noformat}
Collect failed: Failed to create the ingress qdisc on mesos61099: Link 'mesos61099' is not found.
{noformat}

It does take us quite a few time to figure out the root cause.",Bug,Major,jieyu,,10020,Accepted,In Progress,Add checks to make sure isolators and the launcher are compatible.,2015-11-04T00:51:27.000+0000,MESOS-3814,2.0,mesos,
hartem,2015-10-23T22:20:58.000+0000,hartem,Mesos containerizer attempts to create a Linux launcher by default without verifying whether the necessary prerequisites (such as availability of cgroups) are met.,Bug,Major,hartem,2015-10-26T20:12:05.000+0000,5,Resolved,Complete,Containerizer attempts to create Linux launcher by default ,2015-11-08T12:51:43.000+0000,MESOS-3800,3.0,mesos,Mesosphere Sprint 21
jamespeach,2015-10-22T23:31:20.000+0000,kaysoky,"From a comment in [MESOS-3771]:

Master should not be storing the {{data}} fields from {{ExecutorInfo}}.  We currently [store the entire object|https://github.com/apache/mesos/blob/master/src/master/master.hpp#L262-L271], which means master would be at high risk of OOM-ing if a bunch of executors were started with big {{data}} blobs.
* Master should scrub out unneeded bloat from {{ExecutorInfo}} before storing it.
* We can use an alternate internal object, like we do for {{TaskInfo}} vs {{Task}}; see [this|https://github.com/apache/mesos/blob/master/src/messages/messages.proto#L39-L41].",Bug,Critical,kaysoky,,10020,Accepted,In Progress,Master should not store arbitrarily sized data in ExecutorInfo,2015-11-23T19:50:31.000+0000,MESOS-3794,3.0,mesos,
jojy,2015-10-22T18:44:57.000+0000,aquamatthias,"We updated the mesos version to 0.25.0 in our Marathon docker image, that runs our integration tests.
We use mesos local for those tests. This fails with this message:

{noformat}
root@a06e4b4eb776:/marathon# mesos local
I1022 18:42:26.852485   136 leveldb.cpp:176] Opened db in 6.103258ms
I1022 18:42:26.853302   136 leveldb.cpp:183] Compacted db in 765740ns
I1022 18:42:26.853343   136 leveldb.cpp:198] Created db iterator in 9001ns
I1022 18:42:26.853355   136 leveldb.cpp:204] Seeked to beginning of db in 1287ns
I1022 18:42:26.853366   136 leveldb.cpp:273] Iterated through 0 keys in the db in 1111ns
I1022 18:42:26.853406   136 replica.cpp:744] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned
I1022 18:42:26.853775   141 recover.cpp:449] Starting replica recovery
I1022 18:42:26.853862   141 recover.cpp:475] Replica is in EMPTY status
I1022 18:42:26.854751   138 replica.cpp:641] Replica in EMPTY status received a broadcasted recover request
I1022 18:42:26.854856   140 recover.cpp:195] Received a recover response from a replica in EMPTY status
I1022 18:42:26.855002   140 recover.cpp:566] Updating replica status to STARTING
I1022 18:42:26.855655   138 master.cpp:376] Master a3f39818-1bda-4710-b96b-2a60ed4d12b8 (a06e4b4eb776) started on 172.17.0.14:5050
I1022 18:42:26.855680   138 master.cpp:378] Flags at startup: --allocation_interval=""1secs"" --allocator=""HierarchicalDRF"" --authenticate=""false"" --authenticate_slaves=""false"" --authenticators=""crammd5"" --authorizers=""local"" --framework_sorter=""drf"" --help=""false"" --hostname_lookup=""true"" --initialize_driver_logging=""true"" --log_auto_initialize=""true"" --logbufsecs=""0"" --logging_level=""INFO"" --max_slave_ping_timeouts=""5"" --quiet=""false"" --recovery_slave_removal_limit=""100%"" --registry=""replicated_log"" --registry_fetch_timeout=""1mins"" --registry_store_timeout=""5secs"" --registry_strict=""false"" --root_submissions=""true"" --slave_ping_timeout=""15secs"" --slave_reregister_timeout=""10mins"" --user_sorter=""drf"" --version=""false"" --webui_dir=""/usr/share/mesos/webui"" --work_dir=""/tmp/mesos/local/AK0XpG"" --zk_session_timeout=""10secs""
I1022 18:42:26.855790   138 master.cpp:425] Master allowing unauthenticated frameworks to register
I1022 18:42:26.855803   138 master.cpp:430] Master allowing unauthenticated slaves to register
I1022 18:42:26.855815   138 master.cpp:467] Using default 'crammd5' authenticator
W1022 18:42:26.855829   138 authenticator.cpp:505] No credentials provided, authentication requests will be refused
I1022 18:42:26.855840   138 authenticator.cpp:512] Initializing server SASL
I1022 18:42:26.856442   136 containerizer.cpp:143] Using isolation: posix/cpu,posix/mem,filesystem/posix
I1022 18:42:26.856943   140 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 1.888185ms
I1022 18:42:26.856987   140 replica.cpp:323] Persisted replica status to STARTING
I1022 18:42:26.857115   140 recover.cpp:475] Replica is in STARTING status
I1022 18:42:26.857270   140 replica.cpp:641] Replica in STARTING status received a broadcasted recover request
I1022 18:42:26.857312   140 recover.cpp:195] Received a recover response from a replica in STARTING status
I1022 18:42:26.857368   140 recover.cpp:566] Updating replica status to VOTING
I1022 18:42:26.857781   140 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 371121ns
I1022 18:42:26.857841   140 replica.cpp:323] Persisted replica status to VOTING
I1022 18:42:26.857895   140 recover.cpp:580] Successfully joined the Paxos group
I1022 18:42:26.857928   140 recover.cpp:464] Recover process terminated
I1022 18:42:26.862455   137 master.cpp:1603] The newly elected leader is master@172.17.0.14:5050 with id a3f39818-1bda-4710-b96b-2a60ed4d12b8
I1022 18:42:26.862498   137 master.cpp:1616] Elected as the leading master!
I1022 18:42:26.862511   137 master.cpp:1376] Recovering from registrar
I1022 18:42:26.862560   137 registrar.cpp:309] Recovering registrar
Failed to create a containerizer: Could not create MesosContainerizer: Failed to create launcher: Failed to create Linux launcher: Failed to mount cgroups hierarchy at '/sys/fs/cgroup/freezer': 'freezer' is already attached to another hierarchy
{noformat}

The setup worked with mesos 0.24.0.
The Dockerfile is here: https://github.com/mesosphere/marathon/blob/mv/mesos_0.25/Dockerfile



{noformat}
root@a06e4b4eb776:/marathon# ls /sys/fs/cgroup/
root@a06e4b4eb776:/marathon# 
{noformat}

{noformat}
root@a06e4b4eb776:/marathon# cat /proc/mounts 
none / aufs rw,relatime,si=6e7ac87f36042e03,dio,dirperm1 0 0
proc /proc proc rw,nosuid,nodev,noexec,relatime 0 0
tmpfs /dev tmpfs rw,nosuid,mode=755 0 0
devpts /dev/pts devpts rw,nosuid,noexec,relatime,gid=5,mode=620,ptmxmode=666 0 0
shm /dev/shm tmpfs rw,nosuid,nodev,noexec,relatime,size=65536k 0 0
mqueue /dev/mqueue mqueue rw,nosuid,nodev,noexec,relatime 0 0
sysfs /sys sysfs ro,nosuid,nodev,noexec,relatime 0 0
/dev/sda1 /etc/resolv.conf ext4 rw,relatime,data=ordered 0 0
/dev/sda1 /etc/hostname ext4 rw,relatime,data=ordered 0 0
/dev/sda1 /etc/hosts ext4 rw,relatime,data=ordered 0 0
devpts /dev/console devpts rw,relatime,mode=600,ptmxmode=000 0 0
proc /proc/bus proc ro,nosuid,nodev,noexec,relatime 0 0
proc /proc/fs proc ro,nosuid,nodev,noexec,relatime 0 0
proc /proc/irq proc ro,nosuid,nodev,noexec,relatime 0 0
proc /proc/sys proc ro,nosuid,nodev,noexec,relatime 0 0
proc /proc/sysrq-trigger proc ro,nosuid,nodev,noexec,relatime 0 0
tmpfs /proc/kcore tmpfs rw,nosuid,mode=755 0 0
tmpfs /proc/timer_stats tmpfs rw,nosuid,mode=755 0 0
{noformat}

[~bernd-mesos] Can you please assign to the correct person?",Bug,Major,aquamatthias,2015-11-25T16:41:18.000+0000,5,Resolved,Complete,Cannot start mesos local on a Debian GNU/Linux 8 docker machine,2016-04-22T21:18:00.000+0000,MESOS-3793,3.0,mesos,Mesosphere Sprint 23
greggomann,2015-10-21T18:40:40.000+0000,greggomann,"As far as I can tell, current practice is to quote code excerpts and object names with backticks when writing comments. For example:

{code}
// You know, `sadPanda` seems extra sad lately.
std::string sadPanda;
sadPanda = ""   :'(   "";
{code}

However, I don't see this documented in our C++ style guide at all. It should be added.",Documentation,Minor,greggomann,2015-11-21T01:14:18.000+0000,5,Resolved,Complete,Backticks are not mentioned in Mesos C++ Style Guide,2015-11-21T01:14:18.000+0000,MESOS-3786,1.0,mesos,Mesosphere Sprint 22
bbannier,2015-10-21T14:52:42.000+0000,bernd-mesos,"Instead of using checksums to trigger fetcher cache updates, we can for starters use the content modification time (mtime), which is available for a number of download protocols, e.g. HTTP and HDFS.

Proposal: Instead of just fetching the content size, we fetch both size  and mtime together. As before, if there is no size, then caching fails and we fall back on direct downloading to the sandbox. 

Assuming a size is given, we compare the mtime from the fetch URI with the mtime known to the cache. If it differs, we update the cache. (As a defensive measure, a difference in size should also trigger an update.) 

Not having an mtime available at the fetch URI is simply treated as a unique valid mtime value that differs from all others. This means that when initially there is no mtime, cache content remains valid until there is one. Thereafter,  anew lack of an mtime invalidates the cache once. In other words: any change from no mtime to having one or back is the same as encountering a new mtime.

Note that this scheme does not require any new protobuf fields.
",Improvement,Major,bernd-mesos,2015-12-04T12:21:44.000+0000,5,Resolved,Complete,Use URI content modification time to trigger fetcher cache updates.,2015-12-04T12:21:44.000+0000,MESOS-3785,5.0,mesos,Mesosphere Sprint 21
lins05,2015-10-21T10:18:34.000+0000,alexr,"The {{MasterAllocatorTest.SlaveLost}} takes more that {{5s}} to complete. A brief look into the code hints that the stopped agent does not quit immediately (and hence its resources are not released by the allocator) because [it waits for the executor to terminate|https://github.com/apache/mesos/blob/master/src/tests/master_allocator_tests.cpp#L717]. {{5s}} timeout comes from {{EXECUTOR_SHUTDOWN_GRACE_PERIOD}} agent constant.

Possible solutions:
* Do not wait until the stopped agent quits (can be flaky, needs deeper analysis).
* Decrease the agent's {{executor_shutdown_grace_period}} flag.
* Terminate the executor faster (this may require some refactoring since the executor driver is created in the {{TestContainerizer}} and we do not have direct access to it.
",Improvement,Minor,alexr,2016-04-12T15:12:28.000+0000,5,Resolved,Complete,MasterAllocatorTest.SlaveLost is slow.,2016-04-12T15:12:28.000+0000,MESOS-3775,1.0,mesos,Mesosphere Sprint 33
jojy,2015-10-20T21:03:03.000+0000,kaysoky,"{{RegistryClientTest.SimpleGetBlob}} fails about 1/5 times.  This was encountered on OSX.

{code:title=Repro}
bin/mesos-tests.sh --gtest_filter=""*RegistryClientTest.SimpleGetBlob*"" --gtest_repeat=10 --gtest_break_on_failure
{code}

{code:title=Example Failure}
[ RUN      ] RegistryClientTest.SimpleGetBlob
../../src/tests/containerizer/provisioner_docker_tests.cpp:946: Failure
Value of: blobResponse
  Actual: ""2015-10-20 20:58:59.579393024+00:00""
Expected: blob.get()
Which is: ""\x15\x3\x3\00(P~\xCA&\xC6<\x4\x16\xE\xB2\xFF\b1a\xB9Z{\xE0\x80\xDA`\xBCt\x5R\x81x6\xF8 \x8B{\xA8\xA9\x4\xAB\xB6"" ""E\xE6\xDE\xCF\xD9*\xCC!\xC2\x15"" ""2015-10-20 20:58:59.579393024+00:00""
*** Aborted at 1445374739 (unix time) try ""date -d @1445374739"" if you are using GNU date ***
PC: @        0x103144ddc testing::UnitTest::AddTestPartResult()
*** SIGSEGV (@0x0) received by PID 49008 (TID 0x7fff73ca3300) stack trace: ***
    @     0x7fff8c58af1a _sigtramp
    @     0x7fff8386e187 malloc
    @        0x1031445b7 testing::internal::AssertHelper::operator=()
    @        0x1030d32e0 mesos::internal::tests::RegistryClientTest_SimpleGetBlob_Test::TestBody()
    @        0x1030d3562 mesos::internal::tests::RegistryClientTest_SimpleGetBlob_Test::TestBody()
    @        0x1031ac8f3 testing::internal::HandleSehExceptionsInMethodIfSupported<>()
    @        0x103192f87 testing::internal::HandleExceptionsInMethodIfSupported<>()
    @        0x1031533f5 testing::Test::Run()
    @        0x10315493b testing::TestInfo::Run()
    @        0x1031555f7 testing::TestCase::Run()
    @        0x103163df3 testing::internal::UnitTestImpl::RunAllTests()
    @        0x1031af8c3 testing::internal::HandleSehExceptionsInMethodIfSupported<>()
    @        0x103195397 testing::internal::HandleExceptionsInMethodIfSupported<>()
    @        0x1031639f2 testing::UnitTest::Run()
    @        0x1025abd41 RUN_ALL_TESTS()
    @        0x1025a8089 main
    @     0x7fff86b155c9 start
{code}

{code:title=Less common failure}
[ RUN      ] RegistryClientTest.SimpleGetBlob
../../src/tests/containerizer/provisioner_docker_tests.cpp:926: Failure
(socket).failure(): Failed accept: connection error: error:00000000:lib(0):func(0):reason(0)
{code}",Bug,Major,kaysoky,2015-12-18T23:17:07.000+0000,5,Resolved,Complete,RegistryClientTest.SimpleGetBlob is flaky,2016-02-27T00:15:34.000+0000,MESOS-3773,4.0,mesos,Mesosphere Sprint 21
ijimenez,2015-10-20T19:50:29.000+0000,neilc,"Example log output:

{quote}
I1020 18:56:02.933956  1790 slave.cpp:1270] Got assigned task 13 for framework 496620b9-4368-4a71-b741-68216f3d909f-0000
I1020 18:56:02.934185  1790 slave.cpp:1386] Launching task 13 for framework 496620b9-4368-4a71-b741-68216f3d909f-0000
I1020 18:56:02.934408  1790 slave.cpp:1618] Queuing task '13' for executor default of framework '496620b9-4368-4a71-b741-68216f3d909f-0000
I1020 18:56:02.935417  1790 slave.cpp:1760] Sending queued task '13' to executor 'default' of framework 496620b9-4368-4a71-b741-68216f3d909f-0000
{quote}

Aside from the typo (unmatched quote) in the third line, these log messages using quoting inconsistently: sometimes task, executor, and framework IDs are quoted, other times they are not.

We should probably adopt a general rule, a la http://www.postgresql.org/docs/9.4/static/error-style-guide.html . My proposal: when interpolating a variable, only use quotes if it is possible that the value might contain whitespace or punctuation (in the latter case, the punctuation should probably be escaped).",Bug,Major,neilc,,10020,Accepted,In Progress,Consistency of quoted strings in error messages,2016-02-04T20:03:37.000+0000,MESOS-3772,3.0,mesos,
kaysoky,2015-10-20T18:05:36.000+0000,stevenschlansker,"Spark encodes some binary data into the ExecutorInfo.data field.  This field is sent as a ""bytes"" Protobuf value, which can have arbitrary non-UTF8 data.

If you have such a field, it seems that it is splatted out into JSON without any regards to proper character encoding:

{code}
0006b0b0  2e 73 70 61 72 6b 2e 65  78 65 63 75 74 6f 72 2e  |.spark.executor.|
0006b0c0  4d 65 73 6f 73 45 78 65  63 75 74 6f 72 42 61 63  |MesosExecutorBac|
0006b0d0  6b 65 6e 64 22 7d 2c 22  64 61 74 61 22 3a 22 ac  |kend""},""data"":"".|
0006b0e0  ed 5c 75 30 30 30 30 5c  75 30 30 30 35 75 72 5c  |.\u0000\u0005ur\|
0006b0f0  75 30 30 30 30 5c 75 30  30 30 66 5b 4c 73 63 61  |u0000\u000f[Lsca|
0006b100  6c 61 2e 54 75 70 6c 65  32 3b 2e cc 5c 75 30 30  |la.Tuple2;..\u00|
{code}

I suspect this is because the HTTP api emits the executorInfo.data directly:

{code}
JSON::Object model(const ExecutorInfo& executorInfo)
{
  JSON::Object object;
  object.values[""executor_id""] = executorInfo.executor_id().value();
  object.values[""name""] = executorInfo.name();
  object.values[""data""] = executorInfo.data();
  object.values[""framework_id""] = executorInfo.framework_id().value();
  object.values[""command""] = model(executorInfo.command());
  object.values[""resources""] = model(executorInfo.resources());
  return object;
}
{code}

I think this may be because the custom JSON processing library in stout seems to not have any idea of what a byte array is.  I'm guessing that some implicit conversion makes it get written as a String instead, but:

{code}
inline std::ostream& operator<<(std::ostream& out, const String& string)
{
  // TODO(benh): This escaping DOES NOT handle unicode, it encodes as ASCII.
  // See RFC4627 for the JSON string specificiation.
  return out << picojson::value(string.value).serialize();
}
{code}

Thank you for any assistance here.  Our cluster is currently entirely down -- the frameworks cannot handle parsing the invalid JSON produced (it is not even valid utf-8)
",Bug,Critical,stevenschlansker,2015-11-05T00:44:13.000+0000,5,Resolved,Complete,Mesos JSON API creates invalid JSON due to lack of binary data / non-ASCII handling,2016-02-17T14:19:03.000+0000,MESOS-3771,2.0,mesos,Mesosphere Sprint 21
gradywang,2015-10-20T04:35:35.000+0000,js84,"As we decided to create a more restful api for managing Quota request.
Therefore we also want to use the HTTP put request and hence need to enable the libprocess/http to send put request besides get and post requests.",Task,Minor,js84,2016-02-10T09:08:20.000+0000,5,Resolved,Complete,Need for http::put request method,2016-05-03T08:15:38.000+0000,MESOS-3763,1.0,mesos,Mesosphere Sprint 27
kaysoky,2015-10-19T22:43:32.000+0000,kaysoky,"In order to write tests that exercise SSL with other components of Mesos, such as the HTTP scheduler library, we need to use the setup/teardown logic found in the {{SSLTest}} fixture.

Currently, the test fixtures have separate inheritance structures like this:
{code}
SSLTest <- ::testing::Test
MesosTest <- TemporaryDirectoryTest <- ::testing::Test
{code}
where {{::testing::Test}} is a gtest class.

The plan is the following:
# Change {{SSLTest}} to inherit from {{TemporaryDirectoryTest}}.  This will require moving the setup (generation of keys and certs) from {{SetUpTestCase}} to {{SetUp}}.  At the same time, *some* of the cleanup logic in the SSLTest will not be needed.
# Move the logic of generating keys/certs into helpers, so that individual tests can call them when needed, much like {{MesosTest}}.
# Write a child class of {{SSLTest}} which has the same functionality as the existing {{SSLTest}}, for use by the existing tests that rely on {{SSLTest}} or the {{RegistryClientTest}}.
# Have {{MesosTest}} inherit from {{SSLTest}} (which might be renamed during the refactor).  If Mesos is not compiled with {{--enable-ssl}}, then {{SSLTest}} could be {{#ifdef}}'d into any empty class.

The resulting structure should be like:
{code}
MesosTest <- SSLTest <- TemporaryDirectoryTest <- ::testing::Test
ChildOfSSLTest /
{code}",Task,Major,kaysoky,2015-10-28T20:30:44.000+0000,5,Resolved,Complete,Refactor SSLTest fixture such that MesosTest can use the same helpers.,2015-11-08T12:51:45.000+0000,MESOS-3762,3.0,mesos,Mesosphere Sprint 21
kaysoky,2015-10-19T18:08:17.000+0000,kaysoky,The messages we pass between Mesos components are largely undocumented.  See this [TODO|https://github.com/apache/mesos/blob/19f14d06bac269b635657960d8ea8b2928b7830c/src/messages/messages.proto#L23].,Improvement,Major,kaysoky,2015-10-21T18:26:41.000+0000,5,Resolved,Complete,Document messages.proto,2015-11-08T12:51:44.000+0000,MESOS-3759,3.0,mesos,Mesosphere Sprint 21
tillt,2015-10-19T13:24:40.000+0000,bernd-mesos,"Manage the release of Apache Mesos version 0.26.0. 

The Mesos 0.26.0 release will aim at being timely and at improving robustness. It will not be gated by new features. However, there may be blockers when it comes to bugs or incompleteness of existing features.

Once these blockers are resolved, we will start deferring unresolved issues by Priority and Status until we are ready to make the first cut.

Here is how you can stay informed and help out.

h3. Users
- Note the ""is blocked by"" links in this ticket for major targeted features.
- Check out the 0.26.0 [dashboard|https://issues.apache.org/jira/secure/Dashboard.jspa?selectPageId=12327111] for status indicators.
- See the in-progress [Release Notes|https://issues.apache.org/jira/secure/ReleaseNote.jspa?version=12333528&styleName=Html&projectId=12311242&Create=Create&atl_token=A5KQ-2QAV-T4JA-FDED%7C675829c365428965ebec16702c62d3637db57d84%7Clin] to see what's committed so far.
- Add comments to issues describing your problems or use cases.

h3. Issue Reporters
- Set Target Version to 0.26.0, if appropriate.
- Set Priority for fixing in 0.26.0.
- Ask around on IRC or dev@ for a Shepherd!

h3. Developers
- Newbies: Check out [Accepted, Unassigned, 'newbie'] issues.
- Looking for something meatier to work on? [Accepted, Unassigned for 0.26]
- For Shepherdless issues, find a Shepherd before diving too deep!!
- Update Target Version and Priority, as needed.
- Discuss your intended design on the JIRA, perhaps sharing a design doc.
- Update the Status to ""In Progress"" and ""Reviewable"" as you go.
- Assign yourself if you are working on it. Un-assign yourself in case you stop before finishing.

h3. Committers
- Accept and Shepherd all relevant [Shepherdless issues].
- Update Target Version and Priority, as needed.
- Add 'newbie' label to any easy ones.

h3. Important JIRA fields
- Target Version: Set to 0.26.0 if you want the issue to be addressed in 0.26.
- Priority: Indicates how important it is for the issue to be fixed in the next release (0.26.0 in this case). If you want to update a Priority, please add a comment explaining your reason, and only change the Priority up/down one level.
- Blocked-by Links: major features and critical tickets can be linked as blockers to this ticket to give a high-level overview of what we plan to land in 0.26. Non-critical issues should just set the ""Target Version"".
",Task,Major,bernd-mesos,2015-12-17T23:12:21.000+0000,5,Resolved,Complete,0.26.0 Release,2016-02-26T21:08:18.000+0000,MESOS-3758,5.0,mesos,Mesosphere Sprint 22
arojas,2015-10-19T10:23:21.000+0000,bernd-mesos,"Libprocess is going to factor out an authentication interface: MESOS-3231

Here we propose that Mesos can provide implementations for this interface as Mesos modules.",Task,Major,bernd-mesos,2016-01-08T03:28:19.000+0000,5,Resolved,Complete,Generalized HTTP Authentication Modules,2016-01-14T10:29:31.000+0000,MESOS-3756,13.0,mesos,Mesosphere Sprint 26
anandmazumdar,2015-10-16T22:27:29.000+0000,kaysoky,"Currently, the HTTP Scheduler library does not support SSL-enabled Mesos.  
(You can manually test this by spinning up an SSL-enabled master and attempt to run the event-call framework example against it.)

We need to add tests that check the HTTP Scheduler library against SSL-enabled Mesos:
* with downgrade support,
* with required framework/client-side certifications,
* with/without verification of certificates (master-side),
* with/without verification of certificates (framework-side),
* with a custom certificate authority (CA)

These options should be controlled by the same environment variables found on the [SSL user doc|http://mesos.apache.org/documentation/latest/ssl/].

Note: This issue will be broken down into smaller sub-issues as bugs/problems are discovered.",Story,Major,kaysoky,,10020,Accepted,In Progress,Test the HTTP Scheduler library with SSL enabled,2016-01-22T20:03:16.000+0000,MESOS-3753,13.0,mesos,
greggomann,2015-10-16T18:30:12.000+0000,greggomann,"It seems the Apache Maven dependencies have changed such that following the Getting Started docs for CentOS 6.6 will fail at Maven installation:

{code}
---> Package apache-maven.noarch 0:3.3.3-2.el6 will be installed
--> Processing Dependency: java-devel >= 1:1.7.0 for package: apache-maven-3.3.3-2.el6.noarch
--> Finished Dependency Resolution
Error: Package: apache-maven-3.3.3-2.el6.noarch (epel-apache-maven)
           Requires: java-devel >= 1:1.7.0
           Available: java-1.5.0-gcj-devel-1.5.0.0-29.1.el6.x86_64 (base)
               java-devel = 1.5.0
           Available: 1:java-1.6.0-openjdk-devel-1.6.0.35-1.13.7.1.el6_6.x86_64 (base)
               java-devel = 1:1.6.0
           Available: 1:java-1.6.0-openjdk-devel-1.6.0.36-1.13.8.1.el6_7.x86_64 (updates)
               java-devel = 1:1.6.0
 You could try using --skip-broken to work around the problem
 You could try running: rpm -Va --nofiles --nodigest
{code}",Documentation,Major,greggomann,2015-11-18T09:35:08.000+0000,5,Resolved,Complete,CentOS 6 dependency install fails at Maven,2015-11-18T09:35:08.000+0000,MESOS-3752,1.0,mesos,
gilbert,2015-10-16T17:45:36.000+0000,cmaloney,"When using --executor_environment_variables, and having MESOS_NATIVE_JAVA_LIBRARY in the environment of mesos-slave, the mesos containerizer does not set MESOS_NATIVE_JAVA_LIBRARY itself.

Relevant code: https://github.com/apache/mesos/blob/14f7967ef307f3d98e3a4b93d92d6b3a56399b20/src/slave/containerizer/containerizer.cpp#L281

It sees that the variable is in the mesos-slave's environment (os::getenv), rather than checking if it is set in the environment variable set.",Bug,Major,cmaloney,2015-10-17T23:37:35.000+0000,5,Resolved,Complete,MESOS_NATIVE_JAVA_LIBRARY not set on MesosContainerize tasks with --executor_environmnent_variables,2015-11-05T05:41:27.000+0000,MESOS-3751,2.0,mesos,Mesosphere Sprint 21
greggomann,2015-10-16T16:46:42.000+0000,greggomann,"The {{\-\-enable-libevent}} and {{\-\-enable-ssl}} config flags are currently not documented in the ""Configuration"" docs with the rest of the flags. They should be added.",Documentation,Major,greggomann,2015-10-26T17:05:32.000+0000,5,Resolved,Complete,Configuration docs are missing --enable-libevent and --enable-ssl,2015-11-08T12:51:43.000+0000,MESOS-3749,1.0,mesos,Mesosphere Sprint 21
kaysoky,2015-10-15T23:24:27.000+0000,kaysoky,"If you pass a nonsense string for ""master"" into a framework using the C++ HTTP scheduler library, the framework segfaults.

For example, using the example frameworks:

{code:title=Scheduler Driver}
build/src/test-framework --master=""asdf://127.0.0.1:5050""
{code}
Results in:
{code}
Failed to create a master detector for 'asdf://127.0.0.1:5050': Failed to parse 'asdf://127.0.0.1:5050'
{code}

{code:title=HTTP Scheduler Library}
export DEFAULT_PRINCIPAL=root
build/src/event-call-framework --master=""asdf://127.0.0.1:5050""
{code}
Results in
{code}
I1015 16:18:45.432075 2062201600 scheduler.cpp:157] Version: 0.26.0
Segmentation fault: 11
{code}

{code:title=Stack Trace}
* thread #2: tid = 0x28b6bb, 0x0000000100ad03ca libmesos-0.26.0.dylib`mesos::v1::scheduler::MesosProcess::initialize(this=0x00000001076031a0) + 42 at scheduler.cpp:213, stop reason = EXC_BAD_ACCESS (code=1, address=0x0)
  * frame #0: 0x0000000100ad03ca libmesos-0.26.0.dylib`mesos::v1::scheduler::MesosProcess::initialize(this=0x00000001076031a0) + 42 at scheduler.cpp:213
    frame #1: 0x0000000100ad05f2 libmesos-0.26.0.dylib`virtual thunk to mesos::v1::scheduler::MesosProcess::initialize(this=0x00000001076031a0) + 34 at scheduler.cpp:210
    frame #2: 0x00000001022b60f3 libmesos-0.26.0.dylib`::resume() + 931 at process.cpp:2449
    frame #3: 0x00000001022c131c libmesos-0.26.0.dylib`::operator()() + 268 at process.cpp:2174
    frame #4: 0x00000001022c0fa2 libmesos-0.26.0.dylib`::__thread_proxy<std::__1::tuple<std::__1::__bind<(lambda at ../../../3rdparty/libprocess/src/process.cpp:2158:35), std::__1::reference_wrapper<const std::__1::atomic<bool> > > > >() [inlined] __invoke<(lambda at ../../../3rdparty/libprocess/src/process.cpp:2158:35) &, const std::__1::atomic<bool> &> + 27 at __functional_base:415
    frame #5: 0x00000001022c0f87 libmesos-0.26.0.dylib`::__thread_proxy<std::__1::tuple<std::__1::__bind<(lambda at ../../../3rdparty/libprocess/src/process.cpp:2158:35), std::__1::reference_wrapper<const std::__1::atomic<bool> > > > >() [inlined] __apply_functor<(lambda at ../../../3rdparty/libprocess/src/process.cpp:2158:35), std::__1::tuple<std::__1::reference_wrapper<const std::__1::atomic<bool> > >, 0, std::__1::tuple<> > + 55 at functional:2060
    frame #6: 0x00000001022c0f50 libmesos-0.26.0.dylib`::__thread_proxy<std::__1::tuple<std::__1::__bind<(lambda at ../../../3rdparty/libprocess/src/process.cpp:2158:35), std::__1::reference_wrapper<const std::__1::atomic<bool> > > > >() [inlined] operator()<> + 41 at functional:2123
    frame #7: 0x00000001022c0f27 libmesos-0.26.0.dylib`::__thread_proxy<std::__1::tuple<std::__1::__bind<(lambda at ../../../3rdparty/libprocess/src/process.cpp:2158:35), std::__1::reference_wrapper<const std::__1::atomic<bool> > > > >() [inlined] __invoke<std::__1::__bind<(lambda at ../../../3rdparty/libprocess/src/process.cpp:2158:35), std::__1::reference_wrapper<const std::__1::atomic<bool> > >> + 14 at __functional_base:415
    frame #8: 0x00000001022c0f19 libmesos-0.26.0.dylib`::__thread_proxy<std::__1::tuple<std::__1::__bind<(lambda at ../../../3rdparty/libprocess/src/process.cpp:2158:35), std::__1::reference_wrapper<const std::__1::atomic<bool> > > > >() [inlined] __thread_execute<std::__1::__bind<(lambda at ../../../3rdparty/libprocess/src/process.cpp:2158:35), std::__1::reference_wrapper<const std::__1::atomic<bool> > >> + 25 at thread:337
    frame #9: 0x00000001022c0f00 libmesos-0.26.0.dylib`::__thread_proxy<std::__1::tuple<std::__1::__bind<(lambda at ../../../3rdparty/libprocess/src/process.cpp:2158:35), std::__1::reference_wrapper<const std::__1::atomic<bool> > > > >() + 368 at thread:347
    frame #10: 0x00007fff964c705a libsystem_pthread.dylib`_pthread_body + 131
    frame #11: 0x00007fff964c6fd7 libsystem_pthread.dylib`_pthread_start + 176
    frame #12: 0x00007fff964c43ed libsystem_pthread.dylib`thread_start + 13
{code}",Bug,Major,kaysoky,2015-10-16T21:31:03.000+0000,5,Resolved,Complete,HTTP scheduler library does not gracefully parse invalid resource identifiers,2015-10-16T21:31:03.000+0000,MESOS-3748,1.0,mesos,Mesosphere Sprint 21
bernd-mesos,2015-10-15T12:35:55.000+0000,bernd-mesos,"When fetching fails, the fetcher has written log output to stderr in the task sandbox, but it is not easy to get to. It may even be impossible to get to if one only has the agent log available and no more access to the sandbox. This is for instance the case when looking at output from a CI run.

The fetcher actor in the agent detects if the external fetcher program claims to have succeeded or not. When it exits with an error code, we could grab the fetcher log from the stderr file in the sandbox and append it to the agent log.

This is similar to this patch: https://reviews.apache.org/r/37813/

The difference is that the output of the latter is triggered by test failures outside the fetcher, whereas what is proposed here is triggering upon failures inside the fetcher.",Bug,Minor,bernd-mesos,2015-10-16T09:01:03.000+0000,5,Resolved,Complete,Provide diagnostic output in agent log when fetching fails,2015-10-16T09:01:03.000+0000,MESOS-3743,2.0,mesos,Mesosphere Sprint 21
,2015-10-14T23:58:02.000+0000,cmaloney,"Docker containers aren't currently passed all the same environment variables that Mesos Containerizer tasks are. See: https://github.com/apache/mesos/blob/master/src/slave/containerizer/containerizer.cpp#L254 for all the environment variables explicitly set for mesos containers.

While some of them don't necessarily make sense for docker containers, when the docker has inside of it a libprocess process (A mesos framework scheduler) and is using {{--net=host}} the task needs to have LIBPROCESS_IP set otherwise the same sort of problems that happen because of MESOS-3553 can happen (libprocess will try to guess the machine's IP address with likely bad results in a number of operating environment).",Bug,Major,cmaloney,,10020,Accepted,In Progress,LIBPROCESS_IP not passed to Docker containers,2016-01-19T10:28:56.000+0000,MESOS-3740,3.0,mesos,Mesosphere Sprint 21
vinodkone,2015-10-14T23:41:11.000+0000,BenWhitehead,"While integrating with the HTTP Scheduler API I encountered the following scenario.

The message below was serialized to protobuf and sent as the POST body
{code:title=message}
call {
  type: ACKNOWLEDGE,
  acknowledge: {
    uuid: <bytes>,
    agentID: { value: ""20151012-182734-16777343-5050-8978-S2"" },
    taskID: { value: ""task-1"" }
  }
}
{code}
{code:title=Request Headers}
POST /api/v1/scheduler HTTP/1.1
Content-Type: application/x-protobuf
Accept: application/x-protobuf
Content-Length: 73
Host: localhost:5050
User-Agent: RxNetty Client
{code}

I received the following response
{code:title=Response Headers}
HTTP/1.1 400 Bad Request
Date: Wed, 14 Oct 2015 23:21:36 GMT
Content-Length: 74

Failed to validate Scheduler::Call: Expecting 'framework_id' to be present
{code}

Even though my accept header made no mention of {{text/plain}} the message body returned to me is {{text/plain}}. Additionally, there is no {{Content-Type}} header set on the response so I can't even do anything intelligently in my response handler.",Bug,Major,BenWhitehead,,10006,Reviewable,New,Mesos does not set Content-Type for 400 Bad Request,2016-04-27T16:13:16.000+0000,MESOS-3739,2.0,mesos,Mesosphere Sprint 33
gilbert,2015-10-14T21:22:52.000+0000,gilbert,"The current local store implements get() using the local puller. For all requests of pulling same docker image at the same time, the local puller just untar the image tarball as many times as those requests are, and cp all of them to the same directory, which wastes time and bear high demand of computation. We should be able to support the local store/puller only do these for the first time, and the simultaneous pulling request should wait for the promised future and get it once the first pulling finishes. ",Improvement,Major,gilbert,2015-11-19T00:34:33.000+0000,5,Resolved,Complete,Support docker local store pull same image simultaneously ,2015-11-19T00:34:33.000+0000,MESOS-3736,3.0,mesos,Mesosphere Sprint 22
neilc,2015-10-14T17:58:49.000+0000,neilc,"The build currently fails on OSX:

{noformat}
../3rdparty/libprocess/3rdparty/protobuf-2.5.0/src/protoc -I../../mesos/include/mesos/containerizer		\
		-I../../mesos/include -I../../mesos/src						\
		--python_out=python/interface/src/mesos/interface ../../mesos/include/mesos/containerizer/containerizer.proto
../../mesos/install-sh -c -d python/interface/src/mesos/v1/interface
sed -i 's/mesos\.mesos_pb2/mesos_pb2/' python/interface/src/mesos/interface/containerizer_pb2.py
sed: 1: ""python/interface/src/me ..."": extra characters at the end of p command
make[1]: *** [python/interface/src/mesos/interface/containerizer_pb2.py] Error 1
{noformat}

This is because the sed command uses the wrong syntax for OSX: you need {code}sed -i """"{code} to instruct sed to not use a backup file.",Bug,Blocker,neilc,2015-10-15T07:27:06.000+0000,5,Resolved,Complete,Incorrect sed syntax for Mac OSX,2015-11-08T12:51:44.000+0000,MESOS-3734,2.0,mesos,Mesosphere Sprint 21
alexr,2015-10-14T15:48:21.000+0000,alexr,"FaultToleranceTest.FrameworkReregister test takes more than one second to complete:
{code}
[ RUN      ] FaultToleranceTest.FrameworkReregister
[       OK ] FaultToleranceTest.FrameworkReregister (1056 ms)
{code}

There must be a {{1s}} timeout somewhere which we should mitigate via {{Clock::advance()}}.",Improvement,Major,alexr,2015-11-25T13:37:47.000+0000,5,Resolved,Complete,Speed up FaultToleranceTest.FrameworkReregister test,2015-12-15T16:43:00.000+0000,MESOS-3732,1.0,mesos,Mesosphere Sprint 21
nfnt,2015-10-13T18:23:47.000+0000,nfnt,"When quotas are requested they should authorize their roles.

This ticket will authorize quota requests with ACLs. The existing authorization support that has been implemented in MESOS-1342 will be extended to add a `request_quotas` ACL.",Task,Major,nfnt,2015-11-09T18:57:03.000+0000,5,Resolved,Complete,Prototype quota request authorization,2015-11-09T18:57:03.000+0000,MESOS-3723,5.0,mesos,Mesosphere Sprint 21
nfnt,2015-10-13T18:16:47.000+0000,nfnt,"Quota requests need to be authenticated.

This ticket will authenticate quota requests using credentials provided by the `Authorization` field of the HTTP request. This is similar to how authentication is implemented in `Master::Http`.",Task,Major,nfnt,2015-11-09T18:51:02.000+0000,5,Resolved,Complete,Prototype quota request authentication,2015-11-09T18:53:49.000+0000,MESOS-3722,5.0,mesos,Mesosphere Sprint 21
alexr,2015-10-13T17:49:28.000+0000,alex-mesos,"Allocator-agnostic tests for quota support in the master. They can be divided into several groups:
* Heuristic check;
* Master failover;
* Functionality and quota guarantees.",Improvement,Major,alexr,2015-11-23T20:58:00.000+0000,5,Resolved,Complete,Tests for Quota support in master,2015-11-23T20:58:00.000+0000,MESOS-3720,5.0,mesos,Mesosphere Sprint 22
alexr,2015-10-13T12:13:05.000+0000,alex-mesos,"The built-in Hierarchical DRF allocator should support Quota. This includes (but not limited to): adding, updating, removing and satisfying quota; avoiding both overcomitting resources and handing them to non-quota'ed roles in presence of master failover.

A [design doc for Quota support in Allocator|https://issues.apache.org/jira/browse/MESOS-2937] provides an overview of a feature set required to be implemented.",Bug,Major,alexr,2015-12-03T19:34:36.000+0000,5,Resolved,Complete,Implement Quota support in allocator,2015-12-04T17:47:41.000+0000,MESOS-3718,5.0,mesos,Mesosphere Sprint 21
alexr,2015-10-13T12:03:58.000+0000,alex-mesos,Quota complicates master failover in several ways. The new master should determine if it is possible to satisfy the total quota and notify an operator in case it's not (imagine simultaneous failovers of multiple agents). The new master should hint the allocator how many agents might reconnect in the future to help it decide how to satisfy quota before the majority of agents reconnect.,Task,Major,alexr,2015-11-22T19:15:42.000+0000,5,Resolved,Complete,Master recovery in presence of quota,2015-11-22T19:15:42.000+0000,MESOS-3717,5.0,mesos,Mesosphere Sprint 22
alexr,2015-10-13T11:46:28.000+0000,alexr,"An allocator should be notified when a quota is being set/updated or removed. Also to support master failover in presence of quota, allocator should be notified about the reregistering agents and allocations towards quota.",Bug,Major,alexr,2015-11-09T17:35:34.000+0000,5,Resolved,Complete,Update Allocator interface to support quota,2015-11-09T17:35:34.000+0000,MESOS-3716,3.0,mesos,Mesosphere Sprint 21
arojas,2015-10-12T16:02:16.000+0000,arojas,"[HTTP 1.1 Pipelining|https://en.wikipedia.org/wiki/HTTP_pipelining] describes a mechanism by which multiple HTTP request can be performed over a single socket. The requirement here is that responses should be send in the same order as requests are being made.

Libprocess has some mechanisms built in to deal with pipelining when multiple HTTP requests are made, it is still, however, possible to create a situation in which responses are scrambled respected to the requests arrival.

Consider the situation in which there are two libprocess processes, {{processA}} and {{processB}}, each running in a different thread, {{thread2}} and {{thread3}} respectively. The [{{ProcessManager}}|https://github.com/apache/mesos/blob/1d68eed9089659b06a1e710f707818dbcafeec52/3rdparty/libprocess/src/process.cpp#L374] runs in {{thread1}}.

{{processA}} is of type {{ProcessA}} which looks roughly as follows:

{code}
class ProcessA : public ProcessBase<ProcessA>
{
public:
  ProcessA() {}

  Future<http::Response> foo(const http::Request&) {
    // … Do something …
   return http::Ok();
  }

protected:
  virtual void initialize() {
    route(""/foo"", None(), &ProcessA::foo);
  }
}
{code}

{{processB}} is from type {{ProcessB}} which is just like {{ProcessA}} but routes {{""bar""}} instead of {{""foo""}}.

The situation in which the bug arises is the following:

# Two requests, one for {{""http://server_uri/(1)/foo""}} and one for {{""http://server_uri/(2)//bar""}} are made over the same socket.
# The first request arrives to [{{ProcessManager::handle}}|https://github.com/apache/mesos/blob/1d68eed9089659b06a1e710f707818dbcafeec52/3rdparty/libprocess/src/process.cpp#L2202] which is still running in {{thread1}}. This one creates an {{HttpEvent}} and delivers to the handler, in this case {{processA}}.
# [{{ProcessManager::deliver}}|https://github.com/apache/mesos/blob/1d68eed9089659b06a1e710f707818dbcafeec52/3rdparty/libprocess/src/process.cpp#L2361] enqueues the HTTP event in to the {{processA}} queue. This happens in {{thread1}}.
# The second request arrives to [{{ProcessManager::handle}}|https://github.com/apache/mesos/blob/1d68eed9089659b06a1e710f707818dbcafeec52/3rdparty/libprocess/src/process.cpp#L2202] which is still running in {{thread1}}. Another {{HttpEvent}} is created and delivered to the handler, in this case {{processB}}.
# [{{ProcessManager::deliver}}|https://github.com/apache/mesos/blob/1d68eed9089659b06a1e710f707818dbcafeec52/3rdparty/libprocess/src/process.cpp#L2361] enqueues the HTTP event in to the {{processB}} queue. This happens in {{thread1}}.
# {{Thread2}} is blocked, so {{processA}} cannot handle the first request, it is stuck in the queue.
# {{Thread3}} is idle, so it picks up the request to {{processB}} immediately.
# [{{ProcessBase::visit(HttpEvent)}}|https://github.com/apache/mesos/blob/1d68eed9089659b06a1e710f707818dbcafeec52/3rdparty/libprocess/src/process.cpp#L3073] is called in {{thread3}}, this one in turn [dispatches|https://github.com/apache/mesos/blob/1d68eed9089659b06a1e710f707818dbcafeec52/3rdparty/libprocess/src/process.cpp#L3106] the response's future to the {{HttpProxy}} associated with the socket where the request came.

At the last point, the bug is evident, the request to {{processB}} will be send before the request to {{processA}} even if the handler takes a long time and the {{processA::bar()}} actually finishes before. The responses are not send in the order the requests are done.

h1. Reproducer

The following is a test which successfully reproduces the issue:

{code:title=3rdparty/libprocess/src/tests/http_tests.cpp}
#include <process/latch.hpp

using process::Latch;
using testing::InvokeWithoutArgs;

// This tests tries to force a situation in which HTTP Pipelining is scrambled.
// It does so by having two actors to which three requests are made, the first
// two requests to the first actor and a third request to the second actor.
// The first request will block the first actor long enough to allow the second
// actor to process the third request. Since the first actor will not be able to
// handle any event until it is done processing the first request, the third
// request is finished before the second even starts.
// The ultimate goal of the test is to alter the order in which
// `ProcessBase::visit(HttpEvent)` is executed for the different events
// respect to the order in which the requests arrived.
TEST(HTTPConnectionTest, ComplexPipelining)
{
  Http server1, server2;

  Future<http::Request> get1, get2, get3;
  Latch latch;

  EXPECT_CALL(*server1.process, get(_))
    .WillOnce(DoAll(FutureArg<0>(&get1),
                    InvokeWithoutArgs([&latch]() { latch.await(); }),
                    Return(http::OK(""1""))))
    .WillOnce(DoAll(FutureArg<0>(&get2),
                    Return(http::OK(""2""))));

  EXPECT_CALL(*server2.process, get(_))
    .WillOnce(DoAll(FutureArg<0>(&get3),
                    Return(http::OK(""3""))));

    auto url1 = http::URL(
      ""http"",
      server1.process->self().address.ip,
      server1.process->self().address.port,
      server1.process->self().id + ""/get"");
    auto url2 = http::URL(
      ""http"",
      server1.process->self().address.ip,
      server1.process->self().address.port,
      server2.process->self().id + ""/get"");

  // Create a connection to the server for HTTP pipelining.
  Future<http::Connection> connect = http::connect(url1);

  AWAIT_READY(connect);

  http::Connection connection = connect.get();

  http::Request request1;
  request1.method = ""GET"";
  request1.url = url1;
  request1.keepAlive = true;
  request1.body = ""1"";
  Future<http::Response> response1 = connection.send(request1);

  http::Request request2 = request1;
  request2.body = ""2"";
  Future<http::Response> response2 = connection.send(request2);

  http::Request request3;
  request3.method = ""GET"";
  request3.url = url2;
  request3.keepAlive = true;
  request3.body = ""3"";
  Future<http::Response> response3 = connection.send(request3);

  // Verify that request1 arrived at server1 and it is the right request.
  // Now server1 is blocked processing request1 and cannot pick up more events
  // in the queue.
  AWAIT_READY(get1);
  EXPECT_EQ(request1.body, get1->body);

  // Verify that request3 arrived at server2 and it is the right request.
  AWAIT_READY(get3);
  EXPECT_EQ(request3.body, get3->body);

  // Request2 hasn't been picked up since server1 is still blocked serving
  // request1.
  EXPECT_TRUE(get2.isPending());

  // Free server1 so it can serve request2.
  latch.trigger();

  // Verify that request2 arrived at server1 and it is the right request.
  AWAIT_READY(get2);
  EXPECT_EQ(request2.body, get2->body);

  // Wait for all responses.
  AWAIT_READY(response1);
  AWAIT_READY(response2);
  AWAIT_READY(response3);

  // If pipelining works as expected, even though server2 finished processing
  // its request before server1 even began with request2, the responses should
  // arrive in the order they were made.
  EXPECT_EQ(request1.body, response1->body);
  EXPECT_EQ(request2.body, response2->body);
  EXPECT_EQ(request3.body, response3->body);

  AWAIT_READY(connection.disconnect());
  AWAIT_READY(connection.disconnected());
}
{code}",Bug,Major,arojas,2015-11-12T12:36:28.000+0000,5,Resolved,Complete,HTTP Pipelining doesn't keep order of requests,2016-05-05T18:53:28.000+0000,MESOS-3705,3.0,mesos,Mesosphere Sprint 21
,2015-10-12T15:31:47.000+0000,karya,"Currently, if the signature of a hook function changes, we don't get any compile time errors if the hook implementation is not updated. This results in a hook that is never called.",Task,Major,karya,,1,Open,New,Allow easier detection when hook signature changes,2015-10-12T15:33:08.000+0000,MESOS-3704,2.0,mesos,
gyliu,2015-10-12T03:25:10.000+0000,gyliu,This parameter should be deprecated after 0.23.0 release as it has no use now. ,Bug,Major,gyliu,2015-10-13T00:32:13.000+0000,5,Resolved,Complete,Deprecate resource_monitoring_interval flag,2015-10-13T00:32:13.000+0000,MESOS-3700,1.0,mesos,
greggomann,2015-10-11T01:55:02.000+0000,greggomann,"Picojson supports a streaming mode in which a stream containing a series of JSON values can be repeatedly parsed. For this reason, it does not return an error when passed a string containing a valid JSON value followed by non-whitespace trailing characters.

However, in addition to the four-argument {{picojson::parse()}} that we're using, picojson contains a two-argument {{parse()}} function (https://github.com/kazuho/picojson/blob/master/picojson.h#L938-L942) which accepts a {{std::string}} and should probably validate its input to ensure it doesn't contain trailing characters. A pull request has been filed for this change at https://github.com/kazuho/picojson/pull/70 and if it's merged, we can switch to the two-argument function call. In the meantime, we should provide such input validation ourselves in {{JSON::parse()}}.",Bug,Major,greggomann,2015-10-21T08:18:46.000+0000,5,Resolved,Complete,JSON parsing allows non-whitespace trailing characters,2015-10-21T08:18:46.000+0000,MESOS-3698,1.0,mesos,Mesosphere Sprint 21
hartem,2015-10-09T22:42:13.000+0000,hartem,We should make it easy for everyone to modify the website and be able to generate it locally before pushing to upstream. ,Bug,Major,hartem,2015-10-21T06:51:55.000+0000,5,Resolved,Complete,Enable building mesos.apache.org locally in a Docker container.,2015-10-21T06:51:55.000+0000,MESOS-3694,3.0,mesos,Mesosphere Sprint 21
marco-mesos,2015-10-09T15:35:50.000+0000,frankscholten,"When deploying a framework I encountered the error message 'could not chown work directory'.

It took me a while to figure out that this happened because my framework was registered as a user on my host machine which did not exist on the Docker container and the agent was running as root.

I suggest to clarify this message by pointing out to either set {{--switch-user}}  to {{false}} or to run the framework as the same user as the agent.",Documentation,Minor,frankscholten,2015-10-20T00:16:25.000+0000,5,Resolved,Complete,Clarify error message 'could not chown work directory',2016-01-16T20:42:19.000+0000,MESOS-3692,1.0,mesos,
karya,2015-10-09T10:48:37.000+0000,rdifazio,"We want to get the Docker Name (or Docker ID, or both) when launching a container task with mesos. The container name is generated by mesos itself (i.e. mesos-77e5fde6-83e7-4618-a2dd-d5b10f2b4d25, obtained with ""docker ps"") and it would be nice to expose this information to frameworks so that this information can be used, for example by Marathon to give this information to users via a REST API. 
To go a bit in depth with our use case, we have files created by fluentd logdriver that are named with Docker Name or Docker ID (full or short) and we need a mapping for the users of the REST API and thus the first step is to make this information available from mesos. ",Improvement,Major,rdifazio,,3,In Progress,In Progress,Get Container Name information when launching a container task,2016-01-14T23:20:39.000+0000,MESOS-3688,3.0,mesos,
hartem,2015-10-08T20:50:28.000+0000,hartem,"From Adam's email on dev@ list:

I have used the '-g' feature for github PRs in the past, and we should
continue to support that model, so that new Mesos contributors don't have
to create new RB accounts and learn a new process just for quick
documentation changes, etc.

As a side note, now that the Myriad incubator project has migrated to
Apache git and we can no longer merge PRs directly, we were hoping to take
advantage of a tool like apply-reviews to apply our PR patches. It looks
like apply-reviews.sh only specifies 'mesos' in the GITHUB_URL/API_URL.
Would apply-reviews.py be just as easy to reuse for another project (i.e.
Myriad)?",Task,Major,hartem,2015-11-09T15:20:33.000+0000,5,Resolved,Complete,Add support for github and variable base URLs to apply-reviews.py,2015-11-09T15:20:33.000+0000,MESOS-3625,3.0,mesos,
hausdorff,2015-10-08T20:17:23.000+0000,hausdorff,"Important subset of changes this depends on:

slave/state.cpp: pid, os, path, protobuf, paths, state
pid.hpp: address.hpp, ip.hpp
address.hpp: ip.hpp, net.hpp
net.hpp: ip, networking stuff
state: type_utils, pid, os, path, protobuf, uuid
type_utils.hpp: uuid.hpp",Task,Major,hausdorff,2015-12-10T01:48:06.000+0000,5,Resolved,Complete,Port slave/state.cpp,2015-12-10T01:48:06.000+0000,MESOS-3615,3.0,mesos,Mesosphere Sprint 24
hausdorff,2015-10-08T20:12:55.000+0000,hausdorff,"Important subset of dependency tree of changes necessary:

slave/paths.cpp: os, path",Task,Major,hausdorff,2016-01-29T06:02:05.000+0000,5,Resolved,Complete,Port slave/paths.cpp to Windows,2016-01-29T07:20:19.000+0000,MESOS-3613,1.0,mesos,Mesosphere Sprint 27
mcypark,2015-10-08T00:26:59.000+0000,mcypark,"The example persistent volume framework test does not pass in OS X El Capitan. It seems to be executing the {{<build_dir>/src/.libs/mesos-executor}} directly while it should be executing the wrapper script at {{<build_dir>/src/mesos-executor}} instead. The no-executor framework passes however, which seem to have a very similar configuration with the persistent volume framework. The following is the output that shows the {{dyld}} load error:

{noformat}
I1008 01:22:52.280140 4284416 launcher.cpp:132] Forked child with pid '1706' for contain
er 'b6d3bd96-2ebd-47b1-a16a-a22ffba992aa'
I1008 01:22:52.280300 4284416 containerizer.cpp:873] Checkpointing executor's forked pid
 1706 to '/var/folders/p6/nfxknpz52dzfc6zqnz23tq180000gn/T/mesos-XXXXXX.5OZ3locB/0/meta/
slaves/34d6329e-69cb-4a72-aee4-fe892bf1c70b-S2/frameworks/34d6329e-69cb-4a72-aee4-fe892b
f1c70b-0000/executors/dec188d4-d2dc-40c5-ac4d-881adc3d81c0/runs/b6d3bd96-2ebd-47b1-a16a-
a22ffba992aa/pids/forked.pid'
dyld: Library not loaded: /usr/local/lib/libmesos-0.26.0.dylib
  Referenced from: /Users/mpark/Projects/mesos/build/src/.libs/mesos-executor
  Reason: image not found
dyld: Library not loaded: /usr/local/lib/libmesos-0.26.0.dylib
  Referenced from: /Users/mpark/Projects/mesos/build/src/.libs/mesos-executor
  Reason: image not found
dyld: Library not loaded: /usr/local/lib/libmesos-0.26.0.dylib
  Referenced from: /Users/mpark/Projects/mesos/build/src/.libs/mesos-executor
  Reason: image not found
I1008 01:22:52.365397 3211264 containerizer.cpp:1284] Executor for container '06b649be-88c8-4047-8fb5-e89bdd096b66' has exited
I1008 01:22:52.365433 3211264 containerizer.cpp:1097] Destroying container '06b649be-88c8-4047-8fb5-e89bdd096b66'
{noformat}",Bug,Major,mcypark,2015-10-19T19:15:58.000+0000,5,Resolved,Complete,ExamplesTest.PersistentVolumeFramework does not work in OS X El Capitan,2016-04-02T00:04:44.000+0000,MESOS-3604,3.0,mesos,Mesosphere Sprint 21
karya,2015-10-07T23:23:27.000+0000,karya,"Compilation fails on OpenSUSE Tumbleweed (Linux 4.1.6, gcc 5.1.1, glibc 2.22) with the following errors:

{code}
In file included from ../../src/tests/values_tests.cpp:22:0: 
../3rdparty/libprocess/3rdparty/gmock-1.7.0/gtest/include/gtest/gtest.h: In instantiatio
n of ‘testing::AssertionResult testing::internal::CmpHelperEQ(const char*, const char*, 
const T1&, const T2&) [with T1 = int; T2 = long unsigned int]’: 
../3rdparty/libprocess/3rdparty/gmock-1.7.0/gtest/include/gtest/gtest.h:1484:23:   requi
red from ‘static testing::AssertionResult testing::internal::EqHelper<lhs_is_null_litera
l>::Compare(const char*, const char*, const T1&, const T2&) [with T1 = int; T2 = long un
signed int; bool lhs_is_null_literal = false]’ 
../../src/tests/values_tests.cpp:287:3:   required from here 
../3rdparty/libprocess/3rdparty/gmock-1.7.0/gtest/include/gtest/gtest.h:1448:16: error: 
comparison between signed and unsigned integer expressions [-Werror=sign-compare] 
  if (expected == actual) { 
               ^ 
 CXX      tests/containerizer/mesos_tests-provisioner_docker_tests.o 
^CMakefile:6779: recipe for target 'tests/mesos_tests-values_tests.o' failed 
make[3]: *** [tests/mesos_tests-values_tests.o] Interrupt
{code}",Bug,Blocker,karya,2015-10-12T20:20:19.000+0000,5,Resolved,Complete,Test build failure due to comparison between signed and unsigned integers,2015-11-08T12:51:45.000+0000,MESOS-3603,1.0,mesos,
mchadha,2015-10-06T17:34:27.000+0000,mchadha,"When running multi framework instances per process, if the number of framework created exceeds the libprocess threads then during master failover the zookeeper updates can cause deadlock. E.g. On a machine with 24 cpus, if the framework instance count exceeds 24 ( per process)  then when the master fails over all the libprocess threads block updating the cache ( GroupProcess) leading to deadlock. Below is the stack trace of one the libprocess thread :

{code}
Thread 101 (Thread 0x7f42821f1700 (LWP 5974)):
#0  0x000000314100b5bc in pthread_cond_wait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0
#1  0x00007f42870d1637 in Gate::arrive(long) () from /Users/mchadha/venv/lib/python2.7/site-packages/mesos.native-0.22.1003-py2.7-linux-x86_64.egg/mesos/native/_mesos.so
#2  0x00007f42870be87c in process::ProcessManager::wait(process::UPID const&) () from /Users/mchadha/venv/lib/python2.7/site-packages/mesos.native-0.22.1003-py2.7-linux-x86_64.eg
g/mesos/native/_mesos.so
#3  0x00007f42870c25f7 in process::wait(process::UPID const&, Duration const&) () from /Users/mchadha/venv/lib/python2.7/site-packages/mesos.native-0.22.1003-py2.7-linux-x86_64.e
gg/mesos/native/_mesos.so
#4  0x00007f428708e294 in process::Latch::await(Duration const&) () from /Users/mchadha/venv/lib/python2.7/site-packages/mesos.native-0.22.1003-py2.7-linux-x86_64.egg/mesos/nativ
e/_mesos.so
#5  0x00007f4286b67dea in process::Future<int>::await(Duration const&) const () from /Users/mchadha/venv/lib/python2.7/site-packages/mesos.native-0.22.1003-py2.7-linux-x86_64.egg
/mesos/native/_mesos.so
#6  0x00007f4286b5a0df in process::Future<int>::get() const () from /Users/mchadha/venv/lib/python2.7/site-packages/mesos.native-0.22.1003-py2.7-linux-x86_64.egg/mesos/native/_me
sos.so
#7  0x00007f4286ff0508 in ZooKeeper::getChildren(std::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, bool, std::vector<std::basic_string<char, std::cha
r_traits<char>, std::allocator<char> >, std::allocator<std::basic_string<char, std::char_traits<char>, std::allocator<char> > > >*) () from /Users/mchadha/venv/lib/python2.7/site
-packages/mesos.native-0.22.1003-py2.7-linux-x86_64.egg/mesos/native/_mesos.so
#8  0x00007f4286cb394e in zookeeper::GroupProcess::cache() () from /Users/mchadha/venv/lib/python2.7/site-packages/mesos.native-0.22.1003-py2.7-linux-x86_64.egg/mesos/native/_mes
os.so
#9  0x00007f4286cb1e63 in zookeeper::GroupProcess::updated(long, std::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) () from /Users/mchadha/venv/lib/py
thon2.7/site-packages/mesos.native-0.22.1003-py2.7-linux-x86_64.egg/mesos/native/_mesos.so
#10 0x00007f4286ce027a in std::tr1::_Mem_fn<void (zookeeper::GroupProcess::*)(long, std::basic_string<char, std::char_traits<char>, std::allocator<char> > const&)>::operator()(zo
okeeper::GroupProcess*, long, std::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) const () from /Users/mchadha/venv/lib/python2.7/site-packages/mesos.n
ative-0.22.1003-py2.7-linux-x86_64.egg/mesos/native/_mesos.so
#11 0x00007f4286ce0067 in std::tr1::result_of<std::tr1::_Mem_fn<void (zookeeper::GroupProcess::*)(long, std::basic_string<char, std::char_traits<char>, std::allocator<char> > con
st&)> ()(std::tr1::result_of<std::tr1::_Mu<std::tr1::_Placeholder<1>, false, true> ()(std::tr1::_Placeholder<1>, std::tr1::tuple<zookeeper::GroupProcess*&>)>::type, std::tr1::res
ult_of<std::tr1::_Mu<long, false, false> ()(long, std::tr1::_Mu<std::tr1::_Placeholder<1>, false, true> ()(std::tr1::_Placeholder<1>, std::tr1::tuple<zookeeper::GroupProcess*&>))
>::type, std::tr1::result_of<std::tr1::_Mu<std::basic_string<char, std::char_traits<char>, std::allocator<char> >, false, false> ()(std::basic_string<char, std::char_traits<char>
, std::allocator<char> >, std::tr1::_Mu<std::tr1::_Placeholder<1>, false, true> ()(std::tr1::_Placeholder<1>, std::tr1::tuple<zookeeper::GroupProcess*&>))>::type)>::type std::tr1
::_Bind<std::tr1::_Mem_fn<void (zookeeper::GroupProcess::*)(long, std::basic_string<char, std::char_traits<char>, std::allocator<char> > const&)> ()(std::tr1::_Placeholder<1>, lo
ng, std::basic_string<char, std::char_traits<char>, std::allocator<char> >)>::__call<zookeeper::GroupProcess*&, 0, 1, 2>(std::tr1::_Mu<std::tr1::_Placeholder<1>, false, true> ( c
onst&)(std::tr1::_Placeholder<1>, std::tr1::tuple<zookeeper::GroupProcess*&>), std::tr1::_Index_tuple<0, 1, 2>) () from /Users/mchadha/venv/lib/python2.7/site-packages/mesos.nati
ve-0.22.1003-py2.7-linux-x86_64.egg/mesos/native/_mesos.so
#12 0x00007f4286cdfd16 in std::tr1::result_of<std::tr1::_Mem_fn<void (zookeeper::GroupProcess::*)(long, std::basic_string<char, std::char_traits<char>, std::allocator<char> > con
st&)> ()(std::tr1::result_of<std::tr1::_Mu<std::tr1::_Placeholder<1>, false, true> ()(std::tr1::_Placeholder<1>, std::tr1::tuple<zookeeper::GroupProcess*>)>::type, std::tr1::resu
lt_of<std::tr1::_Mu<long, false, false> ()(long, std::tr1::_Mu<std::tr1::_Placeholder<1>, false, true> ()(std::tr1::_Placeholder<1>, std::tr1::tuple<zookeeper::GroupProcess*>))>:
:type, std::tr1::result_of<std::tr1::_Mu<std::basic_string<char, std::char_traits<char>, std::allocator<char> >, false, false> ()(std::basic_string<char, std::char_traits<char>,
std::allocator<char> >, std::tr1::_Mu<std::tr1::_Placeholder<1>, false, true> ()(std::tr1::_Placeholder<1>, std::tr1::tuple<zookeeper::GroupProcess*>))>::type)>::type std::tr1::_
Bind<std::tr1::_Mem_fn<void (zookeeper::GroupProcess::*)(long, std::basic_string<char, std::char_traits<char>, std::allocator<char> > const&)> ()(std::tr1::_Placeholder<1>, long,
 std::basic_string<char, std::char_traits<char>, std::allocator<char> >)>::operator()<zookeeper::GroupProcess*>(zookeeper::GroupProcess*&) () from /Users/mchadha/venv/lib/python2
.7/site-packages/mesos.native-0.22.1003-py2.7-linux-x86_64.egg/mesos/native/_mesos.so
#13 0x00007f4286cdf8be in std::tr1::_Function_handler<void ()(zookeeper::GroupProcess*), std::tr1::_Bind<std::tr1::_Mem_fn<void (zookeeper::GroupProcess::*)(long, std::basic_stri
ng<char, std::char_traits<char>, std::allocator<char> > const&)> ()(std::tr1::_Placeholder<1>, long, std::basic_string<char, std::char_traits<char>, std::allocator<char> >)> >::_
M_invoke(std::tr1::_Any_data const&, zookeeper::GroupProcess*) () from /Users/mchadha/venv/lib/python2.7/site-packages/mesos.native-0.22.1003-py2.7-linux-x86_64.egg/mesos/native/
_mesos.so
#14 0x00007f4286cc2394 in std::tr1::function<void ()(zookeeper::GroupProcess*)>::operator()(zookeeper::GroupProcess*) const () from /Users/mchadha/venv/lib/python2.7/site-package
s/mesos.native-0.22.1003-py2.7-linux-x86_64.egg/mesos/native/_mesos.so
#15 0x00007f4286cbc3a2 in void process::internal::vdispatcher<zookeeper::GroupProcess>(process::ProcessBase*, std::tr1::shared_ptr<std::tr1::function<void ()(zookeeper::GroupProc
ess*)> >) () from /Users/mchadha/venv/lib/python2.7/site-packages/mesos.native-0.22.1003-py2.7-linux-x86_64.egg/mesos/native/_mesos.so
#16 0x00007f4286ccdca5 in std::tr1::result_of<void (*()(std::tr1::result_of<std::tr1::_Mu<std::tr1::_Placeholder<1>, false, true> ()(std::tr1::_Placeholder<1>, std::tr1::tuple<pr
ocess::ProcessBase*&>)>::type, std::tr1::result_of<std::tr1::_Mu<std::tr1::shared_ptr<std::tr1::function<void ()(zookeeper::GroupProcess*)> >, false, false> ()(std::tr1::shared_p
tr<std::tr1::function<void ()(zookeeper::GroupProcess*)> >, std::tr1::_Mu<std::tr1::_Placeholder<1>, false, true> ()(std::tr1::_Placeholder<1>, std::tr1::tuple<process::ProcessBa
se*&>))>::type))(process::ProcessBase*, std::tr1::shared_ptr<std::tr1::function<void ()(zookeeper::GroupProcess*)> >)>::type std::tr1::_Bind<void (*()(std::tr1::_Placeholder<1>,
std::tr1::shared_ptr<std::tr1::function<void ()(zookeeper::GroupProcess*)> >))(process::ProcessBase*, std::tr1::shared_ptr<std::tr1::function<void ()(zookeeper::GroupProcess*)> >
)>::__call<process::ProcessBase*&, 0, 1>(std::tr1::_Mu<std::tr1::_Placeholder<1>, false, true> ( const&)(std::tr1::_Placeholder<1>, std::tr1::tuple<process::ProcessBase*&>), std:
:tr1::_Index_tuple<0, 1>) () from /Users/mchadha/venv/lib/python2.7/site-packages/mesos.native-0.22.1003-py2.7-linux-x86_64.egg/mesos/native/_mesos.so
#17 0x00007f4286cc7a5a in std::tr1::result_of<void (*()(std::tr1::result_of<std::tr1::_Mu<std::tr1::_Placeholder<1>, false, true> ()(std::tr1::_Placeholder<1>, std::tr1::tuple<pr
ocess::ProcessBase*>)>::type, std::tr1::result_of<std::tr1::_Mu<std::tr1::shared_ptr<std::tr1::function<void ()(zookeeper::GroupProcess*)> >, false, false> ()(std::tr1::shared_pt
r<std::tr1::function<void ()(zookeeper::GroupProcess*)> >, std::tr1::_Mu<std::tr1::_Placeholder<1>, false, true> ()(std::tr1::_Placeholder<1>, std::tr1::tuple<process::ProcessBas
e*>))>::type))(process::ProcessBase*, std::tr1::shared_ptr<std::tr1::function<void ()(zookeeper::GroupProcess*)> >)>::type std::tr1::_Bind<void (*()(std::tr1::_Placeholder<1>, st
d::tr1::shared_ptr<std::tr1::function<void ()(zookeeper::GroupProcess*)> >))(process::ProcessBase*, std::tr1::shared_ptr<std::tr1::function<void ()(zookeeper::GroupProcess*)> >)>
::operator()<process::ProcessBase*>(process::ProcessBase*&) () from /Users/mchadha/venv/lib/python2.7/site-packages/mesos.native-0.22.1003-py2.7-linux-x86_64.egg/mesos/native/_me
sos.so
#18 0x00007f4286cc2480 in std::tr1::_Function_handler<void ()(process::ProcessBase*), std::tr1::_Bind<void (*()(std::tr1::_Placeholder<1>, std::tr1::shared_ptr<std::tr1::function
<void ()(zookeeper::GroupProcess*)> >))(process::ProcessBase*, std::tr1::shared_ptr<std::tr1::function<void ()(zookeeper::GroupProcess*)> >)> >::_M_invoke(std::tr1::_Any_data con
st&, process::ProcessBase*) () from /Users/mchadha/venv/lib/python2.7/site-packages/mesos.native-0.22.1003-py2.7-linux-x86_64.egg/mesos/native/_mesos.so
#19 0x00007f42870db546 in std::tr1::function<void ()(process::ProcessBase*)>::operator()(process::ProcessBase*) const () from /Users/mchadha/venv/lib/python2.7/site-packages/meso
s.native-0.22.1003-py2.7-linux-x86_64.egg/mesos/native/_mesos.so
#20 0x00007f42870c1013 in process::ProcessBase::visit(process::DispatchEvent const&) () from /Users/mchadha/venv/lib/python2.7/site-packages/mesos.native-0.22.1003-py2.7-linux-x8
6_64.egg/mesos/native/_mesos.so
#21 0x00007f42870c5582 in process::DispatchEvent::visit(process::EventVisitor*) const () from /Users/mchadha/venv/lib/python2.7/site-packages/mesos.native-0.22.1003-py2.7-linux-x
86_64.egg/mesos/native/_mesos.so
#22 0x00007f428666680e in process::ProcessBase::serve(process::Event const&) () from /Users/mchadha/venv/lib/python2.7/site-packages/mesos.native-0.22.1003-py2.7-linux-x86_64.egg
/mesos/native/_mesos.so
#23 0x00007f42870bd88f in process::ProcessManager::resume(process::ProcessBase*) () from /Users/mchadha/venv/lib/python2.7/site-packages/mesos.native-0.22.1003-py2.7-linux-x86_64
.egg/mesos/native/_mesos.so
#24 0x00007f42870b1cb9 in process::schedule(void*) () from /Users/mchadha/venv/lib/python2.7/site-packages/mesos.native-0.22.1003-py2.7-linux-x86_64.egg/mesos/native/_mesos.so
#25 0x00000031410079d1 in start_thread () from /lib64/libpthread.so.0
#26 0x00000031408e88fd in clone () from /lib64/libc.so.6
{code}

Solution: 
 Create master detector per url instead of per framework.
Will send the review request. 

",Bug,Major,mchadha,2016-01-26T09:03:11.000+0000,5,Resolved,Complete,Framework process hangs after master failover when number frameworks > libprocess thread pool size,2016-01-26T09:03:11.000+0000,MESOS-3595,3.0,mesos,Mesosphere Sprint 24
karya,2015-10-06T01:11:41.000+0000,karya,"Currently, if {{Isolator::prepare}} fails for some isolator(s), we simply return a generic message about container being destroyed during launch.

It would be especially helpful if a third-party isolator modules could report the error back to the framework.",Task,Major,karya,2015-11-06T13:42:49.000+0000,5,Resolved,Complete,Propagate Isolator::prepare() failures to the framework,2015-11-06T13:42:49.000+0000,MESOS-3593,2.0,mesos,
,2015-10-05T22:42:06.000+0000,bmahler,"FWICT, this is just a consequence of some technical debt in the master code. When an active framework fails over, we do not go through the deactivation->activation code paths, and so:

(1) The framework's filters in the allocator remain after the failover.
(2) The failed over framework does not receive an immediate allocation (it has to wait for the next allocation interval).

If the framework had disconnected first, then the failover goes through the deactivation->activation code paths.

This also means that some tests take longer to run than necessary.",Bug,Minor,bmahler,,10020,Accepted,In Progress,Framework failover when framework is 'active' does not trigger allocation.,2015-10-23T15:34:38.000+0000,MESOS-3587,5.0,mesos,
kaysoky,2015-10-05T22:31:41.000+0000,bernadinm,"I am install Mesos 0.24.0 on 4 servers which have very similar hardware and software configurations. 

After performing {{../configure}}, {{make}}, and {{make check}} some servers have completed successfully and other failed on test {{[ RUN      ] MemoryPressureMesosTest.CGROUPS_ROOT_Statistics}}.

Is there something I should check in this test? 

{code}
PERFORMED MAKE CHECK NODE-001
[ RUN      ] MemoryPressureMesosTest.CGROUPS_ROOT_Statistics
I1005 14:37:35.585067 38479 exec.cpp:133] Version: 0.24.0
I1005 14:37:35.593789 38497 exec.cpp:207] Executor registered on slave 20151005-143735-2393768202-35106-27900-S0
Registered executor on svdidac038.techlabs.accenture.com
Starting task 010b2fe9-4eac-4136-8a8a-6ce7665488b0
Forked command at 38510
sh -c 'while true; do dd count=512 bs=1M if=/dev/zero of=./temp; done'


PERFORMED MAKE CHECK NODE-002
[ RUN      ] MemoryPressureMesosTest.CGROUPS_ROOT_Statistics
I1005 14:38:58.794112 36997 exec.cpp:133] Version: 0.24.0
I1005 14:38:58.802851 37022 exec.cpp:207] Executor registered on slave 20151005-143857-2360213770-50427-26325-S0
Registered executor on svdidac039.techlabs.accenture.com
Starting task 9bb317ba-41cb-44a4-b507-d1c85ceabc28
sh -c 'while true; do dd count=512 bs=1M if=/dev/zero of=./temp; done'
Forked command at 37028
../../src/tests/containerizer/memory_pressure_tests.cpp:145: Failure
Expected: (usage.get().mem_medium_pressure_counter()) >= (usage.get().mem_critical_pressure_counter()), actual: 5 vs 6
2015-10-05 14:39:00,130:26325(0x2af08cc78700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:37198] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
[  FAILED  ] MemoryPressureMesosTest.CGROUPS_ROOT_Statistics (4303 ms)
{code}",Bug,Major,bernadinm,2015-12-09T11:00:22.000+0000,5,Resolved,Complete,MemoryPressureMesosTest.CGROUPS_ROOT_Statistics and CGROUPS_ROOT_SlaveRecovery are flaky,2016-03-14T20:01:14.000+0000,MESOS-3586,1.0,mesos,Mesosphere Sprint 23
karya,2015-10-05T20:55:24.000+0000,karya,"With the addition of {{NetworkInfo}} to allow frameworks to request IP-per-container for their tasks, we should add a simple module that mimics the behavior of a real network-isolation module for testing purposes. We can then add this module in {{src/examples}} and write some tests against it.

This module can also serve as a template module for third-party network isolation provides for building their own network isolator modules.",Task,Major,karya,,10020,Accepted,In Progress,Add a test module for ip-per-container support,2016-01-19T10:27:58.000+0000,MESOS-3585,3.0,mesos,
karya,2015-10-05T19:59:50.000+0000,jamespeach,"Stout tests are in a binary named {{stout-tests}}, Mesos tests are in {{mesos-tests}}, but libprocess tests are just {{tests}}. It would be helpful to name them {{libprocess-tests}} ",Bug,Trivial,jamespeach,2015-10-09T08:59:46.000+0000,5,Resolved,Complete,"rename libprocess tests to ""libprocess-tests""",2015-10-09T08:59:46.000+0000,MESOS-3584,1.0,mesos,Mesosphere Sprint 20
greggomann,2015-10-05T17:47:31.000+0000,anandmazumdar,"Currently, the HTTP Scheduler API has no concept of Sessions aka {{SessionID}} or a {{TokenID}}. This is useful in some failure scenarios. As of now, if a framework fails over and then subscribes again with the same {{FrameworkID}} with the {{force}} option set, the Mesos master would subscribe it.

If the previous instance of the framework/scheduler tries to send a Call , e.g. {{Call::KILL}} with the same previous {{FrameworkID}} set, it would be still accepted by the master leading to erroneously killing a task.

This is possible because we do not have a way currently of distinguishing connections. It used to work in the previous driver implementation due to the master also performing a {{UPID}} check to verify if they matched and only then allowing the call. Following the design process, we will implemented ""stream IDs"" for Mesos HTTP schedulers; each ID will be associated with a single subscription connection, and the scheduler must include it as a header in all non-subscribe calls sent to the master.",Task,Major,anandmazumdar,2016-03-03T21:57:48.000+0000,5,Resolved,Complete,Introduce stream IDs in HTTP Scheduler API,2016-03-03T21:57:49.000+0000,MESOS-3583,5.0,mesos,Mesosphere Sprint 30
bbannier,2015-10-05T08:38:27.000+0000,bbannier,"Currently license headers are commented in something resembling Javadoc style,

{code}
/**
* Licensed ...
{code}

Since we use Javadoc-style comment blocks for doxygen documentation all license headers appear in the generated documentation, potentially and likely hiding the actual documentation.

Using {{/*}} to start the comment blocks would be enough to hide them from doxygen, but would likely also result in a largish (though mostly uninteresting) patch.",Documentation,Minor,bbannier,2015-12-03T18:44:09.000+0000,5,Resolved,Complete,License headers show up all over doxygen documentation.,2015-12-03T18:44:09.000+0000,MESOS-3581,2.0,mesos,Mesosphere Sprint 22
bbannier,2015-10-04T01:02:01.000+0000,anandmazumdar,"From ASF CI:
https://builds.apache.org/job/Mesos/866/COMPILER=clang,CONFIGURATION=--verbose,OS=ubuntu:14.04,label_exp=docker%7C%7CHadoop/console

{code}
[ RUN      ] FetcherCacheTest.LocalUncachedExtract
Using temporary directory '/tmp/FetcherCacheTest_LocalUncachedExtract_jHBfeA'
I0925 19:15:39.541198 27410 leveldb.cpp:176] Opened db in 3.43934ms
I0925 19:15:39.542362 27410 leveldb.cpp:183] Compacted db in 1.136184ms
I0925 19:15:39.542428 27410 leveldb.cpp:198] Created db iterator in 35866ns
I0925 19:15:39.542448 27410 leveldb.cpp:204] Seeked to beginning of db in 8807ns
I0925 19:15:39.542459 27410 leveldb.cpp:273] Iterated through 0 keys in the db in 6325ns
I0925 19:15:39.542505 27410 replica.cpp:744] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned
I0925 19:15:39.543143 27438 recover.cpp:449] Starting replica recovery
I0925 19:15:39.543393 27438 recover.cpp:475] Replica is in EMPTY status
I0925 19:15:39.544373 27436 replica.cpp:641] Replica in EMPTY status received a broadcasted recover request
I0925 19:15:39.544791 27433 recover.cpp:195] Received a recover response from a replica in EMPTY status
I0925 19:15:39.545284 27433 recover.cpp:566] Updating replica status to STARTING
I0925 19:15:39.546155 27436 master.cpp:376] Master c8bf1c95-50f4-4832-a570-c560f0b466ae (f57fd4291168) started on 172.17.1.195:41781
I0925 19:15:39.546257 27433 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 747249ns
I0925 19:15:39.546288 27433 replica.cpp:323] Persisted replica status to STARTING
I0925 19:15:39.546483 27434 recover.cpp:475] Replica is in STARTING status
I0925 19:15:39.546187 27436 master.cpp:378] Flags at startup: --acls="""" --allocation_interval=""1secs"" --allocator=""HierarchicalDRF"" --authenticate=""true"" --authenticate_slaves=""true"" --authenticators=""crammd5"" --authorizers=""local"" --credentials=""/tmp/FetcherCacheTest_LocalUncachedExtract_jHBfeA/credentials"" --framework_sorter=""drf"" --help=""false"" --hostname_lookup=""true"" --initialize_driver_logging=""true"" --log_auto_initialize=""true"" --logbufsecs=""0"" --logging_level=""INFO"" --max_slave_ping_timeouts=""5"" --quiet=""false"" --recovery_slave_removal_limit=""100%"" --registry=""replicated_log"" --registry_fetch_timeout=""1mins"" --registry_store_timeout=""25secs"" --registry_strict=""true"" --root_submissions=""true"" --slave_ping_timeout=""15secs"" --slave_reregister_timeout=""10mins"" --user_sorter=""drf"" --version=""false"" --webui_dir=""/mesos/mesos-0.26.0/_inst/share/mesos/webui"" --work_dir=""/tmp/FetcherCacheTest_LocalUncachedExtract_jHBfeA/master"" --zk_session_timeout=""10secs""
I0925 19:15:39.546567 27436 master.cpp:423] Master only allowing authenticated frameworks to register
I0925 19:15:39.546617 27436 master.cpp:428] Master only allowing authenticated slaves to register
I0925 19:15:39.546632 27436 credentials.hpp:37] Loading credentials for authentication from '/tmp/FetcherCacheTest_LocalUncachedExtract_jHBfeA/credentials'
I0925 19:15:39.546931 27436 master.cpp:467] Using default 'crammd5' authenticator
I0925 19:15:39.547044 27436 master.cpp:504] Authorization enabled
I0925 19:15:39.547276 27441 whitelist_watcher.cpp:79] No whitelist given
I0925 19:15:39.547320 27434 hierarchical.hpp:468] Initialized hierarchical allocator process
I0925 19:15:39.547471 27438 replica.cpp:641] Replica in STARTING status received a broadcasted recover request
I0925 19:15:39.548318 27443 recover.cpp:195] Received a recover response from a replica in STARTING status
I0925 19:15:39.549067 27435 recover.cpp:566] Updating replica status to VOTING
I0925 19:15:39.549115 27440 master.cpp:1603] The newly elected leader is master@172.17.1.195:41781 with id c8bf1c95-50f4-4832-a570-c560f0b466ae
I0925 19:15:39.549162 27440 master.cpp:1616] Elected as the leading master!
I0925 19:15:39.549190 27440 master.cpp:1376] Recovering from registrar
I0925 19:15:39.549342 27434 registrar.cpp:309] Recovering registrar
I0925 19:15:39.549666 27430 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 418187ns
I0925 19:15:39.549753 27430 replica.cpp:323] Persisted replica status to VOTING
I0925 19:15:39.550089 27442 recover.cpp:580] Successfully joined the Paxos group
I0925 19:15:39.550320 27442 recover.cpp:464] Recover process terminated
I0925 19:15:39.550904 27430 log.cpp:661] Attempting to start the writer
I0925 19:15:39.551955 27434 replica.cpp:477] Replica received implicit promise request with proposal 1
I0925 19:15:39.552351 27434 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 380746ns
I0925 19:15:39.552372 27434 replica.cpp:345] Persisted promised to 1
I0925 19:15:39.552896 27436 coordinator.cpp:231] Coordinator attemping to fill missing position
I0925 19:15:39.554003 27432 replica.cpp:378] Replica received explicit promise request for position 0 with proposal 2
I0925 19:15:39.554534 27432 leveldb.cpp:343] Persisting action (8 bytes) to leveldb took 510572ns
I0925 19:15:39.554558 27432 replica.cpp:679] Persisted action at 0
I0925 19:15:39.555516 27443 replica.cpp:511] Replica received write request for position 0
I0925 19:15:39.555595 27443 leveldb.cpp:438] Reading position from leveldb took 65355ns
I0925 19:15:39.556177 27443 leveldb.cpp:343] Persisting action (14 bytes) to leveldb took 542757ns
I0925 19:15:39.556200 27443 replica.cpp:679] Persisted action at 0
I0925 19:15:39.556813 27429 replica.cpp:658] Replica received learned notice for position 0
I0925 19:15:39.557251 27429 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 422272ns
I0925 19:15:39.557281 27429 replica.cpp:679] Persisted action at 0
I0925 19:15:39.557315 27429 replica.cpp:664] Replica learned NOP action at position 0
I0925 19:15:39.558061 27442 log.cpp:677] Writer started with ending position 0
I0925 19:15:39.559294 27434 leveldb.cpp:438] Reading position from leveldb took 56536ns
I0925 19:15:39.560333 27432 registrar.cpp:342] Successfully fetched the registry (0B) in 10.936064ms
I0925 19:15:39.560469 27432 registrar.cpp:441] Applied 1 operations in 41340ns; attempting to update the 'registry'
I0925 19:15:39.561244 27441 log.cpp:685] Attempting to append 176 bytes to the log
I0925 19:15:39.561378 27436 coordinator.cpp:341] Coordinator attempting to write APPEND action at position 1
I0925 19:15:39.562126 27439 replica.cpp:511] Replica received write request for position 1
I0925 19:15:39.562515 27439 leveldb.cpp:343] Persisting action (195 bytes) to leveldb took 364968ns
I0925 19:15:39.562539 27439 replica.cpp:679] Persisted action at 1
I0925 19:15:39.563160 27438 replica.cpp:658] Replica received learned notice for position 1
I0925 19:15:39.563699 27438 leveldb.cpp:343] Persisting action (197 bytes) to leveldb took 455933ns
I0925 19:15:39.563730 27438 replica.cpp:679] Persisted action at 1
I0925 19:15:39.563753 27438 replica.cpp:664] Replica learned APPEND action at position 1
I0925 19:15:39.564749 27434 registrar.cpp:486] Successfully updated the 'registry' in 4.214016ms
I0925 19:15:39.564893 27434 registrar.cpp:372] Successfully recovered registrar
I0925 19:15:39.564950 27442 log.cpp:704] Attempting to truncate the log to 1
I0925 19:15:39.565039 27429 coordinator.cpp:341] Coordinator attempting to write TRUNCATE action at position 2
I0925 19:15:39.565172 27430 master.cpp:1413] Recovered 0 slaves from the Registry (137B) ; allowing 10mins for slaves to re-register
I0925 19:15:39.565946 27429 replica.cpp:511] Replica received write request for position 2
I0925 19:15:39.566349 27429 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 375473ns
I0925 19:15:39.566371 27429 replica.cpp:679] Persisted action at 2
I0925 19:15:39.566994 27431 replica.cpp:658] Replica received learned notice for position 2
I0925 19:15:39.567440 27431 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 437095ns
I0925 19:15:39.567483 27431 leveldb.cpp:401] Deleting ~1 keys from leveldb took 31954ns
I0925 19:15:39.567498 27431 replica.cpp:679] Persisted action at 2
I0925 19:15:39.567514 27431 replica.cpp:664] Replica learned TRUNCATE action at position 2
I0925 19:15:39.576660 27410 containerizer.cpp:143] Using isolation: posix/cpu,posix/mem,filesystem/posix
W0925 19:15:39.577055 27410 backend.cpp:50] Failed to create 'bind' backend: BindBackend requires root privileges
I0925 19:15:39.583020 27443 slave.cpp:190] Slave started on 46)@172.17.1.195:41781
I0925 19:15:39.583062 27443 slave.cpp:191] Flags at startup: --appc_store_dir=""/tmp/mesos/store/appc"" --authenticatee=""crammd5"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --credential=""/tmp/FetcherCacheTest_LocalUncachedExtract_LwfzK4/credential"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_kill_orphans=""true"" --docker_local_archives_dir=""/tmp/mesos/images/docker"" --docker_puller=""local"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/tmp/mesos/store/docker"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/tmp/FetcherCacheTest_LocalUncachedExtract_LwfzK4/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --hostname_lookup=""true"" --image_provisioner_backend=""copy"" --initialize_driver_logging=""true"" --isolation=""posix/cpu,posix/mem"" --launcher_dir=""/mesos/mesos-0.26.0/_build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""10ms"" --resource_monitoring_interval=""1secs"" --resources=""cpus(*):1000; mem(*):1000"" --revocable_cpu_low_priority=""true"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --systemd_runtime_directory=""/run/systemd/system"" --version=""false"" --work_dir=""/tmp/FetcherCacheTest_LocalUncachedExtract_LwfzK4""
I0925 19:15:39.583472 27443 credentials.hpp:85] Loading credential for authentication from '/tmp/FetcherCacheTest_LocalUncachedExtract_LwfzK4/credential'
I0925 19:15:39.583752 27443 slave.cpp:321] Slave using credential for: test-principal
I0925 19:15:39.584249 27443 slave.cpp:354] Slave resources: cpus(*):1000; mem(*):1000; disk(*):3.70122e+06; ports(*):[31000-32000]
I0925 19:15:39.584344 27443 slave.cpp:390] Slave hostname: f57fd4291168
I0925 19:15:39.584362 27443 slave.cpp:395] Slave checkpoint: true
I0925 19:15:39.585180 27428 state.cpp:54] Recovering state from '/tmp/FetcherCacheTest_LocalUncachedExtract_LwfzK4/meta'
I0925 19:15:39.585383 27440 status_update_manager.cpp:202] Recovering status update manager
I0925 19:15:39.585636 27435 containerizer.cpp:386] Recovering containerizer
I0925 19:15:39.586380 27438 slave.cpp:4110] Finished recovery
I0925 19:15:39.586845 27438 slave.cpp:4267] Querying resource estimator for oversubscribable resources
I0925 19:15:39.587059 27430 status_update_manager.cpp:176] Pausing sending status updates
I0925 19:15:39.587064 27438 slave.cpp:705] New master detected at master@172.17.1.195:41781
I0925 19:15:39.587139 27438 slave.cpp:768] Authenticating with master master@172.17.1.195:41781
I0925 19:15:39.587163 27438 slave.cpp:773] Using default CRAM-MD5 authenticatee
I0925 19:15:39.587321 27438 slave.cpp:741] Detecting new master
I0925 19:15:39.587357 27434 authenticatee.cpp:115] Creating new client SASL connection
I0925 19:15:39.587574 27438 slave.cpp:4281] Received oversubscribable resources  from the resource estimator
I0925 19:15:39.587739 27442 master.cpp:5138] Authenticating slave(46)@172.17.1.195:41781
I0925 19:15:39.587853 27441 authenticator.cpp:407] Starting authentication session for crammd5_authenticatee(139)@172.17.1.195:41781
I0925 19:15:39.588052 27439 authenticator.cpp:92] Creating new server SASL connection
I0925 19:15:39.588248 27431 authenticatee.cpp:206] Received SASL authentication mechanisms: CRAM-MD5
I0925 19:15:39.588297 27431 authenticatee.cpp:232] Attempting to authenticate with mechanism 'CRAM-MD5'
I0925 19:15:39.588443 27437 authenticator.cpp:197] Received SASL authentication start
I0925 19:15:39.588506 27437 authenticator.cpp:319] Authentication requires more steps
I0925 19:15:39.588677 27443 authenticatee.cpp:252] Received SASL authentication step
I0925 19:15:39.588814 27436 authenticator.cpp:225] Received SASL authentication step
I0925 19:15:39.588855 27436 auxprop.cpp:102] Request to lookup properties for user: 'test-principal' realm: 'f57fd4291168' server FQDN: 'f57fd4291168' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0925 19:15:39.588876 27436 auxprop.cpp:174] Looking up auxiliary property '*userPassword'
I0925 19:15:39.588937 27436 auxprop.cpp:174] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0925 19:15:39.588979 27436 auxprop.cpp:102] Request to lookup properties for user: 'test-principal' realm: 'f57fd4291168' server FQDN: 'f57fd4291168' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0925 19:15:39.588997 27436 auxprop.cpp:124] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0925 19:15:39.589011 27436 auxprop.cpp:124] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0925 19:15:39.589036 27436 authenticator.cpp:311] Authentication success
I0925 19:15:39.589126 27443 authenticatee.cpp:292] Authentication success
I0925 19:15:39.589192 27437 master.cpp:5168] Successfully authenticated principal 'test-principal' at slave(46)@172.17.1.195:41781
I0925 19:15:39.589238 27433 authenticator.cpp:425] Authentication session cleanup for crammd5_authenticatee(139)@172.17.1.195:41781
I0925 19:15:39.589412 27440 slave.cpp:836] Successfully authenticated with master master@172.17.1.195:41781
I0925 19:15:39.589540 27440 slave.cpp:1230] Will retry registration in 13.562027ms if necessary
I0925 19:15:39.589745 27436 master.cpp:3862] Registering slave at slave(46)@172.17.1.195:41781 (f57fd4291168) with id c8bf1c95-50f4-4832-a570-c560f0b466ae-S0
I0925 19:15:39.590121 27438 registrar.cpp:441] Applied 1 operations in 70627ns; attempting to update the 'registry'
I0925 19:15:39.590831 27430 log.cpp:685] Attempting to append 345 bytes to the log
I0925 19:15:39.590927 27439 coordinator.cpp:341] Coordinator attempting to write APPEND action at position 3
I0925 19:15:39.591809 27430 replica.cpp:511] Replica received write request for position 3
I0925 19:15:39.592072 27430 leveldb.cpp:343] Persisting action (364 bytes) to leveldb took 221734ns
I0925 19:15:39.592099 27430 replica.cpp:679] Persisted action at 3
I0925 19:15:39.592643 27442 replica.cpp:658] Replica received learned notice for position 3
I0925 19:15:39.593215 27442 leveldb.cpp:343] Persisting action (366 bytes) to leveldb took 560946ns
I0925 19:15:39.593237 27442 replica.cpp:679] Persisted action at 3
I0925 19:15:39.593255 27442 replica.cpp:664] Replica learned APPEND action at position 3
I0925 19:15:39.594663 27433 registrar.cpp:486] Successfully updated the 'registry' in 4.472832ms
I0925 19:15:39.594874 27431 log.cpp:704] Attempting to truncate the log to 3
I0925 19:15:39.595407 27429 slave.cpp:3138] Received ping from slave-observer(45)@172.17.1.195:41781
I0925 19:15:39.595450 27433 coordinator.cpp:341] Coordinator attempting to write TRUNCATE action at position 4
I0925 19:15:39.596017 27442 replica.cpp:511] Replica received write request for position 4
I0925 19:15:39.596029 27429 hierarchical.hpp:675] Added slave c8bf1c95-50f4-4832-a570-c560f0b466ae-S0 (f57fd4291168) with cpus(*):1000; mem(*):1000; disk(*):3.70122e+06; ports(*):[31000-32000] (allocated: )
I0925 19:15:39.595952 27441 master.cpp:3930] Registered slave c8bf1c95-50f4-4832-a570-c560f0b466ae-S0 at slave(46)@172.17.1.195:41781 (f57fd4291168) with cpus(*):1000; mem(*):1000; disk(*):3.70122e+06; ports(*):[31000-32000]
I0925 19:15:39.596240 27429 hierarchical.hpp:1326] No resources available to allocate!
I0925 19:15:39.596263 27439 slave.cpp:880] Registered with master master@172.17.1.195:41781; given slave ID c8bf1c95-50f4-4832-a570-c560f0b466ae-S0
I0925 19:15:39.596341 27439 fetcher.cpp:77] Clearing fetcher cache
I0925 19:15:39.596345 27429 hierarchical.hpp:1421] No inverse offers to send out!
I0925 19:15:39.596367 27429 hierarchical.hpp:1239] Performed allocation for slave c8bf1c95-50f4-4832-a570-c560f0b466ae-S0 in 299337ns
I0925 19:15:39.596524 27434 status_update_manager.cpp:183] Resuming sending status updates
I0925 19:15:39.596571 27442 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 575374ns
I0925 19:15:39.596662 27442 replica.cpp:679] Persisted action at 4
I0925 19:15:39.596984 27439 slave.cpp:903] Checkpointing SlaveInfo to '/tmp/FetcherCacheTest_LocalUncachedExtract_LwfzK4/meta/slaves/c8bf1c95-50f4-4832-a570-c560f0b466ae-S0/slave.info'
I0925 19:15:39.597522 27434 replica.cpp:658] Replica received learned notice for position 4
I0925 19:15:39.597553 27410 sched.cpp:164] Version: 0.26.0
I0925 19:15:39.597746 27439 slave.cpp:939] Forwarding total oversubscribed resources 
I0925 19:15:39.598021 27429 master.cpp:4272] Received update of slave c8bf1c95-50f4-4832-a570-c560f0b466ae-S0 at slave(46)@172.17.1.195:41781 (f57fd4291168) with total oversubscribed resources 
I0925 19:15:39.598070 27434 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 531503ns
I0925 19:15:39.598162 27434 leveldb.cpp:401] Deleting ~2 keys from leveldb took 79081ns
I0925 19:15:39.598170 27428 sched.cpp:262] New master detected at master@172.17.1.195:41781
I0925 19:15:39.598206 27434 replica.cpp:679] Persisted action at 4
I0925 19:15:39.598238 27434 replica.cpp:664] Replica learned TRUNCATE action at position 4
I0925 19:15:39.598276 27428 sched.cpp:318] Authenticating with master master@172.17.1.195:41781
I0925 19:15:39.598296 27428 sched.cpp:325] Using default CRAM-MD5 authenticatee
I0925 19:15:39.598950 27430 hierarchical.hpp:735] Slave c8bf1c95-50f4-4832-a570-c560f0b466ae-S0 (f57fd4291168) updated with oversubscribed resources  (total: cpus(*):1000; mem(*):1000; disk(*):3.70122e+06; ports(*):[31000-32000], allocated: )
I0925 19:15:39.599242 27430 hierarchical.hpp:1326] No resources available to allocate!
I0925 19:15:39.599282 27430 hierarchical.hpp:1421] No inverse offers to send out!
I0925 19:15:39.599341 27430 hierarchical.hpp:1239] Performed allocation for slave c8bf1c95-50f4-4832-a570-c560f0b466ae-S0 in 327742ns
I0925 19:15:39.599632 27437 authenticatee.cpp:115] Creating new client SASL connection
I0925 19:15:39.600005 27428 master.cpp:5138] Authenticating scheduler-dda30e8e-47b7-4b1d-9a96-32364754be63@172.17.1.195:41781
I0925 19:15:39.600170 27435 authenticator.cpp:407] Starting authentication session for crammd5_authenticatee(140)@172.17.1.195:41781
I0925 19:15:39.600518 27433 authenticator.cpp:92] Creating new server SASL connection
I0925 19:15:39.600788 27436 authenticatee.cpp:206] Received SASL authentication mechanisms: CRAM-MD5
I0925 19:15:39.600831 27436 authenticatee.cpp:232] Attempting to authenticate with mechanism 'CRAM-MD5'
I0925 19:15:39.600944 27433 authenticator.cpp:197] Received SASL authentication start
I0925 19:15:39.601019 27433 authenticator.cpp:319] Authentication requires more steps
I0925 19:15:39.601150 27436 authenticatee.cpp:252] Received SASL authentication step
I0925 19:15:39.601284 27436 authenticator.cpp:225] Received SASL authentication step
I0925 19:15:39.601326 27436 auxprop.cpp:102] Request to lookup properties for user: 'test-principal' realm: 'f57fd4291168' server FQDN: 'f57fd4291168' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0925 19:15:39.601341 27436 auxprop.cpp:174] Looking up auxiliary property '*userPassword'
I0925 19:15:39.601387 27436 auxprop.cpp:174] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0925 19:15:39.601413 27436 auxprop.cpp:102] Request to lookup properties for user: 'test-principal' realm: 'f57fd4291168' server FQDN: 'f57fd4291168' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0925 19:15:39.601421 27436 auxprop.cpp:124] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0925 19:15:39.601428 27436 auxprop.cpp:124] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0925 19:15:39.601439 27436 authenticator.cpp:311] Authentication success
I0925 19:15:39.601508 27433 authenticatee.cpp:292] Authentication success
I0925 19:15:39.601644 27433 master.cpp:5168] Successfully authenticated principal 'test-principal' at scheduler-dda30e8e-47b7-4b1d-9a96-32364754be63@172.17.1.195:41781
I0925 19:15:39.601671 27436 authenticator.cpp:425] Authentication session cleanup for crammd5_authenticatee(140)@172.17.1.195:41781
I0925 19:15:39.601842 27434 sched.cpp:407] Successfully authenticated with master master@172.17.1.195:41781
I0925 19:15:39.601869 27434 sched.cpp:714] Sending SUBSCRIBE call to master@172.17.1.195:41781
I0925 19:15:39.601955 27434 sched.cpp:747] Will retry registration in 749.975107ms if necessary
I0925 19:15:39.602046 27443 master.cpp:2179] Received SUBSCRIBE call for framework 'default' at scheduler-dda30e8e-47b7-4b1d-9a96-32364754be63@172.17.1.195:41781
W0925 19:15:39.602128 27443 master.cpp:2186] Framework at scheduler-dda30e8e-47b7-4b1d-9a96-32364754be63@172.17.1.195:41781 (authenticated as 'test-principal') does not set 'principal' in FrameworkInfo
I0925 19:15:39.602149 27443 master.cpp:1642] Authorizing framework principal '' to receive offers for role '*'
I0925 19:15:39.602375 27437 master.cpp:2250] Subscribing framework default with checkpointing enabled and capabilities [  ]
I0925 19:15:39.602712 27429 hierarchical.hpp:515] Added framework c8bf1c95-50f4-4832-a570-c560f0b466ae-0000
I0925 19:15:39.602859 27437 sched.cpp:641] Framework registered with c8bf1c95-50f4-4832-a570-c560f0b466ae-0000
I0925 19:15:39.602905 27437 sched.cpp:655] Scheduler::registered took 30086ns
I0925 19:15:39.603204 27429 hierarchical.hpp:1421] No inverse offers to send out!
I0925 19:15:39.603234 27429 hierarchical.hpp:1221] Performed allocation for 1 slaves in 506104ns
I0925 19:15:39.603520 27438 master.cpp:4967] Sending 1 offers to framework c8bf1c95-50f4-4832-a570-c560f0b466ae-0000 (default) at scheduler-dda30e8e-47b7-4b1d-9a96-32364754be63@172.17.1.195:41781
I0925 19:15:39.603962 27431 sched.cpp:811] Scheduler::resourceOffers took 123790ns
I0925 19:15:39.605443 27432 master.cpp:2918] Processing ACCEPT call for offers: [ c8bf1c95-50f4-4832-a570-c560f0b466ae-O0 ] on slave c8bf1c95-50f4-4832-a570-c560f0b466ae-S0 at slave(46)@172.17.1.195:41781 (f57fd4291168) for framework c8bf1c95-50f4-4832-a570-c560f0b466ae-0000 (default) at scheduler-dda30e8e-47b7-4b1d-9a96-32364754be63@172.17.1.195:41781
I0925 19:15:39.605485 27432 master.cpp:2714] Authorizing framework principal '' to launch task 0 as user 'mesos'
I0925 19:15:39.606487 27432 master.hpp:176] Adding task 0 with resources cpus(*):1; mem(*):1 on slave c8bf1c95-50f4-4832-a570-c560f0b466ae-S0 (f57fd4291168)
I0925 19:15:39.606586 27432 master.cpp:3248] Launching task 0 of framework c8bf1c95-50f4-4832-a570-c560f0b466ae-0000 (default) at scheduler-dda30e8e-47b7-4b1d-9a96-32364754be63@172.17.1.195:41781 with resources cpus(*):1; mem(*):1 on slave c8bf1c95-50f4-4832-a570-c560f0b466ae-S0 at slave(46)@172.17.1.195:41781 (f57fd4291168)
I0925 19:15:39.606875 27440 slave.cpp:1270] Got assigned task 0 for framework c8bf1c95-50f4-4832-a570-c560f0b466ae-0000
I0925 19:15:39.607050 27439 hierarchical.hpp:1103] Recovered cpus(*):999; mem(*):999; disk(*):3.70122e+06; ports(*):[31000-32000] (total: cpus(*):1000; mem(*):1000; disk(*):3.70122e+06; ports(*):[31000-32000], allocated: cpus(*):1; mem(*):1) on slave c8bf1c95-50f4-4832-a570-c560f0b466ae-S0 from framework c8bf1c95-50f4-4832-a570-c560f0b466ae-0000
I0925 19:15:39.607087 27440 slave.cpp:4773] Checkpointing FrameworkInfo to '/tmp/FetcherCacheTest_LocalUncachedExtract_LwfzK4/meta/slaves/c8bf1c95-50f4-4832-a570-c560f0b466ae-S0/frameworks/c8bf1c95-50f4-4832-a570-c560f0b466ae-0000/framework.info'
I0925 19:15:39.607103 27439 hierarchical.hpp:1140] Framework c8bf1c95-50f4-4832-a570-c560f0b466ae-0000 filtered slave c8bf1c95-50f4-4832-a570-c560f0b466ae-S0 for 5secs
I0925 19:15:39.607573 27440 slave.cpp:4784] Checkpointing framework pid 'scheduler-dda30e8e-47b7-4b1d-9a96-32364754be63@172.17.1.195:41781' to '/tmp/FetcherCacheTest_LocalUncachedExtract_LwfzK4/meta/slaves/c8bf1c95-50f4-4832-a570-c560f0b466ae-S0/frameworks/c8bf1c95-50f4-4832-a570-c560f0b466ae-0000/framework.pid'
I0925 19:15:39.608544 27440 slave.cpp:1386] Launching task 0 for framework c8bf1c95-50f4-4832-a570-c560f0b466ae-0000
I0925 19:15:39.615109 27440 slave.cpp:5209] Checkpointing ExecutorInfo to '/tmp/FetcherCacheTest_LocalUncachedExtract_LwfzK4/meta/slaves/c8bf1c95-50f4-4832-a570-c560f0b466ae-S0/frameworks/c8bf1c95-50f4-4832-a570-c560f0b466ae-0000/executors/0/executor.info'
I0925 19:15:39.616000 27440 slave.cpp:4852] Launching executor 0 of framework c8bf1c95-50f4-4832-a570-c560f0b466ae-0000 with resources cpus(*):0.1; mem(*):32 in work directory '/tmp/FetcherCacheTest_LocalUncachedExtract_LwfzK4/slaves/c8bf1c95-50f4-4832-a570-c560f0b466ae-S0/frameworks/c8bf1c95-50f4-4832-a570-c560f0b466ae-0000/executors/0/runs/4bc31eb2-709b-4b09-a5a9-21a8387e355a'
I0925 19:15:39.616510 27441 containerizer.cpp:640] Starting container '4bc31eb2-709b-4b09-a5a9-21a8387e355a' for executor '0' of framework 'c8bf1c95-50f4-4832-a570-c560f0b466ae-0000'
I0925 19:15:39.616612 27440 slave.cpp:5232] Checkpointing TaskInfo to '/tmp/FetcherCacheTest_LocalUncachedExtract_LwfzK4/meta/slaves/c8bf1c95-50f4-4832-a570-c560f0b466ae-S0/frameworks/c8bf1c95-50f4-4832-a570-c560f0b466ae-0000/executors/0/runs/4bc31eb2-709b-4b09-a5a9-21a8387e355a/tasks/0/task.info'
I0925 19:15:39.617144 27440 slave.cpp:1604] Queuing task '0' for executor 0 of framework 'c8bf1c95-50f4-4832-a570-c560f0b466ae-0000
I0925 19:15:39.617277 27440 slave.cpp:658] Successfully attached file '/tmp/FetcherCacheTest_LocalUncachedExtract_LwfzK4/slaves/c8bf1c95-50f4-4832-a570-c560f0b466ae-S0/frameworks/c8bf1c95-50f4-4832-a570-c560f0b466ae-0000/executors/0/runs/4bc31eb2-709b-4b09-a5a9-21a8387e355a'
I0925 19:15:39.619359 27437 launcher.cpp:132] Forked child with pid '30069' for container '4bc31eb2-709b-4b09-a5a9-21a8387e355a'
I0925 19:15:39.619583 27437 containerizer.cpp:873] Checkpointing executor's forked pid 30069 to '/tmp/FetcherCacheTest_LocalUncachedExtract_LwfzK4/meta/slaves/c8bf1c95-50f4-4832-a570-c560f0b466ae-S0/frameworks/c8bf1c95-50f4-4832-a570-c560f0b466ae-0000/executors/0/runs/4bc31eb2-709b-4b09-a5a9-21a8387e355a/pids/forked.pid'
I0925 19:15:39.622011 27441 fetcher.cpp:299] Starting to fetch URIs for container: 4bc31eb2-709b-4b09-a5a9-21a8387e355a, directory: /tmp/FetcherCacheTest_LocalUncachedExtract_LwfzK4/slaves/c8bf1c95-50f4-4832-a570-c560f0b466ae-S0/frameworks/c8bf1c95-50f4-4832-a570-c560f0b466ae-0000/executors/0/runs/4bc31eb2-709b-4b09-a5a9-21a8387e355a
I0925 19:15:39.633872 27441 fetcher.cpp:756] Fetching URIs using command '/mesos/mesos-0.26.0/_build/src/mesos-fetcher'
E0925 19:15:39.724884 27430 fetcher.cpp:515] Failed to run mesos-fetcher: Failed to fetch all URIs for container '4bc31eb2-709b-4b09-a5a9-21a8387e355a' with exit status: 256
Failed to synchronize with slave (it's probably exited)
E0925 19:15:39.725486 27443 slave.cpp:3342] Container '4bc31eb2-709b-4b09-a5a9-21a8387e355a' for executor '0' of framework 'c8bf1c95-50f4-4832-a570-c560f0b466ae-0000' failed to start: Failed to fetch all URIs for container '4bc31eb2-709b-4b09-a5a9-21a8387e355a' with exit status: 256
I0925 19:15:39.725620 27430 containerizer.cpp:1097] Destroying container '4bc31eb2-709b-4b09-a5a9-21a8387e355a'
I0925 19:15:39.725651 27430 containerizer.cpp:1126] Waiting for the isolators to complete for container '4bc31eb2-709b-4b09-a5a9-21a8387e355a'
I0925 19:15:39.825744 27443 containerizer.cpp:1284] Executor for container '4bc31eb2-709b-4b09-a5a9-21a8387e355a' has exited
I0925 19:15:39.827075 27429 slave.cpp:3440] Executor '0' of framework c8bf1c95-50f4-4832-a570-c560f0b466ae-0000 exited with status 1
I0925 19:15:39.827324 27429 slave.cpp:2717] Handling status update TASK_FAILED (UUID: 6bb8651c-0668-4724-8fbd-76db8a91adb7) for task 0 of framework c8bf1c95-50f4-4832-a570-c560f0b466ae-0000 from @0.0.0.0:0
I0925 19:15:39.827514 27429 slave.cpp:5147] Terminating task 0
W0925 19:15:39.827745 27436 containerizer.cpp:988] Ignoring update for unknown container: 4bc31eb2-709b-4b09-a5a9-21a8387e355a
I0925 19:15:39.828073 27440 status_update_manager.cpp:322] Received status update TASK_FAILED (UUID: 6bb8651c-0668-4724-8fbd-76db8a91adb7) for task 0 of framework c8bf1c95-50f4-4832-a570-c560f0b466ae-0000
I0925 19:15:39.828168 27440 status_update_manager.cpp:499] Creating StatusUpdate stream for task 0 of framework c8bf1c95-50f4-4832-a570-c560f0b466ae-0000
I0925 19:15:39.828661 27440 status_update_manager.cpp:826] Checkpointing UPDATE for status update TASK_FAILED (UUID: 6bb8651c-0668-4724-8fbd-76db8a91adb7) for task 0 of framework c8bf1c95-50f4-4832-a570-c560f0b466ae-0000
I0925 19:15:39.830041 27440 status_update_manager.cpp:376] Forwarding update TASK_FAILED (UUID: 6bb8651c-0668-4724-8fbd-76db8a91adb7) for task 0 of framework c8bf1c95-50f4-4832-a570-c560f0b466ae-0000 to the slave
I0925 19:15:39.830292 27434 slave.cpp:3016] Forwarding the update TASK_FAILED (UUID: 6bb8651c-0668-4724-8fbd-76db8a91adb7) for task 0 of framework c8bf1c95-50f4-4832-a570-c560f0b466ae-0000 to master@172.17.1.195:41781
I0925 19:15:39.830492 27434 slave.cpp:2940] Status update manager successfully handled status update TASK_FAILED (UUID: 6bb8651c-0668-4724-8fbd-76db8a91adb7) for task 0 of framework c8bf1c95-50f4-4832-a570-c560f0b466ae-0000
I0925 19:15:39.830641 27432 master.cpp:4415] Status update TASK_FAILED (UUID: 6bb8651c-0668-4724-8fbd-76db8a91adb7) for task 0 of framework c8bf1c95-50f4-4832-a570-c560f0b466ae-0000 from slave c8bf1c95-50f4-4832-a570-c560f0b466ae-S0 at slave(46)@172.17.1.195:41781 (f57fd4291168)
I0925 19:15:39.830682 27432 master.cpp:4454] Forwarding status update TASK_FAILED (UUID: 6bb8651c-0668-4724-8fbd-76db8a91adb7) for task 0 of framework c8bf1c95-50f4-4832-a570-c560f0b466ae-0000
I0925 19:15:39.830842 27432 master.cpp:6081] Updating the latest state of task 0 of framework c8bf1c95-50f4-4832-a570-c560f0b466ae-0000 to TASK_FAILED
I0925 19:15:39.831075 27431 sched.cpp:918] Scheduler::statusUpdate took 176815ns
I0925 19:15:39.831204 27439 hierarchical.hpp:1103] Recovered cpus(*):1; mem(*):1 (total: cpus(*):1000; mem(*):1000; disk(*):3.70122e+06; ports(*):[31000-32000], allocated: ) on slave c8bf1c95-50f4-4832-a570-c560f0b466ae-S0 from framework c8bf1c95-50f4-4832-a570-c560f0b466ae-0000
I0925 19:15:39.831357 27432 master.cpp:6149] Removing task 0 with resources cpus(*):1; mem(*):1 of framework c8bf1c95-50f4-4832-a570-c560f0b466ae-0000 on slave c8bf1c95-50f4-4832-a570-c560f0b466ae-S0 at slave(46)@172.17.1.195:41781 (f57fd4291168)
I0925 19:15:39.831491 27432 master.cpp:3606] Processing ACKNOWLEDGE call 6bb8651c-0668-4724-8fbd-76db8a91adb7 for task 0 of framework c8bf1c95-50f4-4832-a570-c560f0b466ae-0000 (default) at scheduler-dda30e8e-47b7-4b1d-9a96-32364754be63@172.17.1.195:41781 on slave c8bf1c95-50f4-4832-a570-c560f0b466ae-S0
I0925 19:15:39.831763 27437 status_update_manager.cpp:394] Received status update acknowledgement (UUID: 6bb8651c-0668-4724-8fbd-76db8a91adb7) for task 0 of framework c8bf1c95-50f4-4832-a570-c560f0b466ae-0000
I0925 19:15:39.831957 27437 status_update_manager.cpp:826] Checkpointing ACK for status update TASK_FAILED (UUID: 6bb8651c-0668-4724-8fbd-76db8a91adb7) for task 0 of framework c8bf1c95-50f4-4832-a570-c560f0b466ae-0000
I0925 19:15:39.833057 27437 status_update_manager.cpp:530] Cleaning up status update stream for task 0 of framework c8bf1c95-50f4-4832-a570-c560f0b466ae-0000
I0925 19:15:39.833407 27432 slave.cpp:2319] Status update manager successfully handled status update acknowledgement (UUID: 6bb8651c-0668-4724-8fbd-76db8a91adb7) for task 0 of framework c8bf1c95-50f4-4832-a570-c560f0b466ae-0000
I0925 19:15:39.833447 27432 slave.cpp:5188] Completing task 0
I0925 19:15:39.833470 27432 slave.cpp:3544] Cleaning up executor '0' of framework c8bf1c95-50f4-4832-a570-c560f0b466ae-0000
I0925 19:15:39.833768 27437 gc.cpp:56] Scheduling '/tmp/FetcherCacheTest_LocalUncachedExtract_LwfzK4/slaves/c8bf1c95-50f4-4832-a570-c560f0b466ae-S0/frameworks/c8bf1c95-50f4-4832-a570-c560f0b466ae-0000/executors/0/runs/4bc31eb2-709b-4b09-a5a9-21a8387e355a' for gc 6.99999035100741days in the future
I0925 19:15:39.833933 27437 gc.cpp:56] Scheduling '/tmp/FetcherCacheTest_LocalUncachedExtract_LwfzK4/slaves/c8bf1c95-50f4-4832-a570-c560f0b466ae-S0/frameworks/c8bf1c95-50f4-4832-a570-c560f0b466ae-0000/executors/0' for gc 6.99999034949333days in the future
I0925 19:15:39.834005 27432 slave.cpp:3633] Cleaning up framework c8bf1c95-50f4-4832-a570-c560f0b466ae-0000
I0925 19:15:39.834031 27437 gc.cpp:56] Scheduling '/tmp/FetcherCacheTest_LocalUncachedExtract_LwfzK4/meta/slaves/c8bf1c95-50f4-4832-a570-c560f0b466ae-S0/frameworks/c8bf1c95-50f4-4832-a570-c560f0b466ae-0000/executors/0/runs/4bc31eb2-709b-4b09-a5a9-21a8387e355a' for gc 6.99999034847111days in the future
I0925 19:15:39.834106 27430 status_update_manager.cpp:284] Closing status update streams for framework c8bf1c95-50f4-4832-a570-c560f0b466ae-0000
I0925 19:15:39.834121 27437 gc.cpp:56] Scheduling '/tmp/FetcherCacheTest_LocalUncachedExtract_LwfzK4/meta/slaves/c8bf1c95-50f4-4832-a570-c560f0b466ae-S0/frameworks/c8bf1c95-50f4-4832-a570-c560f0b466ae-0000/executors/0' for gc 6.99999034757926days in the future
I0925 19:15:39.834266 27437 gc.cpp:56] Scheduling '/tmp/FetcherCacheTest_LocalUncachedExtract_LwfzK4/slaves/c8bf1c95-50f4-4832-a570-c560f0b466ae-S0/frameworks/c8bf1c95-50f4-4832-a570-c560f0b466ae-0000' for gc 6.99999034594963days in the future
I0925 19:15:39.834360 27437 gc.cpp:56] Scheduling '/tmp/FetcherCacheTest_LocalUncachedExtract_LwfzK4/meta/slaves/c8bf1c95-50f4-4832-a570-c560f0b466ae-S0/frameworks/c8bf1c95-50f4-4832-a570-c560f0b466ae-0000' for gc 6.99999034517333days in the future
I0925 19:15:40.549545 27428 hierarchical.hpp:1421] No inverse offers to send out!
I0925 19:15:40.549640 27428 hierarchical.hpp:1221] Performed allocation for 1 slaves in 849712ns
I0925 19:15:40.550092 27442 master.cpp:4967] Sending 1 offers to framework c8bf1c95-50f4-4832-a570-c560f0b466ae-0000 (default) at scheduler-dda30e8e-47b7-4b1d-9a96-32364754be63@172.17.1.195:41781
I0925 19:15:40.550679 27442 sched.cpp:811] Scheduler::resourceOffers took 157498ns
I0925 19:15:40.551633 27432 master.cpp:2918] Processing ACCEPT call for offers: [ c8bf1c95-50f4-4832-a570-c560f0b466ae-O1 ] on slave c8bf1c95-50f4-4832-a570-c560f0b466ae-S0 at slave(46)@172.17.1.195:41781 (f57fd4291168) for framework c8bf1c95-50f4-4832-a570-c560f0b466ae-0000 (default) at scheduler-dda30e8e-47b7-4b1d-9a96-32364754be63@172.17.1.195:41781
I0925 19:15:40.552602 27432 hierarchical.hpp:1103] Recovered cpus(*):1000; mem(*):1000; disk(*):3.70122e+06; ports(*):[31000-32000] (total: cpus(*):1000; mem(*):1000; disk(*):3.70122e+06; ports(*):[31000-32000], allocated: ) on slave c8bf1c95-50f4-4832-a570-c560f0b466ae-S0 from framework c8bf1c95-50f4-4832-a570-c560f0b466ae-0000
I0925 19:15:40.552672 27432 hierarchical.hpp:1140] Framework c8bf1c95-50f4-4832-a570-c560f0b466ae-0000 filtered slave c8bf1c95-50f4-4832-a570-c560f0b466ae-S0 for 5secs
I0925 19:15:41.551115 27428 hierarchical.hpp:1521] Filtered offer with cpus(*):1000; mem(*):1000; disk(*):3.70122e+06; ports(*):[31000-32000] on slave c8bf1c95-50f4-4832-a570-c560f0b466ae-S0 for framework c8bf1c95-50f4-4832-a570-c560f0b466ae-0000
I0925 19:15:41.551200 27428 hierarchical.hpp:1326] No resources available to allocate!
I0925 19:15:41.551224 27428 hierarchical.hpp:1421] No inverse offers to send out!
I0925 19:15:41.551239 27428 hierarchical.hpp:1221] Performed allocation for 1 slaves in 595589ns
I0925 19:15:42.552183 27433 hierarchical.hpp:1521] Filtered offer with cpus(*):1000; mem(*):1000; disk(*):3.70122e+06; ports(*):[31000-32000] on slave c8bf1c95-50f4-4832-a570-c560f0b466ae-S0 for framework c8bf1c95-50f4-4832-a570-c560f0b466ae-0000
I0925 19:15:42.552254 27433 hierarchical.hpp:1326] No resources available to allocate!
I0925 19:15:42.552271 27433 hierarchical.hpp:1421] No inverse offers to send out!
I0925 19:15:42.552281 27433 hierarchical.hpp:1221] Performed allocation for 1 slaves in 496429ns
I0925 19:15:43.553062 27442 hierarchical.hpp:1521] Filtered offer with cpus(*):1000; mem(*):1000; disk(*):3.70122e+06; ports(*):[31000-32000] on slave c8bf1c95-50f4-4832-a570-c560f0b466ae-S0 for framework c8bf1c95-50f4-4832-a570-c560f0b466ae-0000
I0925 19:15:43.553134 27442 hierarchical.hpp:1326] No resources available to allocate!
I0925 19:15:43.553151 27442 hierarchical.hpp:1421] No inverse offers to send out!
I0925 19:15:43.553163 27442 hierarchical.hpp:1221] Performed allocation for 1 slaves in 482544ns
I0925 19:15:44.554844 27443 hierarchical.hpp:1521] Filtered offer with cpus(*):1000; mem(*):1000; disk(*):3.70122e+06; ports(*):[31000-32000] on slave c8bf1c95-50f4-4832-a570-c560f0b466ae-S0 for framework c8bf1c95-50f4-4832-a570-c560f0b466ae-0000
I0925 19:15:44.554930 27443 hierarchical.hpp:1326] No resources available to allocate!
I0925 19:15:44.554954 27443 hierarchical.hpp:1421] No inverse offers to send out!
I0925 19:15:44.554970 27443 hierarchical.hpp:1221] Performed allocation for 1 slaves in 699469ns
I0925 19:15:45.556754 27442 hierarchical.hpp:1421] No inverse offers to send out!
I0925 19:15:45.556805 27442 hierarchical.hpp:1221] Performed allocation for 1 slaves in 702577ns
I0925 19:15:45.557119 27437 master.cpp:4967] Sending 1 offers to framework c8bf1c95-50f4-4832-a570-c560f0b466ae-0000 (default) at scheduler-dda30e8e-47b7-4b1d-9a96-32364754be63@172.17.1.195:41781
I0925 19:15:45.557569 27435 sched.cpp:811] Scheduler::resourceOffers took 122887ns
I0925 19:15:45.558279 27433 master.cpp:2918] Processing ACCEPT call for offers: [ c8bf1c95-50f4-4832-a570-c560f0b466ae-O2 ] on slave c8bf1c95-50f4-4832-a570-c560f0b466ae-S0 at slave(46)@172.17.1.195:41781 (f57fd4291168) for framework c8bf1c95-50f4-4832-a570-c560f0b466ae-0000 (default) at scheduler-dda30e8e-47b7-4b1d-9a96-32364754be63@172.17.1.195:41781
I0925 19:15:45.559015 27441 hierarchical.hpp:1103] Recovered cpus(*):1000; mem(*):1000; disk(*):3.70122e+06; ports(*):[31000-32000] (total: cpus(*):1000; mem(*):1000; disk(*):3.70122e+06; ports(*):[31000-32000], allocated: ) on slave c8bf1c95-50f4-4832-a570-c560f0b466ae-S0 from framework c8bf1c95-50f4-4832-a570-c560f0b466ae-0000
I0925 19:15:45.559070 27441 hierarchical.hpp:1140] Framework c8bf1c95-50f4-4832-a570-c560f0b466ae-0000 filtered slave c8bf1c95-50f4-4832-a570-c560f0b466ae-S0 for 5secs
I0925 19:15:46.558176 27439 hierarchical.hpp:1521] Filtered offer with cpus(*):1000; mem(*):1000; disk(*):3.70122e+06; ports(*):[31000-32000] on slave c8bf1c95-50f4-4832-a570-c560f0b466ae-S0 for framework c8bf1c95-50f4-4832-a570-c560f0b466ae-0000
I0925 19:15:46.558245 27439 hierarchical.hpp:1326] No resources available to allocate!
I0925 19:15:46.558262 27439 hierarchical.hpp:1421] No inverse offers to send out!
I0925 19:15:46.558274 27439 hierarchical.hpp:1221] Performed allocation for 1 slaves in 509658ns
I0925 19:15:47.559289 27429 hierarchical.hpp:1521] Filtered offer with cpus(*):1000; mem(*):1000; disk(*):3.70122e+06; ports(*):[31000-32000] on slave c8bf1c95-50f4-4832-a570-c560f0b466ae-S0 for framework c8bf1c95-50f4-4832-a570-c560f0b466ae-0000
I0925 19:15:47.559360 27429 hierarchical.hpp:1326] No resources available to allocate!
I0925 19:15:47.559376 27429 hierarchical.hpp:1421] No inverse offers to send out!
I0925 19:15:47.559386 27429 hierarchical.hpp:1221] Performed allocation for 1 slaves in 495131ns
I0925 19:15:48.560979 27442 hierarchical.hpp:1521] Filtered offer with cpus(*):1000; mem(*):1000; disk(*):3.70122e+06; ports(*):[31000-32000] on slave c8bf1c95-50f4-4832-a570-c560f0b466ae-S0 for framework c8bf1c95-50f4-4832-a570-c560f0b466ae-0000
I0925 19:15:48.561064 27442 hierarchical.hpp:1326] No resources available to allocate!
I0925 19:15:48.561087 27442 hierarchical.hpp:1421] No inverse offers to send out!
I0925 19:15:48.561101 27442 hierarchical.hpp:1221] Performed allocation for 1 slaves in 710782ns
I0925 19:15:49.562594 27431 hierarchical.hpp:1521] Filtered offer with cpus(*):1000; mem(*):1000; disk(*):3.70122e+06; ports(*):[31000-32000] on slave c8bf1c95-50f4-4832-a570-c560f0b466ae-S0 for framework c8bf1c95-50f4-4832-a570-c560f0b466ae-0000
I0925 19:15:49.562666 27431 hierarchical.hpp:1326] No resources available to allocate!
I0925 19:15:49.562683 27431 hierarchical.hpp:1421] No inverse offers to send out!
I0925 19:15:49.562695 27431 hierarchical.hpp:1221] Performed allocation for 1 slaves in 525867ns
I0925 19:15:50.564564 27428 hierarchical.hpp:1421] No inverse offers to send out!
I0925 19:15:50.564620 27428 hierarchical.hpp:1221] Performed allocation for 1 slaves in 621850ns
I0925 19:15:50.565004 27432 master.cpp:4967] Sending 1 offers to framework c8bf1c95-50f4-4832-a570-c560f0b466ae-0000 (default) at scheduler-dda30e8e-47b7-4b1d-9a96-32364754be63@172.17.1.195:41781
I0925 19:15:50.565457 27428 sched.cpp:811] Scheduler::resourceOffers took 110220ns
I0925 19:15:50.566159 27437 master.cpp:2918] Processing ACCEPT call for offers: [ c8bf1c95-50f4-4832-a570-c560f0b466ae-O3 ] on slave c8bf1c95-50f4-4832-a570-c560f0b466ae-S0 at slave(46)@172.17.1.195:41781 (f57fd4291168) for framework c8bf1c95-50f4-4832-a570-c560f0b466ae-0000 (default) at scheduler-dda30e8e-47b7-4b1d-9a96-32364754be63@172.17.1.195:41781
I0925 19:15:50.566815 27428 hierarchical.hpp:1103] Recovered cpus(*):1000; mem(*):1000; disk(*):3.70122e+06; ports(*):[31000-32000] (total: cpus(*):1000; mem(*):1000; disk(*):3.70122e+06; ports(*):[31000-32000], allocated: ) on slave c8bf1c95-50f4-4832-a570-c560f0b466ae-S0 from framework c8bf1c95-50f4-4832-a570-c560f0b466ae-0000
I0925 19:15:50.566869 27428 hierarchical.hpp:1140] Framework c8bf1c95-50f4-4832-a570-c560f0b466ae-0000 filtered slave c8bf1c95-50f4-4832-a570-c560f0b466ae-S0 for 5secs
I0925 19:15:51.565913 27433 hierarchical.hpp:1521] Filtered offer with cpus(*):1000; mem(*):1000; disk(*):3.70122e+06; ports(*):[31000-32000] on slave c8bf1c95-50f4-4832-a570-c560f0b466ae-S0 for framework c8bf1c95-50f4-4832-a570-c560f0b466ae-0000
I0925 19:15:51.565981 27433 hierarchical.hpp:1326] No resources available to allocate!
I0925 19:15:51.565999 27433 hierarchical.hpp:1421] No inverse offers to send out!
I0925 19:15:51.566009 27433 hierarchical.hpp:1221] Performed allocation for 1 slaves in 504883ns
I0925 19:15:52.567260 27432 hierarchical.hpp:1521] Filtered offer with cpus(*):1000; mem(*):1000; disk(*):3.70122e+06; ports(*):[31000-32000] on slave c8bf1c95-50f4-4832-a570-c560f0b466ae-S0 for framework c8bf1c95-50f4-4832-a570-c560f0b466ae-0000
I0925 19:15:52.567333 27432 hierarchical.hpp:1326] No resources available to allocate!
I0925 19:15:52.567350 27432 hierarchical.hpp:1421] No inverse offers to send out!
I0925 19:15:52.567361 27432 hierarchical.hpp:1221] Performed allocation for 1 slaves in 513500ns
I0925 19:15:53.568176 27438 hierarchical.hpp:1521] Filtered offer with cpus(*):1000; mem(*):1000; disk(*):3.70122e+06; ports(*):[31000-32000] on slave c8bf1c95-50f4-4832-a570-c560f0b466ae-S0 for framework c8bf1c95-50f4-4832-a570-c560f0b466ae-0000
I0925 19:15:53.568248 27438 hierarchical.hpp:1326] No resources available to allocate!
I0925 19:15:53.568266 27438 hierarchical.hpp:1421] No inverse offers to send out!
I0925 19:15:53.568281 27438 hierarchical.hpp:1221] Performed allocation for 1 slaves in 522293ns
I0925 19:15:54.570142 27430 hierarchical.hpp:1521] Filtered offer with cpus(*):1000; mem(*):1000; disk(*):3.70122e+06; ports(*):[31000-32000] on slave c8bf1c95-50f4-4832-a570-c560f0b466ae-S0 for framework c8bf1c95-50f4-4832-a570-c560f0b466ae-0000
I0925 19:15:54.570226 27430 hierarchical.hpp:1326] No resources available to allocate!
I0925 19:15:54.570250 27430 hierarchical.hpp:1421] No inverse offers to send out!
I0925 19:15:54.570264 27430 hierarchical.hpp:1221] Performed allocation for 1 slaves in 626798ns
I0925 19:15:54.588251 27442 slave.cpp:4267] Querying resource estimator for oversubscribable resources
I0925 19:15:54.588673 27443 slave.cpp:4281] Received oversubscribable resources  from the resource estimator
I0925 19:15:54.596678 27428 slave.cpp:3138] Received ping from slave-observer(45)@172.17.1.195:41781
../../src/tests/fetcher_cache_tests.cpp:681: Failure
Failed to wait 15secs for awaitFinished(task.get())
I0925 19:15:54.606274 27410 sched.cpp:1771] Asked to stop the driver
I0925 19:15:54.606623 27439 master.cpp:1119] Framework c8bf1c95-50f4-4832-a570-c560f0b466ae-0000 (default) at scheduler-dda30e8e-47b7-4b1d-9a96-32364754be63@172.17.1.195:41781 disconnected
I0925 19:15:54.606679 27439 master.cpp:2475] Disconnecting framework c8bf1c95-50f4-4832-a570-c560f0b466ae-0000 (default) at scheduler-dda30e8e-47b7-4b1d-9a96-32364754be63@172.17.1.195:41781
I0925 19:15:54.606855 27439 master.cpp:2499] Deactivating framework c8bf1c95-50f4-4832-a570-c560f0b466ae-0000 (default) at scheduler-dda30e8e-47b7-4b1d-9a96-32364754be63@172.17.1.195:41781
I0925 19:15:54.607441 27439 master.cpp:1143] Giving framework c8bf1c95-50f4-4832-a570-c560f0b466ae-0000 (default) at scheduler-dda30e8e-47b7-4b1d-9a96-32364754be63@172.17.1.195:41781 0ns to failover
I0925 19:15:54.607770 27433 hierarchical.hpp:599] Deactivated framework c8bf1c95-50f4-4832-a570-c560f0b466ae-0000
I0925 19:15:54.609256 27432 master.cpp:4815] Framework failover timeout, removing framework c8bf1c95-50f4-4832-a570-c560f0b466ae-0000 (default) at scheduler-dda30e8e-47b7-4b1d-9a96-32364754be63@172.17.1.195:41781
I0925 19:15:54.609297 27432 master.cpp:5571] Removing framework c8bf1c95-50f4-4832-a570-c560f0b466ae-0000 (default) at scheduler-dda30e8e-47b7-4b1d-9a96-32364754be63@172.17.1.195:41781
I0925 19:15:54.609501 27433 slave.cpp:1980] Asked to shut down framework c8bf1c95-50f4-4832-a570-c560f0b466ae-0000 by master@172.17.1.195:41781
W0925 19:15:54.609549 27433 slave.cpp:1995] Cannot shut down unknown framework c8bf1c95-50f4-4832-a570-c560f0b466ae-0000
I0925 19:15:54.609881 27432 master.cpp:919] Master terminating
I0925 19:15:54.610255 27440 hierarchical.hpp:552] Removed framework c8bf1c95-50f4-4832-a570-c560f0b466ae-0000
I0925 19:15:54.610627 27440 hierarchical.hpp:706] Removed slave c8bf1c95-50f4-4832-a570-c560f0b466ae-S0
I0925 19:15:54.611197 27436 slave.cpp:3184] master@172.17.1.195:41781 exited
W0925 19:15:54.611233 27436 slave.cpp:3187] Master disconnected! Waiting for a new master to be elected
I0925 19:15:54.616207 27410 slave.cpp:585] Slave terminating
[  FAILED  ] FetcherCacheTest.LocalUncachedExtract (15091 ms)
{code}
",Bug,Major,anandmazumdar,2015-12-03T14:40:58.000+0000,5,Resolved,Complete,FetcherCacheTest.LocalUncachedExtract is flaky,2015-12-03T14:40:58.000+0000,MESOS-3579,2.0,mesos,Mesosphere Sprint 23
anandmazumdar,2015-10-02T19:07:55.000+0000,jvanremoortere,"The java/python protos for the V1 api should be generated according to the Makefile; however, they do not show up in the generated build directory.",Bug,Blocker,jvanremoortere,2015-10-03T20:03:29.000+0000,5,Resolved,Complete,V1 API java/python protos are not generated,2015-10-03T20:03:29.000+0000,MESOS-3575,2.0,mesos,Mesosphere Sprint 20
anandmazumdar,2015-10-02T12:26:18.000+0000,bobrik,"After upgrade to 0.24.0 we noticed hanging containers appearing. Looks like there were changes between 0.23.0 and 0.24.0 that broke cleanup.

Here's how to trigger this bug:

1. Deploy app in docker container.
2. Kill corresponding mesos-docker-executor process
3. Observe hanging container

Here are the logs after kill:

{noformat}
slave_1    | I1002 12:12:59.362002  7791 docker.cpp:1576] Executor for container 'f083aaa2-d5c3-43c1-b6ba-342de8829fa8' has exited
slave_1    | I1002 12:12:59.362284  7791 docker.cpp:1374] Destroying container 'f083aaa2-d5c3-43c1-b6ba-342de8829fa8'
slave_1    | I1002 12:12:59.363404  7791 docker.cpp:1478] Running docker stop on container 'f083aaa2-d5c3-43c1-b6ba-342de8829fa8'
slave_1    | I1002 12:12:59.363876  7791 slave.cpp:3399] Executor 'sleepy.87eb6191-68fe-11e5-9444-8eb895523b9c' of framework 20150923-122130-2153451692-5050-1-0000 terminated with signal Terminated
slave_1    | I1002 12:12:59.367570  7791 slave.cpp:2696] Handling status update TASK_FAILED (UUID: 4a1b2387-a469-4f01-bfcb-0d1cccbde550) for task sleepy.87eb6191-68fe-11e5-9444-8eb895523b9c of framework 20150923-122130-2153451692-5050-1-0000 from @0.0.0.0:0
slave_1    | I1002 12:12:59.367842  7791 slave.cpp:5094] Terminating task sleepy.87eb6191-68fe-11e5-9444-8eb895523b9c
slave_1    | W1002 12:12:59.368484  7791 docker.cpp:986] Ignoring updating unknown container: f083aaa2-d5c3-43c1-b6ba-342de8829fa8
slave_1    | I1002 12:12:59.368671  7791 status_update_manager.cpp:322] Received status update TASK_FAILED (UUID: 4a1b2387-a469-4f01-bfcb-0d1cccbde550) for task sleepy.87eb6191-68fe-11e5-9444-8eb895523b9c of framework 20150923-122130-2153451692-5050-1-0000
slave_1    | I1002 12:12:59.368741  7791 status_update_manager.cpp:826] Checkpointing UPDATE for status update TASK_FAILED (UUID: 4a1b2387-a469-4f01-bfcb-0d1cccbde550) for task sleepy.87eb6191-68fe-11e5-9444-8eb895523b9c of framework 20150923-122130-2153451692-5050-1-0000
slave_1    | I1002 12:12:59.370636  7791 status_update_manager.cpp:376] Forwarding update TASK_FAILED (UUID: 4a1b2387-a469-4f01-bfcb-0d1cccbde550) for task sleepy.87eb6191-68fe-11e5-9444-8eb895523b9c of framework 20150923-122130-2153451692-5050-1-0000 to the slave
slave_1    | I1002 12:12:59.371335  7791 slave.cpp:2975] Forwarding the update TASK_FAILED (UUID: 4a1b2387-a469-4f01-bfcb-0d1cccbde550) for task sleepy.87eb6191-68fe-11e5-9444-8eb895523b9c of framework 20150923-122130-2153451692-5050-1-0000 to master@172.16.91.128:5050
slave_1    | I1002 12:12:59.371908  7791 slave.cpp:2899] Status update manager successfully handled status update TASK_FAILED (UUID: 4a1b2387-a469-4f01-bfcb-0d1cccbde550) for task sleepy.87eb6191-68fe-11e5-9444-8eb895523b9c of framework 20150923-122130-2153451692-5050-1-0000
master_1   | I1002 12:12:59.372047    11 master.cpp:4069] Status update TASK_FAILED (UUID: 4a1b2387-a469-4f01-bfcb-0d1cccbde550) for task sleepy.87eb6191-68fe-11e5-9444-8eb895523b9c of framework 20150923-122130-2153451692-5050-1-0000 from slave 20151002-120829-2153451692-5050-1-S0 at slave(1)@172.16.91.128:5051 (172.16.91.128)
master_1   | I1002 12:12:59.372534    11 master.cpp:4108] Forwarding status update TASK_FAILED (UUID: 4a1b2387-a469-4f01-bfcb-0d1cccbde550) for task sleepy.87eb6191-68fe-11e5-9444-8eb895523b9c of framework 20150923-122130-2153451692-5050-1-0000
master_1   | I1002 12:12:59.373018    11 master.cpp:5576] Updating the latest state of task sleepy.87eb6191-68fe-11e5-9444-8eb895523b9c of framework 20150923-122130-2153451692-5050-1-0000 to TASK_FAILED
master_1   | I1002 12:12:59.373447    11 hierarchical.hpp:814] Recovered cpus(*):0.1; mem(*):16; ports(*):[31685-31685] (total: cpus(*):4; mem(*):1001; disk(*):52869; ports(*):[31000-32000], allocated: cpus(*):8.32667e-17) on slave 20151002-120829-2153451692-5050-1-S0 from framework 20150923-122130-2153451692-5050-1-0000
{noformat}

Another issue: if you restart mesos-slave on the host with orphaned docker containers, they are not getting killed. This was the case before and I hoped for this trick to kill hanging containers, but it doesn't work now.

Marking this as critical because it hoards cluster resources and blocks scheduling.",Bug,Major,bobrik,2016-04-07T22:52:33.000+0000,5,Resolved,Complete,Mesos does not kill orphaned docker containers,2016-04-07T22:52:33.000+0000,MESOS-3573,5.0,mesos,Mesosphere Sprint 32
jojy,2015-10-01T17:52:54.000+0000,jojy,"Refactor registry client component to:

- Make methods shorter for readability
- Pull out structs not related to registry client into common namespace.",Task,Major,jojy,2015-11-09T21:05:37.000+0000,5,Resolved,Complete,Refactor registry_client,2015-11-09T21:05:37.000+0000,MESOS-3571,5.0,mesos,Mesosphere Sprint 20
anandmazumdar,2015-10-01T16:54:25.000+0000,anandmazumdar,"Currently, the scheduler library sends calls in order by chaining them and sending them only when it has received a response for the earlier call. This was done because there was no HTTP Pipelining abstraction in Libprocess {{process::post}}.

However once {{MESOS-3332}} is resolved, we should be now able to use the new abstraction.",Bug,Major,anandmazumdar,2016-02-28T18:01:11.000+0000,5,Resolved,Complete,Make Scheduler Library use HTTP Pipelining Abstraction in Libprocess,2016-03-03T21:25:39.000+0000,MESOS-3570,8.0,mesos,Mesosphere Sprint 27
vinodkone,2015-09-30T22:09:18.000+0000,maximk,"The slave's state.json reports revocable task resources as zero:

{noformat}
resources: {
cpus: 0,
disk: 3071,
mem: 1248,
ports: ""[31715-31715]""
},
{noformat}

Also, there is no indication that a task uses revocable CPU. It would be great to have this type of info.",Bug,Major,maximk,2015-10-16T19:34:59.000+0000,5,Resolved,Complete,Revocable task CPU shows as zero in /state.json,2015-10-16T19:34:59.000+0000,MESOS-3563,2.0,mesos,Twitter Mesos Q3 Sprint 6
ijimenez,2015-09-30T20:31:16.000+0000,mcypark,"Specifying the following credentials file:
{code}
{
  “credentials”: [
    {
      “principal”: “user”,
      “secret”: “password”
    }
  ]
}
{code}

Then hitting a master endpoint with:
{code}
curl -i -u “user:password” ...
{code}

Does not work. This is contrary to the text-based credentials file which works:
{code}
user password
{code}

Currently, the password in a JSON-based credentials file needs to be base64-encoded in order for it to work:
{code}
{
  “credentials”: [
    {
      “principal”: “user”,
      “secret”: “cGFzc3dvcmQ=”
    }
  ]
}
{code}",Bug,Major,mcypark,2015-10-13T09:15:17.000+0000,5,Resolved,Complete,JSON-based credential files do not work correctly,2016-03-28T19:54:05.000+0000,MESOS-3560,1.0,mesos,Mesosphere Sprint 20
anandmazumdar,2015-09-30T20:18:53.000+0000,anandmazumdar,We should make the Command Scheduler in {{src/cli/executor.cpp}} use the Scheduler Library {{src/scheduler/scheduler.cpp}} instead of the Scheduler Driver.,Task,Major,anandmazumdar,2016-03-29T23:21:09.000+0000,5,Resolved,Complete,Make the Command Scheduler use the HTTP Scheduler Library,2016-03-29T23:21:09.000+0000,MESOS-3559,3.0,mesos,Mesosphere Sprint 32
qianzhang,2015-09-30T20:14:18.000+0000,anandmazumdar,"Instead of using the {{MesosExecutorDriver}} , we should make the {{CommandExecutor}} in {{src/launcher/executor.cpp}} use the new Executor HTTP Library that we create in {{MESOS-3550}}. 

This would act as a good validation of the {{HTTP API}} implementation.",Task,Major,anandmazumdar,,10006,Reviewable,New,Implement  HTTPCommandExecutor that uses the Executor Library ,2016-04-27T16:13:16.000+0000,MESOS-3558,13.0,mesos,Mesosphere Sprint 33
marco-mesos,2015-09-30T08:03:00.000+0000,radekg,"The issue was initially reported on the mailing list: http://www.mail-archive.com/user@mesos.apache.org/msg04670.html

The format of the master data stored in zookeeper has changed but the mesos.cli does not reflect these changes causing tools like {{mesos-tail}} and {{mesos-ps}} to fail.

Example error from {{mesos-tail}}:

{noformat}
mesos-master ~$ mesos tail -f -n 50 service
Traceback (most recent call last):
  File ""/usr/local/bin/mesos-tail"", line 11, in <module>
    sys.exit(main())
  File ""/usr/local/lib/python2.7/dist-packages/mesos/cli/cli.py"", line 61, in 
wrapper
    return fn(*args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/mesos/cli/cmds/tail.py"", line 
55, in main
    args.task, args.file, fail=(not args.follow)):
  File ""/usr/local/lib/python2.7/dist-packages/mesos/cli/cluster.py"", line 27, 
in files
    tlist = MASTER.tasks(fltr)
  File ""/usr/local/lib/python2.7/dist-packages/mesos/cli/master.py"", line 174, 
in tasks
    self._task_list(active_only))))
  File ""/usr/local/lib/python2.7/dist-packages/mesos/cli/master.py"", line 153, 
in _task_list
    *[util.merge(x, *keys) for x in self.frameworks(active_only)])
  File ""/usr/local/lib/python2.7/dist-packages/mesos/cli/master.py"", line 185, 
in frameworks
    return util.merge(self.state, *keys)
  File ""/usr/local/lib/python2.7/dist-packages/mesos/cli/util.py"", line 58, in 
__get__
    value = self.fget(inst)
  File ""/usr/local/lib/python2.7/dist-packages/mesos/cli/master.py"", line 123, 
in state
    return self.fetch(""/master/state.json"").json()
  File ""/usr/local/lib/python2.7/dist-packages/mesos/cli/master.py"", line 64, 
in fetch
    return requests.get(urlparse.urljoin(self.host, url), **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/requests/api.py"", line 69, in get
    return request('get', url, params=params, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/requests/api.py"", line 50, in 
request
    response = session.request(method=method, url=url, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/requests/sessions.py"", line 451, 
in request
    prep = self.prepare_request(req)
  File ""/usr/local/lib/python2.7/dist-packages/requests/sessions.py"", line 382, 
in prepare_request
    hooks=merge_hooks(request.hooks, self.hooks),
  File ""/usr/local/lib/python2.7/dist-packages/requests/models.py"", line 304, 
in prepare
    self.prepare_url(url, params)
  File ""/usr/local/lib/python2.7/dist-packages/requests/models.py"", line 357, 
in prepare_url
    raise InvalidURL(*e.args)
requests.exceptions.InvalidURL: Failed to parse: 
10.100.1.100:5050"",""port"":5050,""version"":""0.24.1""}
{noformat}

The problem exists in https://github.com/mesosphere/mesos-cli/blob/master/mesos/cli/master.py#L107. The code should be along the lines of:

{noformat}
            try:
                parsed =  json.loads(val)
                return parsed[""address""][""ip""] + "":"" + str(parsed[""address""][""port""])
            except Exception:
                return val.split(""@"")[-1]
{noformat}

This causes the master address to come back correctly.",Bug,Major,radekg,2015-10-09T14:41:33.000+0000,5,Resolved,Complete,mesos.cli broken in 0.24.x,2015-10-14T21:39:22.000+0000,MESOS-3556,1.0,mesos,Mesosphere Sprint 20
jvanremoortere,2015-09-30T00:08:45.000+0000,jvanremoortere,"Due to the templatized nature of the allocator, even small changes trigger large recompiles of the code-base. This make iterating on changes expensive for developers.",Improvement,Major,jvanremoortere,2015-11-09T13:16:59.000+0000,5,Resolved,Complete,Allocator changes trigger large re-compiles,2015-11-09T13:16:59.000+0000,MESOS-3554,3.0,mesos,Mesosphere Sprint 20
greggomann,2015-09-30T00:01:10.000+0000,greggomann,"When the executor's environment is specified explicitly via {{\-\-executor_environment_variables}}, {{LIBPROCESS_IP}} will not be passed, leading to errors in some cases - for example, when no DNS is available.",Bug,Major,greggomann,2015-10-09T23:15:54.000+0000,5,Resolved,Complete,LIBPROCESS_IP not passed when executor's environment is specified,2016-04-10T08:52:57.000+0000,MESOS-3553,2.0,mesos,Mesosphere Sprint 20
mchadha,2015-09-29T21:17:13.000+0000,mchadha,"result.cpus() == cpus() check is failing due to ( double == double ) comparison problem. 


Root Cause : 

Framework requested 0.1 cpu reservation for the first task. So far so good. Next Reserve operation — lead to double operations resulting in following double values :

 results.cpus() : 23.9999999999999964472863211995 cpus() : 24

And the check ( result.cpus() == cpus() ) failed. 

 The double arithmetic operations caused results.cpus() value to be :  23.9999999999999964472863211995 and hence ( 23.9999999999999964472863211995 == 24 ) failed.


",Task,Major,mchadha,2016-02-03T08:15:15.000+0000,5,Resolved,Complete,CHECK failure due to floating point precision on reservation request,2016-02-03T08:15:15.000+0000,MESOS-3552,3.0,mesos,Mesosphere Sprint 28
bbannier,2015-09-29T19:39:45.000+0000,bmahler,"{{strerror()}} is not required to be thread safe by POSIX and is listed as unsafe on Linux:

http://pubs.opengroup.org/onlinepubs/9699919799/
http://man7.org/linux/man-pages/man3/strerror.3.html

I don't believe we've seen any issues reported due to this. We should replace occurrences of strerror accordingly, possibly offering a wrapper in stout to simplify callsites.",Bug,Major,bmahler,2015-11-13T17:15:53.000+0000,5,Resolved,Complete,Replace use of strerror with thread-safe alternatives strerror_r / strerror_l.,2015-11-13T17:15:53.000+0000,MESOS-3551,3.0,mesos,Mesosphere Sprint 22
anandmazumdar,2015-09-29T19:01:25.000+0000,anandmazumdar,"Similar to the Scheduler Library {{src/scheduler/scheduler.cpp}} , we would need a Executor Library that speaks the new Executor HTTP API. ",Task,Major,anandmazumdar,2016-01-26T23:25:55.000+0000,5,Resolved,Complete,Create a Executor Library based on the new Executor HTTP API,2016-02-27T00:05:07.000+0000,MESOS-3550,5.0,mesos,Mesosphere Sprint 21
jvanremoortere,2015-09-28T22:44:14.000+0000,jvanremoortere,"When the libevent loop terminates and we unblock the {{SIGPIPE}} signal, the pending {{SIGPIPE}} instantly triggers and causes a broken pipe when the test binary stops running.
{code}
Program received signal SIGPIPE, Broken pipe.
[Switching to Thread 0x7ffff18b4700 (LWP 16270)]
pthread_sigmask (how=1, newmask=<optimized out>, oldmask=0x7ffff18b3d80) at ../sysdeps/unix/sysv/linux/pthread_sigmask.c:53
53	../sysdeps/unix/sysv/linux/pthread_sigmask.c: No such file or directory.
(gdb) bt
#0  pthread_sigmask (how=1, newmask=<optimized out>, oldmask=0x7ffff18b3d80) at ../sysdeps/unix/sysv/linux/pthread_sigmask.c:53
#1  0x00000000006fd9a4 in unblock () at ../../../3rdparty/libprocess/3rdparty/stout/include/stout/os/posix/signals.hpp:90
#2  0x00000000007d7915 in run () at ../../../3rdparty/libprocess/src/libevent.cpp:125
#3  0x00000000007950cb in _M_invoke<>(void) () at /usr/include/c++/4.9/functional:1700
#4  0x0000000000795000 in operator() () at /usr/include/c++/4.9/functional:1688
#5  0x0000000000794f6e in _M_run () at /usr/include/c++/4.9/thread:115
#6  0x00007ffff668de30 in ?? () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6
#7  0x00007ffff79a16aa in start_thread (arg=0x7ffff18b4700) at pthread_create.c:333
#8  0x00007ffff5df1eed in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:109
{code}",Bug,Major,jvanremoortere,2015-09-29T02:00:42.000+0000,5,Resolved,Complete,Libevent termination triggers Broken Pipe,2015-09-29T02:00:42.000+0000,MESOS-3540,2.0,mesos,Mesosphere Sprint 20
jieyu,2015-09-28T22:31:02.000+0000,jieyu,"To address this TODO in the code:

{noformat}
src/slave/containerizer/isolators/filesystem/linux.cpp +122


// TODO(jieyu): Currently, we don't check if the slave's work_dir
// mount is a shared mount or not. We just assume it is. We cannot
// simply mark the slave as shared again because that will create a
// new peer group for the mounts. This is a temporary workaround for
// now while we are thinking about fixes.
{noformat}",Bug,Major,jieyu,2015-09-29T23:22:12.000+0000,5,Resolved,Complete,Validate that slave's work_dir is a shared mount in its own peer group when LinuxFilesystemIsolator is used.,2015-09-29T23:22:12.000+0000,MESOS-3539,3.0,mesos,Twitter Mesos Q3 Sprint 6
hausdorff,2015-09-26T01:05:58.000+0000,hausdorff,"We need to make sure people don't try to compile Mesos on 32-bit architectures. We don't want a Windows repeat of something like this:

https://issues.apache.org/jira/browse/MESOS-267",Task,Major,hausdorff,2016-03-01T18:41:20.000+0000,5,Resolved,Complete,Figure out how to enforce 64-bit builds on Windows.,2016-03-01T18:41:21.000+0000,MESOS-3525,3.0,mesos,Mesosphere Sprint 30
chzhcn,2015-09-25T20:31:10.000+0000,chzhcn,"In order to avoid missing {{close()}} calls on file descriptors, or double-closing file descriptors, it would be nice to add a reference counted {{FileDescriptor}} in a similar way to what we've done for Socket. This will be closed automatically when the last reference goes away, and double closes can be prevented via internal state.",Improvement,Major,chzhcn,,10006,Reviewable,New,Add an abstraction to manage the life cycle of file descriptors.,2016-01-27T23:25:32.000+0000,MESOS-3520,5.0,mesos,Twitter Mesos Q3 Sprint 6
anandmazumdar,2015-09-25T02:28:59.000+0000,anandmazumdar,We need to add a {{subscribe(...)}} method in {{src/slave/slave.cpp}} to introduce the ability for HTTP based executors to subscribe and then receive events on the persistent HTTP connection. Most of the functionality needed would be similar to {{Master::subscribe}} in {{src/master/master.cpp}}.,Task,Major,anandmazumdar,2015-12-04T23:28:46.000+0000,5,Resolved,Complete,Support Subscribe Call for HTTP based Executors,2016-02-27T00:05:04.000+0000,MESOS-3515,5.0,mesos,Mesosphere Sprint 19
pbrett,2015-09-24T19:30:09.000+0000,pbrett,"Running make check on centos 6.6 causes all tests to abort due to CHECK_SOME test in CgroupsFIlter:

{code}
Build directory: /home/jenkins/workspace/mesos-config-centos6/build
F0923 23:00:49.748896 27362 environment.cpp:132] CHECK_SOME(hierarchies_): Failed to determine canonical path of /sys/fs/cgroup/freezer: No such file or directory 
*** Check failure stack trace: ***
    @     0x7fb786ca0c4d  google::LogMessage::Fail()
    @     0x7fb786ca298c  google::LogMessage::SendToLog()
    @     0x7fb786ca083c  google::LogMessage::Flush()
    @     0x7fb786ca3289  google::LogMessageFatal::~LogMessageFatal()
    @           0x58e66c  mesos::internal::tests::CgroupsFilter::CgroupsFilter()
    @           0x58712f  mesos::internal::tests::Environment::Environment()
    @           0x4c882f  main
    @     0x7fb782767d5d  __libc_start_main
    @           0x4d6331  (unknown)
make[3]: *** [check-local] Aborted
{code}",Bug,Major,pbrett,2015-09-24T19:45:48.000+0000,5,Resolved,Complete,Cgroups Test Filters aborts tests on Centos 6.6 ,2015-09-24T19:45:48.000+0000,MESOS-3513,1.0,mesos,Twitter Mesos Q3 Sprint 5
bmahler,2015-09-24T19:29:26.000+0000,bmahler,"On Linux, retrying close on EINTR is dangerous because the fd is already released and we may accidentally close a newly opened fd (from another thread), see:

http://ewontfix.com/4/
http://lwn.net/Articles/576478/
http://lwn.net/Articles/576591/

It appears that other OSes, like HPUX, require a retry of close on EINTR. The Austin Group recently proposed changes to POSIX to require that the EINTR case need a retry, but EINPROGRESS be used for when a retry should not occur:

http://austingroupbugs.net/view.php?id=529

However, Linux does not follow this and so we need to remove our EINTR retries.

Some more links for posterity:

https://github.com/wahern/cqueues/issues/56#issuecomment-108656004
https://code.google.com/p/chromium/issues/detail?id=269623
https://codereview.chromium.org/23455051/
",Bug,Major,bmahler,2015-09-24T22:09:21.000+0000,5,Resolved,Complete,Don't retry close() on EINTR.,2015-09-24T22:09:33.000+0000,MESOS-3512,1.0,mesos,
greggomann,2015-09-23T22:41:33.000+0000,greggomann,Neglecting to run {{sudo yum update}} on CentOS 6.6 currently causes the build to break when building {{mesos-0.25.0.jar}}. The build instructions for this platform on the Getting Started page should be changed accordingly.,Bug,Major,greggomann,2015-11-03T05:12:58.000+0000,5,Resolved,Complete,Build instructions for CentOS 6.6 should include `sudo yum update`,2015-11-03T05:12:58.000+0000,MESOS-3506,1.0,mesos,Mesosphere Sprint 21
jieyu,2015-09-23T21:50:40.000+0000,jieyu,"Similar to Docker containerizer, if a container changes rootfs, we'll have two environment variables:

MESOS_DIRECTORY: the path in the host filesystem
MESOS_SANDBOX: the path in the container filesystem",Task,Major,jieyu,2015-09-25T21:01:27.000+0000,5,Resolved,Complete,Introduce MESOS_SANDBOX environment variable in Mesos containerizer.,2015-09-25T21:01:28.000+0000,MESOS-3504,3.0,mesos,Twitter Mesos Q3 Sprint 5
greggomann,2015-09-23T17:57:07.000+0000,greggomann,"If libevent is installed via {{sudo yum install libevent-headers}}, running {{../configure --enable-libevent}} will fail to discover the libevent headers:

{code}
checking event2/event.h usability... no
checking event2/event.h presence... no
checking for event2/event.h... no
configure: error: cannot find libevent headers
-------------------------------------------------------------------
libevent is required for libprocess to build.
-------------------------------------------------------------------
{code}",Bug,Major,greggomann,2015-10-26T14:51:10.000+0000,5,Resolved,Complete,configure cannot find libevent headers in CentOS 6,2015-11-08T12:51:45.000+0000,MESOS-3501,2.0,mesos,
jojy,2015-09-22T23:59:16.000+0000,jojy,https://reviews.apache.org/r/38747/,Task,Major,jojy,2015-12-15T00:55:38.000+0000,5,Resolved,Complete,Add implementation for sha256 based file content verification.,2015-12-15T00:55:38.000+0000,MESOS-3497,3.0,mesos,Mesosphere Sprint 19
jojy,2015-09-22T23:54:48.000+0000,jojy,"Add interface for digest verifier so that we can add implementations for digest types like sha256, sha512 etc",Task,Major,jojy,2015-12-17T23:55:06.000+0000,5,Resolved,Complete,Create interface for digest verifier,2015-12-17T23:55:06.000+0000,MESOS-3496,2.0,mesos,Mesosphere Sprint 19
kaysoky,2015-09-22T18:49:57.000+0000,kaysoky,"The committed docs can be found here:
http://mesos.apache.org/documentation/latest/maintenance/

We need to add a link to {{docs/home.md}}
Also, the doc needs some minor formatting tweaks.",Task,Trivial,kaysoky,2015-09-22T20:40:24.000+0000,5,Resolved,Complete,Expose maintenance user doc via the documentation home page,2015-09-25T22:46:29.000+0000,MESOS-3492,1.0,mesos,Mesosphere Sprint 19
ijimenez,2015-09-22T17:26:50.000+0000,vinodkone,"I've disabled ubuntu:14.04 builds on ASF CI because the job randomly fails on fetching packages.

{code}
Get:406 http://archive.ubuntu.com/ubuntu/ trusty-updates/main gdisk amd64 0.8.8-1ubuntu0.1 [185 kB]
Err http://archive.ubuntu.com/ubuntu/ trusty-security/main libldap-2.4-2 amd64 2.4.31-1+nmu2ubuntu8.1
  404  Not Found [IP: 91.189.91.15 80]
Err http://archive.ubuntu.com/ubuntu/ trusty-security/main libfreetype6 amd64 2.5.2-1ubuntu2.4
  404  Not Found [IP: 91.189.91.15 80]
Err http://archive.ubuntu.com/ubuntu/ trusty-security/main libicu52 amd64 52.1-3ubuntu0.3
  404  Not Found [IP: 91.189.91.15 80]
Fetched 213 MB in 1min 57s (1812 kB/s)
 [91mE [0m [91m: Failed to fetch http://archive.ubuntu.com/ubuntu/pool/main/o/openldap/libldap-2.4-2_2.4.31-1+nmu2ubuntu8.1_amd64.deb  404  Not Found [IP: 91.189.91.15 80]

E: Failed to fetch http://archive.ubuntu.com/ubuntu/pool/main/f/freetype/libfreetype6_2.5.2-1ubuntu2.4_amd64.deb  404  Not Found [IP: 91.189.91.15 80]

E: Failed to fetch http://archive.ubuntu.com/ubuntu/pool/main/i/icu/libicu52_52.1-3ubuntu0.3_amd64.deb  404  Not Found [IP: 91.189.91.15 80]

E: Failed to fetch http://archive.ubuntu.com/ubuntu/pool/main/g/gvfs/gvfs-common_1.20.3-0ubuntu1.1_all.deb  404  Not Found [IP: 91.189.91.15 80]

E: Failed to fetch http://archive.ubuntu.com/ubuntu/pool/main/g/gvfs/gvfs-libs_1.20.3-0ubuntu1.1_amd64.deb  404  Not Found [IP: 91.189.91.15 80]

E: Failed to fetch http://archive.ubuntu.com/ubuntu/pool/main/g/gvfs/gvfs-daemons_1.20.3-0ubuntu1.1_amd64.deb  404  Not Found [IP: 91.189.91.15 80]

E: Failed to fetch http://archive.ubuntu.com/ubuntu/pool/main/g/gvfs/gvfs_1.20.3-0ubuntu1.1_amd64.deb  404  Not Found [IP: 91.189.91.15 80]

E: Unable to fetch some archives, maybe run apt-get update or try with --fix-missing?
 [0mThe command '/bin/sh -c apt-get -y install build-essential clang git maven autoconf libtool' returned a non-zero code: 100
{code}

We need to figure out what the problem is and fix it before enabling testing on ubuntu.",Task,Major,vinodkone,2015-09-23T16:54:11.000+0000,5,Resolved,Complete,Enable ubuntu builds in ASF CI,2015-09-23T16:54:11.000+0000,MESOS-3491,1.0,mesos,Mesosphere Sprint 19
kaysoky,2015-09-22T16:53:23.000+0000,ijimenez,"The Mesos UI is broken, it seems to fail to represent JSON from /state.
This may have been introduced with https://reviews.apache.org/r/38028 ",Bug,Major,ijimenez,2015-09-22T18:21:15.000+0000,5,Resolved,Complete,Mesos UI fails to represent JSON entities,2015-09-25T22:46:28.000+0000,MESOS-3490,1.0,mesos,Mesosphere Sprint 19
kaysoky,2015-09-22T16:16:44.000+0000,hartem,"Current implementation of maintenance primitives does not support exposing Accept/Decline responses of frameworks to the cluster operators. 

This functionality is necessary to provide visibility to operators into whether a given framework is ready to comply with the posted maintenance schedule.",Bug,Major,hartem,2015-09-23T21:19:58.000+0000,5,Resolved,Complete,Add support for exposing Accept/Decline responses for inverse offers,2015-09-25T22:46:27.000+0000,MESOS-3489,2.0,mesos,Mesosphere Sprint 19
haosdent@gmail.com,2015-09-21T23:55:05.000+0000,flx42,"Currently, when using multiple hooks of the same type, the execution order is implementation-defined. 

This is because in src/hook/manager.cpp, the list of available hooks is stored in a {{hashmap<string, Hook*>}}. A hashmap is probably unnecessary for this task since the number of hooks should remain reasonable. A data structure preserving ordering should be used instead to allow the user to predict the execution order of the hooks. I suggest that the execution order should be the order in which hooks are specified with {{--hooks}} when starting an agent/master.

This will be useful when combining multiple hooks after MESOS-3366 is done.",Improvement,Major,flx42,2016-01-19T10:20:21.000+0000,5,Resolved,Complete,Make hook execution order deterministic,2016-01-19T10:20:21.000+0000,MESOS-3485,3.0,mesos,
jieyu,2015-09-21T17:55:21.000+0000,jieyu,"So that when a user task is forked, it does not hold extra references to the sandbox mount and provisioner bind backend mounts. If we don't do that, we could get the following error message when cleaning up bind backend mount points and sandbox mount points.

{noformat}
E0921 17:35:57.268159 47010 bind.cpp:182] Failed to remove rootfs mount point '/var/lib/mesos/provisioner/containers/07eb6660-25ff-4e83-8b2f-06955567e04a/backends/bind/rootfses/30f7e5e2-55d0-4d4d-a662-f8aad0d56b33': Device or resource busy
E0921 17:35:57.268349 47010 provisioner.cpp:403] Failed to remove the provisioned container directory at '/var/lib/mesos/provisioner/containers/07eb6660-25ff-4e83-8b2f-06955567e04a': Device or resource busy
{noformat}",Bug,Major,jieyu,2015-09-22T00:19:12.000+0000,5,Resolved,Complete,LinuxFilesystemIsolator should make the slave's work_dir a shared mount.,2015-09-22T00:19:12.000+0000,MESOS-3483,3.0,mesos,Twitter Mesos Q3 Sprint 5
dongdong,2015-09-21T16:26:33.000+0000,kaysoky,"It would make sense to have an accessor to the master's flags, especially for tests.

For example, see [this test|https://github.com/apache/mesos/blob/2876b8c918814347dd56f6f87d461e414a90650a/src/tests/master_maintenance_tests.cpp#L1231-L1235].",Task,Trivial,kaysoky,,10006,Reviewable,New,Add const accessor to Master flags,2016-02-23T05:22:43.000+0000,MESOS-3481,2.0,mesos,
anandmazumdar,2015-09-21T15:51:51.000+0000,anandmazumdar,"Currently, the {{struct Executor}} in slave only supports executors connected via message passing (driver). We should refactor it to add support for HTTP based Executors similar to what was done for the Scheduler API {{struct Framework}} in {{src/master/master.hpp}}",Task,Major,anandmazumdar,2015-10-23T17:14:15.000+0000,5,Resolved,Complete,Refactor Executor struct in Slave to handle HTTP based executors,2016-02-27T00:05:05.000+0000,MESOS-3480,3.0,mesos,Mesosphere Sprint 19
anandmazumdar,2015-09-20T19:19:18.000+0000,anandmazumdar,"Currently, receiving a status update sent from slave to itself , {{runTask}} , {{killTask}} and status updates from executors are handled by the {{Slave::statusUpdate}} method on Slave. The signature of the method is {{void Slave::statusUpdate(StatusUpdate update, const UPID& pid)}}. 

We need to create another overload of it that can also handle HTTP based executors which the previous PID based function can also call into. The signature of the new function could be:

{{void Slave::statusUpdate(StatusUpdate update, Executor* executor)}}

The HTTP Executor would also call into this new function via {{src/slave/http.cpp}}",Task,Major,anandmazumdar,2015-11-24T00:13:56.000+0000,5,Resolved,Complete,Refactor Status Update method on Agent to handle HTTP based Executors,2016-02-27T00:05:09.000+0000,MESOS-3476,8.0,mesos,Mesosphere Sprint 20
neilc,2015-09-20T00:32:19.000+0000,hartem,"RegistryTokenTest.ExpiredToken test is flaky. Here is the error I got on OSX after running it for several times:

{noformat}
[ RUN      ] RegistryTokenTest.ExpiredToken
../../src/tests/containerizer/provisioner_docker_tests.cpp:167: Failure
Value of: token.isError()
  Actual: false
Expected: true
libc++abi.dylib: terminating with uncaught exception of type testing::internal::GoogleTestFailureException: ../../src/tests/containerizer/provisioner_docker_tests.cpp:167: Failure
Value of: token.isError()
  Actual: false
Expected: true
*** Aborted at 1442708631 (unix time) try ""date -d @1442708631"" if you are using GNU date ***
PC: @     0x7fff925fd286 __pthread_kill
*** SIGABRT (@0x7fff925fd286) received by PID 7082 (TID 0x7fff7d7ad300) stack trace: ***
    @     0x7fff9041af1a _sigtramp
    @     0x7fff59759968 (unknown)
    @     0x7fff9bb429b3 abort
    @     0x7fff90ce1a21 abort_message
    @     0x7fff90d099b9 default_terminate_handler()
    @     0x7fff994767eb _objc_terminate()
    @     0x7fff90d070a1 std::__terminate()
    @     0x7fff90d06d48 __cxa_rethrow
    @        0x10781bb16 testing::internal::HandleExceptionsInMethodIfSupported<>()
    @        0x1077e9d30 testing::UnitTest::Run()
    @        0x106d59a91 RUN_ALL_TESTS()
    @        0x106d55d47 main
    @     0x7fff8fc395c9 start
    @                0x3 (unknown)
Abort trap: 6
~/src/mesos/build ((3ee82e3...)) $
{noformat}",Bug,Major,hartem,2016-01-13T01:27:45.000+0000,5,Resolved,Complete,RegistryTokenTest.ExpiredToken test is flaky,2016-01-13T01:27:59.000+0000,MESOS-3472,3.0,mesos,Mesosphere Sprint 26
haosdent@gmail.com,2015-09-19T19:19:01.000+0000,haosdent@gmail.com,UserCgroupIsolatorTest use /sys/fs/cgroup as cgroups_hierarchy. But CentOS 6.6 cgroups_hierarchy is /cgroup. Need change to follow the way in ContainerizerTest.,Bug,Major,haosdent@gmail.com,2015-11-24T14:00:32.000+0000,5,Resolved,Complete,UserCgroupIsolatorTest failed on CentOS 6.6,2015-11-27T12:40:18.000+0000,MESOS-3470,1.0,mesos,Mesosphere Sprint 23
hartem,2015-09-18T23:13:02.000+0000,vinodkone,Currently the support/apply-review.sh script allows an user (typically committer) to apply a single review on top the HEAD. Since Mesos contributors typically submit a chain of reviews for a given issue it makes sense for the script to apply the whole chain recursively.,Improvement,Major,vinodkone,2015-11-09T15:14:41.000+0000,5,Resolved,Complete,Improve apply_reviews.sh script to apply chain of reviews,2015-11-09T15:14:41.000+0000,MESOS-3468,8.0,mesos,Mesosphere Sprint 20
,2015-09-18T21:31:09.000+0000,xujyan,"In the first phase of filesystem provisioning and isolation we are disallowing (or at least should, especially in the case of CopyBackend) users to write outside the sandbox without explicitly mounting specific volumes into the container. We do this even when OverlayBackend can potentially support a empty writable top layer.

However in the real world use of containers (and for people coming from the VM world), users and applications often are used to being able to write to the full filesystem (restricted by plain file system permissions) with reasons ranging from applications being non-portable (filesystem-wise) to the need to do custom installs at run time to system directories (inside its container).

In general, it's a good practice to restrict the application to write to confined locations and software dependencies can be managed through pre-packaged layers but these often introduce a high entry barrier for users.

We should discuss a solution that gives the users the option to write to a full filesystem with a filesystem layer on top of provisioned images and optionally enable persistence of that layer through persistent volumes. This has implication in the management of user namespaces and resource reservations and requires a thorough design.",Story,Major,xujyan,,10020,Accepted,In Progress,Provide the users with a fully writable filesystem,2016-01-07T20:10:39.000+0000,MESOS-3467,13.0,mesos,
xujyan,2015-09-18T18:51:46.000+0000,jieyu,"We need to know about:

1) Errors encountered while provisioning root filesystems
2) Errors encountered while cleaning up root filesystems
3) Number of containers changing root filesystem
...",Task,Major,jieyu,2015-09-23T20:42:35.000+0000,5,Resolved,Complete,Add metrics for filesystem isolation and image provisioning.,2015-09-23T20:44:30.000+0000,MESOS-3466,2.0,mesos,Twitter Mesos Q3 Sprint 5
kaysoky,2015-09-17T23:38:32.000+0000,kaysoky,"With [MESOS-3312] committed, the {{/machine/up}} and {{/machine/down}} endpoints should also take an input as an array.

It is important to change this before maintenance primitives are released:
https://reviews.apache.org/r/38011/

Also, a minor change to the error message from these endpoints:
https://reviews.apache.org/r/37969/",Task,Major,kaysoky,2015-09-18T20:28:53.000+0000,5,Resolved,Complete,Change /machine/up and /machine/down endpoints to take an array,2015-09-25T22:46:26.000+0000,MESOS-3459,1.0,mesos,Mesosphere Sprint 19
kaysoky,2015-09-17T23:31:50.000+0000,kaysoky,"Discovered while writing a test for filters (in regards to inverse offers).

Fix here: https://reviews.apache.org/r/38470/",Bug,Blocker,kaysoky,2015-09-18T20:35:44.000+0000,5,Resolved,Complete,Segfault when accepting or declining inverse offers,2015-09-25T22:46:26.000+0000,MESOS-3458,1.0,mesos,Mesosphere Sprint 19
marco-mesos,2015-09-17T22:35:12.000+0000,marco-mesos,"In testing / buildinging DCOS we've found that we need to set --hostname explicitly on the masters. For our uses IP and `hostname` must always be the same thing. 

More in general, under certain circumstances, dynamic lookup of {{hostname}}, while successful, provides undesirable results; we would also like, in those circumstances, be able to just set the hostname to the chosen
IP address (possibly set via the {{\-\- ip_discovery_command}} method).

We suggest adding a {{\-\-no-hostname-lookup}}. 
Note that we can introduce this flag as {{--hostname-lookup}} with a default to 'true' (which is the current semantics) and that way someone can do {{\-\-no-hostname-lookup}} or {{\-\-hostname-lookup=false}}.
",Improvement,Major,cmaloney,2015-09-24T04:52:18.000+0000,5,Resolved,Complete,Add flag to disable hostname lookup,2015-09-25T22:46:26.000+0000,MESOS-3457,3.0,mesos,Mesosphere Sprint 19
jojy,2015-09-17T14:28:30.000+0000,jojy,"Since mesos code is based on the actor model and dispatching an interface

asynchronously is a large part of the code base, generalizing the concept of

asynchronously dispatching an interface would eliminate the need to manual

programming of the dispatch boilerplate.

An example usage:

For a simple interface like:

{code}
class Interface                                                                  
{                                                                                
  virtual Future<size_t> writeToFile(const char* data) = 0;                      
  virtual ~Interface();                                                          
};     

{code}

Today the developer has to do the following:

a. Write a wrapper class that implements the same interface to add the

dispatching boilerplate.

b. Spend precious time in reviews.

c. Risk introducing bugs.

None of the above steps add any value to the executable binary.

The wrapper class would look like:

{code}
// -- hpp file                                                                   
class InterfaceProcess;                                                          

class InterfaceImpl : public Interface                                           
{                                                                                
public:                                                                          
  Try<Owned<InterfaceImpl>> create(const Flags& flags);                          

  virtual Future<size_t> writeToFile(const char* data);                          

  ~InterfaceImpl();
private:                                                                         
  Owned<InterfaceProcess> process;                                               
};                                                                               

// -- cpp file                                                                   
Try<Owned<InterfaceImpl>> create(const Flags& flags)                             
{                                                                                
  // Code to create the InterfaceProcess class.                                  
}                                                                                

Future Future<size_t> InterfaceImpl::writeToFile(const char* data)               
{                                                                                
  process->dispatch(                                                             
    &InterfaceProcess::writeToFile,                                              
    data);                                                                       
}                                                                                

InterfaceImpl::InterfaceImpl()                                                   
{                                                                                
  // Code to spawn the process                                                   
}                                                                                

InterfaceImpl::~InterfaceImpl()                                                  
{                                                                                
  // Code to stop the process.                                                   
}   

{code}
At the caller/client site, the code would look like:

{code}
Try<Owned<Interface>> in = InterfaceImpl::create(flags);                         
Future<size_t> result =                                                          
  in->writeToFile(data);   

{code}

                                                                    
Proposal

We should use C++'s rich language semnatics to express the intent and avoid

the boilerplate we write manually.

The basic intent of the code that leads to all the boilerplate above is:

a. An interface that provides a set of functionality.

b. An implementation of the interface.

c. Ability to dispatch that interface asynchronously using actor.

C++ has a rich set of generics that can be used to express above.

Components

ProcessDispatcher

This component will ""dispatch"" an interface implementation asychronously using 
the process framework.

This component can be expressed as:

{code}
ProcessDispatcher<Interface, InterfaceImplmentation>   
{code}

DispatchInterface

Any interface that provides an implementation that can be ""dispatched"" can be

expressed using this component.

This component can be expressed as:

{code}
Dispatchable<Interface>  
{code}


Usage:

Simple usage

{code}
Try<Owned<Dispatchable<Interface>>> dispatcher =                                 
  ProcessDispatcher<Interface, InterfaceImpl>::create(flags);                    

Future<size_t> result =                                                          
  dispatcher->dispatch(                                                          
    Interface::writeToFile,                                                      
    data);      
{code}
                                                                 
Collecting the interface in a container

{code}
vector<Owned<Dispatchable<Interface>>> dispatchCollection;                       

Try<Owned<Dispatchable<Interface>>> dispatcher1 =                                
ProcessDispatcher<Interface, InterfaceImpl1>::create(flags);                   

Try<Owned<Dispatchable<Interface>>> dispatcher2 =                                
ProcessDispatcher<Interface, InterfaceImpl2>::create(""test"");                  

dispatchCollection.push_back(dispatcher1);                                       
dispatchCollection.push_back(dispatcher2);    
{code}

The advantages of using the generic dispatcher:

Saves time by avoiding to write all the boilerplate and going through review
cycles.
Less bugs.
Focus on real problem and not boilerplate.
",Bug,Major,jojy,,10020,Accepted,In Progress,Higher level construct for expressing process dispatch,2015-12-15T00:52:37.000+0000,MESOS-3455,6.0,mesos,
kaysoky,2015-09-16T22:05:42.000+0000,kaysoky,"The previous changes (MESOS-3345) to support integer precision when converting JSON <-> Protobuf did not support precision for unsigned integers between {{INT64_MAX}} and {{UINT64_MAX}}.  (There's some loss, but the conversion is still as good/bad as it was with doubles.)

This problem is due to a limitation in the JSON parsing library we use (PicoJSON), which parses integers as {{int64_t}}.

Some possible solutions or things to investigate:
* We can patch PicoJSON to parse some large values as {{uint64_t}}.
* We can investigate using another parsing library.
* If we want extra precision beyond 64 or 80 bits per double, one possibility is the [GMP library|https://gmplib.org/].  We'd still need to change the parsing library though.",Bug,Minor,kaysoky,,10020,Accepted,In Progress,Expand the range of integer precision in json <-> protobuf conversions to include unsigned integers,2015-11-27T15:36:18.000+0000,MESOS-3449,5.0,mesos,
xujyan,2015-09-15T20:59:32.000+0000,xujyan,"As described in this [TODO|https://github.com/apache/mesos/blob/e601e469c64594dd8339352af405cbf26a574ea8/src/slave/containerizer/isolators/filesystem/linux.cpp#L418]:
{noformat:title=}
  // TODO(jieyu): Try to unmount work directory mounts and persistent
  // volume mounts for other containers to release the extra
  // references to those mounts.
{noformat}

This will a best effort attempt to alleviate the race condition between provisioner's container cleanup and new containers copying host mount table.",Task,Major,xujyan,2015-09-17T22:42:01.000+0000,5,Resolved,Complete,Unmount irrelevant host mounts in the new container's mount namespace.,2015-09-17T22:42:01.000+0000,MESOS-3433,3.0,mesos,Twitter Mesos Q3 Sprint 5
jieyu,2015-09-15T18:05:38.000+0000,jieyu,"The current design uses separate provisioner implementation for each type of image (e.g., APPC, DOCKER).

This creates a lot of code duplications. Since we already have a unified provisioner backend (e.g., copy, bind, overlayfs), we should be able to unify the implementations of image provisioners and hide the image specific logics in the corresponding 'Store' implementation.",Task,Major,jieyu,2015-09-16T23:10:38.000+0000,5,Resolved,Complete,Unify the implementations of the image provisioners.,2015-09-16T23:10:38.000+0000,MESOS-3432,5.0,mesos,Twitter Mesos Q3 Sprint 5
jieyu,2015-09-15T03:13:43.000+0000,marco-mesos,"Just ran ROOT tests on CentOS 7.1 and had the following failure (clean build, just pulled from {{master}}):
{noformat}
[ RUN      ] LinuxFilesystemIsolatorTest.ROOT_PersistentVolumeWithoutRootFilesystem
../../src/tests/containerizer/filesystem_isolator_tests.cpp:498: Failure
(wait).failure(): Failed to clean up an isolator when destroying container '366b6d37-b326-4ed1-8a5f-43d483dbbace' :Failed to unmount volume '/tmp/LinuxFilesystemIsolatorTest_ROOT_PersistentVolumeWithoutRootFilesystem_KXgvoH/sandbox/volume': Failed to unmount '/tmp/LinuxFilesystemIsolatorTest_ROOT_PersistentVolumeWithoutRootFilesystem_KXgvoH/sandbox/volume': Invalid argument
../../src/tests/utils.cpp:75: Failure
os::rmdir(sandbox.get()): Device or resource busy
[  FAILED  ] LinuxFilesystemIsolatorTest.ROOT_PersistentVolumeWithoutRootFilesystem (1943 ms)
[----------] 1 test from LinuxFilesystemIsolatorTest (1943 ms total)

[----------] Global test environment tear-down
[==========] 1 test from 1 test case ran. (1951 ms total)
[  PASSED  ] 0 tests.
[  FAILED  ] 1 test, listed below:
[  FAILED  ] LinuxFilesystemIsolatorTest.ROOT_PersistentVolumeWithoutRootFilesystem
{noformat}",Bug,Major,marco-mesos,2015-09-17T22:37:19.000+0000,5,Resolved,Complete,LinuxFilesystemIsolatorTest.ROOT_PersistentVolumeWithoutRootFilesystem fails on CentOS 7.1,2015-09-25T22:46:28.000+0000,MESOS-3430,2.0,mesos,Twitter Mesos Q3 Sprint 5
bmahler,2015-09-14T20:11:06.000+0000,bmahler,"When aggregating futures with collect, one may discard the outer future:

{code}
Promise<int> p1;
Promise<string> p2;

Future<int, string> collect = process::collect(p1.future(), p2.future());

collect.discard();

// collect will transition to DISCARDED

// However, p{1,2}.future().hasDiscard() remains false
// as there is no discard propagation!
{code}

Discard requests should propagate down into the inner futures being collected.",Bug,Major,bmahler,2015-09-15T19:04:57.000+0000,5,Resolved,Complete,process::collect and process::await do not perform discard propagation.,2015-09-15T19:04:57.000+0000,MESOS-3426,3.0,mesos,Twitter Mesos Q3 Sprint 5
jvanremoortere,2015-09-14T20:03:40.000+0000,jvanremoortere,"Implement the solution described in MESOS-3352 in the LinuxLauncher

In order to avoid the migration of cgroup pids by Systemd we can use the {{delegate=true}} flag. This guards Systemd from migrating the pids that are descendants of the process launched by a Systemd unit.

In order for this strategy to work, the {{delegate}} flag must be supported by the Systemd version. Support for this was introduced in Systemd v218; however, it has also been backported to v208 for RHEL7 and CentOS7 [here|http://centoserrata.nagater.net/item/CEBA-2015-0037-CentOS-7.i386.x86_64.html] with the package [systemd-208-20|https://rhn.redhat.com/errata/RHBA-2015-1155.html]. It is highly recommended to upgrade to this package if running those operating systems.

Once the {{delegate=true}} flag has been set, the cgroups that are manually manipulated by the agent will no longer be migrated *during the lifetime of the agent*.

This still leaves the problem of tasks being migrated _after the agent has stopped running_ (voluntarily or not). In order to deal with the problem we propose the following solution:

If an agent is running on a Systemd initialized machine, then the agent will create a Systemd slice with a life-time that is independent of the agent and {{delegate=true}}. The linux launcher (used when cgroups isolators are enabled) will then assign the cgroup name for any executor that is launched to this separate slice. The consequence of this is that when the agent unit is terminated, the separate slice will continue to delegate the cgroups preventing Systemd from migrating the pids. A side benefit of this is that we can maintain the {{KillMode=control-group}} flag on the agent and terminate all agent specific services such as the {{fetcher}} without terminating the tasks. This provides for a nice clean-up.

This solution will still require that the agent unit be launched with the {{delegate=true}} flag such that there is no race during the transition of the pids from the agent to the separate slice.

The agent will be responsible for verifying the slice is still available upon recovery, and warning the operator if it notices that the tasks it is recovering are no longer associated with this separate slice, as this can cause *silent* loss of isolation of existing tasks.",Task,Major,jvanremoortere,2015-09-24T04:19:44.000+0000,5,Resolved,Complete,Modify LinuxLauncher to support Systemd,2015-09-25T22:46:27.000+0000,MESOS-3425,8.0,mesos,Mesosphere Sprint 19
jojy,2015-09-14T18:39:00.000+0000,vinodkone,"So far AppC store is read only and depends on out of band mechanisms to get the images. We need to design a way to support fetching in a native way.

As commented on MESOS-2824:
It's unacceptable to have either have:
* the slave to be blocked for extended period of time (minutes) which delays the communication between the executor and scheduler, or
* the first task that uses this image to be blocked for a long time to wait for the container image to be ready.

The solution needs to enable the operator to prefetch a list of ""preferred images"" without introducing the above problems.",Task,Major,vinodkone,2016-02-27T00:03:25.000+0000,5,Resolved,Complete,Support fetching AppC images into the store,2016-02-27T00:03:25.000+0000,MESOS-3424,5.0,mesos,Mesosphere Sprint 29
bmahler,2015-09-14T18:29:26.000+0000,vinodkone,"Currently the perf event isolator times out a sample after a fixed extra time of 2 seconds on top of the sample time elapses:

{code}
    Duration timeout = flags.perf_duration + Seconds(2);
{code}

This should be based on the reap interval maximum.

Also, the code stops sampling altogether when a single timeout occurs. We've observed time outs during normal operation, so it would be better for the isolator to continue performing perf sampling in the case of timeouts. It may also make sense to continue sampling in the case of errors, since these may be transient.",Bug,Major,vinodkone,2015-09-15T19:07:13.000+0000,5,Resolved,Complete,Perf event isolator stops performing sampling if a single timeout occurs.,2015-09-15T19:07:13.000+0000,MESOS-3423,3.0,mesos,Twitter Mesos Q3 Sprint 5
gyliu,2015-09-13T20:48:19.000+0000,jvanremoortere,"We currently have some helper functionality for V1 API tests. This is copied in a few test files.
Factor this out into a common place once the API is stabilized.
{code}
// Helper class for using EXPECT_CALL since the Mesos scheduler API
  // is callback based.
  class Callbacks
  {
  public:
    MOCK_METHOD0(connected, void(void));
    MOCK_METHOD0(disconnected, void(void));
    MOCK_METHOD1(received, void(const std::queue<Event>&));
  };
{code}
{code}
// Enqueues all received events into a libprocess queue.
// TODO(jmlvanre): Factor this common code out of tests into V1
// helper.
ACTION_P(Enqueue, queue)
{
  std::queue<Event> events = arg0;
  while (!events.empty()) {
    // Note that we currently drop HEARTBEATs because most of these tests
    // are not designed to deal with heartbeats.
    // TODO(vinod): Implement DROP_HTTP_CALLS that can filter heartbeats.
    if (events.front().type() == Event::HEARTBEAT) {
      VLOG(1) << ""Ignoring HEARTBEAT event"";
    } else {
      queue->put(events.front());
    }
    events.pop();
  }
}
{code}

We can also update the helpers in {{/tests/mesos.hpp}} to support the V1 API.  This would let us get ride of lines like:
{code}
v1::TaskInfo taskInfo = evolve(createTask(devolve(offer), """", DEFAULT_EXECUTOR_ID));
{code}
In favor of:
{code}
v1::TaskInfo taskInfo = createTask(offer, """", DEFAULT_EXECUTOR_ID);
{code}",Improvement,Major,jvanremoortere,2016-02-16T17:54:51.000+0000,5,Resolved,Complete,Factor out V1 API test helper functions,2016-02-16T17:54:54.000+0000,MESOS-3418,2.0,mesos,
neilc,2015-09-12T05:55:39.000+0000,cmaloney,"Currently Mesos doesn't log what machine a replicated log status broadcast was recieved from:
{code}
Sep 11 21:41:14 master-01 mesos-master[15625]: I0911 21:41:14.320164 15637 replica.cpp:641] Replica in EMPTY status received a broadcasted recover request
Sep 11 21:41:14 master-01 mesos-dns[15583]: I0911 21:41:14.321097   15583 detect.go:118] ignoring children-changed event, leader has not changed: /mesos
Sep 11 21:41:14 master-01 mesos-master[15625]: I0911 21:41:14.353914 15639 replica.cpp:641] Replica in EMPTY status received a broadcasted recover request
Sep 11 21:41:14 master-01 mesos-master[15625]: I0911 21:41:14.479132 15639 replica.cpp:641] Replica in EMPTY status received a broadcasted recover request
{code}

It would be really useful for debugging replicated log startup issues to have info about where the message came from (libprocess address, ip, or hostname) the message came from",Improvement,Minor,cmaloney,2015-10-08T04:25:29.000+0000,5,Resolved,Complete,Log source address replicated log recieved broadcasts,2015-10-08T04:25:30.000+0000,MESOS-3417,2.0,mesos,Mesosphere Sprint 20
vinodkone,2015-09-11T23:47:14.000+0000,wfarner,"0.24.0 was released, but the python egg has not been published.",Task,Major,wfarner,2015-09-14T21:00:04.000+0000,5,Resolved,Complete,Publish egg for 0.24.0 to PyPI,2015-09-14T21:01:57.000+0000,MESOS-3416,1.0,mesos,Twitter Mesos Q3 Sprint 5
tnachen,2015-09-11T14:06:11.000+0000,neunhoef,"For the ArangoDB framework I am trying to use the persistent primitives. nearly all is working, but I am missing a crucial piece at the end: I have successfully created a persistent disk resource and have set the persistence and volume information in the DiskInfo message. However, I do not see any way to find out what directory on the host the mesos slave has reserved for us. I know it is ${MESOS_SLAVE_WORKDIR}/volumes/roles/<myRole>/<NAME>_<UUID> but we have no way to query this information anywhere. The docker containerizer does not automatically mount this directory into our docker container, or symlinks it into our sandbox. Therefore, I have essentially no access to it. Note that the mesos containerizer (which I cannot use for other reasons) seems to create a symlink in the sandbox to the actual path for the persistent volume. With that, I could mount the volume into our docker container and all would be well.",Bug,Major,neunhoef,2016-02-18T22:03:23.000+0000,5,Resolved,Complete,Docker containerizer does not symlink persistent volumes into sandbox,2016-02-19T00:42:21.000+0000,MESOS-3413,5.0,mesos,Mesosphere Sprint 28
gilbert,2015-09-10T18:22:05.000+0000,kaysoky,"Two functions in the Docker-related code take a string and parse it to JSON:
* {{Docker::Container::create}} in {{src/docker/docker.cpp}}
* {{Token::create}} in {{src/slave/containerizer/provisioners/docker/token_manager.cpp}}

This JSON is then validated (lots of if-elses) and used via the {{JSON::Value}} accessors.  We could instead use a protobuf and the related Stout JSON->Protobuf conversion function.",Improvement,Minor,kaysoky,,10020,Accepted,In Progress,Refactor the plain JSON parsing in the docker containerizer,2015-09-30T18:38:09.000+0000,MESOS-3409,3.0,mesos,
tanderegg,2015-09-10T00:39:03.000+0000,meatmanek,mesos-execute does not appear to support passing credentials. This makes it impossible to use on a cluster where framework authentication is required.,Bug,Major,meatmanek,2016-04-22T20:28:17.000+0000,5,Resolved,Complete,mesos-execute does not support credentials,2016-04-22T20:28:17.000+0000,MESOS-3402,2.0,mesos,Mesosphere Sprint 33
wangcong,2015-09-09T00:51:17.000+0000,wangcong,"Our current code base invokes and parses `perf stat`, which sucks, because cmdline output is not a stable ABI at all, it can break our code at any time, for example MESOS-2834.

We should use the stable API perf_event_open(2). With this patch https://reviews.apache.org/r/37540/, we already have the infrastructure for the implementation, so it should not be hard to rewrite all the perf events code.",Task,Minor,wangcong,,3,In Progress,In Progress,Rewrite perf events code,2015-09-28T18:21:32.000+0000,MESOS-3399,5.0,mesos,Twitter Mesos Q3 Sprint 5
ijimenez,2015-09-08T22:26:44.000+0000,ijimenez,"The executor protobuf definition living outside the v1/ directory is unused, it should  be removed to avoid confusion.",Task,Major,ijimenez,2015-09-09T17:57:05.000+0000,5,Resolved,Complete,Remove unused executor protobuf,2016-02-27T00:05:05.000+0000,MESOS-3393,1.0,mesos,
alexr,2015-09-07T16:08:48.000+0000,alex-mesos,We use {{Clock::advance()}} extensively in tests to expedite event firing and minimize overall {{make check}} time. Document this pattern for posterity.,Documentation,Minor,alexr,2015-10-20T10:18:39.000+0000,5,Resolved,Complete,Document a test pattern for expediting event firing,2015-10-20T10:19:49.000+0000,MESOS-3378,3.0,mesos,Mesosphere Sprint 18
ijimenez,2015-09-05T02:34:25.000+0000,ijimenez,"A new protobuf for Executor was introduced in Mesos for the HTTP API, it needs to be added to /v1 so it reflects changes made on v1/mesos.proto. This protobuf is ought to be changed as the executor HTTP API design evolves.",Task,Major,ijimenez,2015-09-16T21:09:18.000+0000,5,Resolved,Complete,Add executor protobuf to v1,2016-02-27T00:05:07.000+0000,MESOS-3375,1.0,mesos,Mesosphere Sprint 18
a10gupta,2015-09-04T18:16:55.000+0000,nnielsen,"Add support for [device cgroups|https://www.kernel.org/doc/Documentation/cgroup-v1/devices.txt] to aid isolators controlling access to devices.

In the future, we could think about how to numerate and control access to devices as resource or task/container policy",Task,Major,nnielsen,2016-03-29T02:03:29.000+0000,5,Resolved,Complete,Add device support in cgroups abstraction,2016-03-29T02:03:29.000+0000,MESOS-3368,3.0,mesos,Mesosphere Sprint 31
,2015-09-04T01:36:06.000+0000,flx42,"In heterogeneous clusters, tasks sometimes have strong constraints on the type of hardware they need to execute on. The current solution is to use custom resources and attributes on the agents. Detecting non-standard resources/attributes requires wrapping the ""mesos-slave"" binary behind a script and use custom code to probe the agent. Unfortunately, this approach doesn't allow composition. The solution would be to provide a hook/module mechanism to allow users to use custom code performing resources/attributes discovery.

Please review the detailed document below:
https://docs.google.com/document/d/15OkebDezFxzeyLsyQoU0upB0eoVECAlzEkeg0HQAX9w

Feel free to express comments/concerns by annotating the document or by replying to this issue.
",Improvement,Major,flx42,2015-10-13T18:24:22.000+0000,5,Resolved,Complete,Allow resources/attributes discovery,2015-11-08T12:51:46.000+0000,MESOS-3366,3.0,mesos,
wangcong,2015-09-04T01:16:33.000+0000,wangcong,"We need to export the per container SNMP statistics too, from its /proc/net/snmp.",Task,Major,wangcong,2016-02-18T17:28:23.000+0000,5,Resolved,Complete,Export per container SNMP statistics,2016-02-18T17:28:23.000+0000,MESOS-3365,5.0,mesos,Twitter Mesos Q3 Sprint 4
alexr,2015-09-02T12:48:38.000+0000,alex-mesos,"We got plenty of feedback from different parties, which we would like to persist in the design doc for posterity.",Documentation,Major,alexr,2015-09-29T05:51:06.000+0000,5,Resolved,Complete,Update quota design doc based on user comments and offline syncs,2015-11-07T00:14:19.000+0000,MESOS-3357,3.0,mesos,Mesosphere Sprint 18
kaysoky,2015-09-01T20:48:00.000+0000,kaysoky,"For the background, see the parent story [MESOS-3348].

For the work/design/discussion, see the linked design document (below).

",Task,Major,kaysoky,2015-12-07T19:32:11.000+0000,5,Resolved,Complete,Scope out approaches to deal with logging to finite disks (i.e. log rotation|capped-size logging).,2015-12-07T19:32:11.000+0000,MESOS-3356,5.0,mesos,Mesosphere Sprint 18
jvanremoortere,2015-09-01T15:06:59.000+0000,jvanremoortere,"There have been many reports of cgroups related issues when running Mesos on Systemd.
Many of these issues are rooted in the manual manipulation of the cgroups filesystem by Mesos.
This task is to describe the problem in a 1-page summary, and elaborate on the suggested 2 part solution:
1. Using the {{delegate=true}} flag for the slave
2. Implementing a Systemd launcher to run executors with tighter Systemd integration.",Task,Major,jvanremoortere,2015-09-14T15:01:32.000+0000,5,Resolved,Complete,Problem Statement Summary for Systemd Cgroup Launcher,2015-11-19T02:06:32.000+0000,MESOS-3352,5.0,mesos,Mesosphere Sprint 18
jieyu,2015-09-01T02:26:07.000+0000,bmahler,"When running the tests as root, we found PersistentVolumeTest.AccessPersistentVolume fails consistently on some platforms.

{noformat}
[ RUN      ] PersistentVolumeTest.AccessPersistentVolume
I0901 02:17:26.435140 39432 exec.cpp:133] Version: 0.25.0
I0901 02:17:26.442129 39461 exec.cpp:207] Executor registered on slave 20150901-021726-1828659978-52102-32604-S0
Registered executor on hostname
Starting task d8ff1f00-e720-4a61-b440-e111009dfdc3
sh -c 'echo abc > path1/file'
Forked command at 39484
Command exited with status 0 (pid: 39484)
../../src/tests/persistent_volume_tests.cpp:579: Failure
Value of: os::exists(path::join(directory, ""path1""))
  Actual: true
Expected: false
[  FAILED  ] PersistentVolumeTest.AccessPersistentVolume (777 ms)
{noformat}

Turns out that the 'rmdir' after the 'umount' fails with EBUSY because there's still some references to the mount.

FYI [~jieyu] [~mcypark]",Bug,Major,bmahler,2015-09-14T23:39:06.000+0000,5,Resolved,Complete,Removing mount point fails with EBUSY in LinuxFilesystemIsolator.,2016-01-16T08:46:50.000+0000,MESOS-3349,5.0,mesos,Twitter Mesos Q3 Sprint 5
kaysoky,2015-09-01T00:44:50.000+0000,kaysoky,"Tasks currently log their output (i.e. stdout/stderr) to files (the ""sandbox"") on an agent's disk.  In some cases, the accumulation of these logs can completely fill up the agent's disk and thereby kill the task or machine.

To prevent this, we should either implement a log rotation mechanism or capped-size logging.  This would be used by executors to control the amount of logs they keep.  

Master/agent logs will not be affected.

We will first scope out several possible approaches for log rotation/capping in a design document (see [MESOS-3356]).  Once an approach is chosen, this story will be broken down into some corresponding issues.",Story,Major,kaysoky,2015-12-07T20:11:18.000+0000,5,Resolved,Complete,Add either log rotation or capped-size logging (for tasks),2015-12-07T20:11:18.000+0000,MESOS-3348,13.0,mesos,
hartem,2015-08-31T23:18:54.000+0000,hartem,"A filter attached to the inverse offer can be used by the framework to control when it wants to be contacted again with the inverse offer, since future circumstances may change the viability of the maintenance schedule.  The “filter” for InverseOffers is identical to the existing mechanism for re-offering Offers to frameworks.",Task,Major,hartem,2015-09-20T18:40:59.000+0000,5,Resolved,Complete,Add filter support for inverse offers,2015-09-25T22:46:29.000+0000,MESOS-3346,5.0,mesos,Mesosphere Sprint 18
kaysoky,2015-08-31T21:56:22.000+0000,kaysoky,"For [MESOS-3299], we added some protobufs to represent time with integer precision.  However, this precision is not maintained through protobuf <-> JSON conversion, because of how our JSON encoders/decoders convert numbers to floating point.

To maintain precision, we can try one of the following:
* Try using a {{long double}} to represent a number.
* Add logic to stringify/parse numbers without loss when possible.
* Try representing {{int64_t}} as a string and parse it as such?
* Update PicoJson and add a compiler flag, i.e. {{-DPICOJSON_USE_INT64}} 

In all cases, we'll need to make sure that:
* Integers are properly stringified without loss.
* The JSON decoder parses the integer without loss.
* We have some unit tests for big (close to {{INT32_MAX}}/{{INT64_MAX}}) and small integers.",Task,Minor,kaysoky,2015-09-16T21:58:21.000+0000,5,Resolved,Complete,Expand the range of integer precision when converting into/out of json.,2015-09-27T23:33:40.000+0000,MESOS-3345,5.0,mesos,Mesosphere Sprint 18
,2015-08-31T20:57:50.000+0000,anandmazumdar,"We need to build rate limiting functionality for frameworks connecting via the Scheduler HTTP API similar to the PID based frameworks.

Link to the rate-limiting section from design doc:
https://docs.google.com/document/d/1pnIY_HckimKNvpqhKRhbc9eSItWNFT-priXh_urR-T0/edit#heading=h.kzgdk4d5fmba

- This ticket deals with refactoring the existing PID based framework functionality and extend it for HTTP frameworks.
- The second part of notifying the framework when rate-limiting is active i.e. returning a status of 429 can be undertook as part of MESOS-1664",Task,Major,anandmazumdar,,1,Open,New,Rate Limiting functionality for HTTP Frameworks,2016-01-22T20:05:29.000+0000,MESOS-3343,5.0,mesos,
klaus1982,2015-08-31T18:06:29.000+0000,marco-mesos,"Currently, it appears that re-defining a flag on the command-line that was already defined via a OS Env var ({{MESOS_*}}) causes the Master to fail with a not very helpful message.

For example, if one has {{MESOS_QUORUM}} defined, this happens:
{noformat}
$ ./mesos-master --zk=zk://192.168.1.4/mesos --quorum=1 --hostname=192.168.1.4 --ip=192.168.1.4
Duplicate flag 'quorum' on command line
{noformat}

which is not very helpful.

Ideally, we would parse the flags with a ""well-known"" priority (command-line first, environment last) - but at the very least, the error message should be more helpful in explaining what the issue is.",Improvement,Major,marco-mesos,2015-09-19T14:40:53.000+0000,5,Resolved,Complete,Command-line flags should take precedence over OS Env variables,2015-09-19T14:40:53.000+0000,MESOS-3340,2.0,mesos,
anandmazumdar,2015-08-31T16:26:12.000+0000,anandmazumdar,"Currently, our testing infrastructure does not have a mechanism of filtering/dropping HTTP events of a particular type from the Scheduler API response stream.  We need a {{DROP_HTTP_CALLS}} abstraction that can help us to filter a particular event type.

{code}
// Enqueues all received events into a libprocess queue.
ACTION_P(Enqueue, queue)
{
  std::queue<Event> events = arg0;
  while (!events.empty()) {
    // Note that we currently drop HEARTBEATs because most of these tests
    // are not designed to deal with heartbeats.
    // TODO(vinod): Implement DROP_HTTP_CALLS that can filter heartbeats.
    if (events.front().type() == Event::HEARTBEAT) {
      VLOG(1) << ""Ignoring HEARTBEAT event"";
    } else {
      queue->put(events.front());
    }
    events.pop();
  }
}
{code}

This helper code is duplicated in at least two places currently, Scheduler Library/Maintenance Primitives tests. 
- The solution can be as trivial as moving this helper function to a common test-header.
- Implement a {{DROP_HTTP_CALLS}} similar to what we do for other protobufs via {{DROP_CALLS}}.",Task,Major,anandmazumdar,2016-02-28T17:58:14.000+0000,5,Resolved,Complete,Implement filtering mechanism for (Scheduler API Events) Testing,2016-02-28T17:58:14.000+0000,MESOS-3339,3.0,mesos,Mesosphere Sprint 20
gyliu,2015-08-31T15:53:39.000+0000,alex-mesos,"Dynamically reserved resources should be considered used or allocated and hence reflected in Mesos bookkeeping structures and {{state.json}}.

I expanded the {{ReservationTest.ReserveThenUnreserve}} test with the following section:
{code}
  // Check that the Master counts the reservation as a used resource.
  {
    Future<process::http::Response> response =
      process::http::get(master.get(), ""state.json"");
    AWAIT_READY(response);

    Try<JSON::Object> parse = JSON::parse<JSON::Object>(response.get().body);
    ASSERT_SOME(parse);

    Result<JSON::Number> cpus =
      parse.get().find<JSON::Number>(""slaves[0].used_resources.cpus"");

    ASSERT_SOME_EQ(JSON::Number(1), cpus);
  }
{code}
and got
{noformat}
../../../src/tests/reservation_tests.cpp:168: Failure
Value of: (cpus).get()
  Actual: 0
Expected: JSON::Number(1)
Which is: 1
{noformat}

Idea for new resources states: https://docs.google.com/drawings/d/1aquVIqPY8D_MR-cQjZu-wz5nNn3cYP3jXqegUHl-Kzc/edit",Bug,Minor,alexr,,10020,Accepted,In Progress,Dynamic reservations are not counted as used resources in the master,2016-01-20T09:21:41.000+0000,MESOS-3338,3.0,mesos,
jojy,2015-08-31T14:31:13.000+0000,jojy, Refactor SSL test fixture to be available for reuse by other projects. Currently the fixture class and its the symbols it depends on are not present in libproces's include files.,Task,Major,jojy,2015-08-31T14:40:55.000+0000,5,Resolved,Complete,Refactored libprocess SSL tests.,2015-08-31T19:04:52.000+0000,MESOS-3337,3.0,mesos,Mesosphere Sprint 17
bmahler,2015-08-28T20:20:38.000+0000,anandmazumdar,"Currently , {{http::post}} in libprocess, does not support HTTP pipelining. Each call as of know sends in the {{Connection: close}} header, thereby, signaling to the server to close the TCP socket after the response.

We either need to create a new interface for supporting HTTP pipelining , or modify the existing {{http::post}} to do so.

This is needed for the Scheduler/Executor library implementations to make sure ""Calls"" are sent in order to the master. Currently, in order to do so, we send in the next request only after we have received a response for an earlier call that results in degraded performance.

",Task,Major,anandmazumdar,2015-10-06T00:13:11.000+0000,5,Resolved,Complete,Support HTTP Pipelining in libprocess (http::post),2015-10-06T00:13:11.000+0000,MESOS-3332,8.0,mesos,Twitter Mesos Q3 Sprint 4
neilc,2015-08-28T00:11:23.000+0000,neilc,"Now that we require C++11, we can make use of std::atomic. For example:

* libprocess/process.cpp uses a bare int + __sync_synchronize() for ""running""
* __sync_synchronize() is used in logging.hpp in libprocess and fork.hpp in stout
* sched/sched.cpp uses a volatile int for ""running"" -- this is wrong, ""volatile"" is not sufficient to ensure safe concurrent access
* ""volatile"" is used in a few other places -- most are probably dubious but I haven't looked closely",Bug,Major,neilc,2015-10-20T20:42:31.000+0000,5,Resolved,Complete,Make use of C++11 atomics,2015-10-21T15:06:49.000+0000,MESOS-3326,2.0,mesos,Mesosphere Sprint 21
,2015-08-27T01:44:48.000+0000,alex-mesos,Stout protobufs (AFAIK right now it's just a single file {{protobuf_tests.proto}}) are not generated automatically. Including proto generation step would be cleaner and more convenient.,Improvement,Minor,alexr,,10020,Accepted,In Progress,Auto-generate protos for stout tests,2015-12-08T19:52:53.000+0000,MESOS-3323,2.0,mesos,
bernd-mesos,2015-08-26T19:53:46.000+0000,karya,"The fetcher emits a spurious log message about not extracting an archive with "".tgz"" extension, even though the tarball is extracted correctly.

{code}
I0826 19:02:08.304914  2109 logging.cpp:172] INFO level logging started!
I0826 19:02:08.305253  2109 fetcher.cpp:413] Fetcher Info: {""cache_directory"":""\/tmp\/mesos\/fetch\/slaves\/20150826-185716-251662764-5050-1-S0\/root"",""items"":[{""action"":""BYPASS_CACHE"",""uri"":{""extract"":true,""value"":""file:\/\/\/mesos\/sampleflaskapp.tgz""}}],""sandbox_directory"":""\/tmp\/mesos\/slaves\/20150826-185716-251662764-5050-1-S0\/frameworks\/20150826-185716-251662764-5050-1-0000\/executors\/sample-flask-app.f222d202-4c24-11e5-a628-0242ac110011\/runs\/e71f50b8-816d-46d5-bcc6-f9850a0402ed"",""user"":""root""}
I0826 19:02:08.306834  2109 fetcher.cpp:368] Fetching URI 'file:///mesos/sampleflaskapp.tgz'
I0826 19:02:08.306864  2109 fetcher.cpp:242] Fetching directly into the sandbox directory
I0826 19:02:08.306884  2109 fetcher.cpp:179] Fetching URI 'file:///mesos/sampleflaskapp.tgz'
I0826 19:02:08.306900  2109 fetcher.cpp:159] Copying resource with command:cp '/mesos/sampleflaskapp.tgz' '/tmp/mesos/slaves/20150826-185716-251662764-5050-1-S0/frameworks/20150826-185716-251662764-5050-1-0000/executors/sample-flask-app.f222d202-4c24-11e5-a628-0242ac110011/runs/e71f50b8-816d-46d5-bcc6-f9850a0402ed/sampleflaskapp.tgz'
I0826 19:02:08.309063  2109 fetcher.cpp:76] Extracting with command: tar -C '/tmp/mesos/slaves/20150826-185716-251662764-5050-1-S0/frameworks/20150826-185716-251662764-5050-1-0000/executors/sample-flask-app.f222d202-4c24-11e5-a628-0242ac110011/runs/e71f50b8-816d-46d5-bcc6-f9850a0402ed' -xf '/tmp/mesos/slaves/20150826-185716-251662764-5050-1-S0/frameworks/20150826-185716-251662764-5050-1-0000/executors/sample-flask-app.f222d202-4c24-11e5-a628-0242ac110011/runs/e71f50b8-816d-46d5-bcc6-f9850a0402ed/sampleflaskapp.tgz'
I0826 19:02:08.315313  2109 fetcher.cpp:84] Extracted '/tmp/mesos/slaves/20150826-185716-251662764-5050-1-S0/frameworks/20150826-185716-251662764-5050-1-0000/executors/sample-flask-app.f222d202-4c24-11e5-a628-0242ac110011/runs/e71f50b8-816d-46d5-bcc6-f9850a0402ed/sampleflaskapp.tgz' into '/tmp/mesos/slaves/20150826-185716-251662764-5050-1-S0/frameworks/20150826-185716-251662764-5050-1-0000/executors/sample-flask-app.f222d202-4c24-11e5-a628-0242ac110011/runs/e71f50b8-816d-46d5-bcc6-f9850a0402ed'
W0826 19:02:08.315381  2109 fetcher.cpp:264] Copying instead of extracting resource from URI with 'extract' flag, because it does not seem to be an archive: file:///mesos/sampleflaskapp.tgz
I0826 19:02:08.315604  2109 fetcher.cpp:445] Fetched 'file:///mesos/sampleflaskapp.tgz' to '/tmp/mesos/slaves/20150826-185716-251662764-5050-1-S0/frameworks/20150826-185716-251662764-5050-1-0000/executors/sample-flask-app.f222d202-4c24-11e5-a628-0242ac110011/runs/e71f50b8-816d-46d5-bcc6-f9850a0402ed/sampleflaskapp.tgz'
{code}",Bug,Trivial,karya,2015-08-28T15:53:14.000+0000,5,Resolved,Complete,Spurious fetcher message about extracting an archive,2015-08-28T15:53:14.000+0000,MESOS-3321,1.0,mesos,Mesosphere Sprint 17
greggomann,2015-08-26T18:56:29.000+0000,greggomann,"Mesos configured with {{--enable-perftools}} currently will not build on OSX 10.10.4 or Ubuntu 14.04, possibly because the bundled gperftools-2.0 is not current. The stable release is now 2.4, which builds successfully on both of these platforms.

This issue is resolved when Mesos will build successfully out of the box with gperftools enabled. After this ticket is resolved, the libprocess profiler should be tested to confirm that it still works and if not, it should be fixed.",Bug,Major,greggomann,2016-04-26T20:30:57.000+0000,5,Resolved,Complete,Mesos will not build when configured with gperftools enabled,2016-04-27T13:15:49.000+0000,MESOS-3319,2.0,mesos,Mesosphere Sprint 33
hartem,2015-08-26T04:26:40.000+0000,hartem,"Mesos Jenkins build script needs to be reworked to support the following:

- Wider test coverage (libevent, libssl, root tests, Docker tests).
- More OS/compiler Docker images for testing Mesos.
- Excluding tests on per-image basis.
- Reproducing the test image locally.
",Task,Major,hartem,,10006,Reviewable,New,Rework Jenkins build script,2015-11-09T07:23:46.000+0000,MESOS-3313,3.0,mesos,Mesosphere Sprint 17
alexr,2015-08-26T02:37:16.000+0000,alex-mesos,"In general, we have the collection of protobuf messages as another protobuf message, which makes JSON -> protobuf conversion straightforward. This is not always the case, for example, {{Resources}} class is not a protobuf, though protobuf-convertible.

To facilitate conversions like JSON -> {{Resources}} and avoid writing code for each particular case, we propose to introduce {{JSON::Array}} -> {{repeated protobuf}} conversion. With this in place, {{JSON::Array}} -> {{Resources}} boils down to {{JSON::Array}} -> {{repeated Resource}} -> (extra c-tor call) -> {{Resources}}.",Improvement,Major,alexr,2015-09-15T08:29:51.000+0000,5,Resolved,Complete,Factor out JSON to repeated protobuf conversion,2015-10-14T10:00:43.000+0000,MESOS-3312,2.0,mesos,Mesosphere Sprint 17
anandmazumdar,2015-08-25T22:18:40.000+0000,vinodkone,"Observed on ASF CI

{code}
[ RUN      ] SlaveTest.HTTPSchedulerSlaveRestart
Using temporary directory '/tmp/SlaveTest_HTTPSchedulerSlaveRestart_CXyDrA'
I0825 22:07:36.809872 27610 leveldb.cpp:176] Opened db in 3.751801ms
I0825 22:07:36.811115 27610 leveldb.cpp:183] Compacted db in 1.2194ms
I0825 22:07:36.811175 27610 leveldb.cpp:198] Created db iterator in 30669ns
I0825 22:07:36.811197 27610 leveldb.cpp:204] Seeked to beginning of db in 7829ns
I0825 22:07:36.811208 27610 leveldb.cpp:273] Iterated through 0 keys in the db in 6017ns
I0825 22:07:36.811245 27610 replica.cpp:744] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned
I0825 22:07:36.811722 27638 recover.cpp:449] Starting replica recovery
I0825 22:07:36.811980 27638 recover.cpp:475] Replica is in EMPTY status
I0825 22:07:36.813033 27641 replica.cpp:641] Replica in EMPTY status received a broadcasted recover request
I0825 22:07:36.813355 27635 recover.cpp:195] Received a recover response from a replica in EMPTY status
I0825 22:07:36.813756 27628 recover.cpp:566] Updating replica status to STARTING
I0825 22:07:36.814434 27636 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 570160ns
I0825 22:07:36.814471 27636 replica.cpp:323] Persisted replica status to STARTING
I0825 22:07:36.814743 27642 recover.cpp:475] Replica is in STARTING status
I0825 22:07:36.814965 27638 master.cpp:378] Master 20150825-220736-234885548-51219-27610 (09c6504e3a31) started on 172.17.0.14:51219
I0825 22:07:36.814999 27638 master.cpp:380] Flags at startup: --acls="""" --allocation_interval=""1secs"" --allocator=""HierarchicalDRF"" --authenticate=""true"" --authenticate_slaves=""true"" --authenticators=""crammd5"" --authorizers=""local"" --credentials=""/tmp/SlaveTest_HTTPSchedulerSlaveRestart_CXyDrA/credentials"" --framework_sorter=""drf"" --help=""false"" --initialize_driver_logging=""true"" --log_auto_initialize=""true"" --logbufsecs=""0"" --logging_level=""INFO"" --max_slave_ping_timeouts=""5"" --quiet=""false"" --recovery_slave_removal_limit=""100%"" --registry=""replicated_log"" --registry_fetch_timeout=""1mins"" --registry_store_timeout=""25secs"" --registry_strict=""true"" --root_submissions=""true"" --slave_ping_timeout=""15secs"" --slave_reregister_timeout=""10mins"" --user_sorter=""drf"" --version=""false"" --webui_dir=""/mesos/mesos-0.25.0/_inst/share/mesos/webui"" --work_dir=""/tmp/SlaveTest_HTTPSchedulerSlaveRestart_CXyDrA/master"" --zk_session_timeout=""10secs""
I0825 22:07:36.815347 27638 master.cpp:425] Master only allowing authenticated frameworks to register
I0825 22:07:36.815371 27638 master.cpp:430] Master only allowing authenticated slaves to register
I0825 22:07:36.815402 27638 credentials.hpp:37] Loading credentials for authentication from '/tmp/SlaveTest_HTTPSchedulerSlaveRestart_CXyDrA/credentials'
I0825 22:07:36.815634 27632 replica.cpp:641] Replica in STARTING status received a broadcasted recover request
I0825 22:07:36.815752 27638 master.cpp:469] Using default 'crammd5' authenticator
I0825 22:07:36.815904 27638 master.cpp:506] Authorization enabled
I0825 22:07:36.815979 27643 recover.cpp:195] Received a recover response from a replica in STARTING status
I0825 22:07:36.816185 27637 whitelist_watcher.cpp:79] No whitelist given
I0825 22:07:36.816186 27641 hierarchical.hpp:346] Initialized hierarchical allocator process
I0825 22:07:36.816519 27630 recover.cpp:566] Updating replica status to VOTING
I0825 22:07:36.817258 27639 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 475231ns
I0825 22:07:36.817296 27639 replica.cpp:323] Persisted replica status to VOTING
I0825 22:07:36.817420 27637 master.cpp:1525] The newly elected leader is master@172.17.0.14:51219 with id 20150825-220736-234885548-51219-27610
I0825 22:07:36.817467 27637 master.cpp:1538] Elected as the leading master!
I0825 22:07:36.817483 27637 master.cpp:1308] Recovering from registrar
I0825 22:07:36.817509 27635 recover.cpp:580] Successfully joined the Paxos group
I0825 22:07:36.817708 27633 registrar.cpp:311] Recovering registrar
I0825 22:07:36.817844 27635 recover.cpp:464] Recover process terminated
I0825 22:07:36.818439 27631 log.cpp:661] Attempting to start the writer
I0825 22:07:36.819694 27636 replica.cpp:477] Replica received implicit promise request with proposal 1
I0825 22:07:36.820133 27636 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 421255ns
I0825 22:07:36.820168 27636 replica.cpp:345] Persisted promised to 1
I0825 22:07:36.820804 27630 coordinator.cpp:231] Coordinator attemping to fill missing position
I0825 22:07:36.822105 27638 replica.cpp:378] Replica received explicit promise request for position 0 with proposal 2
I0825 22:07:36.822597 27638 leveldb.cpp:343] Persisting action (8 bytes) to leveldb took 468065ns
I0825 22:07:36.822625 27638 replica.cpp:679] Persisted action at 0
I0825 22:07:36.823737 27637 replica.cpp:511] Replica received write request for position 0
I0825 22:07:36.823796 27637 leveldb.cpp:438] Reading position from leveldb took 39603ns
I0825 22:07:36.824267 27637 leveldb.cpp:343] Persisting action (14 bytes) to leveldb took 446655ns
I0825 22:07:36.824296 27637 replica.cpp:679] Persisted action at 0
I0825 22:07:36.824961 27634 replica.cpp:658] Replica received learned notice for position 0
I0825 22:07:36.825340 27634 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 362236ns
I0825 22:07:36.825369 27634 replica.cpp:679] Persisted action at 0
I0825 22:07:36.825388 27634 replica.cpp:664] Replica learned NOP action at position 0
I0825 22:07:36.825975 27642 log.cpp:677] Writer started with ending position 0
I0825 22:07:36.826997 27628 leveldb.cpp:438] Reading position from leveldb took 56us
I0825 22:07:36.829946 27639 registrar.cpp:344] Successfully fetched the registry (0B) in 12.187136ms
I0825 22:07:36.830077 27639 registrar.cpp:443] Applied 1 operations in 40874ns; attempting to update the 'registry'
I0825 22:07:36.832870 27635 log.cpp:685] Attempting to append 174 bytes to the log
I0825 22:07:36.833088 27641 coordinator.cpp:341] Coordinator attempting to write APPEND action at position 1
I0825 22:07:36.833845 27636 replica.cpp:511] Replica received write request for position 1
I0825 22:07:36.834293 27636 leveldb.cpp:343] Persisting action (193 bytes) to leveldb took 425175ns
I0825 22:07:36.834324 27636 replica.cpp:679] Persisted action at 1
I0825 22:07:36.835077 27643 replica.cpp:658] Replica received learned notice for position 1
I0825 22:07:36.835500 27643 leveldb.cpp:343] Persisting action (195 bytes) to leveldb took 404831ns
I0825 22:07:36.835532 27643 replica.cpp:679] Persisted action at 1
I0825 22:07:36.835574 27643 replica.cpp:664] Replica learned APPEND action at position 1
I0825 22:07:36.836545 27643 registrar.cpp:488] Successfully updated the 'registry' in 6.393088ms
I0825 22:07:36.836707 27643 registrar.cpp:374] Successfully recovered registrar
I0825 22:07:36.836874 27639 log.cpp:704] Attempting to truncate the log to 1
I0825 22:07:36.837174 27632 master.cpp:1335] Recovered 0 slaves from the Registry (135B) ; allowing 10mins for slaves to re-register
I0825 22:07:36.837291 27634 coordinator.cpp:341] Coordinator attempting to write TRUNCATE action at position 2
I0825 22:07:36.838249 27639 replica.cpp:511] Replica received write request for position 2
I0825 22:07:36.838685 27639 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 412214ns
I0825 22:07:36.838716 27639 replica.cpp:679] Persisted action at 2
I0825 22:07:36.839735 27628 replica.cpp:658] Replica received learned notice for position 2
I0825 22:07:36.840304 27628 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 547841ns
I0825 22:07:36.840375 27628 leveldb.cpp:401] Deleting ~1 keys from leveldb took 51256ns
I0825 22:07:36.840401 27628 replica.cpp:679] Persisted action at 2
I0825 22:07:36.840428 27628 replica.cpp:664] Replica learned TRUNCATE action at position 2
I0825 22:07:36.849371 27610 containerizer.cpp:143] Using isolation: posix/cpu,posix/mem,filesystem/posix
I0825 22:07:36.856500 27633 slave.cpp:190] Slave started on 286)@172.17.0.14:51219
I0825 22:07:36.856541 27633 slave.cpp:191] Flags at startup: --appc_store_dir=""/tmp/mesos/store/appc"" --authenticatee=""crammd5"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --credential=""/tmp/SlaveTest_HTTPSchedulerSlaveRestart_ukkA8L/credential"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_kill_orphans=""true"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/tmp/SlaveTest_HTTPSchedulerSlaveRestart_ukkA8L/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --initialize_driver_logging=""true"" --isolation=""posix/cpu,posix/mem"" --launcher_dir=""/mesos/mesos-0.25.0/_build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""10ms"" --resource_monitoring_interval=""1secs"" --resources=""cpus:2;mem:1024;disk:1024;ports:[31000-32000]"" --revocable_cpu_low_priority=""true"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --version=""false"" --work_dir=""/tmp/SlaveTest_HTTPSchedulerSlaveRestart_ukkA8L""
I0825 22:07:36.857074 27633 credentials.hpp:85] Loading credential for authentication from '/tmp/SlaveTest_HTTPSchedulerSlaveRestart_ukkA8L/credential'
I0825 22:07:36.857275 27633 slave.cpp:321] Slave using credential for: test-principal
I0825 22:07:36.857822 27633 slave.cpp:354] Slave resources: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I0825 22:07:36.857936 27633 slave.cpp:384] Slave hostname: 09c6504e3a31
I0825 22:07:36.857959 27633 slave.cpp:389] Slave checkpoint: true
I0825 22:07:36.858886 27637 state.cpp:54] Recovering state from '/tmp/SlaveTest_HTTPSchedulerSlaveRestart_ukkA8L/meta'
I0825 22:07:36.859130 27638 status_update_manager.cpp:202] Recovering status update manager
I0825 22:07:36.859465 27636 containerizer.cpp:379] Recovering containerizer
I0825 22:07:36.860631 27634 slave.cpp:4069] Finished recovery
I0825 22:07:36.861034 27634 slave.cpp:4226] Querying resource estimator for oversubscribable resources
I0825 22:07:36.861239 27643 status_update_manager.cpp:176] Pausing sending status updates
I0825 22:07:36.861240 27634 slave.cpp:684] New master detected at master@172.17.0.14:51219
I0825 22:07:36.861322 27634 slave.cpp:747] Authenticating with master master@172.17.0.14:51219
I0825 22:07:36.861343 27634 slave.cpp:752] Using default CRAM-MD5 authenticatee
I0825 22:07:36.861450 27634 slave.cpp:720] Detecting new master
I0825 22:07:36.861495 27628 authenticatee.cpp:115] Creating new client SASL connection
I0825 22:07:36.861569 27634 slave.cpp:4240] Received oversubscribable resources  from the resource estimator
I0825 22:07:36.861716 27632 master.cpp:4694] Authenticating slave(286)@172.17.0.14:51219
I0825 22:07:36.861799 27629 authenticator.cpp:407] Starting authentication session for crammd5_authenticatee(665)@172.17.0.14:51219
I0825 22:07:36.862045 27642 authenticator.cpp:92] Creating new server SASL connection
I0825 22:07:36.862308 27635 authenticatee.cpp:206] Received SASL authentication mechanisms: CRAM-MD5
I0825 22:07:36.862337 27635 authenticatee.cpp:232] Attempting to authenticate with mechanism 'CRAM-MD5'
I0825 22:07:36.862421 27629 authenticator.cpp:197] Received SASL authentication start
I0825 22:07:36.862478 27629 authenticator.cpp:319] Authentication requires more steps
I0825 22:07:36.862579 27633 authenticatee.cpp:252] Received SASL authentication step
I0825 22:07:36.862679 27628 authenticator.cpp:225] Received SASL authentication step
I0825 22:07:36.862707 27628 auxprop.cpp:102] Request to lookup properties for user: 'test-principal' realm: '09c6504e3a31' server FQDN: '09c6504e3a31' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0825 22:07:36.862717 27628 auxprop.cpp:174] Looking up auxiliary property '*userPassword'
I0825 22:07:36.862754 27628 auxprop.cpp:174] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0825 22:07:36.862785 27628 auxprop.cpp:102] Request to lookup properties for user: 'test-principal' realm: '09c6504e3a31' server FQDN: '09c6504e3a31' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0825 22:07:36.862797 27628 auxprop.cpp:124] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0825 22:07:36.862802 27628 auxprop.cpp:124] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0825 22:07:36.862817 27628 authenticator.cpp:311] Authentication success
I0825 22:07:36.862884 27629 authenticatee.cpp:292] Authentication success
I0825 22:07:36.862921 27630 master.cpp:4724] Successfully authenticated principal 'test-principal' at slave(286)@172.17.0.14:51219
I0825 22:07:36.862969 27642 authenticator.cpp:425] Authentication session cleanup for crammd5_authenticatee(665)@172.17.0.14:51219
I0825 22:07:36.863139 27639 slave.cpp:815] Successfully authenticated with master master@172.17.0.14:51219
I0825 22:07:36.863256 27639 slave.cpp:1209] Will retry registration in 15.028678ms if necessary
I0825 22:07:36.863382 27643 master.cpp:3636] Registering slave at slave(286)@172.17.0.14:51219 (09c6504e3a31) with id 20150825-220736-234885548-51219-27610-S0
I0825 22:07:36.863899 27610 sched.cpp:164] Version: 0.25.0
I0825 22:07:36.863940 27636 registrar.cpp:443] Applied 1 operations in 94492ns; attempting to update the 'registry'
I0825 22:07:36.864670 27632 sched.cpp:262] New master detected at master@172.17.0.14:51219
I0825 22:07:36.864790 27632 sched.cpp:318] Authenticating with master master@172.17.0.14:51219
I0825 22:07:36.864821 27632 sched.cpp:325] Using default CRAM-MD5 authenticatee
I0825 22:07:36.865095 27637 authenticatee.cpp:115] Creating new client SASL connection
I0825 22:07:36.865453 27643 master.cpp:4694] Authenticating scheduler-6c5ddcdb-9dd1-4b38-b051-5f714d3c1c55@172.17.0.14:51219
I0825 22:07:36.865603 27629 authenticator.cpp:407] Starting authentication session for crammd5_authenticatee(666)@172.17.0.14:51219
I0825 22:07:36.865840 27638 authenticator.cpp:92] Creating new server SASL connection
I0825 22:07:36.866217 27630 authenticatee.cpp:206] Received SASL authentication mechanisms: CRAM-MD5
I0825 22:07:36.866260 27630 authenticatee.cpp:232] Attempting to authenticate with mechanism 'CRAM-MD5'
I0825 22:07:36.866433 27639 authenticator.cpp:197] Received SASL authentication start
I0825 22:07:36.866513 27639 authenticator.cpp:319] Authentication requires more steps
I0825 22:07:36.866710 27630 authenticatee.cpp:252] Received SASL authentication step
I0825 22:07:36.866999 27638 authenticator.cpp:225] Received SASL authentication step
I0825 22:07:36.867051 27638 auxprop.cpp:102] Request to lookup properties for user: 'test-principal' realm: '09c6504e3a31' server FQDN: '09c6504e3a31' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0825 22:07:36.867077 27638 auxprop.cpp:174] Looking up auxiliary property '*userPassword'
I0825 22:07:36.867130 27638 auxprop.cpp:174] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0825 22:07:36.867162 27638 auxprop.cpp:102] Request to lookup properties for user: 'test-principal' realm: '09c6504e3a31' server FQDN: '09c6504e3a31' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0825 22:07:36.867175 27638 auxprop.cpp:124] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0825 22:07:36.867183 27638 auxprop.cpp:124] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0825 22:07:36.867202 27638 authenticator.cpp:311] Authentication success
I0825 22:07:36.867426 27636 authenticatee.cpp:292] Authentication success
I0825 22:07:36.867434 27633 authenticator.cpp:425] Authentication session cleanup for crammd5_authenticatee(666)@172.17.0.14:51219
I0825 22:07:36.867627 27630 master.cpp:4724] Successfully authenticated principal 'test-principal' at scheduler-6c5ddcdb-9dd1-4b38-b051-5f714d3c1c55@172.17.0.14:51219
I0825 22:07:36.867951 27641 sched.cpp:407] Successfully authenticated with master master@172.17.0.14:51219
I0825 22:07:36.867986 27641 sched.cpp:713] Sending SUBSCRIBE call to master@172.17.0.14:51219
I0825 22:07:36.868114 27641 sched.cpp:746] Will retry registration in 1.352726078secs if necessary
I0825 22:07:36.868233 27634 log.cpp:685] Attempting to append 344 bytes to the log
I0825 22:07:36.868268 27638 master.cpp:2094] Received SUBSCRIBE call for framework 'default' at scheduler-6c5ddcdb-9dd1-4b38-b051-5f714d3c1c55@172.17.0.14:51219
I0825 22:07:36.868305 27638 master.cpp:1564] Authorizing framework principal 'test-principal' to receive offers for role '*'
I0825 22:07:36.868373 27631 coordinator.cpp:341] Coordinator attempting to write APPEND action at position 3
I0825 22:07:36.868614 27642 master.cpp:2164] Subscribing framework default with checkpointing enabled and capabilities [  ]
I0825 22:07:36.868999 27643 hierarchical.hpp:391] Added framework 20150825-220736-234885548-51219-27610-0000
I0825 22:07:36.869030 27643 hierarchical.hpp:1010] No resources available to allocate!
I0825 22:07:36.869046 27643 hierarchical.hpp:910] Performed allocation for 0 slaves in 34654ns
I0825 22:07:36.869215 27631 sched.cpp:640] Framework registered with 20150825-220736-234885548-51219-27610-0000
I0825 22:07:36.869215 27643 replica.cpp:511] Replica received write request for position 3
I0825 22:07:36.869268 27631 sched.cpp:654] Scheduler::registered took 29976ns
I0825 22:07:36.869453 27643 leveldb.cpp:343] Persisting action (363 bytes) to leveldb took 181689ns
I0825 22:07:36.869477 27643 replica.cpp:679] Persisted action at 3
I0825 22:07:36.870075 27629 replica.cpp:658] Replica received learned notice for position 3
I0825 22:07:36.870542 27629 leveldb.cpp:343] Persisting action (365 bytes) to leveldb took 469081ns
I0825 22:07:36.870589 27629 replica.cpp:679] Persisted action at 3
I0825 22:07:36.870622 27629 replica.cpp:664] Replica learned APPEND action at position 3
I0825 22:07:36.872133 27632 registrar.cpp:488] Successfully updated the 'registry' in 8.113152ms
I0825 22:07:36.872354 27639 log.cpp:704] Attempting to truncate the log to 3
I0825 22:07:36.872470 27632 coordinator.cpp:341] Coordinator attempting to write TRUNCATE action at position 4
I0825 22:07:36.872879 27637 slave.cpp:3058] Received ping from slave-observer(274)@172.17.0.14:51219
I0825 22:07:36.873015 27636 master.cpp:3699] Registered slave 20150825-220736-234885548-51219-27610-S0 at slave(286)@172.17.0.14:51219 (09c6504e3a31) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I0825 22:07:36.873180 27637 slave.cpp:859] Registered with master master@172.17.0.14:51219; given slave ID 20150825-220736-234885548-51219-27610-S0
I0825 22:07:36.873219 27637 fetcher.cpp:77] Clearing fetcher cache
I0825 22:07:36.873410 27634 status_update_manager.cpp:183] Resuming sending status updates
I0825 22:07:36.873379 27628 hierarchical.hpp:542] Added slave 20150825-220736-234885548-51219-27610-S0 (09c6504e3a31) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (allocated: )
I0825 22:07:36.873482 27642 replica.cpp:511] Replica received write request for position 4
I0825 22:07:36.873661 27637 slave.cpp:882] Checkpointing SlaveInfo to '/tmp/SlaveTest_HTTPSchedulerSlaveRestart_ukkA8L/meta/slaves/20150825-220736-234885548-51219-27610-S0/slave.info'
I0825 22:07:36.874042 27642 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 538208ns
I0825 22:07:36.874078 27642 replica.cpp:679] Persisted action at 4
I0825 22:07:36.874196 27628 hierarchical.hpp:928] Performed allocation for slave 20150825-220736-234885548-51219-27610-S0 in 739900ns
I0825 22:07:36.874204 27637 slave.cpp:918] Forwarding total oversubscribed resources 
I0825 22:07:36.874824 27635 master.cpp:4613] Sending 1 offers to framework 20150825-220736-234885548-51219-27610-0000 (default) at scheduler-6c5ddcdb-9dd1-4b38-b051-5f714d3c1c55@172.17.0.14:51219
I0825 22:07:36.874958 27639 replica.cpp:658] Replica received learned notice for position 4
I0825 22:07:36.875074 27635 master.cpp:3998] Received update of slave 20150825-220736-234885548-51219-27610-S0 at slave(286)@172.17.0.14:51219 (09c6504e3a31) with total oversubscribed resources 
I0825 22:07:36.875485 27636 sched.cpp:803] Scheduler::resourceOffers took 243089ns
I0825 22:07:36.875450 27638 hierarchical.hpp:602] Slave 20150825-220736-234885548-51219-27610-S0 (09c6504e3a31) updated with oversubscribed resources  (total: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000], allocated: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000])
I0825 22:07:36.875495 27639 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 462264ns
I0825 22:07:36.875643 27639 leveldb.cpp:401] Deleting ~2 keys from leveldb took 109856ns
I0825 22:07:36.875682 27639 replica.cpp:679] Persisted action at 4
I0825 22:07:36.875717 27639 replica.cpp:664] Replica learned TRUNCATE action at position 4
I0825 22:07:36.876045 27638 hierarchical.hpp:1010] No resources available to allocate!
I0825 22:07:36.876072 27638 hierarchical.hpp:928] Performed allocation for slave 20150825-220736-234885548-51219-27610-S0 in 541099ns
I0825 22:07:36.879416 27639 master.cpp:2739] Processing ACCEPT call for offers: [ 20150825-220736-234885548-51219-27610-O0 ] on slave 20150825-220736-234885548-51219-27610-S0 at slave(286)@172.17.0.14:51219 (09c6504e3a31) for framework 20150825-220736-234885548-51219-27610-0000 (default) at scheduler-6c5ddcdb-9dd1-4b38-b051-5f714d3c1c55@172.17.0.14:51219
I0825 22:07:36.879475 27639 master.cpp:2570] Authorizing framework principal 'test-principal' to launch task b89d1df8-f2fb-44be-8f60-9352cf32a79d as user 'mesos'
I0825 22:07:36.880975 27639 master.hpp:170] Adding task b89d1df8-f2fb-44be-8f60-9352cf32a79d with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave 20150825-220736-234885548-51219-27610-S0 (09c6504e3a31)
I0825 22:07:36.881124 27639 master.cpp:3069] Launching task b89d1df8-f2fb-44be-8f60-9352cf32a79d of framework 20150825-220736-234885548-51219-27610-0000 (default) at scheduler-6c5ddcdb-9dd1-4b38-b051-5f714d3c1c55@172.17.0.14:51219 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave 20150825-220736-234885548-51219-27610-S0 at slave(286)@172.17.0.14:51219 (09c6504e3a31)
I0825 22:07:36.882314 27636 slave.cpp:1249] Got assigned task b89d1df8-f2fb-44be-8f60-9352cf32a79d for framework 20150825-220736-234885548-51219-27610-0000
I0825 22:07:36.882470 27636 slave.cpp:4720] Checkpointing FrameworkInfo to '/tmp/SlaveTest_HTTPSchedulerSlaveRestart_ukkA8L/meta/slaves/20150825-220736-234885548-51219-27610-S0/frameworks/20150825-220736-234885548-51219-27610-0000/framework.info'
I0825 22:07:36.882984 27636 slave.cpp:4731] Checkpointing framework pid '@0.0.0.0:0' to '/tmp/SlaveTest_HTTPSchedulerSlaveRestart_ukkA8L/meta/slaves/20150825-220736-234885548-51219-27610-S0/frameworks/20150825-220736-234885548-51219-27610-0000/framework.pid'
I0825 22:07:36.884068 27636 slave.cpp:1365] Launching task b89d1df8-f2fb-44be-8f60-9352cf32a79d for framework 20150825-220736-234885548-51219-27610-0000
I0825 22:07:36.895586 27636 slave.cpp:5156] Checkpointing ExecutorInfo to '/tmp/SlaveTest_HTTPSchedulerSlaveRestart_ukkA8L/meta/slaves/20150825-220736-234885548-51219-27610-S0/frameworks/20150825-220736-234885548-51219-27610-0000/executors/b89d1df8-f2fb-44be-8f60-9352cf32a79d/executor.info'
I0825 22:07:36.896765 27636 slave.cpp:4799] Launching executor b89d1df8-f2fb-44be-8f60-9352cf32a79d of framework 20150825-220736-234885548-51219-27610-0000 with resources cpus(*):0.1; mem(*):32 in work directory '/tmp/SlaveTest_HTTPSchedulerSlaveRestart_ukkA8L/slaves/20150825-220736-234885548-51219-27610-S0/frameworks/20150825-220736-234885548-51219-27610-0000/executors/b89d1df8-f2fb-44be-8f60-9352cf32a79d/runs/1499299a-93dd-4982-9249-ad0e19d1c06c'
I0825 22:07:36.897374 27643 containerizer.cpp:633] Starting container '1499299a-93dd-4982-9249-ad0e19d1c06c' for executor 'b89d1df8-f2fb-44be-8f60-9352cf32a79d' of framework '20150825-220736-234885548-51219-27610-0000'
I0825 22:07:36.897414 27636 slave.cpp:5179] Checkpointing TaskInfo to '/tmp/SlaveTest_HTTPSchedulerSlaveRestart_ukkA8L/meta/slaves/20150825-220736-234885548-51219-27610-S0/frameworks/20150825-220736-234885548-51219-27610-0000/executors/b89d1df8-f2fb-44be-8f60-9352cf32a79d/runs/1499299a-93dd-4982-9249-ad0e19d1c06c/tasks/b89d1df8-f2fb-44be-8f60-9352cf32a79d/task.info'
I0825 22:07:36.897974 27636 slave.cpp:1583] Queuing task 'b89d1df8-f2fb-44be-8f60-9352cf32a79d' for executor b89d1df8-f2fb-44be-8f60-9352cf32a79d of framework '20150825-220736-234885548-51219-27610-0000
I0825 22:07:36.898123 27636 slave.cpp:637] Successfully attached file '/tmp/SlaveTest_HTTPSchedulerSlaveRestart_ukkA8L/slaves/20150825-220736-234885548-51219-27610-S0/frameworks/20150825-220736-234885548-51219-27610-0000/executors/b89d1df8-f2fb-44be-8f60-9352cf32a79d/runs/1499299a-93dd-4982-9249-ad0e19d1c06c'
I0825 22:07:36.902439 27641 launcher.cpp:131] Forked child with pid '2326' for container '1499299a-93dd-4982-9249-ad0e19d1c06c'
I0825 22:07:36.902752 27641 containerizer.cpp:855] Checkpointing executor's forked pid 2326 to '/tmp/SlaveTest_HTTPSchedulerSlaveRestart_ukkA8L/meta/slaves/20150825-220736-234885548-51219-27610-S0/frameworks/20150825-220736-234885548-51219-27610-0000/executors/b89d1df8-f2fb-44be-8f60-9352cf32a79d/runs/1499299a-93dd-4982-9249-ad0e19d1c06c/pids/forked.pid'
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0825 22:07:37.029348  2340 process.cpp:1012] libprocess is initialized on 172.17.0.14:42774 for 16 cpus
I0825 22:07:37.030342  2340 logging.cpp:177] Logging to STDERR
I0825 22:07:37.032822  2340 exec.cpp:133] Version: 0.25.0
I0825 22:07:37.038837  2355 exec.cpp:183] Executor started at: executor(1)@172.17.0.14:42774 with pid 2340
I0825 22:07:37.041252 27638 slave.cpp:2358] Got registration for executor 'b89d1df8-f2fb-44be-8f60-9352cf32a79d' of framework 20150825-220736-234885548-51219-27610-0000 from executor(1)@172.17.0.14:42774
I0825 22:07:37.041371 27638 slave.cpp:2444] Checkpointing executor pid 'executor(1)@172.17.0.14:42774' to '/tmp/SlaveTest_HTTPSchedulerSlaveRestart_ukkA8L/meta/slaves/20150825-220736-234885548-51219-27610-S0/frameworks/20150825-220736-234885548-51219-27610-0000/executors/b89d1df8-f2fb-44be-8f60-9352cf32a79d/runs/1499299a-93dd-4982-9249-ad0e19d1c06c/pids/libprocess.pid'
I0825 22:07:37.044067 27634 slave.cpp:1739] Sending queued task 'b89d1df8-f2fb-44be-8f60-9352cf32a79d' to executor 'b89d1df8-f2fb-44be-8f60-9352cf32a79d' of framework 20150825-220736-234885548-51219-27610-0000
I0825 22:07:37.044256  2358 exec.cpp:207] Executor registered on slave 20150825-220736-234885548-51219-27610-S0
I0825 22:07:37.046058  2358 exec.cpp:219] Executor::registered took 239083ns
Registered executor on 09c6504e3a31
Starting task b89d1df8-f2fb-44be-8f60-9352cf32a79d
I0825 22:07:37.046394  2358 exec.cpp:294] Executor asked to run task 'b89d1df8-f2fb-44be-8f60-9352cf32a79d'
I0825 22:07:37.046493  2358 exec.cpp:303] Executor::launchTask took 84034ns
sh -c 'sleep 1000'
Forked command at 2371
I0825 22:07:37.049942  2366 exec.cpp:516] Executor sending status update TASK_RUNNING (UUID: 98c4a799-ad82-497d-be1e-6dfb56a0894e) for task b89d1df8-f2fb-44be-8f60-9352cf32a79d of framework 20150825-220736-234885548-51219-27610-0000
I0825 22:07:37.050977 27635 slave.cpp:2696] Handling status update TASK_RUNNING (UUID: 98c4a799-ad82-497d-be1e-6dfb56a0894e) for task b89d1df8-f2fb-44be-8f60-9352cf32a79d of framework 20150825-220736-234885548-51219-27610-0000 from executor(1)@172.17.0.14:42774
I0825 22:07:37.051316 27632 status_update_manager.cpp:322] Received status update TASK_RUNNING (UUID: 98c4a799-ad82-497d-be1e-6dfb56a0894e) for task b89d1df8-f2fb-44be-8f60-9352cf32a79d of framework 20150825-220736-234885548-51219-27610-0000
I0825 22:07:37.051379 27632 status_update_manager.cpp:499] Creating StatusUpdate stream for task b89d1df8-f2fb-44be-8f60-9352cf32a79d of framework 20150825-220736-234885548-51219-27610-0000
I0825 22:07:37.052251 27632 status_update_manager.cpp:826] Checkpointing UPDATE for status update TASK_RUNNING (UUID: 98c4a799-ad82-497d-be1e-6dfb56a0894e) for task b89d1df8-f2fb-44be-8f60-9352cf32a79d of framework 20150825-220736-234885548-51219-27610-0000
I0825 22:07:37.053840 27632 status_update_manager.cpp:376] Forwarding update TASK_RUNNING (UUID: 98c4a799-ad82-497d-be1e-6dfb56a0894e) for task b89d1df8-f2fb-44be-8f60-9352cf32a79d of framework 20150825-220736-234885548-51219-27610-0000 to the slave
I0825 22:07:37.054127 27642 slave.cpp:2975] Forwarding the update TASK_RUNNING (UUID: 98c4a799-ad82-497d-be1e-6dfb56a0894e) for task b89d1df8-f2fb-44be-8f60-9352cf32a79d of framework 20150825-220736-234885548-51219-27610-0000 to master@172.17.0.14:51219
I0825 22:07:37.054364 27642 slave.cpp:2899] Status update manager successfully handled status update TASK_RUNNING (UUID: 98c4a799-ad82-497d-be1e-6dfb56a0894e) for task b89d1df8-f2fb-44be-8f60-9352cf32a79d of framework 20150825-220736-234885548-51219-27610-0000
I0825 22:07:37.054407 27642 slave.cpp:2905] Sending acknowledgement for status update TASK_RUNNING (UUID: 98c4a799-ad82-497d-be1e-6dfb56a0894e) for task b89d1df8-f2fb-44be-8f60-9352cf32a79d of framework 20150825-220736-234885548-51219-27610-0000 to executor(1)@172.17.0.14:42774
I0825 22:07:37.054469 27635 master.cpp:4069] Status update TASK_RUNNING (UUID: 98c4a799-ad82-497d-be1e-6dfb56a0894e) for task b89d1df8-f2fb-44be-8f60-9352cf32a79d of framework 20150825-220736-234885548-51219-27610-0000 from slave 20150825-220736-234885548-51219-27610-S0 at slave(286)@172.17.0.14:51219 (09c6504e3a31)
I0825 22:07:37.054519 27635 master.cpp:4108] Forwarding status update TASK_RUNNING (UUID: 98c4a799-ad82-497d-be1e-6dfb56a0894e) for task b89d1df8-f2fb-44be-8f60-9352cf32a79d of framework 20150825-220736-234885548-51219-27610-0000
I0825 22:07:37.054743 27635 master.cpp:5576] Updating the latest state of task b89d1df8-f2fb-44be-8f60-9352cf32a79d of framework 20150825-220736-234885548-51219-27610-0000 to TASK_RUNNING
I0825 22:07:37.055011 27641 sched.cpp:910] Scheduler::statusUpdate took 169426ns
I0825 22:07:37.055639 27634 master.cpp:3398] Processing ACKNOWLEDGE call 98c4a799-ad82-497d-be1e-6dfb56a0894e for task b89d1df8-f2fb-44be-8f60-9352cf32a79d of framework 20150825-220736-234885548-51219-27610-0000 (default) at scheduler-6c5ddcdb-9dd1-4b38-b051-5f714d3c1c55@172.17.0.14:51219 on slave 20150825-220736-234885548-51219-27610-S0
I0825 22:07:37.055665  2359 exec.cpp:340] Executor received status update acknowledgement 98c4a799-ad82-497d-be1e-6dfb56a0894e for task b89d1df8-f2fb-44be-8f60-9352cf32a79d of framework 20150825-220736-234885548-51219-27610-0000
I0825 22:07:37.055886 27640 slave.cpp:564] Slave terminating
I0825 22:07:37.056210 27634 master.cpp:1012] Slave 20150825-220736-234885548-51219-27610-S0 at slave(286)@172.17.0.14:51219 (09c6504e3a31) disconnected
I0825 22:07:37.056257 27634 master.cpp:2415] Disconnecting slave 20150825-220736-234885548-51219-27610-S0 at slave(286)@172.17.0.14:51219 (09c6504e3a31)
I0825 22:07:37.056339 27634 master.cpp:2434] Deactivating slave 20150825-220736-234885548-51219-27610-S0 at slave(286)@172.17.0.14:51219 (09c6504e3a31)
I0825 22:07:37.056675 27643 hierarchical.hpp:635] Slave 20150825-220736-234885548-51219-27610-S0 deactivated
I0825 22:07:37.059391 27610 containerizer.cpp:143] Using isolation: posix/cpu,posix/mem,filesystem/posix
I0825 22:07:37.066619 27641 slave.cpp:190] Slave started on 287)@172.17.0.14:51219
I0825 22:07:37.066668 27641 slave.cpp:191] Flags at startup: --appc_store_dir=""/tmp/mesos/store/appc"" --authenticatee=""crammd5"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --credential=""/tmp/SlaveTest_HTTPSchedulerSlaveRestart_ukkA8L/credential"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_kill_orphans=""true"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/tmp/SlaveTest_HTTPSchedulerSlaveRestart_ukkA8L/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --initialize_driver_logging=""true"" --isolation=""posix/cpu,posix/mem"" --launcher_dir=""/mesos/mesos-0.25.0/_build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""10ms"" --resource_monitoring_interval=""1secs"" --resources=""cpus:2;mem:1024;disk:1024;ports:[31000-32000]"" --revocable_cpu_low_priority=""true"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --version=""false"" --work_dir=""/tmp/SlaveTest_HTTPSchedulerSlaveRestart_ukkA8L""
I0825 22:07:37.067343 27641 credentials.hpp:85] Loading credential for authentication from '/tmp/SlaveTest_HTTPSchedulerSlaveRestart_ukkA8L/credential'
I0825 22:07:37.067643 27641 slave.cpp:321] Slave using credential for: test-principal
I0825 22:07:37.068413 27641 slave.cpp:354] Slave resources: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I0825 22:07:37.068580 27641 slave.cpp:384] Slave hostname: 09c6504e3a31
I0825 22:07:37.068613 27641 slave.cpp:389] Slave checkpoint: true
I0825 22:07:37.069970 27636 state.cpp:54] Recovering state from '/tmp/SlaveTest_HTTPSchedulerSlaveRestart_ukkA8L/meta'
I0825 22:07:37.070089 27636 state.cpp:690] Failed to find resources file '/tmp/SlaveTest_HTTPSchedulerSlaveRestart_ukkA8L/meta/resources/resources.info'
I0825 22:07:37.075319 27628 fetcher.cpp:77] Clearing fetcher cache
I0825 22:07:37.075393 27628 slave.cpp:4157] Recovering framework 20150825-220736-234885548-51219-27610-0000
I0825 22:07:37.075475 27628 slave.cpp:4908] Recovering executor 'b89d1df8-f2fb-44be-8f60-9352cf32a79d' of framework 20150825-220736-234885548-51219-27610-0000
I0825 22:07:37.076370 27641 status_update_manager.cpp:202] Recovering status update manager
I0825 22:07:37.076409 27641 status_update_manager.cpp:210] Recovering executor 'b89d1df8-f2fb-44be-8f60-9352cf32a79d' of framework 20150825-220736-234885548-51219-27610-0000
I0825 22:07:37.076504 27641 status_update_manager.cpp:499] Creating StatusUpdate stream for task b89d1df8-f2fb-44be-8f60-9352cf32a79d of framework 20150825-220736-234885548-51219-27610-0000
I0825 22:07:37.077056 27641 status_update_manager.cpp:802] Replaying status update stream for task b89d1df8-f2fb-44be-8f60-9352cf32a79d
I0825 22:07:37.077715 27628 slave.cpp:637] Successfully attached file '/tmp/SlaveTest_HTTPSchedulerSlaveRestart_ukkA8L/slaves/20150825-220736-234885548-51219-27610-S0/frameworks/20150825-220736-234885548-51219-27610-0000/executors/b89d1df8-f2fb-44be-8f60-9352cf32a79d/runs/1499299a-93dd-4982-9249-ad0e19d1c06c'
I0825 22:07:37.078111 27634 containerizer.cpp:379] Recovering containerizer
I0825 22:07:37.078229 27634 containerizer.cpp:434] Recovering container '1499299a-93dd-4982-9249-ad0e19d1c06c' for executor 'b89d1df8-f2fb-44be-8f60-9352cf32a79d' of framework 20150825-220736-234885548-51219-27610-0000
I0825 22:07:37.079934 27640 slave.cpp:4010] Sending reconnect request to executor b89d1df8-f2fb-44be-8f60-9352cf32a79d of framework 20150825-220736-234885548-51219-27610-0000 at executor(1)@172.17.0.14:42774
I0825 22:07:37.081012  2354 exec.cpp:253] Received reconnect request from slave 20150825-220736-234885548-51219-27610-S0
I0825 22:07:37.081893 27631 slave.cpp:2508] Re-registering executor b89d1df8-f2fb-44be-8f60-9352cf32a79d of framework 20150825-220736-234885548-51219-27610-0000
I0825 22:07:37.082904  2362 exec.cpp:230] Executor re-registered on slave 20150825-220736-234885548-51219-27610-S0
Re-registered executor on 09c6504e3a31
I0825 22:07:37.084738  2362 exec.cpp:242] Executor::reregistered took 119419ns
I0825 22:07:37.816828 27634 hierarchical.hpp:1010] No resources available to allocate!
I0825 22:07:37.816884 27634 hierarchical.hpp:910] Performed allocation for 1 slaves in 129850ns
I0825 22:07:38.817526 27629 hierarchical.hpp:1010] No resources available to allocate!
I0825 22:07:38.817607 27629 hierarchical.hpp:910] Performed allocation for 1 slaves in 152923ns
I0825 22:07:39.081434 27637 slave.cpp:2645] Cleaning up un-reregistered executors
I0825 22:07:39.081596 27637 slave.cpp:4069] Finished recovery
I0825 22:07:39.082165 27637 slave.cpp:4226] Querying resource estimator for oversubscribable resources
I0825 22:07:39.082417 27637 status_update_manager.cpp:176] Pausing sending status updates
I0825 22:07:39.082442 27643 slave.cpp:684] New master detected at master@172.17.0.14:51219
I0825 22:07:39.082602 27643 slave.cpp:747] Authenticating with master master@172.17.0.14:51219
I0825 22:07:39.082628 27643 slave.cpp:752] Using default CRAM-MD5 authenticatee
I0825 22:07:39.082830 27643 slave.cpp:720] Detecting new master
I0825 22:07:39.082919 27638 authenticatee.cpp:115] Creating new client SASL connection
I0825 22:07:39.082973 27643 slave.cpp:4240] Received oversubscribable resources  from the resource estimator
I0825 22:07:39.083277 27631 master.cpp:4694] Authenticating slave(287)@172.17.0.14:51219
I0825 22:07:39.083427 27635 authenticator.cpp:407] Starting authentication session for crammd5_authenticatee(667)@172.17.0.14:51219
I0825 22:07:39.083731 27630 authenticator.cpp:92] Creating new server SASL connection
I0825 22:07:39.083982 27634 authenticatee.cpp:206] Received SASL authentication mechanisms: CRAM-MD5
I0825 22:07:39.084025 27634 authenticatee.cpp:232] Attempting to authenticate with mechanism 'CRAM-MD5'
I0825 22:07:39.084106 27634 authenticator.cpp:197] Received SASL authentication start
I0825 22:07:39.084168 27634 authenticator.cpp:319] Authentication requires more steps
I0825 22:07:39.084300 27639 authenticatee.cpp:252] Received SASL authentication step
I0825 22:07:39.084527 27628 authenticator.cpp:225] Received SASL authentication step
I0825 22:07:39.084625 27628 auxprop.cpp:102] Request to lookup properties for user: 'test-principal' realm: '09c6504e3a31' server FQDN: '09c6504e3a31' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0825 22:07:39.084650 27628 auxprop.cpp:174] Looking up auxiliary property '*userPassword'
I0825 22:07:39.084709 27628 auxprop.cpp:174] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0825 22:07:39.084738 27628 auxprop.cpp:102] Request to lookup properties for user: 'test-principal' realm: '09c6504e3a31' server FQDN: '09c6504e3a31' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0825 22:07:39.084750 27628 auxprop.cpp:124] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0825 22:07:39.084763 27628 auxprop.cpp:124] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0825 22:07:39.084780 27628 authenticator.cpp:311] Authentication success
I0825 22:07:39.084905 27642 authenticatee.cpp:292] Authentication success
I0825 22:07:39.085000 27637 master.cpp:4724] Successfully authenticated principal 'test-principal' at slave(287)@172.17.0.14:51219
I0825 22:07:39.085234 27642 slave.cpp:815] Successfully authenticated with master master@172.17.0.14:51219
I0825 22:07:39.085610 27642 slave.cpp:1209] Will retry registration in 6.014445ms if necessary
I0825 22:07:39.085907 27643 authenticator.cpp:425] Authentication session cleanup for crammd5_authenticatee(667)@172.17.0.14:51219
I0825 22:07:39.092914 27640 master.cpp:3773] Re-registering slave 20150825-220736-234885548-51219-27610-S0 at slave(286)@172.17.0.14:51219 (09c6504e3a31)
I0825 22:07:39.093181 27630 slave.cpp:1209] Will retry registration in 20.588077ms if necessary
I0825 22:07:39.093858 27635 slave.cpp:959] Re-registered with master master@172.17.0.14:51219
I0825 22:07:39.093879 27638 hierarchical.hpp:621] Slave 20150825-220736-234885548-51219-27610-S0 reactivated
I0825 22:07:39.093855 27640 master.cpp:3936] Sending updated checkpointed resources  to slave 20150825-220736-234885548-51219-27610-S0 at slave(287)@172.17.0.14:51219 (09c6504e3a31)
I0825 22:07:39.094110 27631 status_update_manager.cpp:183] Resuming sending status updates
I0825 22:07:39.094130 27635 slave.cpp:995] Forwarding total oversubscribed resources 
W0825 22:07:39.094172 27631 status_update_manager.cpp:190] Resending status update TASK_RUNNING (UUID: 98c4a799-ad82-497d-be1e-6dfb56a0894e) for task b89d1df8-f2fb-44be-8f60-9352cf32a79d of framework 20150825-220736-234885548-51219-27610-0000
I0825 22:07:39.094211 27631 status_update_manager.cpp:376] Forwarding update TASK_RUNNING (UUID: 98c4a799-ad82-497d-be1e-6dfb56a0894e) for task b89d1df8-f2fb-44be-8f60-9352cf32a79d of framework 20150825-220736-234885548-51219-27610-0000 to the slave
I0825 22:07:39.094435 27640 master.cpp:3773] Re-registering slave 20150825-220736-234885548-51219-27610-S0 at slave(287)@172.17.0.14:51219 (09c6504e3a31)
I0825 22:07:39.094602 27635 slave.cpp:2227] Updated checkpointed resources from  to 
I0825 22:07:39.095346 27640 master.cpp:3936] Sending updated checkpointed resources  to slave 20150825-220736-234885548-51219-27610-S0 at slave(287)@172.17.0.14:51219 (09c6504e3a31)
I0825 22:07:39.095775 27635 slave.cpp:2975] Forwarding the update TASK_RUNNING (UUID: 98c4a799-ad82-497d-be1e-6dfb56a0894e) for task b89d1df8-f2fb-44be-8f60-9352cf32a79d of framework 20150825-220736-234885548-51219-27610-0000 to master@172.17.0.14:51219
I0825 22:07:39.095803 27640 master.cpp:3998] Received update of slave 20150825-220736-234885548-51219-27610-S0 at slave(287)@172.17.0.14:51219 (09c6504e3a31) with total oversubscribed resources 
I0825 22:07:39.096372 27635 slave.cpp:2131] Updating framework 20150825-220736-234885548-51219-27610-0000 pid to @0.0.0.0:0
I0825 22:07:39.096467 27635 slave.cpp:2147] Checkpointing framework pid '@0.0.0.0:0' to '/tmp/SlaveTest_HTTPSchedulerSlaveRestart_ukkA8L/meta/slaves/20150825-220736-234885548-51219-27610-S0/frameworks/20150825-220736-234885548-51219-27610-0000/framework.pid'
I0825 22:07:39.096544 27640 hierarchical.hpp:602] Slave 20150825-220736-234885548-51219-27610-S0 (09c6504e3a31) updated with oversubscribed resources  (total: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000], allocated: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000])
I0825 22:07:39.096652 27639 master.cpp:4069] Status update TASK_RUNNING (UUID: 98c4a799-ad82-497d-be1e-6dfb56a0894e) for task b89d1df8-f2fb-44be-8f60-9352cf32a79d of framework 20150825-220736-234885548-51219-27610-0000 from slave 20150825-220736-234885548-51219-27610-S0 at slave(287)@172.17.0.14:51219 (09c6504e3a31)
I0825 22:07:39.096709 27639 master.cpp:4108] Forwarding status update TASK_RUNNING (UUID: 98c4a799-ad82-497d-be1e-6dfb56a0894e) for task b89d1df8-f2fb-44be-8f60-9352cf32a79d of framework 20150825-220736-234885548-51219-27610-0000
I0825 22:07:39.096978 27639 master.cpp:5576] Updating the latest state of task b89d1df8-f2fb-44be-8f60-9352cf32a79d of framework 20150825-220736-234885548-51219-27610-0000 to TASK_RUNNING
I0825 22:07:39.097105 27639 status_update_manager.cpp:183] Resuming sending status updates
W0825 22:07:39.097187 27635 slave.cpp:976] Already re-registered with master master@172.17.0.14:51219
I0825 22:07:39.097229 27635 slave.cpp:995] Forwarding total oversubscribed resources 
W0825 22:07:39.097230 27639 status_update_manager.cpp:190] Resending status update TASK_RUNNING (UUID: 98c4a799-ad82-497d-be1e-6dfb56a0894e) for task b89d1df8-f2fb-44be-8f60-9352cf32a79d of framework 20150825-220736-234885548-51219-27610-0000
I0825 22:07:39.097290 27639 status_update_manager.cpp:376] Forwarding update TASK_RUNNING (UUID: 98c4a799-ad82-497d-be1e-6dfb56a0894e) for task b89d1df8-f2fb-44be-8f60-9352cf32a79d of framework 20150825-220736-234885548-51219-27610-0000 to the slave
I0825 22:07:39.097373 27643 sched.cpp:910] Scheduler::statusUpdate took 76470ns
I0825 22:07:39.097450 27635 slave.cpp:2131] Updating framework 20150825-220736-234885548-51219-27610-0000 pid to scheduler-6c5ddcdb-9dd1-4b38-b051-5f714d3c1c55@172.17.0.14:51219
I0825 22:07:39.097473 27640 hierarchical.hpp:1010] No resources available to allocate!
I0825 22:07:39.097497 27640 hierarchical.hpp:928] Performed allocation for slave 20150825-220736-234885548-51219-27610-S0 in 818746ns
I0825 22:07:39.097525 27635 slave.cpp:2147] Checkpointing framework pid 'scheduler-6c5ddcdb-9dd1-4b38-b051-5f714d3c1c55@172.17.0.14:51219' to '/tmp/SlaveTest_HTTPSchedulerSlaveRestart_ukkA8L/meta/slaves/20150825-220736-234885548-51219-27610-S0/frameworks/20150825-220736-234885548-51219-27610-0000/framework.pid'
I0825 22:07:39.097991 27640 status_update_manager.cpp:183] Resuming sending status updates
W0825 22:07:39.098043 27640 status_update_manager.cpp:190] Resending status update TASK_RUNNING (UUID: 98c4a799-ad82-497d-be1e-6dfb56a0894e) for task b89d1df8-f2fb-44be-8f60-9352cf32a79d of framework 20150825-220736-234885548-51219-27610-0000
I0825 22:07:39.098093 27640 status_update_manager.cpp:376] Forwarding update TASK_RUNNING (UUID: 98c4a799-ad82-497d-be1e-6dfb56a0894e) for task b89d1df8-f2fb-44be-8f60-9352cf32a79d of framework 20150825-220736-234885548-51219-27610-0000 to the slave
I0825 22:07:39.098242 27635 slave.cpp:2227] Updated checkpointed resources from  to 
I0825 22:07:39.098433 27635 slave.cpp:3043] Sending message for framework 20150825-220736-234885548-51219-27610-0000 to scheduler-6c5ddcdb-9dd1-4b38-b051-5f714d3c1c55@172.17.0.14:51219
I0825 22:07:39.098480 27636 master.cpp:3998] Received update of slave 20150825-220736-234885548-51219-27610-S0 at slave(287)@172.17.0.14:51219 (09c6504e3a31) with total oversubscribed resources 
I0825 22:07:39.098639 27635 slave.cpp:2975] Forwarding the update TASK_RUNNING (UUID: 98c4a799-ad82-497d-be1e-6dfb56a0894e) for task b89d1df8-f2fb-44be-8f60-9352cf32a79d of framework 20150825-220736-234885548-51219-27610-0000 to master@172.17.0.14:51219
I0825 22:07:39.098755 27634 sched.cpp:1006] Scheduler::frameworkMessage took 68683ns
I0825 22:07:39.098882 27636 master.cpp:3398] Processing ACKNOWLEDGE call 98c4a799-ad82-497d-be1e-6dfb56a0894e for task b89d1df8-f2fb-44be-8f60-9352cf32a79d of framework 20150825-220736-234885548-51219-27610-0000 (default) at scheduler-6c5ddcdb-9dd1-4b38-b051-5f714d3c1c55@172.17.0.14:51219 on slave 20150825-220736-234885548-51219-27610-S0
I0825 22:07:39.098906 27635 slave.cpp:2975] Forwarding the update TASK_RUNNING (UUID: 98c4a799-ad82-497d-be1e-6dfb56a0894e) for task b89d1df8-f2fb-44be-8f60-9352cf32a79d of framework 20150825-220736-234885548-51219-27610-0000 to master@172.17.0.14:51219
I0825 22:07:39.099019 27641 hierarchical.hpp:602] Slave 20150825-220736-234885548-51219-27610-S0 (09c6504e3a31) updated with oversubscribed resources  (total: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000], allocated: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000])
I0825 22:07:39.099192 27636 master.cpp:4069] Status update TASK_RUNNING (UUID: 98c4a799-ad82-497d-be1e-6dfb56a0894e) for task b89d1df8-f2fb-44be-8f60-9352cf32a79d of framework 20150825-220736-234885548-51219-27610-0000 from slave 20150825-220736-234885548-51219-27610-S0 at slave(287)@172.17.0.14:51219 (09c6504e3a31)
I0825 22:07:39.099244 27636 master.cpp:4108] Forwarding status update TASK_RUNNING (UUID: 98c4a799-ad82-497d-be1e-6dfb56a0894e) for task b89d1df8-f2fb-44be-8f60-9352cf32a79d of framework 20150825-220736-234885548-51219-27610-0000
I0825 22:07:39.099369 27641 hierarchical.hpp:1010] No resources available to allocate!
I0825 22:07:39.099395 27641 hierarchical.hpp:928] Performed allocation for slave 20150825-220736-234885548-51219-27610-S0 in 332336ns
I0825 22:07:39.099403 27636 master.cpp:5576] Updating the latest state of task b89d1df8-f2fb-44be-8f60-9352cf32a79d of framework 20150825-220736-234885548-51219-27610-0000 to TASK_RUNNING
I0825 22:07:39.099426 27635 status_update_manager.cpp:394] Received status update acknowledgement (UUID: 98c4a799-ad82-497d-be1e-6dfb56a0894e) for task b89d1df8-f2fb-44be-8f60-9352cf32a79d of framework 20150825-220736-234885548-51219-27610-0000
I0825 22:07:39.099609 27641 sched.cpp:910] Scheduler::statusUpdate took 90272ns
I0825 22:07:39.099617 27636 master.cpp:4069] Status update TASK_RUNNING (UUID: 98c4a799-ad82-497d-be1e-6dfb56a0894e) for task b89d1df8-f2fb-44be-8f60-9352cf32a79d of framework 20150825-220736-234885548-51219-27610-0000 from slave 20150825-220736-234885548-51219-27610-S0 at slave(287)@172.17.0.14:51219 (09c6504e3a31)
I0825 22:07:39.099669 27636 master.cpp:4108] Forwarding status update TASK_RUNNING (UUID: 98c4a799-ad82-497d-be1e-6dfb56a0894e) for task b89d1df8-f2fb-44be-8f60-9352cf32a79d of framework 20150825-220736-234885548-51219-27610-0000
I0825 22:07:39.099607 27635 status_update_manager.cpp:826] Checkpointing ACK for status update TASK_RUNNING (UUID: 98c4a799-ad82-497d-be1e-6dfb56a0894e) for task b89d1df8-f2fb-44be-8f60-9352cf32a79d of framework 20150825-220736-234885548-51219-27610-0000
I0825 22:07:39.099834 27636 master.cpp:5576] Updating the latest state of task b89d1df8-f2fb-44be-8f60-9352cf32a79d of framework 20150825-220736-234885548-51219-27610-0000 to TASK_RUNNING
I0825 22:07:39.099992 27643 sched.cpp:910] Scheduler::statusUpdate took 29331ns
I0825 22:07:39.100038 27636 master.cpp:3398] Processing ACKNOWLEDGE call 98c4a799-ad82-497d-be1e-6dfb56a0894e for task b89d1df8-f2fb-44be-8f60-9352cf32a79d of framework 20150825-220736-234885548-51219-27610-0000 (default) at scheduler-6c5ddcdb-9dd1-4b38-b051-5f714d3c1c55@172.17.0.14:51219 on slave 20150825-220736-234885548-51219-27610-S0
I0825 22:07:39.100381 27636 master.cpp:3398] Processing ACKNOWLEDGE call 98c4a799-ad82-497d-be1e-6dfb56a0894e for task b89d1df8-f2fb-44be-8f60-9352cf32a79d of framework 20150825-220736-234885548-51219-27610-0000 (default) at scheduler-6c5ddcdb-9dd1-4b38-b051-5f714d3c1c55@172.17.0.14:51219 on slave 20150825-220736-234885548-51219-27610-S0
I0825 22:07:39.102119 27635 status_update_manager.cpp:394] Received status update acknowledgement (UUID: 98c4a799-ad82-497d-be1e-6dfb56a0894e) for task b89d1df8-f2fb-44be-8f60-9352cf32a79d of framework 20150825-220736-234885548-51219-27610-0000
I0825 22:07:39.102120 27637 slave.cpp:2298] Status update manager successfully handled status update acknowledgement (UUID: 98c4a799-ad82-497d-be1e-6dfb56a0894e) for task b89d1df8-f2fb-44be-8f60-9352cf32a79d of framework 20150825-220736-234885548-51219-27610-0000
I0825 22:07:39.102375 27635 status_update_manager.cpp:394] Received status update acknowledgement (UUID: 98c4a799-ad82-497d-be1e-6dfb56a0894e) for task b89d1df8-f2fb-44be-8f60-9352cf32a79d of framework 20150825-220736-234885548-51219-27610-0000
E0825 22:07:39.102407 27633 slave.cpp:2291] Failed to handle status update acknowledgement (UUID: 98c4a799-ad82-497d-be1e-6dfb56a0894e) for task b89d1df8-f2fb-44be-8f60-9352cf32a79d of framework 20150825-220736-234885548-51219-27610-0000: Unexpected status update acknowledgment (UUID: 98c4a799-ad82-497d-be1e-6dfb56a0894e) for task b89d1df8-f2fb-44be-8f60-9352cf32a79d of framework 20150825-220736-234885548-51219-27610-0000
E0825 22:07:39.102546 27636 slave.cpp:2291] Failed to handle status update acknowledgement (UUID: 98c4a799-ad82-497d-be1e-6dfb56a0894e) for task b89d1df8-f2fb-44be-8f60-9352cf32a79d of framework 20150825-220736-234885548-51219-27610-0000: Unexpected status update acknowledgment (UUID: 98c4a799-ad82-497d-be1e-6dfb56a0894e) for task b89d1df8-f2fb-44be-8f60-9352cf32a79d of framework 20150825-220736-234885548-51219-27610-0000
I0825 22:07:39.819394 27637 hierarchical.hpp:1010] No resources available to allocate!
I0825 22:07:39.819452 27637 hierarchical.hpp:910] Performed allocation for 1 slaves in 536774ns
2015-08-25 22:07:40,051:27610(0x2b3b2870c700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:40031] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
I0825 22:07:40.820246 27633 hierarchical.hpp:1010] No resources available to allocate!
I0825 22:07:40.820302 27633 hierarchical.hpp:910] Performed allocation for 1 slaves in 511814ns
I0825 22:07:41.821671 27637 hierarchical.hpp:1010] No resources available to allocate!
I0825 22:07:41.821719 27637 hierarchical.hpp:910] Performed allocation for 1 slaves in 518909ns
I0825 22:07:42.822906 27628 hierarchical.hpp:1010] No resources available to allocate!
I0825 22:07:42.822959 27628 hierarchical.hpp:910] Performed allocation for 1 slaves in 659816ns
2015-08-25 22:07:43,388:27610(0x2b3b2870c700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:40031] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
I0825 22:07:43.824976 27632 hierarchical.hpp:1010] No resources available to allocate!
I0825 22:07:43.825032 27632 hierarchical.hpp:910] Performed allocation for 1 slaves in 727197ns
I0825 22:07:44.825883 27641 hierarchical.hpp:1010] No resources available to allocate!
I0825 22:07:44.825932 27641 hierarchical.hpp:910] Performed allocation for 1 slaves in 422745ns
I0825 22:07:45.828217 27634 hierarchical.hpp:1010] No resources available to allocate!
I0825 22:07:45.828445 27634 hierarchical.hpp:910] Performed allocation for 1 slaves in 1.288273ms
2015-08-25 22:07:46,724:27610(0x2b3b2870c700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:40031] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
I0825 22:07:46.829910 27632 hierarchical.hpp:1010] No resources available to allocate!
I0825 22:07:46.829953 27632 hierarchical.hpp:910] Performed allocation for 1 slaves in 483478ns
I0825 22:07:47.830860 27636 hierarchical.hpp:1010] No resources available to allocate!
I0825 22:07:47.830922 27636 hierarchical.hpp:910] Performed allocation for 1 slaves in 551674ns
I0825 22:07:48.832027 27628 hierarchical.hpp:1010] No resources available to allocate!
I0825 22:07:48.832078 27628 hierarchical.hpp:910] Performed allocation for 1 slaves in 417868ns
I0825 22:07:49.833906 27629 hierarchical.hpp:1010] No resources available to allocate!
I0825 22:07:49.833962 27629 hierarchical.hpp:910] Performed allocation for 1 slaves in 472647ns
2015-08-25 22:07:50,060:27610(0x2b3b2870c700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:40031] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
I0825 22:07:50.835659 27630 hierarchical.hpp:1010] No resources available to allocate!
I0825 22:07:50.835718 27630 hierarchical.hpp:910] Performed allocation for 1 slaves in 522864ns
I0825 22:07:51.837473 27638 hierarchical.hpp:1010] No resources available to allocate!
I0825 22:07:51.837537 27638 hierarchical.hpp:910] Performed allocation for 1 slaves in 575837ns
I0825 22:07:52.839296 27641 hierarchical.hpp:1010] No resources available to allocate!
I0825 22:07:52.839350 27641 hierarchical.hpp:910] Performed allocation for 1 slaves in 558642ns
2015-08-25 22:07:53,397:27610(0x2b3b2870c700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:40031] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
I0825 22:07:53.840854 27630 hierarchical.hpp:1010] No resources available to allocate!
I0825 22:07:53.840904 27630 hierarchical.hpp:910] Performed allocation for 1 slaves in 557112ns
I0825 22:07:54.083889 27631 slave.cpp:4226] Querying resource estimator for oversubscribable resources
I0825 22:07:54.084323 27629 slave.cpp:4240] Received oversubscribable resources  from the resource estimator
../../src/tests/slave_tests.cpp:2651: Failure
Failed to wait 15secs for executorToFrameworkMessage1
I0825 22:07:54.098143 27629 master.cpp:1051] Framework 20150825-220736-234885548-51219-27610-0000 (default) at scheduler-6c5ddcdb-9dd1-4b38-b051-5f714d3c1c55@172.17.0.14:51219 disconnected
I0825 22:07:54.098212 27629 master.cpp:2370] Disconnecting framework 20150825-220736-234885548-51219-27610-0000 (default) at scheduler-6c5ddcdb-9dd1-4b38-b051-5f714d3c1c55@172.17.0.14:51219
I0825 22:07:54.098254 27629 master.cpp:2394] Deactivating framework 20150825-220736-234885548-51219-27610-0000 (default) at scheduler-6c5ddcdb-9dd1-4b38-b051-5f714d3c1c55@172.17.0.14:51219
I0825 22:07:54.098363 27629 master.cpp:1075] Giving framework 20150825-220736-234885548-51219-27610-0000 (default) at scheduler-6c5ddcdb-9dd1-4b38-b051-5f714d3c1c55@172.17.0.14:51219 0ns to failover
I0825 22:07:54.098448 27631 hierarchical.hpp:474] Deactivated framework 20150825-220736-234885548-51219-27610-0000
I0825 22:07:54.098830 27641 master.cpp:4469] Framework failover timeout, removing framework 20150825-220736-234885548-51219-27610-0000 (default) at scheduler-6c5ddcdb-9dd1-4b38-b051-5f714d3c1c55@172.17.0.14:51219
I0825 22:07:54.098867 27641 master.cpp:5112] Removing framework 20150825-220736-234885548-51219-27610-0000 (default) at scheduler-6c5ddcdb-9dd1-4b38-b051-5f714d3c1c55@172.17.0.14:51219
I0825 22:07:54.099156 27629 slave.cpp:1959] Asked to shut down framework 20150825-220736-234885548-51219-27610-0000 by master@172.17.0.14:51219
I0825 22:07:54.099211 27629 slave.cpp:1984] Shutting down framework 20150825-220736-234885548-51219-27610-0000
I0825 22:07:54.099198 27641 master.cpp:5576] Updating the latest state of task b89d1df8-f2fb-44be-8f60-9352cf32a79d of framework 20150825-220736-234885548-51219-27610-0000 to TASK_KILLED
I0825 22:07:54.099328 27629 slave.cpp:3710] Shutting down executor 'b89d1df8-f2fb-44be-8f60-9352cf32a79d' of framework 20150825-220736-234885548-51219-27610-0000
I0825 22:07:54.099913 27641 master.cpp:5644] Removing task b89d1df8-f2fb-44be-8f60-9352cf32a79d with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] of framework 20150825-220736-234885548-51219-27610-0000 on slave 20150825-220736-234885548-51219-27610-S0 at slave(287)@172.17.0.14:51219 (09c6504e3a31)
I0825 22:07:54.099987 27632 hierarchical.hpp:816] Recovered cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (total: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000], allocated: ) on slave 20150825-220736-234885548-51219-27610-S0 from framework 20150825-220736-234885548-51219-27610-0000
I0825 22:07:54.100440 27641 hierarchical.hpp:428] Removed framework 20150825-220736-234885548-51219-27610-0000
I0825 22:07:54.100608 27643 master.cpp:860] Master terminating
I0825 22:07:54.100778  2360 exec.cpp:380] Executor asked to shutdown
II0825 22:07:54.100929 27641 hierarchical.hpp:573] Removed slave 20150825-220736-234885548-51219-27610-S0
0825 22:07:54.100896  2364 exec.cpp:79] Scheduling shutdown of the executor
I0825 22:07:54.100958  2360 exec.cpp:395] Executor::shutdown took 75333ns
Shutting down
Sending SIGTERM to process tree at pid 2371
I0825 22:07:54.101748 27640 slave.cpp:3143] master@172.17.0.14:51219 exited
W0825 22:07:54.101866 27640 slave.cpp:3146] Master disconnected! Waiting for a new master to be elected
I0825 22:07:54.106029 27632 containerizer.cpp:1079] Destroying container '1499299a-93dd-4982-9249-ad0e19d1c06c'
Killing the following process trees:
[ 
-+- 2371 sh -c sleep 1000 
 \--- 2372 sleep 1000 
]
I0825 22:07:54.211082 27639 containerizer.cpp:1266] Executor for container '1499299a-93dd-4982-9249-ad0e19d1c06c' has exited
I0825 22:07:54.211087 27630 containerizer.cpp:1266] Executor for container '1499299a-93dd-4982-9249-ad0e19d1c06c' has exited
I0825 22:07:54.211143 27639 containerizer.cpp:1079] Destroying container '1499299a-93dd-4982-9249-ad0e19d1c06c'
I0825 22:07:54.212609 27637 slave.cpp:3399] Executor 'b89d1df8-f2fb-44be-8f60-9352cf32a79d' of framework 20150825-220736-234885548-51219-27610-0000 terminated with signal Killed
I0825 22:07:54.212685 27637 slave.cpp:3503] Cleaning up executor 'b89d1df8-f2fb-44be-8f60-9352cf32a79d' of framework 20150825-220736-234885548-51219-27610-0000
I0825 22:07:54.213062 27631 gc.cpp:56] Scheduling '/tmp/SlaveTest_HTTPSchedulerSlaveRestart_ukkA8L/slaves/20150825-220736-234885548-51219-27610-S0/frameworks/20150825-220736-234885548-51219-27610-0000/executors/b89d1df8-f2fb-44be-8f60-9352cf32a79d/runs/1499299a-93dd-4982-9249-ad0e19d1c06c' for gc 6.99999753474667days in the future
I0825 22:07:54.214745 27630 gc.cpp:56] Scheduling '/tmp/SlaveTest_HTTPSchedulerSlaveRestart_ukkA8L/slaves/20150825-220736-234885548-51219-27610-S0/frameworks/20150825-220736-234885548-51219-27610-0000/executors/b89d1df8-f2fb-44be-8f60-9352cf32a79d' for gc 6.99999753268444days in the future
I0825 22:07:54.214859 27630 gc.cpp:56] Scheduling '/tmp/SlaveTest_HTTPSchedulerSlaveRestart_ukkA8L/meta/slaves/20150825-220736-234885548-51219-27610-S0/frameworks/20150825-220736-234885548-51219-27610-0000/executors/b89d1df8-f2fb-44be-8f60-9352cf32a79d/runs/1499299a-93dd-4982-9249-ad0e19d1c06c' for gc 6.99999751446815days in the future
I0825 22:07:54.214921 27637 slave.cpp:3592] Cleaning up framework 20150825-220736-234885548-51219-27610-0000
I0825 22:07:54.215047 27630 gc.cpp:56] Scheduling '/tmp/SlaveTest_HTTPSchedulerSlaveRestart_ukkA8L/meta/slaves/20150825-220736-234885548-51219-27610-S0/frameworks/20150825-220736-234885548-51219-27610-0000/executors/b89d1df8-f2fb-44be-8f60-9352cf32a79d' for gc 6.99999751310222days in the future
I0825 22:07:54.215140 27634 status_update_manager.cpp:284] Closing status update streams for framework 20150825-220736-234885548-51219-27610-0000
I0825 22:07:54.215338 27634 status_update_manager.cpp:530] Cleaning up status update stream for task b89d1df8-f2fb-44be-8f60-9352cf32a79d of framework 20150825-220736-234885548-51219-27610-0000
I0825 22:07:54.215358 27637 slave.cpp:564] Slave terminating
I0825 22:07:54.215347 27630 gc.cpp:56] Scheduling '/tmp/SlaveTest_HTTPSchedulerSlaveRestart_ukkA8L/slaves/20150825-220736-234885548-51219-27610-S0/frameworks/20150825-220736-234885548-51219-27610-0000' for gc 6.99999751012741days in the future
I0825 22:07:54.215608 27630 gc.cpp:56] Scheduling '/tmp/SlaveTest_HTTPSchedulerSlaveRestart_ukkA8L/meta/slaves/20150825-220736-234885548-51219-27610-S0/frameworks/20150825-220736-234885548-51219-27610-0000' for gc 6.99999750907259days in the future
../../3rdparty/libprocess/include/process/gmock.hpp:365: Failure
Actual function call count doesn't match EXPECT_CALL(filter->mock, filter(testing::A<const MessageEvent&>()))...
    Expected args: message matcher (8-byte object <58-B2 02-68 3A-2B 00-00>, 1, 1)
         Expected: to be called once
           Actual: never called - unsatisfied and active
../../3rdparty/libprocess/include/process/gmock.hpp:365: Failure
Actual function call count doesn't match EXPECT_CALL(filter->mock, filter(testing::A<const MessageEvent&>()))...
    Expected args: message matcher (8-byte object <58-B2 02-68 3A-2B 00-00>, 1, 1-byte object <A8>)
         Expected: to be called once
           Actual: never called - unsatisfied and active
[  FAILED  ] SlaveTest.HTTPSchedulerSlaveRestart (17413 ms)
{code}",Bug,Major,vinodkone,2015-09-01T22:41:33.000+0000,5,Resolved,Complete,SlaveTest.HTTPSchedulerSlaveRestart,2015-09-02T16:32:19.000+0000,MESOS-3311,2.0,mesos,Mesosphere Sprint 18
jieyu,2015-08-24T19:48:06.000+0000,jieyu,"This is related to MESOS-3095 and MESOS-3227.

The idea is that we should allow command executor to run under host filesystem and provision the filesystem for the user. The command line executor will then chroot into user's root filesystem.

This solves the issue that the command executor is not launchable in the user specified root filesystem. 

The design doc is here:
https://docs.google.com/document/d/16hyLVRL0nz-KBts1J5stGyxZPniFPbPbs7R-ZRQVCH4/edit?usp=sharing",Task,Major,jieyu,2015-08-26T21:27:51.000+0000,5,Resolved,Complete,Support provisioning images specified in volumes.,2015-08-26T21:27:51.000+0000,MESOS-3310,3.0,mesos,Twitter Mesos Q3 Sprint 3
xujyan,2015-08-24T18:08:59.000+0000,xujyan,"A few motivations:

1) Given the design in MESOS-3004 it became apparent that we need to support multiple images in a container and these images can be of different image types. (There are no sufficient reasons or major obstacles that force us not to allow it and it obviously gives the users more flexibility).

2) Also, even though we currently allow only one backend for each provisioner, when we update a running slave there can be multiple backends left in each container that we need to launch tasks with, or at least recover. We should evaluate in the future whether to support multiple backends and choose among them dynamically based on image characteristics.

3) Since the rootfs' lifecycle tie with the running containers and should be cleaned up after containers die, it fits into the pattern of {{word_dir}} and we can manage them inside the work dir without needing to ask the operator to specify more flags.
",Task,Major,xujyan,2015-08-31T17:59:35.000+0000,5,Resolved,Complete,Define the container rootfs directories within the slave work_dir.,2015-09-08T19:29:42.000+0000,MESOS-3308,2.0,mesos,
klueska,2015-08-24T16:50:12.000+0000,bobrik,"We try to make Mesos work with multiple frameworks and mesos-dns at the same time. The goal is to have set of frameworks per team / project on a single Mesos cluster.

At this point our mesos state.json is at 4mb and it takes a while to assembly. 5 mesos-dns instances hit state.json every 5 seconds, effectively pushing mesos-master CPU usage through the roof. It's at 100%+ all the time.

Here's the problem:

{noformat}
mesos λ curl -s http://mesos-master:5050/master/state.json | jq .frameworks[].completed_tasks[].framework_id | sort | uniq -c | sort -n
   1 ""20150606-001827-252388362-5050-5982-0003""
  16 ""20150606-001827-252388362-5050-5982-0005""
  18 ""20150606-001827-252388362-5050-5982-0029""
  73 ""20150606-001827-252388362-5050-5982-0007""
 141 ""20150606-001827-252388362-5050-5982-0009""
 154 ""20150820-154817-302720010-5050-15320-0000""
 289 ""20150606-001827-252388362-5050-5982-0004""
 510 ""20150606-001827-252388362-5050-5982-0012""
 666 ""20150606-001827-252388362-5050-5982-0028""
 923 ""20150116-002612-269165578-5050-32204-0003""
1000 ""20150606-001827-252388362-5050-5982-0001""
1000 ""20150606-001827-252388362-5050-5982-0006""
1000 ""20150606-001827-252388362-5050-5982-0010""
1000 ""20150606-001827-252388362-5050-5982-0011""
1000 ""20150606-001827-252388362-5050-5982-0027""

mesos λ fgrep 1000 -r src/master
src/master/constants.cpp:const size_t MAX_REMOVED_SLAVES = 100000;
src/master/constants.cpp:const uint32_t MAX_COMPLETED_TASKS_PER_FRAMEWORK = 1000;
{noformat}

Active tasks are just 6% of state.json response:

{noformat}
mesos λ cat ~/temp/mesos-state.json | jq -c . | wc
       1   14796 4138942
mesos λ cat ~/temp/mesos-state.json | jq .frameworks[].tasks | jq -c . | wc
      16      37  252774
{noformat}

I see four options that can improve the situation:

1. Add query string param to exclude completed tasks from state.json and use it in mesos-dns and similar tools. There is no need for mesos-dns to know about completed tasks, it's just extra load on master and mesos-dns.

2. Make history size configurable.

3. Make JSON serialization faster. With 10000s of tasks even without history it would take a lot of time to serialize tasks for mesos-dns. Doing it every 60 seconds instead of every 5 seconds isn't really an option.

4. Create event bus for mesos master. Marathon has it and it'd be nice to have it in Mesos. This way mesos-dns could avoid polling master state and switch to listening for events.

All can be done independently.

Note to mesosphere folks: please start distributing debug symbols with your distribution. I was asking for it for a while and it is really helpful: https://github.com/mesosphere/marathon/issues/1497#issuecomment-104182501

Perf report for leading master: 

!http://i.imgur.com/iz7C3o0.png!

I'm on 0.23.0.",Bug,Major,bobrik,2016-01-15T07:32:36.000+0000,5,Resolved,Complete,Configurable size of completed task / framework history,2016-02-27T00:40:32.000+0000,MESOS-3307,3.0,mesos,Mesosphere Sprint 26
greggomann,2015-08-21T23:47:52.000+0000,greggomann,"As seen in MESOS-1283, LIBPROCESS_STATISTICS_WINDOW is no longer needed since metrics now require specification of a window size, and default to no history if not provided.

Some commented-out code remnants associated with this environment variable still remain and should be removed.",Improvement,Trivial,greggomann,2015-09-04T19:16:05.000+0000,5,Resolved,Complete,Remove remnants of LIBPROCESS_STATISTICS_WINDOW,2015-09-04T19:16:05.000+0000,MESOS-3304,1.0,mesos,Mesosphere Sprint 17
kaysoky,2015-08-19T21:46:55.000+0000,kaysoky,"Existing timestamps in the protobufs use {{double}} to encode time.  Generally, the field represents seconds (with the decimal component to represent smaller denominations of time).  This is less than ideal.

Instead, we should use integers, so as to not lose data (and to be able to compare value reliably).

Something like:
{code}
message Time {
  int64 seconds;
  int32 nanoseconds;
}
{code}",Task,Major,kaysoky,2015-08-27T22:53:54.000+0000,5,Resolved,Complete,Add a protobuf to represent time with integer precision.,2015-09-25T22:46:29.000+0000,MESOS-3299,1.0,mesos,Mesosphere Sprint 17
greggomann,2015-08-18T23:50:39.000+0000,marco-mesos,"h2. MesosContainerizerLaunchTest

This is one of several ROOT failing tests: we want to track them *individually* and for each of them decide whether to:

* fix;
* remove; OR
* redesign.

(full verbose logs attached)

h2. Steps to Reproduce

Completely cleaned the build, removed directory, clean pull from {{master}} (SHA: {{fb93d93}}) - same results, 9 failed tests:

{noformat}
[==========] 751 tests from 114 test cases ran. (231218 ms total)
[  PASSED  ] 742 tests.
[  FAILED  ] 9 tests, listed below:
[  FAILED  ] LimitedCpuIsolatorTest.ROOT_CGROUPS_Pids_and_Tids
[  FAILED  ] UserCgroupIsolatorTest/1.ROOT_CGROUPS_UserCgroup, where TypeParam = mesos::internal::slave::CgroupsCpushareIsolatorProcess
[  FAILED  ] ContainerizerTest.ROOT_CGROUPS_BalloonFramework
[  FAILED  ] LinuxFilesystemIsolatorTest.ROOT_ChangeRootFilesystem
[  FAILED  ] LinuxFilesystemIsolatorTest.ROOT_VolumeFromSandbox
[  FAILED  ] LinuxFilesystemIsolatorTest.ROOT_VolumeFromHost
[  FAILED  ] LinuxFilesystemIsolatorTest.ROOT_VolumeFromHostSandboxMountPoint
[  FAILED  ] LinuxFilesystemIsolatorTest.ROOT_PersistentVolumeWithRootFilesystem
[  FAILED  ] MesosContainerizerLaunchTest.ROOT_ChangeRootfs

 9 FAILED TESTS
  YOU HAVE 10 DISABLED TESTS
{noformat}",Bug,Blocker,marco-mesos,2015-08-22T00:18:01.000+0000,5,Resolved,Complete,Failing ROOT_ tests on CentOS 7.1 - MesosContainerizerLaunchTest,2015-08-31T23:47:52.000+0000,MESOS-3297,5.0,mesos,Mesosphere Sprint 16
greggomann,2015-08-18T23:49:18.000+0000,marco-mesos,"h2. LinuxFilesystemIsolatorTest

This is one of several ROOT failing tests: we want to track them *individually* and for each of them decide whether to:

* fix;
* remove; OR
* redesign.

(full verbose logs attached)

h2. Steps to Reproduce

Completely cleaned the build, removed directory, clean pull from {{master}} (SHA: {{fb93d93}}) - same results, 9 failed tests:

{noformat}
[==========] 751 tests from 114 test cases ran. (231218 ms total)
[  PASSED  ] 742 tests.
[  FAILED  ] 9 tests, listed below:
[  FAILED  ] LimitedCpuIsolatorTest.ROOT_CGROUPS_Pids_and_Tids
[  FAILED  ] UserCgroupIsolatorTest/1.ROOT_CGROUPS_UserCgroup, where TypeParam = mesos::internal::slave::CgroupsCpushareIsolatorProcess
[  FAILED  ] ContainerizerTest.ROOT_CGROUPS_BalloonFramework
[  FAILED  ] LinuxFilesystemIsolatorTest.ROOT_ChangeRootFilesystem
[  FAILED  ] LinuxFilesystemIsolatorTest.ROOT_VolumeFromSandbox
[  FAILED  ] LinuxFilesystemIsolatorTest.ROOT_VolumeFromHost
[  FAILED  ] LinuxFilesystemIsolatorTest.ROOT_VolumeFromHostSandboxMountPoint
[  FAILED  ] LinuxFilesystemIsolatorTest.ROOT_PersistentVolumeWithRootFilesystem
[  FAILED  ] MesosContainerizerLaunchTest.ROOT_ChangeRootfs

 9 FAILED TESTS
  YOU HAVE 10 DISABLED TESTS
{noformat}",Bug,Blocker,marco-mesos,2015-11-19T01:55:50.000+0000,5,Resolved,Complete,Failing ROOT_ tests on CentOS 7.1 - LinuxFilesystemIsolatorTest,2016-02-12T03:13:13.000+0000,MESOS-3296,5.0,mesos,Mesosphere Sprint 16
marco-mesos,2015-08-18T23:48:01.000+0000,marco-mesos,"h2. ContainerizerTest.ROOT_CGROUPS_BalloonFramework

This is one of several ROOT failing tests: we want to track them *individually* and for each of them decide whether to:

* fix;
* remove; OR
* redesign.

(full verbose logs attached)

h2. Steps to Reproduce

Completely cleaned the build, removed directory, clean pull from {{master}} (SHA: {{fb93d93}}) - same results, 9 failed tests:

{noformat}
[==========] 751 tests from 114 test cases ran. (231218 ms total)
[  PASSED  ] 742 tests.
[  FAILED  ] 9 tests, listed below:
[  FAILED  ] LimitedCpuIsolatorTest.ROOT_CGROUPS_Pids_and_Tids
[  FAILED  ] UserCgroupIsolatorTest/1.ROOT_CGROUPS_UserCgroup, where TypeParam = mesos::internal::slave::CgroupsCpushareIsolatorProcess
[  FAILED  ] ContainerizerTest.ROOT_CGROUPS_BalloonFramework
[  FAILED  ] LinuxFilesystemIsolatorTest.ROOT_ChangeRootFilesystem
[  FAILED  ] LinuxFilesystemIsolatorTest.ROOT_VolumeFromSandbox
[  FAILED  ] LinuxFilesystemIsolatorTest.ROOT_VolumeFromHost
[  FAILED  ] LinuxFilesystemIsolatorTest.ROOT_VolumeFromHostSandboxMountPoint
[  FAILED  ] LinuxFilesystemIsolatorTest.ROOT_PersistentVolumeWithRootFilesystem
[  FAILED  ] MesosContainerizerLaunchTest.ROOT_ChangeRootfs

 9 FAILED TESTS
  YOU HAVE 10 DISABLED TESTS
{noformat}",Bug,Blocker,marco-mesos,2015-09-14T16:24:33.000+0000,5,Resolved,Complete,Failing ROOT_ tests on CentOS 7.1 - ContainerizerTest,2015-09-15T03:08:08.000+0000,MESOS-3295,5.0,mesos,Mesosphere Sprint 16
tnachen,2015-08-18T23:47:00.000+0000,marco-mesos,"h2. UserCgroupIsolatorTest

This is one of several ROOT failing tests: we want to track them *individually* and for each of them decide whether to:

* fix;
* remove; OR
* redesign.

(full verbose logs attached)

h2. Steps to Reproduce

Completely cleaned the build, removed directory, clean pull from {{master}} (SHA: {{fb93d93}}) - same results, 9 failed tests:

{noformat}
[==========] 751 tests from 114 test cases ran. (231218 ms total)
[  PASSED  ] 742 tests.
[  FAILED  ] 9 tests, listed below:
[  FAILED  ] LimitedCpuIsolatorTest.ROOT_CGROUPS_Pids_and_Tids
[  FAILED  ] UserCgroupIsolatorTest/1.ROOT_CGROUPS_UserCgroup, where TypeParam = mesos::internal::slave::CgroupsCpushareIsolatorProcess
[  FAILED  ] ContainerizerTest.ROOT_CGROUPS_BalloonFramework
[  FAILED  ] LinuxFilesystemIsolatorTest.ROOT_ChangeRootFilesystem
[  FAILED  ] LinuxFilesystemIsolatorTest.ROOT_VolumeFromSandbox
[  FAILED  ] LinuxFilesystemIsolatorTest.ROOT_VolumeFromHost
[  FAILED  ] LinuxFilesystemIsolatorTest.ROOT_VolumeFromHostSandboxMountPoint
[  FAILED  ] LinuxFilesystemIsolatorTest.ROOT_PersistentVolumeWithRootFilesystem
[  FAILED  ] MesosContainerizerLaunchTest.ROOT_ChangeRootfs

 9 FAILED TESTS
  YOU HAVE 10 DISABLED TESTS
{noformat}",Bug,Blocker,marco-mesos,2015-08-28T00:11:19.000+0000,5,Resolved,Complete,Failing ROOT_ tests on CentOS 7.1 - UserCgroupIsolatorTest,2015-08-31T23:48:28.000+0000,MESOS-3294,5.0,mesos,Mesosphere Sprint 16
qiujian,2015-08-18T23:40:19.000+0000,marco-mesos,"h2. LimitedCpuIsolatorTest.ROOT_CGROUPS_Pids_and_Tids

This is one of several ROOT failing tests: we want to track them *individually* and for each of them decide whether to:

* fix;
* remove; OR
* redesign.

(full verbose logs attached)

h2. Steps to Reproduce

Completely cleaned the build, removed directory, clean pull from {{master}} (SHA: {{fb93d93}}) - same results, 9 failed tests:

{noformat}
[==========] 751 tests from 114 test cases ran. (231218 ms total)
[  PASSED  ] 742 tests.
[  FAILED  ] 9 tests, listed below:
[  FAILED  ] LimitedCpuIsolatorTest.ROOT_CGROUPS_Pids_and_Tids
[  FAILED  ] UserCgroupIsolatorTest/1.ROOT_CGROUPS_UserCgroup, where TypeParam = mesos::internal::slave::CgroupsCpushareIsolatorProcess
[  FAILED  ] ContainerizerTest.ROOT_CGROUPS_BalloonFramework
[  FAILED  ] LinuxFilesystemIsolatorTest.ROOT_ChangeRootFilesystem
[  FAILED  ] LinuxFilesystemIsolatorTest.ROOT_VolumeFromSandbox
[  FAILED  ] LinuxFilesystemIsolatorTest.ROOT_VolumeFromHost
[  FAILED  ] LinuxFilesystemIsolatorTest.ROOT_VolumeFromHostSandboxMountPoint
[  FAILED  ] LinuxFilesystemIsolatorTest.ROOT_PersistentVolumeWithRootFilesystem
[  FAILED  ] MesosContainerizerLaunchTest.ROOT_ChangeRootfs

 9 FAILED TESTS
  YOU HAVE 10 DISABLED TESTS
{noformat}",Bug,Blocker,marco-mesos,2015-11-05T00:05:04.000+0000,5,Resolved,Complete,Failing ROOT_ tests on CentOS 7.1 - LimitedCpuIsolatorTest,2015-11-08T12:51:42.000+0000,MESOS-3293,5.0,mesos,Mesosphere Sprint 16
vinodkone,2015-08-18T17:47:42.000+0000,vinodkone,"Much like what we do with PID based frameworks, master should drop HTTP calls if it's not the leader and/or still recovering.",Bug,Major,vinodkone,2015-08-18T19:12:09.000+0000,5,Resolved,Complete,Master should drop HTTP calls when it's recovering,2015-08-18T19:27:25.000+0000,MESOS-3290,3.0,mesos,Twitter Mesos Q3 Sprint 3
jojy,2015-08-18T17:10:48.000+0000,jojy,"Add unit tests suite for docker registry implementation.  This could include:

- Creating mock docker registry server
- Using openssl library for digest functions.",Task,Major,jojy,2015-09-25T16:40:04.000+0000,5,Resolved,Complete,Add DockerRegistry unit tests,2015-09-25T16:40:04.000+0000,MESOS-3289,5.0,mesos,Mesosphere Sprint 17
jojy,2015-08-18T17:08:22.000+0000,jojy,"Implement the docker registry client as per design document:

https://docs.google.com/document/d/1kE-HXPQl4lQgamPIiaD4Ytdr-N4HeQc4fnE93WHR4X4/edit",Task,Major,jojy,2015-08-18T19:27:35.000+0000,5,Resolved,Complete,Implement docker registry client,2015-08-18T19:27:35.000+0000,MESOS-3288,5.0,mesos,Mesosphere Sprint 17
marco-mesos,2015-08-18T07:47:13.000+0000,marco-mesos,"This was reported while trying to install Hadoop / Mesos integration:
{noformat}
I0818 05:36:35.058688 24428 fetcher.cpp:409] Fetcher Info: {""cache_directory"":""\/tmp\/mesos\/fetch\/slaves\/20150706-075218-1611773194-5050-28439-S473\/hadoop"",""items"":[{""action"":""BYPASS_CACHE"",""uri"":{""extract"":true,""value"":""hdfs:\/\/hdfs.prod:54310\/user\/ashwanth\/hadoop-with-mesos-2.6.0-cdh5.4.4.tar.gz""}}],""sandbox_directory"":""\/var\/lib\/mesos\/slaves\/20150706-075218-1611773194-5050-28439-S473\/frameworks\/20150706-075218-1611773194-5050-28439-4532\/executors\/executor_Task_Tracker_4129\/runs\/c26f52d4-4055-46fa-b999-11d73f2096dd"",""user"":""hadoop""}
I0818 05:36:35.059806 24428 fetcher.cpp:364] Fetching URI 'hdfs://hdfs.prod:54310/user/ashwanth/hadoop-with-mesos-2.6.0-cdh5.4.4.tar.gz'
I0818 05:36:35.059821 24428 fetcher.cpp:238] Fetching directly into the sandbox directory
I0818 05:36:35.059835 24428 fetcher.cpp:176] Fetching URI 'hdfs://hdfs.prod:54310/user/ashwanth/hadoop-with-mesos-2.6.0-cdh5.4.4.tar.gz'
mesos-fetcher: /tmp/mesos-build/mesos-repo/3rdparty/libprocess/3rdparty/stout/include/stout/try.hpp:90: const string& Try<T>::error() const [with T = bool; std::string = std::basic_string<char>]: Assertion `data.isNone()' failed.
{noformat}

This is, however, a genuine bug in {{src/launcher/fetcher.cpp#L99}}:

{code}
  Try<bool> available = hdfs.available();

  if (available.isError() || !available.get()) {
    return Error(""Skipping fetch with Hadoop Client as""
                 "" Hadoop Client not available: "" + available.error());
  }
{code}
The root cause is that (probably) the HDFS client is not available on the slave; however, we do not {{error()}} but rather return a {{false}} result.

The bug is exposed in the {{return}} line, where we try to retrieve {{available.error()}} (which is not there - it's just `false`).

This was a 'latent' bug that has been exposed by (my) recent refactoring of {{os::shell}} which is used by {{hdfs.available()}} under the covers.",Bug,Major,marco-mesos,2015-08-18T19:25:02.000+0000,5,Resolved,Complete,downloadWithHadoop tries to access Error() for a valid Try<bool>,2015-08-18T19:25:02.000+0000,MESOS-3287,1.0,mesos,Mesosphere Sprint 17
bmahler,2015-08-17T22:36:01.000+0000,bmahler,"Currently we encode 'bytes' fields as UTF-8 strings, which is lossy for binary data due to invalid byte sequences! In order to encode binary data in a lossless fashion, we can encode 'bytes' fields in base64.

Note that this is also how proto3 does its encoding (see [here|https://developers.google.com/protocol-buffers/docs/proto3?hl=en#json]), so this would make migration easier as well.",Bug,Major,bmahler,2015-08-18T00:17:17.000+0000,5,Resolved,Complete,JSON representation of Protobuf should use base64 encoding for 'bytes' fields.,2015-08-18T00:17:17.000+0000,MESOS-3284,3.0,mesos,Twitter Mesos Q3 Sprint 3
vinodkone,2015-08-17T18:25:37.000+0000,vinodkone,We need to convert the design doc into user doc that we can add to our docs folder.,Documentation,Major,vinodkone,2015-08-18T04:51:33.000+0000,5,Resolved,Complete,Create a user doc for Scheduler HTTP API,2015-08-18T04:51:33.000+0000,MESOS-3281,3.0,mesos,Twitter Mesos Q3 Sprint 3
neilc,2015-08-17T16:11:15.000+0000,bernd-mesos,"In a 5 node cluster with 3 masters and 2 slaves, and ZK on each node, when a network partition is forced, all the masters apparently lose access to their replicated log. The leading master halts. Unknown reasons, but presumably related to replicated log access. The others fail to recover from the replicated log. Unknown reasons. This could have to do with ZK setup, but it might also be a Mesos bug. 

This was observed in a Chronos test drive scenario described in detail here:
https://github.com/mesos/chronos/issues/511

With setup instructions here:
https://github.com/mesos/chronos/issues/508

",Bug,Major,bernd-mesos,2015-11-03T21:46:26.000+0000,5,Resolved,Complete,Master fails to access replicated log after network partition,2015-11-03T21:47:48.000+0000,MESOS-3280,8.0,mesos,
anandmazumdar,2015-08-16T19:23:36.000+0000,vinodkone,"Observed this on ASF CI. h/t [~haosdent@gmail.com]

Looks like the HTTP scheduler never sent a SUBSCRIBE request to the master.

{code}
[ RUN      ] ExamplesTest.EventCallFramework
Using temporary directory '/tmp/ExamplesTest_EventCallFramework_k4vXkx'
I0813 19:55:15.643579 26085 exec.cpp:443] Ignoring exited event because the driver is aborted!
Shutting down
Sending SIGTERM to process tree at pid 26061
Killing the following process trees:
[ 

]
Shutting down
Sending SIGTERM to process tree at pid 26062
Shutting down
Killing the following process trees:
[ 

]
Sending SIGTERM to process tree at pid 26063
Killing the following process trees:
[ 

]
Shutting down
Sending SIGTERM to process tree at pid 26098
Killing the following process trees:
[ 

]
Shutting down
Sending SIGTERM to process tree at pid 26099
Killing the following process trees:
[ 

]
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0813 19:55:17.161726 26100 process.cpp:1012] libprocess is initialized on 172.17.2.10:60249 for 16 cpus
I0813 19:55:17.161888 26100 logging.cpp:177] Logging to STDERR
I0813 19:55:17.163625 26100 scheduler.cpp:157] Version: 0.24.0
I0813 19:55:17.175302 26100 leveldb.cpp:176] Opened db in 3.167446ms
I0813 19:55:17.176393 26100 leveldb.cpp:183] Compacted db in 1.047996ms
I0813 19:55:17.176496 26100 leveldb.cpp:198] Created db iterator in 77155ns
I0813 19:55:17.176518 26100 leveldb.cpp:204] Seeked to beginning of db in 8429ns
I0813 19:55:17.176527 26100 leveldb.cpp:273] Iterated through 0 keys in the db in 4219ns
I0813 19:55:17.176708 26100 replica.cpp:744] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned
I0813 19:55:17.178951 26136 recover.cpp:449] Starting replica recovery
I0813 19:55:17.179934 26136 recover.cpp:475] Replica is in EMPTY status
I0813 19:55:17.181970 26126 master.cpp:378] Master 20150813-195517-167907756-60249-26100 (297daca2d01a) started on 172.17.2.10:60249
I0813 19:55:17.182317 26126 master.cpp:380] Flags at startup: --acls=""permissive: false
register_frameworks {
  principals {
    type: SOME
    values: ""test-principal""
  }
  roles {
    type: SOME
    values: ""*""
  }
}
run_tasks {
  principals {
    type: SOME
    values: ""test-principal""
  }
  users {
    type: SOME
    values: ""mesos""
  }
}
"" --allocation_interval=""1secs"" --allocator=""HierarchicalDRF"" --authenticate=""false"" --authenticate_slaves=""false"" --authenticators=""crammd5"" --credentials=""/tmp/ExamplesTest_EventCallFramework_k4vXkx/credentials"" --framework_sorter=""drf"" --help=""false"" --initialize_driver_logging=""true"" --log_auto_initialize=""true"" --logbufsecs=""0"" --logging_level=""INFO"" --max_slave_ping_timeouts=""5"" --quiet=""false"" --recovery_slave_removal_limit=""100%"" --registry=""replicated_log"" --registry_fetch_timeout=""1mins"" --registry_store_timeout=""5secs"" --registry_strict=""false"" --root_submissions=""true"" --slave_ping_timeout=""15secs"" --slave_reregister_timeout=""10mins"" --user_sorter=""drf"" --version=""false"" --webui_dir=""/mesos/mesos-0.24.0/src/webui"" --work_dir=""/tmp/mesos-II8Gua"" --zk_session_timeout=""10secs""
I0813 19:55:17.183475 26126 master.cpp:427] Master allowing unauthenticated frameworks to register
I0813 19:55:17.183536 26126 master.cpp:432] Master allowing unauthenticated slaves to register
I0813 19:55:17.183615 26126 credentials.hpp:37] Loading credentials for authentication from '/tmp/ExamplesTest_EventCallFramework_k4vXkx/credentials'
W0813 19:55:17.183859 26126 credentials.hpp:52] Permissions on credentials file '/tmp/ExamplesTest_EventCallFramework_k4vXkx/credentials' are too open. It is recommended that your credentials file is NOT accessible by others.
I0813 19:55:17.183969 26123 replica.cpp:641] Replica in EMPTY status received a broadcasted recover request
I0813 19:55:17.184306 26126 master.cpp:469] Using default 'crammd5' authenticator
I0813 19:55:17.184661 26126 authenticator.cpp:512] Initializing server SASL
I0813 19:55:17.185104 26138 recover.cpp:195] Received a recover response from a replica in EMPTY status
I0813 19:55:17.185972 26100 containerizer.cpp:143] Using isolation: posix/cpu,posix/mem,filesystem/posix
I0813 19:55:17.186058 26135 recover.cpp:566] Updating replica status to STARTING
I0813 19:55:17.187001 26138 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 654586ns
I0813 19:55:17.187037 26138 replica.cpp:323] Persisted replica status to STARTING
I0813 19:55:17.187499 26134 recover.cpp:475] Replica is in STARTING status
I0813 19:55:17.187605 26126 auxprop.cpp:66] Initialized in-memory auxiliary property plugin
I0813 19:55:17.187710 26126 master.cpp:506] Authorization enabled
I0813 19:55:17.188657 26138 replica.cpp:641] Replica in STARTING status received a broadcasted recover request
I0813 19:55:17.188853 26131 hierarchical.hpp:346] Initialized hierarchical allocator process
I0813 19:55:17.189252 26132 whitelist_watcher.cpp:79] No whitelist given
I0813 19:55:17.189321 26134 recover.cpp:195] Received a recover response from a replica in STARTING status
I0813 19:55:17.190001 26125 recover.cpp:566] Updating replica status to VOTING
I0813 19:55:17.190696 26124 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 357331ns
I0813 19:55:17.190775 26124 replica.cpp:323] Persisted replica status to VOTING
I0813 19:55:17.190970 26133 recover.cpp:580] Successfully joined the Paxos group
I0813 19:55:17.192183 26129 recover.cpp:464] Recover process terminated
I0813 19:55:17.192699 26123 slave.cpp:190] Slave started on 1)@172.17.2.10:60249
I0813 19:55:17.192741 26123 slave.cpp:191] Flags at startup: --authenticatee=""crammd5"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_kill_orphans=""true"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/tmp/mesos/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --initialize_driver_logging=""true"" --isolation=""posix/cpu,posix/mem"" --launcher_dir=""/mesos/mesos-0.24.0/_build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""1secs"" --resource_monitoring_interval=""1secs"" --resources=""cpus:2;mem:10240"" --revocable_cpu_low_priority=""true"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --version=""false"" --work_dir=""/tmp/mesos-II8Gua/0""
I0813 19:55:17.194514 26100 containerizer.cpp:143] Using isolation: posix/cpu,posix/mem,filesystem/posix
I0813 19:55:17.194658 26123 slave.cpp:354] Slave resources: cpus(*):2; mem(*):10240; disk(*):3.70122e+06; ports(*):[31000-32000]
I0813 19:55:17.194854 26123 slave.cpp:384] Slave hostname: 297daca2d01a
I0813 19:55:17.194877 26123 slave.cpp:389] Slave checkpoint: true
I0813 19:55:17.196751 26132 master.cpp:1524] The newly elected leader is master@172.17.2.10:60249 with id 20150813-195517-167907756-60249-26100
I0813 19:55:17.196797 26132 master.cpp:1537] Elected as the leading master!
I0813 19:55:17.196815 26132 master.cpp:1307] Recovering from registrar
I0813 19:55:17.197032 26138 registrar.cpp:311] Recovering registrar
I0813 19:55:17.197845 26132 slave.cpp:190] Slave started on 2)@172.17.2.10:60249
I0813 19:55:17.198420 26125 log.cpp:661] Attempting to start the writer
I0813 19:55:17.197948 26132 slave.cpp:191] Flags at startup: --authenticatee=""crammd5"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_kill_orphans=""true"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/tmp/mesos/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --initialize_driver_logging=""true"" --isolation=""posix/cpu,posix/mem"" --launcher_dir=""/mesos/mesos-0.24.0/_build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""1secs"" --resource_monitoring_interval=""1secs"" --resources=""cpus:2;mem:10240"" --revocable_cpu_low_priority=""true"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --version=""false"" --work_dir=""/tmp/mesos-II8Gua/1""
I0813 19:55:17.199121 26132 slave.cpp:354] Slave resources: cpus(*):2; mem(*):10240; disk(*):3.70122e+06; ports(*):[31000-32000]
I0813 19:55:17.199235 26138 state.cpp:54] Recovering state from '/tmp/mesos-II8Gua/0/meta'
I0813 19:55:17.199322 26132 slave.cpp:384] Slave hostname: 297daca2d01a
I0813 19:55:17.199345 26132 slave.cpp:389] Slave checkpoint: true
I0813 19:55:17.199676 26100 containerizer.cpp:143] Using isolation: posix/cpu,posix/mem,filesystem/posix
I0813 19:55:17.200085 26135 state.cpp:54] Recovering state from '/tmp/mesos-II8Gua/1/meta'
I0813 19:55:17.200317 26132 status_update_manager.cpp:202] Recovering status update manager
I0813 19:55:17.200371 26129 status_update_manager.cpp:202] Recovering status update manager
I0813 19:55:17.202003 26129 replica.cpp:477] Replica received implicit promise request with proposal 1
I0813 19:55:17.202585 26131 slave.cpp:190] Slave started on 3)@172.17.2.10:60249
I0813 19:55:17.202596 26129 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 523191ns
I0813 19:55:17.202756 26129 replica.cpp:345] Persisted promised to 1
I0813 19:55:17.202770 26132 containerizer.cpp:379] Recovering containerizer
I0813 19:55:17.203061 26135 containerizer.cpp:379] Recovering containerizer
I0813 19:55:17.202663 26131 slave.cpp:191] Flags at startup: --authenticatee=""crammd5"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_kill_orphans=""true"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/tmp/mesos/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --initialize_driver_logging=""true"" --isolation=""posix/cpu,posix/mem"" --launcher_dir=""/mesos/mesos-0.24.0/_build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""1secs"" --resource_monitoring_interval=""1secs"" --resources=""cpus:2;mem:10240"" --revocable_cpu_low_priority=""true"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --version=""false"" --work_dir=""/tmp/mesos-II8Gua/2""
I0813 19:55:17.203819 26131 slave.cpp:354] Slave resources: cpus(*):2; mem(*):10240; disk(*):3.70122e+06; ports(*):[31000-32000]
I0813 19:55:17.203930 26131 slave.cpp:384] Slave hostname: 297daca2d01a
I0813 19:55:17.203948 26131 slave.cpp:389] Slave checkpoint: true
I0813 19:55:17.204674 26137 state.cpp:54] Recovering state from '/tmp/mesos-II8Gua/2/meta'
I0813 19:55:17.205178 26135 status_update_manager.cpp:202] Recovering status update manager
I0813 19:55:17.205323 26135 containerizer.cpp:379] Recovering containerizer
I0813 19:55:17.205521 26136 slave.cpp:4069] Finished recovery
I0813 19:55:17.206074 26136 slave.cpp:4226] Querying resource estimator for oversubscribable resources
I0813 19:55:17.206424 26128 slave.cpp:4069] Finished recovery
I0813 19:55:17.206722 26137 status_update_manager.cpp:176] Pausing sending status updates
I0813 19:55:17.206858 26136 slave.cpp:684] New master detected at master@172.17.2.10:60249
I0813 19:55:17.206902 26138 slave.cpp:4069] Finished recovery
I0813 19:55:17.206962 26128 slave.cpp:4226] Querying resource estimator for oversubscribable resources
I0813 19:55:17.208312 26134 scheduler.cpp:272] New master detected at master@172.17.2.10:60249
I0813 19:55:17.208364 26136 slave.cpp:709] No credentials provided. Attempting to register without authentication
I0813 19:55:17.208608 26136 slave.cpp:720] Detecting new master
I0813 19:55:17.208839 26138 slave.cpp:4226] Querying resource estimator for oversubscribable resources
I0813 19:55:17.209216 26123 coordinator.cpp:231] Coordinator attemping to fill missing position
I0813 19:55:17.209247 26127 status_update_manager.cpp:176] Pausing sending status updates
I0813 19:55:17.209259 26128 slave.cpp:684] New master detected at master@172.17.2.10:60249
I0813 19:55:17.209322 26127 status_update_manager.cpp:176] Pausing sending status updates
I0813 19:55:17.209364 26128 slave.cpp:709] No credentials provided. Attempting to register without authentication
I0813 19:55:17.209344 26138 slave.cpp:684] New master detected at master@172.17.2.10:60249
I0813 19:55:17.209455 26128 slave.cpp:720] Detecting new master
I0813 19:55:17.209492 26138 slave.cpp:709] No credentials provided. Attempting to register without authentication
I0813 19:55:17.209573 26128 slave.cpp:4240] Received oversubscribable resources  from the resource estimator
I0813 19:55:17.209601 26138 slave.cpp:720] Detecting new master
I0813 19:55:17.209730 26138 slave.cpp:4240] Received oversubscribable resources  from the resource estimator
I0813 19:55:17.209883 26136 slave.cpp:4240] Received oversubscribable resources  from the resource estimator
I0813 19:55:17.211266 26136 replica.cpp:378] Replica received explicit promise request for position 0 with proposal 2
I0813 19:55:17.211771 26136 leveldb.cpp:343] Persisting action (8 bytes) to leveldb took 462128ns
I0813 19:55:17.211797 26136 replica.cpp:679] Persisted action at 0
I0813 19:55:17.212980 26130 replica.cpp:511] Replica received write request for position 0
I0813 19:55:17.213124 26130 leveldb.cpp:438] Reading position from leveldb took 67075ns
I0813 19:55:17.213580 26130 leveldb.cpp:343] Persisting action (14 bytes) to leveldb took 301649ns
I0813 19:55:17.213603 26130 replica.cpp:679] Persisted action at 0
I0813 19:55:17.214284 26123 replica.cpp:658] Replica received learned notice for position 0
I0813 19:55:17.214622 26123 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 284547ns
I0813 19:55:17.214648 26123 replica.cpp:679] Persisted action at 0
I0813 19:55:17.214675 26123 replica.cpp:664] Replica learned NOP action at position 0
I0813 19:55:17.215420 26136 log.cpp:677] Writer started with ending position 0
I0813 19:55:17.217463 26133 leveldb.cpp:438] Reading position from leveldb took 47943ns
I0813 19:55:17.220762 26125 registrar.cpp:344] Successfully fetched the registry (0B) in 23.649024ms
I0813 19:55:17.221081 26125 registrar.cpp:443] Applied 1 operations in 136902ns; attempting to update the 'registry'
I0813 19:55:17.223667 26133 log.cpp:685] Attempting to append 174 bytes to the log
I0813 19:55:17.223778 26125 coordinator.cpp:341] Coordinator attempting to write APPEND action at position 1
I0813 19:55:17.224516 26127 replica.cpp:511] Replica received write request for position 1
I0813 19:55:17.225009 26127 leveldb.cpp:343] Persisting action (193 bytes) to leveldb took 466230ns
I0813 19:55:17.225042 26127 replica.cpp:679] Persisted action at 1
I0813 19:55:17.225653 26126 replica.cpp:658] Replica received learned notice for position 1
I0813 19:55:17.225953 26126 leveldb.cpp:343] Persisting action (195 bytes) to leveldb took 286966ns
I0813 19:55:17.225975 26126 replica.cpp:679] Persisted action at 1
I0813 19:55:17.226013 26126 replica.cpp:664] Replica learned APPEND action at position 1
I0813 19:55:17.227545 26137 registrar.cpp:488] Successfully updated the 'registry' in 6.328064ms
I0813 19:55:17.227722 26137 registrar.cpp:374] Successfully recovered registrar
I0813 19:55:17.227918 26124 log.cpp:704] Attempting to truncate the log to 1
I0813 19:55:17.228024 26133 coordinator.cpp:341] Coordinator attempting to write TRUNCATE action at position 2
I0813 19:55:17.228193 26131 master.cpp:1334] Recovered 0 slaves from the Registry (135B) ; allowing 10mins for slaves to re-register
I0813 19:55:17.228659 26127 replica.cpp:511] Replica received write request for position 2
I0813 19:55:17.228972 26127 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 297903ns
I0813 19:55:17.229004 26127 replica.cpp:679] Persisted action at 2
I0813 19:55:17.229565 26127 replica.cpp:658] Replica received learned notice for position 2
I0813 19:55:17.229837 26127 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 260326ns
I0813 19:55:17.229899 26127 leveldb.cpp:401] Deleting ~1 keys from leveldb took 48697ns
I0813 19:55:17.229923 26127 replica.cpp:679] Persisted action at 2
I0813 19:55:17.229956 26127 replica.cpp:664] Replica learned TRUNCATE action at position 2
I0813 19:55:17.325634 26138 slave.cpp:1209] Will retry registration in 445.955946ms if necessary
I0813 19:55:17.326088 26124 master.cpp:3635] Registering slave at slave(2)@172.17.2.10:60249 (297daca2d01a) with id 20150813-195517-167907756-60249-26100-S0
I0813 19:55:17.327446 26124 registrar.cpp:443] Applied 1 operations in 231072ns; attempting to update the 'registry'
I0813 19:55:17.330252 26136 log.cpp:685] Attempting to append 344 bytes to the log
I0813 19:55:17.330407 26132 coordinator.cpp:341] Coordinator attempting to write APPEND action at position 3
I0813 19:55:17.331418 26128 replica.cpp:511] Replica received write request for position 3
I0813 19:55:17.331753 26128 leveldb.cpp:343] Persisting action (363 bytes) to leveldb took 264140ns
I0813 19:55:17.331778 26128 replica.cpp:679] Persisted action at 3
I0813 19:55:17.332324 26133 replica.cpp:658] Replica received learned notice for position 3
I0813 19:55:17.332809 26133 leveldb.cpp:343] Persisting action (365 bytes) to leveldb took 313064ns
I0813 19:55:17.332834 26133 replica.cpp:679] Persisted action at 3
I0813 19:55:17.332865 26133 replica.cpp:664] Replica learned APPEND action at position 3
I0813 19:55:17.334211 26132 registrar.cpp:488] Successfully updated the 'registry' in 6.668032ms
I0813 19:55:17.334430 26127 log.cpp:704] Attempting to truncate the log to 3
I0813 19:55:17.334566 26132 coordinator.cpp:341] Coordinator attempting to write TRUNCATE action at position 4
I0813 19:55:17.335283 26129 replica.cpp:511] Replica received write request for position 4
I0813 19:55:17.335615 26127 slave.cpp:3058] Received ping from slave-observer(1)@172.17.2.10:60249
I0813 19:55:17.335816 26129 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 458268ns
I0813 19:55:17.335908 26137 master.cpp:3698] Registered slave 20150813-195517-167907756-60249-26100-S0 at slave(2)@172.17.2.10:60249 (297daca2d01a) with cpus(*):2; mem(*):10240; disk(*):3.70122e+06; ports(*):[31000-32000]
I0813 19:55:17.335983 26129 replica.cpp:679] Persisted action at 4
I0813 19:55:17.336019 26136 slave.cpp:859] Registered with master master@172.17.2.10:60249; given slave ID 20150813-195517-167907756-60249-26100-S0
I0813 19:55:17.336073 26136 fetcher.cpp:77] Clearing fetcher cache
I0813 19:55:17.336220 26127 status_update_manager.cpp:183] Resuming sending status updates
I0813 19:55:17.336328 26128 hierarchical.hpp:540] Added slave 20150813-195517-167907756-60249-26100-S0 (297daca2d01a) with cpus(*):2; mem(*):10240; disk(*):3.70122e+06; ports(*):[31000-32000] (allocated: )
I0813 19:55:17.336599 26138 replica.cpp:658] Replica received learned notice for position 4
I0813 19:55:17.336910 26128 hierarchical.hpp:1008] No resources available to allocate!
I0813 19:55:17.336957 26128 hierarchical.hpp:926] Performed allocation for slave 20150813-195517-167907756-60249-26100-S0 in 580663ns
I0813 19:55:17.337016 26136 slave.cpp:882] Checkpointing SlaveInfo to '/tmp/mesos-II8Gua/1/meta/slaves/20150813-195517-167907756-60249-26100-S0/slave.info'
I0813 19:55:17.337035 26138 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 403607ns
I0813 19:55:17.337138 26138 leveldb.cpp:401] Deleting ~2 keys from leveldb took 77040ns
I0813 19:55:17.337167 26138 replica.cpp:679] Persisted action at 4
I0813 19:55:17.337208 26138 replica.cpp:664] Replica learned TRUNCATE action at position 4
I0813 19:55:17.337514 26136 slave.cpp:918] Forwarding total oversubscribed resources 
I0813 19:55:17.337745 26131 master.cpp:3997] Received update of slave 20150813-195517-167907756-60249-26100-S0 at slave(2)@172.17.2.10:60249 (297daca2d01a) with total oversubscribed resources 
I0813 19:55:17.338240 26131 hierarchical.hpp:600] Slave 20150813-195517-167907756-60249-26100-S0 (297daca2d01a) updated with oversubscribed resources  (total: cpus(*):2; mem(*):10240; disk(*):3.70122e+06; ports(*):[31000-32000], allocated: )
I0813 19:55:17.338479 26131 hierarchical.hpp:1008] No resources available to allocate!
I0813 19:55:17.338505 26131 hierarchical.hpp:926] Performed allocation for slave 20150813-195517-167907756-60249-26100-S0 in 216259ns
I0813 19:55:17.504086 26124 slave.cpp:1209] Will retry registration in 1.92618421secs if necessary
I0813 19:55:17.504408 26124 master.cpp:3635] Registering slave at slave(3)@172.17.2.10:60249 (297daca2d01a) with id 20150813-195517-167907756-60249-26100-S1
I0813 19:55:17.505203 26124 registrar.cpp:443] Applied 1 operations in 144314ns; attempting to update the 'registry'
I0813 19:55:17.507616 26124 log.cpp:685] Attempting to append 511 bytes to the log
I0813 19:55:17.507796 26132 coordinator.cpp:341] Coordinator attempting to write APPEND action at position 5
I0813 19:55:17.508735 26128 replica.cpp:511] Replica received write request for position 5
I0813 19:55:17.509291 26128 leveldb.cpp:343] Persisting action (530 bytes) to leveldb took 527776ns
I0813 19:55:17.509328 26128 replica.cpp:679] Persisted action at 5
I0813 19:55:17.509945 26124 replica.cpp:658] Replica received learned notice for position 5
I0813 19:55:17.510393 26124 leveldb.cpp:343] Persisting action (532 bytes) to leveldb took 438543ns
I0813 19:55:17.510416 26124 replica.cpp:679] Persisted action at 5
I0813 19:55:17.510437 26124 replica.cpp:664] Replica learned APPEND action at position 5
I0813 19:55:17.511907 26125 registrar.cpp:488] Successfully updated the 'registry' in 6624us
I0813 19:55:17.512225 26138 log.cpp:704] Attempting to truncate the log to 5
I0813 19:55:17.512305 26136 coordinator.cpp:341] Coordinator attempting to write TRUNCATE action at position 6
I0813 19:55:17.513066 26133 slave.cpp:3058] Received ping from slave-observer(2)@172.17.2.10:60249
I0813 19:55:17.513242 26133 slave.cpp:859] Registered with master master@172.17.2.10:60249; given slave ID 20150813-195517-167907756-60249-26100-S1
I0813 19:55:17.513221 26126 master.cpp:3698] Registered slave 20150813-195517-167907756-60249-26100-S1 at slave(3)@172.17.2.10:60249 (297daca2d01a) with cpus(*):2; mem(*):10240; disk(*):3.70122e+06; ports(*):[31000-32000]
I0813 19:55:17.513089 26129 replica.cpp:511] Replica received write request for position 6
I0813 19:55:17.513393 26133 fetcher.cpp:77] Clearing fetcher cache
I0813 19:55:17.513380 26138 hierarchical.hpp:540] Added slave 20150813-195517-167907756-60249-26100-S1 (297daca2d01a) with cpus(*):2; mem(*):10240; disk(*):3.70122e+06; ports(*):[31000-32000] (allocated: )
I0813 19:55:17.513805 26132 status_update_manager.cpp:183] Resuming sending status updates
I0813 19:55:17.513949 26129 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 340511ns
I0813 19:55:17.514046 26138 hierarchical.hpp:1008] No resources available to allocate!
I0813 19:55:17.514050 26129 replica.cpp:679] Persisted action at 6
I0813 19:55:17.514195 26133 slave.cpp:882] Checkpointing SlaveInfo to '/tmp/mesos-II8Gua/2/meta/slaves/20150813-195517-167907756-60249-26100-S1/slave.info'
I0813 19:55:17.514140 26138 hierarchical.hpp:926] Performed allocation for slave 20150813-195517-167907756-60249-26100-S1 in 417609ns
I0813 19:55:17.514704 26133 slave.cpp:918] Forwarding total oversubscribed resources 
I0813 19:55:17.514708 26138 replica.cpp:658] Replica received learned notice for position 6
I0813 19:55:17.514880 26133 master.cpp:3997] Received update of slave 20150813-195517-167907756-60249-26100-S1 at slave(3)@172.17.2.10:60249 (297daca2d01a) with total oversubscribed resources 
I0813 19:55:17.515244 26127 hierarchical.hpp:600] Slave 20150813-195517-167907756-60249-26100-S1 (297daca2d01a) updated with oversubscribed resources  (total: cpus(*):2; mem(*):10240; disk(*):3.70122e+06; ports(*):[31000-32000], allocated: )
I0813 19:55:17.515454 26138 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 640882ns
I0813 19:55:17.515522 26138 leveldb.cpp:401] Deleting ~2 keys from leveldb took 56550ns
I0813 19:55:17.515547 26138 replica.cpp:679] Persisted action at 6
I0813 19:55:17.515581 26138 replica.cpp:664] Replica learned TRUNCATE action at position 6
I0813 19:55:17.515802 26127 hierarchical.hpp:1008] No resources available to allocate!
I0813 19:55:17.515866 26127 hierarchical.hpp:926] Performed allocation for slave 20150813-195517-167907756-60249-26100-S1 in 591007ns
I0813 19:55:17.984196 26135 slave.cpp:1209] Will retry registration in 1.542495291secs if necessary
I0813 19:55:17.984391 26138 master.cpp:3635] Registering slave at slave(1)@172.17.2.10:60249 (297daca2d01a) with id 20150813-195517-167907756-60249-26100-S2
I0813 19:55:17.985170 26133 registrar.cpp:443] Applied 1 operations in 202126ns; attempting to update the 'registry'
I0813 19:55:17.987498 26133 log.cpp:685] Attempting to append 678 bytes to the log
I0813 19:55:17.987656 26123 coordinator.cpp:341] Coordinator attempting to write APPEND action at position 7
I0813 19:55:17.988704 26138 replica.cpp:511] Replica received write request for position 7
I0813 19:55:17.989223 26138 leveldb.cpp:343] Persisting action (697 bytes) to leveldb took 490422ns
I0813 19:55:17.989248 26138 replica.cpp:679] Persisted action at 7
I0813 19:55:17.989972 26126 replica.cpp:658] Replica received learned notice for position 7
I0813 19:55:17.990401 26126 leveldb.cpp:343] Persisting action (699 bytes) to leveldb took 404333ns
I0813 19:55:17.990420 26126 replica.cpp:679] Persisted action at 7
I0813 19:55:17.990440 26126 replica.cpp:664] Replica learned APPEND action at position 7
I0813 19:55:17.994066 26123 registrar.cpp:488] Successfully updated the 'registry' in 8.788224ms
I0813 19:55:17.994436 26134 log.cpp:704] Attempting to truncate the log to 7
I0813 19:55:17.994575 26123 coordinator.cpp:341] Coordinator attempting to write TRUNCATE action at position 8
I0813 19:55:17.995070 26134 slave.cpp:3058] Received ping from slave-observer(3)@172.17.2.10:60249
I0813 19:55:17.995291 26134 slave.cpp:859] Registered with master master@172.17.2.10:60249; given slave ID 20150813-195517-167907756-60249-26100-S2
I0813 19:55:17.995319 26134 fetcher.cpp:77] Clearing fetcher cache
I0813 19:55:17.995246 26129 master.cpp:3698] Registered slave 20150813-195517-167907756-60249-26100-S2 at slave(1)@172.17.2.10:60249 (297daca2d01a) with cpus(*):2; mem(*):10240; disk(*):3.70122e+06; ports(*):[31000-32000]
I0813 19:55:17.995565 26123 status_update_manager.cpp:183] Resuming sending status updates
I0813 19:55:17.995579 26129 replica.cpp:511] Replica received write request for position 8
I0813 19:55:17.996016 26134 slave.cpp:882] Checkpointing SlaveInfo to '/tmp/mesos-II8Gua/0/meta/slaves/20150813-195517-167907756-60249-26100-S2/slave.info'
I0813 19:55:17.996039 26129 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 440511ns
I0813 19:55:17.996067 26129 replica.cpp:679] Persisted action at 8
I0813 19:55:17.996294 26128 hierarchical.hpp:540] Added slave 20150813-195517-167907756-60249-26100-S2 (297daca2d01a) with cpus(*):2; mem(*):10240; disk(*):3.70122e+06; ports(*):[31000-32000] (allocated: )
I0813 19:55:17.996556 26134 slave.cpp:918] Forwarding total oversubscribed resources 
I0813 19:55:17.996623 26133 replica.cpp:658] Replica received learned notice for position 8
I0813 19:55:17.997095 26134 master.cpp:3997] Received update of slave 20150813-195517-167907756-60249-26100-S2 at slave(1)@172.17.2.10:60249 (297daca2d01a) with total oversubscribed resources 
I0813 19:55:17.997263 26133 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 442619ns
I0813 19:55:17.997385 26133 leveldb.cpp:401] Deleting ~2 keys from leveldb took 95741ns
I0813 19:55:17.997413 26133 replica.cpp:679] Persisted action at 8
I0813 19:55:17.997465 26133 replica.cpp:664] Replica learned TRUNCATE action at position 8
I0813 19:55:17.997756 26128 hierarchical.hpp:1008] No resources available to allocate!
I0813 19:55:17.997925 26128 hierarchical.hpp:926] Performed allocation for slave 20150813-195517-167907756-60249-26100-S2 in 1.14489ms
I0813 19:55:17.998159 26128 hierarchical.hpp:600] Slave 20150813-195517-167907756-60249-26100-S2 (297daca2d01a) updated with oversubscribed resources  (total: cpus(*):2; mem(*):10240; disk(*):3.70122e+06; ports(*):[31000-32000], allocated: )
I0813 19:55:17.998445 26128 hierarchical.hpp:1008] No resources available to allocate!
I0813 19:55:17.998471 26128 hierarchical.hpp:926] Performed allocation for slave 20150813-195517-167907756-60249-26100-S2 in 218856ns
I0813 19:55:18.190146 26133 hierarchical.hpp:1008] No resources available to allocate!
I0813 19:55:18.190217 26133 hierarchical.hpp:908] Performed allocation for 3 slaves in 637042ns
I0813 19:55:19.191346 26131 hierarchical.hpp:1008] No resources available to allocate!
I0813 19:55:19.191915 26131 hierarchical.hpp:908] Performed allocation for 3 slaves in 1.215355ms
I0813 19:55:20.193631 26135 hierarchical.hpp:1008] No resources available to allocate!
I0813 19:55:20.193709 26135 hierarchical.hpp:908] Performed allocation for 3 slaves in 834491ns
I0813 19:55:21.194805 26134 hierarchical.hpp:1008] No resources available to allocate!
I0813 19:55:21.194870 26134 hierarchical.hpp:908] Performed allocation for 3 slaves in 536547ns
I0813 19:55:22.196143 26137 hierarchical.hpp:1008] No resources available to allocate!
I0813 19:55:22.196216 26137 hierarchical.hpp:908] Performed allocation for 3 slaves in 755140ns
I0813 19:55:23.197412 26132 hierarchical.hpp:1008] No resources available to allocate!
I0813 19:55:23.197979 26132 hierarchical.hpp:908] Performed allocation for 3 slaves in 1.223984ms
I0813 19:55:24.199429 26132 hierarchical.hpp:1008] No resources available to allocate!
I0813 19:55:24.199735 26132 hierarchical.hpp:908] Performed allocation for 3 slaves in 904654ns
I0813 19:55:25.200978 26127 hierarchical.hpp:1008] No resources available to allocate!
I0813 19:55:25.201206 26127 hierarchical.hpp:908] Performed allocation for 3 slaves in 939979ns
I0813 19:55:26.203023 26132 hierarchical.hpp:1008] No resources available to allocate!
I0813 19:55:26.203101 26132 hierarchical.hpp:908] Performed allocation for 3 slaves in 721178ns
I0813 19:55:27.204815 26126 hierarchical.hpp:1008] No resources available to allocate!
I0813 19:55:27.204888 26126 hierarchical.hpp:908] Performed allocation for 3 slaves in 767983ns
I0813 19:55:28.206374 26126 hierarchical.hpp:1008] No resources available to allocate!
I0813 19:55:28.206444 26126 hierarchical.hpp:908] Performed allocation for 3 slaves in 745214ns
I0813 19:55:29.207515 26124 hierarchical.hpp:1008] No resources available to allocate!
I0813 19:55:29.207579 26124 hierarchical.hpp:908] Performed allocation for 3 slaves in 551217ns
I0813 19:55:30.208966 26136 hierarchical.hpp:1008] No resources available to allocate!
I0813 19:55:30.209053 26136 hierarchical.hpp:908] Performed allocation for 3 slaves in 649887ns
I0813 19:55:31.210078 26123 hierarchical.hpp:1008] No resources available to allocate!
I0813 19:55:31.210144 26123 hierarchical.hpp:908] Performed allocation for 3 slaves in 558919ns
I0813 19:55:32.211027 26130 slave.cpp:4226] Querying resource estimator for oversubscribable resources
I0813 19:55:32.211045 26129 slave.cpp:4226] Querying resource estimator for oversubscribable resources
I0813 19:55:32.211084 26132 slave.cpp:4226] Querying resource estimator for oversubscribable resources
I0813 19:55:32.211386 26129 slave.cpp:4240] Received oversubscribable resources  from the resource estimator
I0813 19:55:32.211688 26132 slave.cpp:4240] Received oversubscribable resources  from the resource estimator
I0813 19:55:32.211853 26133 hierarchical.hpp:1008] No resources available to allocate!
I0813 19:55:32.212035 26133 hierarchical.hpp:908] Performed allocation for 3 slaves in 898985ns
I0813 19:55:32.212169 26133 slave.cpp:4240] Received oversubscribable resources  from the resource estimator
I0813 19:55:32.336745 26135 slave.cpp:3058] Received ping from slave-observer(1)@172.17.2.10:60249
I0813 19:55:32.514333 26129 slave.cpp:3058] Received ping from slave-observer(2)@172.17.2.10:60249
I0813 19:55:32.996134 26128 slave.cpp:3058] Received ping from slave-observer(3)@172.17.2.10:60249
I0813 19:55:33.213248 26128 hierarchical.hpp:1008] No resources available to allocate!
I0813 19:55:33.213326 26128 hierarchical.hpp:908] Performed allocation for 3 slaves in 827511ns
I0813 19:55:34.214326 26125 hierarchical.hpp:1008] No resources available to allocate!
I0813 19:55:34.214391 26125 hierarchical.hpp:908] Performed allocation for 3 slaves in 546422ns
I0813 19:55:35.215909 26123 hierarchical.hpp:1008] No resources available to allocate!
I0813 19:55:35.215973 26123 hierarchical.hpp:908] Performed allocation for 3 slaves in 627190ns
I0813 19:55:36.217156 26134 hierarchical.hpp:1008] No resources available to allocate!
I0813 19:55:36.217339 26134 hierarchical.hpp:908] Performed allocation for 3 slaves in 906249ns
I0813 19:55:37.218739 26132 hierarchical.hpp:1008] No resources available to allocate!
I0813 19:55:37.219169 26132 hierarchical.hpp:908] Performed allocation for 3 slaves in 1.102465ms
I0813 19:55:38.220641 26133 hierarchical.hpp:1008] No resources available to allocate!
I0813 19:55:38.220711 26133 hierarchical.hpp:908] Performed allocation for 3 slaves in 643146ns
I0813 19:55:39.221976 26133 hierarchical.hpp:1008] No resources available to allocate!
I0813 19:55:39.222118 26133 hierarchical.hpp:908] Performed allocation for 3 slaves in 845334ns
I0813 19:55:40.223338 26129 hierarchical.hpp:1008] No resources available to allocate!
I0813 19:55:40.223546 26129 hierarchical.hpp:908] Performed allocation for 3 slaves in 849995ns
I0813 19:55:41.225558 26138 hierarchical.hpp:1008] No resources available to allocate!
I0813 19:55:41.225752 26138 hierarchical.hpp:908] Performed allocation for 3 slaves in 958480ns
I0813 19:55:42.227176 26131 hierarchical.hpp:1008] No resources available to allocate!
I0813 19:55:42.227378 26131 hierarchical.hpp:908] Performed allocation for 3 slaves in 927048ns
I0813 19:55:43.228813 26137 hierarchical.hpp:1008] No resources available to allocate!
I0813 19:55:43.229441 26137 hierarchical.hpp:908] Performed allocation for 3 slaves in 1.310118ms
I0813 19:55:44.230828 26135 hierarchical.hpp:1008] No resources available to allocate!
I0813 19:55:44.231142 26135 hierarchical.hpp:908] Performed allocation for 3 slaves in 896369ns
I0813 19:55:45.232656 26135 hierarchical.hpp:1008] No resources available to allocate!
I0813 19:55:45.232903 26135 hierarchical.hpp:908] Performed allocation for 3 slaves in 1.357693ms
I0813 19:55:46.234973 26137 hierarchical.hpp:1008
{code}",Bug,Major,vinodkone,2016-02-08T23:34:47.000+0000,5,Resolved,Complete,EventCall Test Framework is flaky,2016-02-10T09:03:24.000+0000,MESOS-3273,5.0,mesos,Mesosphere Sprint 27
vinodkone,2015-08-14T23:20:04.000+0000,vinodkone,"Currently, we use our own serialization of bytes in json.hpp but we use picojson for deserialization.

We've observed that for some bytes the serialization results in a string that is incorrectly decoded by picojson.

Example:

String = """"\""\\/\b\f\n\r\t\x00\x19 !#[]\x7F\xFF""

Result of our own encoding:  ""\""\\\""\\\\\\/\\b\\f\\n\\r\\t\\u0000\\u0019 !#[]\\u007f\xFF\""""

picojson's encoding: ""\""\\\""\\\\\\/\\b\\f\\n\\r\\t\\u0000\\u0019 !#[]\\u007F\\u00FF\""""

Fix:
We just use picojson to serialize bytes for consistency.",Bug,Major,vinodkone,2015-08-15T02:09:47.000+0000,5,Resolved,Complete,JSON serialization/deserialization of bytes is incorrect,2015-08-15T02:09:47.000+0000,MESOS-3267,2.0,mesos,Twitter Mesos Q3 Sprint 3
jvanremoortere,2015-08-14T23:00:25.000+0000,kaysoky,"After using the {{/maintenance/stop}} endpoint to end maintenance on a machine, any deactivated agents must be reactivated and allowed to register with the master.",Task,Major,kaysoky,2015-09-14T18:06:30.000+0000,5,Resolved,Complete,Stopping/Completing maintenance needs to reactivate agents.,2015-09-25T22:46:28.000+0000,MESOS-3266,5.0,mesos,Mesosphere Sprint 17
jvanremoortere,2015-08-14T22:55:30.000+0000,kaysoky,"After using the {{/maintenance/start}} endpoint to begin maintenance on a machine, agents running on said machine should:
* Be deactivated such that no offers are sent from that agent.  (Investigate if {{Master::deactivate(Slave*)}} can be used or modified for this purpose.)
* Kill all tasks still running on the agent (See MESOS-1475).
* Prevent other agents on that machine from registering or sending out offers.  This will likely involve some modifications to {{Master::register}} and {{Master::reregister}}.

",Task,Major,kaysoky,2015-09-14T18:06:13.000+0000,5,Resolved,Complete,Starting maintenance needs to deactivate agents and kill tasks.,2015-09-25T22:46:30.000+0000,MESOS-3265,8.0,mesos,Mesosphere Sprint 17
vinodkone,2015-08-14T16:34:18.000+0000,vinodkone,"[ RUN      ] HTTPTest.NestedGet
../../../3rdparty/libprocess/src/tests/http_tests.cpp:459: Failure
Value of: response.get().status
  Actual: ""202 Accepted""
Expected: http::statuses[200]
Which is: ""200 OK""
*** Aborted at 1439569965 (unix time) try ""date -d @1439569965"" if you are using GNU date ***
PC: @           0x63abe8 testing::UnitTest::AddTestPartResult()
*** SIGSEGV (@0x0) received by PID 25766 (TID 0x7f499415c780) from PID 0; stack trace: ***
    @     0x7f499224dca0 (unknown)
    @           0x63abe8 testing::UnitTest::AddTestPartResult()
    @           0x62f6af testing::internal::AssertHelper::operator=()
    @           0x43cd78 HTTPTest_NestedGet_Test::TestBody()
    @           0x65935e testing::internal::HandleSehExceptionsInMethodIfSupported<>()
    @           0x653c5e testing::internal::HandleExceptionsInMethodIfSupported<>()
    @           0x6349a3 testing::Test::Run()
    @           0x635128 testing::TestInfo::Run()
    @           0x635778 testing::TestCase::Run()
    @           0x63c0e2 testing::internal::UnitTestImpl::RunAllTests()
    @           0x65a11d testing::internal::HandleSehExceptionsInMethodIfSupported<>()
    @           0x654958 testing::internal::HandleExceptionsInMethodIfSupported<>()
    @           0x63ae08 testing::UnitTest::Run()
    @           0x4877f9 RUN_ALL_TESTS()
    @           0x487613 main
    @     0x7f49915739f4 __libc_start_main
",Bug,Major,vinodkone,2015-08-14T17:44:22.000+0000,5,Resolved,Complete,HTTPTest.NestedGet is flaky,2015-08-14T17:44:22.000+0000,MESOS-3262,2.0,mesos,Twitter Mesos Q3 Sprint 3
pbrett,2015-08-12T00:44:04.000+0000,pbrett,"CHECK in clean up of ContainerizerTest causes test harness to abort rather than fail or skip only perf related tests.

[ RUN      ] SlaveRecoveryTest/0.RestartBeforeContainerizerLaunch
[       OK ] SlaveRecoveryTest/0.RestartBeforeContainerizerLaunch (628 ms)
[----------] 24 tests from SlaveRecoveryTest/0 (38986 ms total)

[----------] 4 tests from MesosContainerizerSlaveRecoveryTest
[ RUN      ] MesosContainerizerSlaveRecoveryTest.ResourceStatistics
../../src/tests/mesos.cpp:720: Failure
cgroups::mount(hierarchy, subsystem): 'perf_event' is already attached to another hierarchy
-------------------------------------------------------------
We cannot run any cgroups tests that require
a hierarchy with subsystem 'perf_event'
because we failed to find an existing hierarchy
or create a new one (tried '/tmp/mesos_test_cgroup/perf_event').
You can either remove all existing
hierarchies, or disable this test case
(i.e., --gtest_filter=-MesosContainerizerSlaveRecoveryTest.*).
-------------------------------------------------------------
F0811 17:23:43.874696 12955 mesos.cpp:774] CHECK_SOME(cgroups): '/tmp/mesos_test_cgroup/perf_event' is not a valid hierarchy 
*** Check failure stack trace: ***
    @     0x7fb2fb4835fd  google::LogMessage::Fail()
    @     0x7fb2fb48543d  google::LogMessage::SendToLog()
    @     0x7fb2fb4831ec  google::LogMessage::Flush()
    @     0x7fb2fb485d39  google::LogMessageFatal::~LogMessageFatal()
    @           0x4e3f98  _CheckFatal::~_CheckFatal()
    @           0x82f25a  mesos::internal::tests::ContainerizerTest<>::TearDown()
    @           0xc030e3  testing::internal::HandleExceptionsInMethodIfSupported<>()
    @           0xbf9050  testing::Test::Run()
    @           0xbf912e  testing::TestInfo::Run()
    @           0xbf9235  testing::TestCase::Run()
    @           0xbf94e8  testing::internal::UnitTestImpl::RunAllTests()
    @           0xbf97a4  testing::UnitTest::Run()
    @           0x4a9df3  main
    @     0x7fb2f9371ec5  (unknown)
    @           0x4b63ee  (unknown)
Build step 'Execute shell' marked build as failure",Bug,Major,pbrett,2015-09-14T18:34:16.000+0000,5,Resolved,Complete,Cgroup CHECK fails test harness,2015-09-14T18:34:16.000+0000,MESOS-3254,2.0,mesos,Twitter Mesos Q3 Sprint 3
pbrett,2015-08-11T19:59:08.000+0000,pbrett,"In PortMappingStatistics::execute, we log the following errors to stderr if the egress rate limiting qdiscs are not configured inside the container.

{code}
Failed to get the network statistics for the htb qdisc on eth0
Failed to get the network statistics for the fq_codel qdisc on eth0
{code}

This can occur because of an error reading the qdisc (statistics function return an error) or because the qdisc does not exist (function returns none).  

We should not log an error when the qdisc does not exist since this is normal behaviour if the container is created without rate limiting.  We do not want to gate this function on the slave rate limiting flag since we would have to compare the behaviour against the flag value at the time the container was created.",Bug,Major,pbrett,2015-08-13T05:34:38.000+0000,5,Resolved,Complete,Ignore no statistics condition for containers with no qdisc,2015-08-13T05:34:38.000+0000,MESOS-3252,2.0,mesos,Twitter Mesos Q3 Sprint 3
jojy,2015-08-11T18:42:37.000+0000,jojy,"Currently libprocess http API sets the ""Host"" header field from the peer socket address (IP:port). The problem is that socket address might not be right HTTP server and might be just a proxy. ",Bug,Major,jojy,2015-08-18T16:41:46.000+0000,5,Resolved,Complete,"http::get API evaluates ""host"" wrongly",2015-08-18T16:41:46.000+0000,MESOS-3251,1.0,mesos,Mesosphere Sprint 16
vinodkone,2015-08-07T21:31:45.000+0000,vinodkone,"For example, if master adds a route ""/api/v1/scheduler"",  a handler named ""api/v1/scheduler"" is added to 'master' libprocess.

But when a request is posted to the above path, process::visit() looks for a http handler named ""api"" instead of ""api/v1/scheduler"".

Ideally libprocess should look for handlers in the following preference order:

""api/v1/scheduler""  --> ""api/v1"" --> ""api""

",Bug,Major,vinodkone,2015-08-11T21:29:45.000+0000,5,Resolved,Complete,HTTP requests with nested path are not properly handled by libprocess,2015-08-11T21:29:45.000+0000,MESOS-3237,2.0,mesos,Twitter Mesos Q3 Sprint 3
karya,2015-08-07T18:43:32.000+0000,karya,"If that task being launched has a command executor, there is no way for
the hook to determine the executor-id for that task. The executor-id is sometimes required by the label decorators for accounting purposes and for preparing ground for executor-environment-decorator (which is not passed the TaskInfo).",Task,Major,karya,2015-08-11T16:06:08.000+0000,5,Resolved,Complete,Updated slave task label decorator hook to pass in ExecutorInfo.,2015-08-11T16:06:08.000+0000,MESOS-3236,1.0,mesos,Mesosphere Sprint 16
haosdent@gmail.com,2015-08-07T17:43:34.000+0000,kaysoky,"On OSX, {{make clean && make -j8 V=0 check}}:
{code}
[----------] 3 tests from FetcherCacheHttpTest
[ RUN      ] FetcherCacheHttpTest.HttpCachedSerialized
HTTP/1.1 200 OK
Date: Fri, 07 Aug 2015 17:23:05 GMT
Content-Length: 30

I0807 10:23:05.673596 2085372672 exec.cpp:133] Version: 0.24.0
E0807 10:23:05.675884 184373248 socket.hpp:173] Shutdown failed on fd=18: Socket is not connected [57]
I0807 10:23:05.675897 182226944 exec.cpp:207] Executor registered on slave 20150807-102305-139395082-52338-52313-S0
E0807 10:23:05.683980 184373248 socket.hpp:173] Shutdown failed on fd=18: Socket is not connected [57]
Registered executor on 10.0.79.8
Starting task 0
Forked command at 54363
sh -c './mesos-fetcher-test-cmd 0'
E0807 10:23:05.694953 184373248 socket.hpp:173] Shutdown failed on fd=18: Socket is not connected [57]
Command exited with status 0 (pid: 54363)
E0807 10:23:05.793927 184373248 socket.hpp:173] Shutdown failed on fd=18: Socket is not connected [57]
I0807 10:23:06.590008 2085372672 exec.cpp:133] Version: 0.24.0
E0807 10:23:06.592244 355938304 socket.hpp:173] Shutdown failed on fd=18: Socket is not connected [57]
I0807 10:23:06.592243 353255424 exec.cpp:207] Executor registered on slave 20150807-102305-139395082-52338-52313-S0
E0807 10:23:06.597995 355938304 socket.hpp:173] Shutdown failed on fd=18: Socket is not connected [57]
Registered executor on 10.0.79.8
Starting task 1
Forked command at 54411
sh -c './mesos-fetcher-test-cmd 1'
E0807 10:23:06.608708 355938304 socket.hpp:173] Shutdown failed on fd=18: Socket is not connected [57]
Command exited with status 0 (pid: 54411)
E0807 10:23:06.707649 355938304 socket.hpp:173] Shutdown failed on fd=18: Socket is not connected [57]
../../src/tests/fetcher_cache_tests.cpp:860: Failure
Failed to wait 15secs for awaitFinished(task.get())
*** Aborted at 1438968214 (unix time) try ""date -d @1438968214"" if you are using GNU date ***
[  FAILED  ] FetcherCacheHttpTest.HttpCachedSerialized (28685 ms)
[ RUN      ] FetcherCacheHttpTest.HttpCachedConcurrent
PC: @        0x113723618 process::Owned<>::get()
*** SIGSEGV (@0x0) received by PID 52313 (TID 0x118d59000) stack trace: ***
    @     0x7fff8fcacf1a _sigtramp
    @     0x7f9bc3109710 (unknown)
    @        0x1136f07e2 mesos::internal::slave::Fetcher::fetch()
    @        0x113862f9d mesos::internal::slave::MesosContainerizerProcess::fetch()
    @        0x1138f1b5d _ZZN7process8dispatchI7NothingN5mesos8internal5slave25MesosContainerizerProcessERKNS2_11ContainerIDERKNS2_11CommandInfoERKNSt3__112basic_stringIcNSC_11char_traitsIcEENSC_9allocatorIcEEEERK6OptionISI_ERKNS2_7SlaveIDES6_S9_SI_SM_SP_EENS_6FutureIT_EERKNS_3PIDIT0_EEMSW_FSU_T1_T2_T3_T4_T5_ET6_T7_T8_T9_T10_ENKUlPNS_11ProcessBaseEE_clES1D_
    @        0x1138f18cf _ZNSt3__110__function6__funcIZN7process8dispatchI7NothingN5mesos8internal5slave25MesosContainerizerProcessERKNS5_11ContainerIDERKNS5_11CommandInfoERKNS_12basic_stringIcNS_11char_traitsIcEENS_9allocatorIcEEEERK6OptionISK_ERKNS5_7SlaveIDES9_SC_SK_SO_SR_EENS2_6FutureIT_EERKNS2_3PIDIT0_EEMSY_FSW_T1_T2_T3_T4_T5_ET6_T7_T8_T9_T10_EUlPNS2_11ProcessBaseEE_NSI_IS1G_EEFvS1F_EEclEOS1F_
    @        0x1143768cf std::__1::function<>::operator()()
    @        0x11435ca7f process::ProcessBase::visit()
    @        0x1143ed6fe process::DispatchEvent::visit()
    @        0x1127aaaa1 process::ProcessBase::serve()
    @        0x114343b4e process::ProcessManager::resume()
    @        0x1143431ca process::internal::schedule()
    @        0x1143da646 _ZNSt3__114__thread_proxyINS_5tupleIJPFvvEEEEEEPvS5_
    @     0x7fff95090268 _pthread_body
    @     0x7fff950901e5 _pthread_start
    @     0x7fff9508e41d thread_start
Failed to synchronize with slave (it's probably exited)
make[3]: *** [check-local] Segmentation fault: 11
make[2]: *** [check-am] Error 2
make[1]: *** [check] Error 2
make: *** [check-recursive] Error 1
{code}

This was encountered just once out of 3+ {{make check}}s.",Bug,Major,kaysoky,,10006,Reviewable,New,FetcherCacheHttpTest.HttpCachedSerialized and FetcherCacheHttpTest.HttpCachedConcurrent are flaky,2016-05-05T17:55:25.000+0000,MESOS-3235,2.0,mesos,Mesosphere Sprint 20
,2015-08-07T01:21:40.000+0000,bmahler,"We currently don't have an abstraction in stout to capture the notion of having a container with many types and a single value. For example, in our abstractions like Try, rather than being able to say {{Either<Error, Value> t}} we must encode two Options ({{Option<Error>}}, {{Option<T>}}) with the implicit invariant that exactly one will be set.

This also comes in handy in many other places in the code. Note that we have the ability to (1) use C++11 unions now, as well as (2) use boost's variant directly instead of introducing Either. However, creating a named union every time this is needed is verbose, and unions require that we externally track which member is set. For variant, we already use this (e.g. json.hpp), but we can benefit from the better naming as Either.

Many languages expose Either as having only two values, left and right. I'd propose making this two or more, as is the case with variant.",Improvement,Major,bmahler,,10020,Accepted,In Progress,Introduce an Either type.,2015-08-07T18:09:48.000+0000,MESOS-3226,5.0,mesos,
bmahler,2015-08-07T00:52:04.000+0000,hartem,"Some variables in 
3rdparty/libprocess/3rdparty/stout/include/stout/version.hpp violate Mesos code style of biding '&' and '*' to the type name  (as opposed to binding to the variable name).",Bug,Minor,hartem,2015-08-07T01:01:40.000+0000,5,Resolved,Complete,some variables in version.hpp use `Type &var` instead of `Type& var`,2015-08-07T01:01:40.000+0000,MESOS-3225,1.0,mesos,
jojy,2015-08-06T20:59:36.000+0000,jojy,"Implement the following:
- A component that fetches JSON web authorization token from a given registry.
- Caches the token keyed on registry, service and scope
- Validates the cache for expiry date

Nice to have:
- Cache gets pruned as tokens are aged beyond expiration time. ",Task,Major,jojy,2015-09-10T20:06:06.000+0000,5,Resolved,Complete,Implement token manager for docker registry,2015-09-10T20:06:06.000+0000,MESOS-3223,4.0,mesos,Mesosphere Sprint 16
jojy,2015-08-06T20:49:38.000+0000,jojy,"Implement the following functionality:

- fetch manifest from remote registry based on authorization method dictated by the registry.
- fetch image layers from remote registry  based on authorization method dictated by the registry..
",Task,Major,jojy,2015-09-25T16:39:43.000+0000,5,Resolved,Complete,Implement docker registry client,2015-09-25T16:39:43.000+0000,MESOS-3222,5.0,mesos,Mesosphere Sprint 17
,2015-08-06T05:47:41.000+0000,hartem,"[ RUN      ] CgroupsAnyHierarchyWithPerfEventTest.ROOT_CGROUPS_Perf
../../src/tests/containerizer/cgroups_tests.cpp:172: Failure
(cgroups::destroy(hierarchy, cgroup)).failure(): Failed to remove cgroup '/sys/fs/cgroup/perf_event/mesos_test': Device or resource busy
../../src/tests/containerizer/cgroups_tests.cpp:190: Failure
(cgroups::destroy(hierarchy, cgroup)).failure(): Failed to remove cgroup '/sys/fs/cgroup/perf_event/mesos_test': Device or resource busy
[  FAILED  ] CgroupsAnyHierarchyWithPerfEventTest.ROOT_CGROUPS_Perf (9 ms)
[----------] 1 test from CgroupsAnyHierarchyWithPerfEventTest (9 ms total)
",Bug,Major,hartem,,10020,Accepted,In Progress,CgroupsAnyHierarchyWithPerfEventTest failing on Ubuntu 14.04,2016-03-01T17:15:46.000+0000,MESOS-3215,3.0,mesos,
jojy,2015-08-05T20:29:14.000+0000,jojy,Create design document for describing the component and interaction between Docker Registry Client and remote Docker Registry for token based authorization.,Task,Major,jojy,2015-08-17T17:55:49.000+0000,5,Resolved,Complete,Design doc for docker registry token manager,2015-08-17T17:55:49.000+0000,MESOS-3213,2.0,mesos,Mesosphere Sprint 16
marco-mesos,2015-08-05T19:08:52.000+0000,marco-mesos,"With the new JSON {{MasterInfo}} published to ZK, we want to provide a simple library class for Java Framework developers to retrieve info about the masters and the leader.",Story,Major,marco-mesos,2015-08-06T20:43:59.000+0000,5,Resolved,Complete,As a Java developer I want a simple way to obtain information about Master from ZooKeeper,2015-08-15T00:26:03.000+0000,MESOS-3212,2.0,mesos,
marco-mesos,2015-08-05T19:08:32.000+0000,marco-mesos,"With the new JSON {{MasterInfo}} published to ZK, we want to provide a simple library class for Python developers to retrieve info about the masters and the leader.",Story,Major,marco-mesos,2015-08-06T08:00:46.000+0000,5,Resolved,Complete,As a Python developer I want a simple way to obtain information about Master from ZooKeeper,2015-08-06T08:00:46.000+0000,MESOS-3211,2.0,mesos,Mesosphere Sprint 16
,2015-08-05T11:34:58.000+0000,bernd-mesos,"This is the first part of phase 1 as described in the comments for MESOS-2073. We add a field to CommandInfo::URI that contains the URI of a checksum file. When this file has new content, then the contents of the associated value URI needs to be refreshed in the fetcher cache. 

In this implementation step, we just add the above basic functionality (download, checksum comparison). In later steps, we will add more control flow to cover corner cases and thus make this feature more useful.
",Improvement,Minor,bernd-mesos,,10020,Accepted,In Progress,Fetch checksum files to inform fetcher cache use,2016-01-11T09:47:00.000+0000,MESOS-3208,3.0,mesos,Mesosphere Sprint 16
bernd-mesos,2015-08-05T09:11:19.000+0000,bernd-mesos,"Some paragraphs at the bottom of docs/mesos-c++-style-guide.md containing code sections are not rendered correctly by the web site generator. It looks fine in a github gist and apparently the syntax used is correct. 

",Bug,Minor,anandmazumdar,2015-08-05T10:01:36.000+0000,5,Resolved,Complete,C++ style guide is not rendered correctly (code section syntax disregarded),2015-08-15T00:26:02.000+0000,MESOS-3207,1.0,mesos,Mesosphere Sprint 16
jieyu,2015-08-04T23:45:15.000+0000,jieyu,"Given the design discussed in [MESOS-3004|https://issues.apache.org/jira/browse/MESOS-3004], one container might have multiple provisioned root filesystems. Only checkpointing the root filesystem for ContainerInfo::image does not make sense.

Also, we realized that checkpointing container root filesystem path is not necessary because each provisioner should be able to destroy root filesystems for a given container based on a canonical directory layout (e.g., <appc_rootfs_dir>/<container_id>/xxx).",Task,Major,jieyu,2015-08-08T00:17:12.000+0000,5,Resolved,Complete,No need to checkpoint container root filesystem path.,2015-08-08T00:17:12.000+0000,MESOS-3205,3.0,mesos,Twitter Mesos Q3 Sprint 3
vinodkone,2015-08-04T22:24:48.000+0000,vinodkone,"[ RUN      ] MasterAuthorizationTest.DuplicateRegistration
Using temporary directory '/tmp/MasterAuthorizationTest_DuplicateRegistration_NKT3f7'
I0804 22:16:01.578500 26185 leveldb.cpp:176] Opened db in 2.188338ms
I0804 22:16:01.579172 26185 leveldb.cpp:183] Compacted db in 645075ns
I0804 22:16:01.579211 26185 leveldb.cpp:198] Created db iterator in 15766ns
I0804 22:16:01.579227 26185 leveldb.cpp:204] Seeked to beginning of db in 1658ns
I0804 22:16:01.579238 26185 leveldb.cpp:273] Iterated through 0 keys in the db in 313ns
I0804 22:16:01.579282 26185 replica.cpp:744] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned
I0804 22:16:01.579787 26212 recover.cpp:449] Starting replica recovery
I0804 22:16:01.580075 26212 recover.cpp:475] Replica is in EMPTY status
I0804 22:16:01.581014 26205 replica.cpp:641] Replica in EMPTY status received a broadcasted recover request
I0804 22:16:01.581357 26211 recover.cpp:195] Received a recover response from a replica in EMPTY status
I0804 22:16:01.581761 26207 recover.cpp:566] Updating replica status to STARTING
I0804 22:16:01.582334 26218 master.cpp:377] Master 20150804-221601-2550141356-59302-26185 (d6d349cd895b) started on 172.17.0.152:59302
I0804 22:16:01.582355 26218 master.cpp:379] Flags at startup: --acls="""" --allocation_interval=""1secs"" --allocator=""HierarchicalDRF"" --authenticate=""true"" --authenticate_slaves=""true"" --authenticators=""crammd5"" --credentials=""/tmp/MasterAuthorizationTest_DuplicateRegistration_NKT3f7/credentials"" --framework_sorter=""drf"" --help=""false"" --initialize_driver_logging=""true"" --log_auto_initialize=""true"" --logbufsecs=""0"" --logging_level=""INFO"" --max_slave_ping_timeouts=""5"" --quiet=""false"" --recovery_slave_removal_limit=""100%"" --registry=""replicated_log"" --registry_fetch_timeout=""1mins"" --registry_store_timeout=""25secs"" --registry_strict=""true"" --root_submissions=""true"" --slave_ping_timeout=""15secs"" --slave_reregister_timeout=""10mins"" --user_sorter=""drf"" --version=""false"" --webui_dir=""/mesos/mesos-0.24.0/_inst/share/mesos/webui"" --work_dir=""/tmp/MasterAuthorizationTest_DuplicateRegistration_NKT3f7/master"" --zk_session_timeout=""10secs""
I0804 22:16:01.582711 26218 master.cpp:424] Master only allowing authenticated frameworks to register
I0804 22:16:01.582722 26218 master.cpp:429] Master only allowing authenticated slaves to register
I0804 22:16:01.582728 26218 credentials.hpp:37] Loading credentials for authentication from '/tmp/MasterAuthorizationTest_DuplicateRegistration_NKT3f7/credentials'
I0804 22:16:01.582929 26204 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 421543ns
I0804 22:16:01.582950 26204 replica.cpp:323] Persisted replica status to STARTING
I0804 22:16:01.583032 26218 master.cpp:468] Using default 'crammd5' authenticator
I0804 22:16:01.583132 26211 recover.cpp:475] Replica is in STARTING status
I0804 22:16:01.583154 26218 master.cpp:505] Authorization enabled
I0804 22:16:01.583356 26214 whitelist_watcher.cpp:79] No whitelist given
I0804 22:16:01.583411 26217 hierarchical.hpp:346] Initialized hierarchical allocator process
I0804 22:16:01.583976 26213 replica.cpp:641] Replica in STARTING status received a broadcasted recover request
I0804 22:16:01.584187 26209 recover.cpp:195] Received a recover response from a replica in STARTING status
I0804 22:16:01.584581 26213 master.cpp:1495] The newly elected leader is master@172.17.0.152:59302 with id 20150804-221601-2550141356-59302-26185
I0804 22:16:01.584609 26213 master.cpp:1508] Elected as the leading master!
I0804 22:16:01.584627 26213 master.cpp:1278] Recovering from registrar
I0804 22:16:01.584656 26204 recover.cpp:566] Updating replica status to VOTING
I0804 22:16:01.584770 26212 registrar.cpp:313] Recovering registrar
I0804 22:16:01.585261 26218 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 370526ns
I0804 22:16:01.585285 26218 replica.cpp:323] Persisted replica status to VOTING
I0804 22:16:01.585412 26216 recover.cpp:580] Successfully joined the Paxos group
I0804 22:16:01.585667 26216 recover.cpp:464] Recover process terminated
I0804 22:16:01.586047 26213 log.cpp:661] Attempting to start the writer
I0804 22:16:01.587164 26211 replica.cpp:477] Replica received implicit promise request with proposal 1
I0804 22:16:01.587549 26211 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 358261ns
I0804 22:16:01.587568 26211 replica.cpp:345] Persisted promised to 1
I0804 22:16:01.588173 26209 coordinator.cpp:230] Coordinator attemping to fill missing position
I0804 22:16:01.589316 26208 replica.cpp:378] Replica received explicit promise request for position 0 with proposal 2
I0804 22:16:01.589700 26208 leveldb.cpp:343] Persisting action (8 bytes) to leveldb took 351778ns
I0804 22:16:01.589721 26208 replica.cpp:679] Persisted action at 0
I0804 22:16:01.590698 26213 replica.cpp:511] Replica received write request for position 0
I0804 22:16:01.590754 26213 leveldb.cpp:438] Reading position from leveldb took 31557ns
I0804 22:16:01.591147 26213 leveldb.cpp:343] Persisting action (14 bytes) to leveldb took 321842ns
I0804 22:16:01.591167 26213 replica.cpp:679] Persisted action at 0
I0804 22:16:01.591790 26217 replica.cpp:658] Replica received learned notice for position 0
I0804 22:16:01.592133 26217 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 315281ns
I0804 22:16:01.592155 26217 replica.cpp:679] Persisted action at 0
I0804 22:16:01.592180 26217 replica.cpp:664] Replica learned NOP action at position 0
I0804 22:16:01.592686 26211 log.cpp:677] Writer started with ending position 0
I0804 22:16:01.593729 26205 leveldb.cpp:438] Reading position from leveldb took 26394ns
I0804 22:16:01.596165 26209 registrar.cpp:346] Successfully fetched the registry (0B) in 11.343104ms
I0804 22:16:01.596281 26209 registrar.cpp:445] Applied 1 operations in 26242ns; attempting to update the 'registry'
I0804 22:16:01.598415 26212 log.cpp:685] Attempting to append 178 bytes to the log
I0804 22:16:01.598563 26215 coordinator.cpp:340] Coordinator attempting to write APPEND action at position 1
I0804 22:16:01.599324 26215 replica.cpp:511] Replica received write request for position 1
I0804 22:16:01.599778 26215 leveldb.cpp:343] Persisting action (197 bytes) to leveldb took 420523ns
I0804 22:16:01.599800 26215 replica.cpp:679] Persisted action at 1
I0804 22:16:01.600349 26204 replica.cpp:658] Replica received learned notice for position 1
I0804 22:16:01.600684 26204 leveldb.cpp:343] Persisting action (199 bytes) to leveldb took 310315ns
I0804 22:16:01.600706 26204 replica.cpp:679] Persisted action at 1
I0804 22:16:01.600723 26204 replica.cpp:664] Replica learned APPEND action at position 1
I0804 22:16:01.601632 26213 registrar.cpp:490] Successfully updated the 'registry' in 5.287936ms
I0804 22:16:01.601747 26213 registrar.cpp:376] Successfully recovered registrar
I0804 22:16:01.601826 26215 log.cpp:704] Attempting to truncate the log to 1
I0804 22:16:01.601948 26210 coordinator.cpp:340] Coordinator attempting to write TRUNCATE action at position 2
I0804 22:16:01.602145 26208 master.cpp:1305] Recovered 0 slaves from the Registry (139B) ; allowing 10mins for slaves to re-register
I0804 22:16:01.602859 26219 replica.cpp:511] Replica received write request for position 2
I0804 22:16:01.603181 26219 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 284713ns
I0804 22:16:01.603209 26219 replica.cpp:679] Persisted action at 2
I0804 22:16:01.603984 26211 replica.cpp:658] Replica received learned notice for position 2
I0804 22:16:01.604313 26211 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 302445ns
I0804 22:16:01.604365 26211 leveldb.cpp:401] Deleting ~1 keys from leveldb took 29354ns
I0804 22:16:01.604387 26211 replica.cpp:679] Persisted action at 2
I0804 22:16:01.604408 26211 replica.cpp:664] Replica learned TRUNCATE action at position 2
I0804 22:16:01.616402 26185 sched.cpp:164] Version: 0.24.0
I0804 22:16:01.616902 26209 sched.cpp:262] New master detected at master@172.17.0.152:59302
I0804 22:16:01.617000 26209 sched.cpp:318] Authenticating with master master@172.17.0.152:59302
I0804 22:16:01.617019 26209 sched.cpp:325] Using default CRAM-MD5 authenticatee
I0804 22:16:01.617324 26212 authenticatee.cpp:115] Creating new client SASL connection
I0804 22:16:01.617550 26209 master.cpp:4405] Authenticating scheduler-ac5e7b68-e2d2-441c-a5f5-60c1ff8cf00c@172.17.0.152:59302
I0804 22:16:01.617641 26212 authenticator.cpp:406] Starting authentication session for crammd5_authenticatee(259)@172.17.0.152:59302
I0804 22:16:01.617858 26208 authenticator.cpp:92] Creating new server SASL connection
I0804 22:16:01.618140 26216 authenticatee.cpp:206] Received SASL authentication mechanisms: CRAM-MD5
I0804 22:16:01.618191 26216 authenticatee.cpp:232] Attempting to authenticate with mechanism 'CRAM-MD5'
I0804 22:16:01.618324 26213 authenticator.cpp:197] Received SASL authentication start
I0804 22:16:01.618413 26213 authenticator.cpp:319] Authentication requires more steps
I0804 22:16:01.618557 26216 authenticatee.cpp:252] Received SASL authentication step
I0804 22:16:01.618664 26216 authenticator.cpp:225] Received SASL authentication step
I0804 22:16:01.618703 26216 auxprop.cpp:102] Request to lookup properties for user: 'test-principal' realm: 'd6d349cd895b' server FQDN: 'd6d349cd895b' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0804 22:16:01.618719 26216 auxprop.cpp:174] Looking up auxiliary property '*userPassword'
I0804 22:16:01.618778 26216 auxprop.cpp:174] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0804 22:16:01.618820 26216 auxprop.cpp:102] Request to lookup properties for user: 'test-principal' realm: 'd6d349cd895b' server FQDN: 'd6d349cd895b' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0804 22:16:01.618834 26216 auxprop.cpp:124] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0804 22:16:01.618839 26216 auxprop.cpp:124] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0804 22:16:01.618857 26216 authenticator.cpp:311] Authentication success
I0804 22:16:01.618954 26219 authenticatee.cpp:292] Authentication success
I0804 22:16:01.619035 26204 master.cpp:4435] Successfully authenticated principal 'test-principal' at scheduler-ac5e7b68-e2d2-441c-a5f5-60c1ff8cf00c@172.17.0.152:59302
I0804 22:16:01.619083 26219 authenticator.cpp:424] Authentication session cleanup for crammd5_authenticatee(259)@172.17.0.152:59302
I0804 22:16:01.619309 26208 sched.cpp:407] Successfully authenticated with master master@172.17.0.152:59302
I0804 22:16:01.619335 26208 sched.cpp:713] Sending SUBSCRIBE call to master@172.17.0.152:59302
I0804 22:16:01.619494 26208 sched.cpp:746] Will retry registration in 439203ns if necessary
I0804 22:16:01.619627 26217 master.cpp:1812] Received SUBSCRIBE call for framework 'default' at scheduler-ac5e7b68-e2d2-441c-a5f5-60c1ff8cf00c@172.17.0.152:59302
I0804 22:16:01.619695 26217 master.cpp:1534] Authorizing framework principal 'test-principal' to receive offers for role '*'
I0804 22:16:01.620848 26217 sched.cpp:713] Sending SUBSCRIBE call to master@172.17.0.152:59302
I0804 22:16:01.620929 26217 sched.cpp:746] Will retry registration in 2.099193326secs if necessary
I0804 22:16:01.621036 26210 master.cpp:1812] Received SUBSCRIBE call for framework 'default' at scheduler-ac5e7b68-e2d2-441c-a5f5-60c1ff8cf00c@172.17.0.152:59302
I0804 22:16:01.621083 26210 master.cpp:1534] Authorizing framework principal 'test-principal' to receive offers for role '*'
I0804 22:16:01.621727 26217 master.cpp:1876] Subscribing framework default with checkpointing disabled and capabilities [  ]
I0804 22:16:01.621981 26208 sched.cpp:262] New master detected at master@172.17.0.152:59302
I0804 22:16:01.622131 26208 sched.cpp:318] Authenticating with master master@172.17.0.152:59302
I0804 22:16:01.622153 26208 sched.cpp:325] Using default CRAM-MD5 authenticatee
I0804 22:16:01.622323 26212 authenticatee.cpp:115] Creating new client SASL connection
I0804 22:16:01.622324 26210 hierarchical.hpp:391] Added framework 20150804-221601-2550141356-59302-26185-0000
I0804 22:16:01.622369 26210 hierarchical.hpp:1008] No resources available to allocate!
I0804 22:16:01.622386 26210 hierarchical.hpp:908] Performed allocation for 0 slaves in 28592ns
I0804 22:16:01.622511 26210 sched.cpp:640] Framework registered with 20150804-221601-2550141356-59302-26185-0000
I0804 22:16:01.622586 26210 sched.cpp:654] Scheduler::registered took 48005ns
I0804 22:16:01.622592 26208 master.cpp:4405] Authenticating scheduler-ac5e7b68-e2d2-441c-a5f5-60c1ff8cf00c@172.17.0.152:59302
I0804 22:16:01.622673 26212 authenticator.cpp:406] Starting authentication session for crammd5_authenticatee(260)@172.17.0.152:59302
I0804 22:16:01.622923 26205 authenticator.cpp:92] Creating new server SASL connection
I0804 22:16:01.623112 26204 authenticatee.cpp:206] Received SASL authentication mechanisms: CRAM-MD5
I0804 22:16:01.623133 26216 master.cpp:1870] Dropping SUBSCRIBE call for framework 'default' at scheduler-ac5e7b68-e2d2-441c-a5f5-60c1ff8cf00c@172.17.0.152:59302: Re-authentication in progress
I0804 22:16:01.623144 26204 authenticatee.cpp:232] Attempting to authenticate with mechanism 'CRAM-MD5'
I0804 22:16:01.623258 26215 authenticator.cpp:197] Received SASL authentication start
I0804 22:16:01.623313 26215 authenticator.cpp:319] Authentication requires more steps
I0804 22:16:01.623394 26215 authenticatee.cpp:252] Received SASL authentication step
I0804 22:16:01.623512 26212 authenticator.cpp:225] Received SASL authentication step
I0804 22:16:01.623546 26212 auxprop.cpp:102] Request to lookup properties for user: 'test-principal' realm: 'd6d349cd895b' server FQDN: 'd6d349cd895b' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0804 22:16:01.623564 26212 auxprop.cpp:174] Looking up auxiliary property '*userPassword'
I0804 22:16:01.623603 26212 auxprop.cpp:174] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0804 22:16:01.623622 26212 auxprop.cpp:102] Request to lookup properties for user: 'test-principal' realm: 'd6d349cd895b' server FQDN: 'd6d349cd895b' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0804 22:16:01.623631 26212 auxprop.cpp:124] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0804 22:16:01.623636 26212 auxprop.cpp:124] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0804 22:16:01.623649 26212 authenticator.cpp:311] Authentication success
I0804 22:16:01.623777 26212 authenticatee.cpp:292] Authentication success
I0804 22:16:01.623846 26212 master.cpp:4435] Successfully authenticated principal 'test-principal' at scheduler-ac5e7b68-e2d2-441c-a5f5-60c1ff8cf00c@172.17.0.152:59302
I0804 22:16:01.623913 26212 authenticator.cpp:424] Authentication session cleanup for crammd5_authenticatee(260)@172.17.0.152:59302
I0804 22:16:01.624130 26212 sched.cpp:407] Successfully authenticated with master master@172.17.0.152:59302
I0804 22:16:02.583772 26218 hierarchical.hpp:1008] No resources available to allocate!
I0804 22:16:02.583818 26218 hierarchical.hpp:908] Performed allocation for 0 slaves in 80538ns
I0804 22:16:03.585110 26211 hierarchical.hpp:1008] No resources available to allocate!
I0804 22:16:03.585156 26211 hierarchical.hpp:908] Performed allocation for 0 slaves in 69272ns
I0804 22:16:04.586539 26214 hierarchical.hpp:1008] No resources available to allocate!
I0804 22:16:04.586586 26214 hierarchical.hpp:908] Performed allocation for 0 slaves in 79232ns
I0804 22:16:05.587239 26209 hierarchical.hpp:1008] No resources available to allocate!
I0804 22:16:05.587293 26209 hierarchical.hpp:908] Performed allocation for 0 slaves in 85128ns
I0804 22:16:06.587935 26212 hierarchical.hpp:1008] No resources available to allocate!
I0804 22:16:06.587985 26212 hierarchical.hpp:908] Performed allocation for 0 slaves in 78141ns
I0804 22:16:07.588817 26214 hierarchical.hpp:1008] No resources available to allocate!
I0804 22:16:07.588865 26214 hierarchical.hpp:908] Performed allocation for 0 slaves in 81433ns
I0804 22:16:08.589857 26214 hierarchical.hpp:1008] No resources available to allocate!
I0804 22:16:08.589906 26214 hierarchical.hpp:908] Performed allocation for 0 slaves in 71929ns
I0804 22:16:09.591085 26207 hierarchical.hpp:1008] No resources available to allocate!
I0804 22:16:09.591133 26207 hierarchical.hpp:908] Performed allocation for 0 slaves in 78223ns
I0804 22:16:10.591737 26207 hierarchical.hpp:1008] No resources available to allocate!
I0804 22:16:10.591785 26207 hierarchical.hpp:908] Performed allocation for 0 slaves in 71894ns
I0804 22:16:11.593166 26210 hierarchical.hpp:1008] No resources available to allocate!
I0804 22:16:11.593221 26210 hierarchical.hpp:908] Performed allocation for 0 slaves in 89782ns
I0804 22:16:12.593647 26212 hierarchical.hpp:1008] No resources available to allocate!
I0804 22:16:12.593689 26212 hierarchical.hpp:908] Performed allocation for 0 slaves in 69426ns
I0804 22:16:13.594154 26210 hierarchical.hpp:1008] No resources available to allocate!
I0804 22:16:13.594202 26210 hierarchical.hpp:908] Performed allocation for 0 slaves in 70581ns
I0804 22:16:14.594712 26207 hierarchical.hpp:1008] No resources available to allocate!
I0804 22:16:14.594758 26207 hierarchical.hpp:908] Performed allocation for 0 slaves in 71201ns
I0804 22:16:15.595412 26219 hierarchical.hpp:1008] No resources available to allocate!
I0804 22:16:15.595464 26219 hierarchical.hpp:908] Performed allocation for 0 slaves in 85183ns
I0804 22:16:16.596201 26217 hierarchical.hpp:1008] No resources available to allocate!
I0804 22:16:16.596247 26217 hierarchical.hpp:908] Performed allocation for 0 slaves in 95132ns
../../src/tests/master_authorization_tests.cpp:794: Failure
Failed to wait 15secs for frameworkRegisteredMessage
I0804 22:16:16.624354 26212 master.cpp:966] Framework 20150804-221601-2550141356-59302-26185-0000 (default) at scheduler-ac5e7b68-e2d2-441c-a5f5-60c1ff8cf00c@172.17.0.152:59302 disconnected
I0804 22:16:16.624398 26212 master.cpp:2092] Disconnecting framework 20150804-221601-2550141356-59302-26185-0000 (default) at scheduler-ac5e7b68-e2d2-441c-a5f5-60c1ff8cf00c@172.17.0.152:59302
I0804 22:16:16.624445 26212 master.cpp:2116] Deactivating framework 20150804-221601-2550141356-59302-26185-0000 (default) at scheduler-ac5e7b68-e2d2-441c-a5f5-60c1ff8cf00c@172.17.0.152:59302
I0804 22:16:16.624686 26212 master.cpp:988] Giving framework 20150804-221601-2550141356-59302-26185-0000 (default) at scheduler-ac5e7b68-e2d2-441c-a5f5-60c1ff8cf00c@172.17.0.152:59302 0ns to failover
I0804 22:16:16.625641 26219 hierarchical.hpp:474] Deactivated framework 20150804-221601-2550141356-59302-26185-0000
I0804 22:16:16.626688 26218 master.cpp:4180] Framework failover timeout, removing framework 20150804-221601-2550141356-59302-26185-0000 (default) at scheduler-ac5e7b68-e2d2-441c-a5f5-60c1ff8cf00c@172.17.0.152:59302
I0804 22:16:16.626734 26218 master.cpp:4759] Removing framework 20150804-221601-2550141356-59302-26185-0000 (default) at scheduler-ac5e7b68-e2d2-441c-a5f5-60c1ff8cf00c@172.17.0.152:59302
I0804 22:16:16.627074 26218 master.cpp:858] Master terminating
I0804 22:16:16.627218 26215 hierarchical.hpp:428] Removed framework 20150804-221601-2550141356-59302-26185-0000
../../3rdparty/libprocess/include/process/gmock.hpp:365: Failure
Actual function call count doesn't match EXPECT_CALL(filter->mock, filter(testing::A<const MessageEvent&>()))...
    Expected args: message matcher (8-byte object <98-98 02-AC 54-2B 00-00>, 1-byte object <97>, 1-byte object <D2>)
         Expected: to be called once
           Actual: never called - unsatisfied and active
[  FAILED  ] MasterAuthorizationTest.DuplicateRegistration (15056 ms)
",Bug,Major,vinodkone,2015-08-07T00:00:48.000+0000,5,Resolved,Complete,MasterAuthorizationTest.DuplicateRegistration test is flaky,2015-08-07T00:00:48.000+0000,MESOS-3203,1.0,mesos,Twitter Mesos Q3 Sprint 3
jvanremoortere,2015-08-04T17:58:28.000+0000,jvanremoortere,"Due to the arbitrary nature of the functions that are executed in handle_async, invoking them under the (A) {{watchers_mutex}} can lead to deadlocks if (B) is acquired before calling {{run_in_event_loop}} and (B) is also acquired within the arbitrary function.
{code}
==82679== Thread #10: lock order ""0x60774F8 before 0x60768C0"" violated
==82679== 
==82679== Observed (incorrect) order is: acquisition of lock at 0x60768C0
==82679==    at 0x4C32145: pthread_mutex_lock (in /usr/lib/valgrind/vgpreload_helgrind-amd64-linux.so)
==82679==    by 0x692C9B: __gthread_mutex_lock(pthread_mutex_t*) (gthr-default.h:748)
==82679==    by 0x6950BF: std::mutex::lock() (mutex:134)
==82679==    by 0x696219: Synchronized<std::mutex> synchronize<std::mutex>(std::mutex*)::{lambda(std::mutex*)#1}::operator()(std::mutex*) const (synchronized.hpp:58)
==82679==    by 0x696238: Synchronized<std::mutex> synchronize<std::mutex>(std::mutex*)::{lambda(std::mutex*)#1}::_FUN(std::mutex*) (synchronized.hpp:58)
==82679==    by 0x6984CF: Synchronized<std::mutex>::Synchronized(std::mutex*, void (*)(std::mutex*), void (*)(std::mutex*)) (synchronized.hpp:35)
==82679==    by 0x6962DE: Synchronized<std::mutex> synchronize<std::mutex>(std::mutex*) (synchronized.hpp:60)
==82679==    by 0x728FE1: process::handle_async(ev_loop*, ev_async*, int) (libev.cpp:48)
==82679==    by 0x761384: ev_invoke_pending (ev.c:2994)
==82679==    by 0x7643C4: ev_run (ev.c:3394)
==82679==    by 0x728E37: ev_loop (ev.h:826)
==82679==    by 0x729469: process::EventLoop::run() (libev.cpp:135)
==82679== 
==82679==  followed by a later acquisition of lock at 0x60774F8
==82679==    at 0x4C32145: pthread_mutex_lock (in /usr/lib/valgrind/vgpreload_helgrind-amd64-linux.so)
==82679==    by 0x4C6F9D: __gthread_mutex_lock(pthread_mutex_t*) (gthr-default.h:748)
==82679==    by 0x4C6FED: __gthread_recursive_mutex_lock(pthread_mutex_t*) (gthr-default.h:810)
==82679==    by 0x4F5D3D: std::recursive_mutex::lock() (mutex:175)
==82679==    by 0x516513: Synchronized<std::recursive_mutex> synchronize<std::recursive_mutex>(std::recursive_mutex*)::{lambda(std::recursive_mutex*)#1}::operator()(std::recursive_mutex*) const (synchronized.hpp:58)
==82679==    by 0x516532: Synchronized<std::recursive_mutex> synchronize<std::recursive_mutex>(std::recursive_mutex*)::{lambda(std::recursive_mutex*)#1}::_FUN(std::recursive_mutex*) (synchronized.hpp:58)
==82679==    by 0x52E619: Synchronized<std::recursive_mutex>::Synchronized(std::recursive_mutex*, void (*)(std::recursive_mutex*), void (*)(std::recursive_mutex*)) (synchronized.hpp:35)
==82679==    by 0x5165D4: Synchronized<std::recursive_mutex> synchronize<std::recursive_mutex>(std::recursive_mutex*) (synchronized.hpp:60)
==82679==    by 0x6BF4E1: process::ProcessManager::use(process::UPID const&) (process.cpp:2127)
==82679==    by 0x6C2B8C: process::ProcessManager::terminate(process::UPID const&, bool, process::ProcessBase*) (process.cpp:2604)
==82679==    by 0x6C6C3C: process::terminate(process::UPID const&, bool) (process.cpp:3107)
==82679==    by 0x692B65: process::Latch::trigger() (latch.cpp:53)
{code}

This was introduced in https://github.com/apache/mesos/commit/849fc4d361e40062073324153ba97e98e294fdf2",Bug,Blocker,jvanremoortere,2015-08-17T23:19:14.000+0000,5,Resolved,Complete,Libev handle_async can deadlock with run_in_event_loop,2015-08-17T23:19:14.000+0000,MESOS-3201,3.0,mesos,Mesosphere Sprint 16
mcypark,2015-08-04T17:41:44.000+0000,mcypark,"There exist {{fatal}} and {{fatalerror}} macros in both {{libprocess}} and {{stout}}. None of them are currently used as we favor {{glog}}'s {{LOG(FATAL)}}, and therefore should be removed.",Task,Major,mcypark,2015-08-05T12:04:49.000+0000,5,Resolved,Complete,Remove unused 'fatal' and 'fatalerror' macros,2015-08-05T12:04:49.000+0000,MESOS-3200,1.0,mesos,Mesosphere Sprint 16
js84,2015-08-04T13:30:23.000+0000,js84,We need to validate quota requests in terms of syntactical and semantical correctness.,Task,Major,js84,2015-11-23T09:24:47.000+0000,5,Resolved,Complete,Validate Quota Requests.,2015-11-23T09:24:47.000+0000,MESOS-3199,3.0,mesos,Mesosphere Sprint 16
hartem,2015-08-03T23:43:34.000+0000,mcypark,"Looks like this is due to {{mlockall}} being unimplemented on OS X.

{noformat}
[----------] 1 test from MemIsolatorTest/0, where TypeParam = N5mesos8internal5slave23PosixMemIsolatorProcessE
[ RUN      ] MemIsolatorTest/0.MemUsage
Failed to allocate RSS memory: Failed to make pages to be mapped unevictable: Function not implemented../../src/tests/containerizer/isolator_tests.cpp:812: Failure
helper.increaseRSS(allocation): Failed to sync with the subprocess
../../src/tests/containerizer/isolator_tests.cpp:815: Failure
(usage).failure(): Failed to get usage: No process found at 40558
[  FAILED  ] MemIsolatorTest/0.MemUsage, where TypeParam = N5mesos8internal5slave23PosixMemIsolatorProcessE (56 ms)
[----------] 1 test from MemIsolatorTest/0 (57 ms total)

[----------] 1 test from MemIsolatorTest/1, where TypeParam = N5mesos8internal5tests6ModuleINS_5slave8IsolatorELNS1_8ModuleIDE0EEE
[ RUN      ] MemIsolatorTest/1.MemUsage
Failed to allocate RSS memory: Failed to make pages to be mapped unevictable: Function not implemented../../src/tests/containerizer/isolator_tests.cpp:812: Failure
helper.increaseRSS(allocation): Failed to sync with the subprocess
../../src/tests/containerizer/isolator_tests.cpp:815: Failure
(usage).failure(): Failed to get usage: No process found at 40572
[  FAILED  ] MemIsolatorTest/1.MemUsage, where TypeParam = N5mesos8internal5tests6ModuleINS_5slave8IsolatorELNS1_8ModuleIDE0EEE (50 ms)
[----------] 1 test from MemIsolatorTest/1 (50 ms total)
{noformat}",Bug,Major,mcypark,2015-08-06T23:01:38.000+0000,5,Resolved,Complete,"MemIsolatorTest/{0,1}.MemUsage fails on OS X",2015-08-06T23:01:38.000+0000,MESOS-3197,2.0,mesos,Mesosphere Sprint 16
karya,2015-08-03T22:58:43.000+0000,karya,"Currently, the Executor doesn't always set TaskStatus.executor_id. This prevents the Slave TaskStatus label decorator hook from knowing the executor id.

An appropriate place to automatically fill in the executor_id is ExecutorProcesS::sendStatusUpdate() since we are already filling in some other information here.",Task,Major,karya,2015-08-05T20:54:28.000+0000,5,Resolved,Complete,Always set TaskStatus.executor_id when sending a status update message from Executor,2015-08-05T20:54:28.000+0000,MESOS-3196,1.0,mesos,Mesosphere Sprint 16
vinodkone,2015-08-03T21:10:02.000+0000,vinodkone,"Currently the master increments metrics for old style messages from the driver but not when it receives Calls. Since the driver is now sending Calls, master should update metrics correctly.",Bug,Major,vinodkone,2015-08-09T02:39:56.000+0000,5,Resolved,Complete,Fix master metrics for scheduler calls,2015-08-09T02:39:56.000+0000,MESOS-3195,3.0,mesos,Twitter Mesos Q3 Sprint 3
xujyan,2015-08-03T19:49:52.000+0000,xujyan,"It's going to be derived from this: https://reviews.apache.org/r/34140/ (and other related patches) but in the initial 'read-only' version the store's content is prepared by out-of-band mechanisms so the store component in Mesos only needs to provide access to images already in it and recover images upon slave restart.

This greatly simplifies the initial version's responsibility and test cases. Features that fetch the images into the store will be added later and they will take into consideration its impact on task start latency and slave restart responsiveness, etc.",Task,Major,xujyan,2015-08-13T22:39:18.000+0000,5,Resolved,Complete,Implement a 'read-only' AppC Image Store,2015-08-13T22:39:18.000+0000,MESOS-3194,5.0,mesos,Twitter Mesos Q3 Sprint 3
,2015-08-03T19:48:38.000+0000,xujyan,"Appc spec specifies two image discovery mechanisms: simple and meta discovery. We need to have an abstraction for image discovery in AppcStore. For MVP, we can implement the simple discovery first.

Update: simple discovery is removed from the spec. Meta discovery is the only discovery mechanism right now in the spec. Simple discovery is already shipped (we support an arbitrary operator specified [URI prefix|https://github.com/apache/mesos/blob/master/docs/container-image.md#appc-support-and-current-limitations]). So this ticket should focus on implementing Meta discovery.",Task,Major,xujyan,,10020,Accepted,In Progress,Implement AppC image discovery.,2016-03-14T18:09:26.000+0000,MESOS-3193,5.0,mesos,Mesosphere Sprint 27
xujyan,2015-08-03T19:47:00.000+0000,xujyan,"As I commented here: https://reviews.apache.org/r/34136/

Currently ContainerInfo::Image::Appc is defined as the following

{noformat:title=}
    message AppC {
      required string name = 1;
      required string id = 2;
      optional Labels labels = 3;
    }
{noformat}

In which the {{id}} is a required field. When users specify the image in tasks they likely will not use an image id (much like when you use docker or rkt to launch containers, you often use {{ubuntu}} or {{ubuntu:latest}} and seldom a SHA512 ID) and we should change it to be optional.

The motivating scenario is that: if the frameworks in the Mesos use something like {{image=ubuntu:14.04""}} to run a task and {{image=ubuntu}} defaults to {{image=ubuntu:latest}}, the operator can swap the latest version for all new tasks requesting {{image=ubuntu}}. If they allow users to specify {{image=ubuntu:live}}, they can swap the live version under the covers as well. This allows the operator to release important image updates (e.g., security patches) and have it picked up by new tasks in the cluster without asking the users to update their job/task configs.",Bug,Major,xujyan,2015-08-13T22:38:35.000+0000,5,Resolved,Complete,ContainerInfo::Image::AppC::id should be optional,2015-08-13T22:38:35.000+0000,MESOS-3192,1.0,mesos,Twitter Mesos Q3 Sprint 3
jojy,2015-08-03T19:40:35.000+0000,xujyan,It is useful for both appc and docker to compute and verify image hash.,Task,Major,xujyan,2016-01-28T17:42:20.000+0000,5,Resolved,Complete,Implement a utility for computing hash,2016-01-28T17:42:20.000+0000,MESOS-3191,2.0,mesos,
jvanremoortere,2015-08-03T19:18:37.000+0000,jvanremoortere,"[ RUN      ] TimeTest.Now
../../../3rdparty/libprocess/src/tests/time_tests.cpp:50: Failure
Expected: (Microseconds(10)) < (Clock::now() - t1), actual: 8-byte object <10-27 00-00 00-00 00-00> vs 0ns
[  FAILED  ] TimeTest.Now (0 ms)",Bug,Major,jvanremoortere,2015-08-31T13:34:33.000+0000,5,Resolved,Complete,TimeTest.Now fails with --enable-libevent,2015-08-31T13:34:52.000+0000,MESOS-3189,2.0,mesos,Mesosphere Sprint 17
pbrett,2015-07-31T21:06:36.000+0000,pbrett,"MESOS-2834 will enhance the perf isolator to support the different output formats provided by difference kernel versions.  In order to achieve this, it requires to execute the ""perf --version"" command. 

We should decompose the existing Subcommand processing in perf so that we can share the implementation between the multiple uses of perf.",Bug,Major,pbrett,2015-09-14T18:32:47.000+0000,5,Resolved,Complete,Refactor Subprocess logic in linux/perf.cpp to use common subroutine,2015-09-14T18:32:47.000+0000,MESOS-3185,3.0,mesos,
,2015-07-30T23:43:06.000+0000,jamesmulcahy,"Any images which are referenced from the generated docs ({{docs/*.md}}) do not show up on the website.  For example:
* [Architecture|http://mesos.apache.org/documentation/latest/architecture/]
* [External Containerizer|http://mesos.apache.org/documentation/latest/external-containerizer/]
* [Fetcher Cache Internals|http://mesos.apache.org/documentation/latest/fetcher-cache-internals/]
* [Maintenance|http://mesos.apache.org/documentation/latest/maintenance/] 	
* [Oversubscription|http://mesos.apache.org/documentation/latest/oversubscription/]
",Documentation,Minor,jamesmulcahy,2015-12-18T23:22:34.000+0000,5,Resolved,Complete,Documentation images do not load,2015-12-18T23:22:34.000+0000,MESOS-3183,3.0,mesos,
vinodkone,2015-07-30T23:37:34.000+0000,vinodkone,Currently Master::subscribe() calls into Master::registerFramework() and Master::reregisterFramework(). We should do it the other way around to be consistent with how we did all the other calls.,Improvement,Major,vinodkone,2015-08-04T17:55:22.000+0000,5,Resolved,Complete,Make Master::registerFramework() and Master::reregisterFramework() call into Master::subscribe(),2015-08-04T17:55:22.000+0000,MESOS-3182,3.0,mesos,Twitter Mesos Q3 Sprint 2
jieyu,2015-07-30T22:06:05.000+0000,jieyu,"Several tests need this abstraction, so it's better to unify them. For example, src/tests/containerizer/launch_tests.cpp needs to create a test rootfs. We also need that to test filesystem isolators.

The test rootfs can be created by copying files/directories from host file system.",Task,Major,jieyu,2015-08-08T00:14:41.000+0000,5,Resolved,Complete,Create a test abstraction for preparing test rootfs.,2015-08-08T00:14:41.000+0000,MESOS-3179,3.0,mesos,Twitter Mesos Q3 Sprint 2
jieyu,2015-07-30T21:59:20.000+0000,jieyu,"Syscall 'pivot_root' requires that the old and the new root are not in the same filesystem. Otherwise, the user will receive a ""Device or resource busy"" error.

Currently, we rely on the provisioner to prepare the rootfs and do proper bind mount if needed so that pivot_root can succeed. The drawback of this approach is that it potentially pollutes the host mount table which requires cleanup logics.

For instance, in the test, we create a test rootfs by copying the host files. We need to do a self bind mount so that we can pivot_root on it. That pollute the host mount table and it might leak mounts if test crashes before we do the lazy umount:
https://github.com/apache/mesos/blob/master/src/tests/containerizer/launch_tests.cpp#L96-L102

What I propose is that we always perform a recursive self bind mount of rootfs itself in fs::chroot::enter (after enter the new mount namespace). Seems that this is also done in libcontainer:
https://github.com/opencontainers/runc/blob/master/libcontainer/rootfs_linux.go#L402",Bug,Major,jieyu,2015-08-03T18:01:42.000+0000,5,Resolved,Complete,Perform a self bind mount of rootfs itself in fs::chroot::enter.,2015-08-03T18:01:50.000+0000,MESOS-3178,2.0,mesos,Twitter Mesos Q3 Sprint 2
gyliu,2015-07-30T11:09:58.000+0000,bernd-mesos,"When fetching an asset while not using the cache, the fetcher may erroneously report this: ""Copying instead of extracting resource from URI with 'extract' flag, because it does not seem to be an archive: "".

This message appears in the stderr log in the sandbox no matter whether extraction succeeded or not. It should be absent after successful extraction.
",Bug,Minor,bernd-mesos,2015-08-31T09:34:35.000+0000,5,Resolved,Complete,Fetcher logs erroneous message when successfully extracting an archive,2015-08-31T13:32:14.000+0000,MESOS-3174,1.0,mesos,
nfnt,2015-07-29T14:18:34.000+0000,nfnt,"The functions Path::basename and Path::dirname in stout/path.hpp are not marked const, although they could. Marking them const would remove some ambiguities in the usage of these functions.",Improvement,Trivial,nfnt,2015-07-29T16:22:14.000+0000,5,Resolved,Complete,"Mark Path::basename, Path::dirname as const functions.",2015-08-15T00:26:03.000+0000,MESOS-3173,1.0,mesos,Mesosphere Sprint 15
js84,2015-07-29T13:09:05.000+0000,js84,The fetcher tests use EXPECT validation for critical measures (e.g. non-empty results) and the subsequent logic releis on this (i.e. by accessing the first element). In such cases we should use ASSERT/CHECK.,Bug,Minor,js84,2015-07-29T13:46:54.000+0000,5,Resolved,Complete,Fetcher Tests use EXPECT while subsequent logic relies on the outcome.,2015-07-29T13:46:55.000+0000,MESOS-3171,1.0,mesos,Mesosphere Sprint 15
gyliu,2015-07-29T08:00:04.000+0000,jvanremoortere,"See Ben Mahler's comment in https://reviews.apache.org/r/32961/
FrameworkInfo should not be updated if the re-registration is invalid. This can happen in a few cases under the branching logic, so this requires some refactoring.
Notice that a {code}FrameworkErrorMessage{code} can be generated  both inside {code}else if (from != framework->pid){code} as well as from inside {code}failoverFramework(framework, from);{code}",Bug,Major,jvanremoortere,2015-09-19T18:07:23.000+0000,5,Resolved,Complete,FrameworkInfo should only be updated if the re-registration is valid,2015-09-19T18:07:23.000+0000,MESOS-3169,2.0,mesos,
vinodkone,2015-07-28T21:01:03.000+0000,vinodkone,"MesosZooKeeperTest fixture doesn't restart the ZooKeeper server for each test. This means if a test shuts down the ZooKeeper server, the next test (using the same fixture) might fail. 

For an example see https://reviews.apache.org/r/36807/",Bug,Major,vinodkone,2015-07-29T00:31:14.000+0000,5,Resolved,Complete,MesosZooKeeperTest fixture can have side effects across tests,2015-07-30T07:27:45.000+0000,MESOS-3168,2.0,mesos,Twitter Mesos Q3 Sprint 2
vinodkone,2015-07-28T20:52:03.000+0000,vinodkone,"In concert with the release of the HTTP API, we would also like to come up with a versioning strategy. This enables to do a meaningful 1.0 release.",Documentation,Major,vinodkone,2015-08-10T19:54:18.000+0000,5,Resolved,Complete,Design doc for versioning the HTTP API,2015-08-10T19:54:18.000+0000,MESOS-3167,3.0,mesos,Twitter Mesos Q3 Sprint 2
jojy,2015-07-28T17:46:49.000+0000,jojy,Create design document for the docker registry Authenticator component so that we have a baseline for the implementation. ,Bug,Major,jojy,2015-08-17T17:53:05.000+0000,5,Resolved,Complete,Design doc for docker image registry client,2015-08-17T17:53:05.000+0000,MESOS-3166,3.0,mesos,Mesosphere Sprint 15
alexr,2015-07-28T17:02:05.000+0000,alex-mesos,"To persist quotas across failovers, the Master should save them in the registry. To support this, we shall:
* Introduce a Quota state variable in registry.proto;
* Extend the Operation interface so that it supports a ‘Quota’ accumulator (see src/master/registrar.hpp);
* Introduce AddQuota / RemoveQuota operations;
* Recover quotas from the registry on failover to the Master’s internal::master::Role struct;
* Extend RegistrarTest with quota-specific tests.

NOTE: Registry variable can be rather big for production clusters (see MESOS-2075). While it should be fine for MVP to add quota information to registry, we should consider storing Quota separately, as this does not need to be in sync with slaves update. However, currently adding more variable is not supported by the registrar.

While the Agents are reregistering (note they may fail to do so), the information about what part of the quota is allocated is only partially available to the Master. In other words, the state of the quota allocation is reconstructed as Agents reregister. During this period, some roles may be under quota from the perspective of the newly elected Master.

The same problem exists on the allocator side: it may think the cluster is under quota and may eagerly try to satisfy quotas before enough Agents reregister, which may result in resources being allocated to frameworks beyond their quota. To address this issue and also to avoid panicking and generating under quota alerts, the Master should give a certain amount of time for the majority (e.g. 80%) of the Agents to reregister before reporting any quota status and notifying the allocator about granted quotas.",Task,Major,alexr,2015-11-22T21:44:28.000+0000,5,Resolved,Complete,Persist and recover quota to/from Registry,2015-11-22T21:44:28.000+0000,MESOS-3165,5.0,mesos,Mesosphere Sprint 22
js84,2015-07-28T16:53:29.000+0000,alex-mesos,A {{QuotaInfo}} protobuf message is internal representation for quota related information (e.g. for persisting quota). The protobuf message should be extendable for future needs and allows for easy aggregation across roles and operator principals. It may also be used to pass quota information to allocators.,Task,Major,alexr,2015-10-02T13:12:53.000+0000,5,Resolved,Complete,Introduce QuotaInfo message,2015-10-23T16:38:54.000+0000,MESOS-3164,3.0,mesos,Mesosphere Sprint 15
,2015-07-28T09:46:36.000+0000,js84,"The libprocess http.cpp post/get handlers currently do not consider query and fragments parts of the path correctly. 
E.g.
{code}
if (path.isSome()) {
    // TODO(benh): Get 'query' and/or 'fragment' out of 'path'.
    url.path = strings::join(""/"", url.path, path.get());
  }
{code}",Task,Major,js84,,10020,Accepted,In Progress,Proper handling of 'query' and/or 'fragment' out of 'path' in http handler.,2015-08-07T17:20:06.000+0000,MESOS-3163,1.0,mesos,
bmahler,2015-07-28T01:55:20.000+0000,bmahler,"If one uses an http::Pipe::Writer to stream a response, one cannot compare the writer with another to see if the connection has changed.

This is useful for example, in the master's http api when there is asynchronous disconnection logic. When we handle the disconnection, it's possible for the scheduler to have re-subscribed, and so the master needs to tell if the disconnection event is relevant for the current connection before taking action.",Task,Major,bmahler,2015-07-28T23:24:45.000+0000,5,Resolved,Complete,Provide a means to check http connection equality for streaming connections.,2015-07-28T23:24:45.000+0000,MESOS-3162,3.0,mesos,Twitter Mesos Q3 Sprint 2
,2015-07-28T01:10:19.000+0000,bmahler,"The [gold linker|https://en.wikipedia.org/wiki/Gold_(linker)] seems to provide a decent speedup (about ~20%) on a parallel build. From a quick test:

{noformat: title=timings for make check -j24 GTEST_FILTER="""" w/ 24 hyperthreaded cores}
gold:

real	7m18.526s
user	81m21.213s
sys	5m17.224s

default ld:

real	9m7.908s
user	85m13.466s
sys	5m52.199s
{noformat}

On CentOS 5 w/ devtoolset-2:

{noformat}
sudo /usr/sbin/alternatives --altdir /opt/rh/devtoolset-2/root/etc/alternatives --admindir /opt/rh/devtoolset-2/root/var/lib/alternatives --set ld /opt/rh/devtoolset-2/root/usr/bin/ld.gold
{noformat}

On Ubuntu:
{noformat}
sudo update-alternatives --install /usr/bin/ld ld /usr/bin/gold 1
{noformat}

Ideally we could this out on the website, with instructions for each OS.",Improvement,Major,bmahler,,10020,Accepted,In Progress,Document using the gold linker for faster development on linux.,2016-03-30T09:29:18.000+0000,MESOS-3161,3.0,mesos,
greggomann,2015-07-27T21:35:12.000+0000,jvanremoortere,"The lack of synchronization between ProcessManager destruction and the thread pool threads running the queued processes means that the shared state that is part of the ProcessManager gets destroyed prematurely.
Synchronizing the ProcessManager destructor with draining the work queues and stopping the workers will allow us to not require leaking the shared state to avoid use beyond destruction.",Improvement,Major,jvanremoortere,2015-09-28T00:42:22.000+0000,5,Resolved,Complete,Libprocess Process: Join runqueue workers during finalization,2015-09-28T00:42:22.000+0000,MESOS-3158,3.0,mesos,Mesosphere Sprint 17
marco-mesos,2015-07-27T18:24:19.000+0000,marco-mesos,"Following from MESOS-2902 we want to enable the same functionality in the Mesos Agents too.

This is probably best done once we implement the new {{os::shell}} semantics, as described in MESOS-3142.",Story,Major,benjaminhindman,2015-08-14T17:42:00.000+0000,5,Resolved,Complete,"Enable Mesos Agent Node to use arbitrary script / module to figure out IP, HOSTNAME",2015-08-14T17:42:00.000+0000,MESOS-3154,1.0,mesos,Mesosphere Sprint 16
,2015-07-27T16:21:43.000+0000,jojy,"Unit tests are lacking for the following cases:

1. HTTPS Post with ""None"" payload. 
2. Verification of HTTPS payload on the SSL socket(maybe decode to a Request object)
3. http -> ssl socket
4. https -> raw socket.",Bug,Minor,jojy,,10020,Accepted,In Progress,Add tests for HTTPS SSL socket communication,2015-12-18T07:05:53.000+0000,MESOS-3153,3.0,mesos,
js84,2015-07-27T13:39:09.000+0000,js84,"As we decided to create a more restful api for managing Quota request.
Therefore we also want to use the HTTP Delete request and hence need to enable the libprocess/http to send delete request besides get and post requests.",Improvement,Major,js84,2015-08-14T11:04:16.000+0000,5,Resolved,Complete,Need for HTTP delete requests,2015-11-25T14:30:45.000+0000,MESOS-3152,1.0,mesos,Mesosphere Sprint 15
marco-mesos,2015-07-25T00:56:16.000+0000,marco-mesos,"See MESOS-2736 for the original issue;
the submitted [Review|https://reviews.apache.org/r/36663] currently has no tests, the one posted in the subsequent [r/3687|https://reviews.apache.org/r/36807] currently hangs when ran after the other {{TEST_F(MasterZooKeeperTest, LostZooKeeperCluster)}}.

The issue is around the {{await()}} in {{StartMaster()}} ({{cluster.hpp #430}}) that waits indefinitely for the master recovery.
",Bug,Major,marco-mesos,2015-07-30T16:19:19.000+0000,5,Resolved,Complete,Resolve issue with hanging tests with Zookeeper,2015-08-15T00:26:03.000+0000,MESOS-3148,1.0,mesos,Mesosphere Sprint 15
mcypark,2015-07-24T21:25:47.000+0000,mcypark,This ticket is to track the {{updateAvailable}} API call being added to the allocator which updates the available resources in the allocator. It's used for master endpoints for dynamic reservation and persistent volumes. {{updateAvailable}} is similar to {{updateSlave}} except that {{updateAvailable}} never leaves the allocator in an over-allocated state.,Task,Major,mcypark,2015-07-24T22:44:14.000+0000,5,Resolved,Complete,Add a new API call to the allocator to update available resources,2015-07-30T23:32:53.000+0000,MESOS-3146,8.0,mesos,Mesosphere Sprint 15
mcypark,2015-07-24T21:21:37.000+0000,mcypark,"The following commands trigger the crash:

{noformat}
$ sudo hostname foo  # an unresolvable hostname
$ sudo ./bin/mesos-master.sh --ip=127.0.0.1 --work_dir=/var/lib/mesos
$ LIBPROCESS_IP=127.0.0.1 ./src/mesos-execute --master=127.0.0.1:5050 --name=bar --command=""while true; do sleep 100; done""
{noformat}

The crash output:

{noformat}
WARNING: Logging before InitGoogleLogging() is written to STDERR
W0724 14:20:39.960733 1925993216 sched.cpp:1487]
**************************************************
Scheduler driver bound to loopback interface! Cannot communicate with remote master(s). You might want to set 'LIBPROCESS_IP' environment variable to use a routable IP address.
**************************************************
ABORT: (../../3rdparty/libprocess/3rdparty/stout/include/stout/try.hpp:85): Try::get() but state == ERROR: nodename nor servname provided, or not known[1]    24560 abort      LIBPROCESS_IP=127.0.0.1 ./src/mesos-execute --master=127.0.0.1:5050
{noformat}",Task,Major,mcypark,2015-07-24T21:29:05.000+0000,5,Resolved,Complete,Using a unresolvable hostname crashes the framework on registration,2015-07-24T21:29:05.000+0000,MESOS-3145,1.0,mesos,Mesosphere Sprint 15
marco-mesos,2015-07-24T19:22:31.000+0000,marco-mesos,"We have pushed a [pull request|https://github.com/Homebrew/homebrew/pull/42099] to Homebrew for the new 0.23 formula.

Once accepted, we must verify that this works on a Mac OSX device.
This would also be a great time to ensure our documentation is up-to-date.

Currently, the Homebrew check fails, as they have deprecated SHA-1 checksums:
{noformat}
Error Message

failed: brew audit mesos
Stacktrace

        Error: 7 problems in 1 formula
mesos:
 * Stable resource ""protobuf"": SHA1 checksums are deprecated, please use SHA256
 * Stable resource ""python-gflags"": SHA1 checksums are deprecated, please use SHA256
 * Stable resource ""six"": SHA1 checksums are deprecated, please use SHA256
 * Stable resource ""google-apputils"": SHA1 checksums are deprecated, please use SHA256
 * Stable resource ""python-dateutil"": SHA1 checksums are deprecated, please use SHA256
 * Stable resource ""boto"": SHA1 checksums are deprecated, please use SHA256
 * Stable resource ""pytz"": SHA1 checksums are deprecated, please use SHA256
{noformat}

Don't know enough about Homebrew to really figure out what is going on here; nor how to fix this.
The Mesos SHA-256 has been correctly entered and computed via the [Online SHA/MD5 calculator|https://md5file.com/calculator].

I guess, we should go download the packages and compute their SHA-256 and/or research from the respective download sites whether they publish the SHA.",Task,Trivial,marco-mesos,2015-07-28T05:50:28.000+0000,5,Resolved,Complete,Update Homebrew formula for Mesos (Mac OSX),2015-07-28T05:50:28.000+0000,MESOS-3144,1.0,mesos,Mesosphere Sprint 15
haosdent@gmail.com,2015-07-24T10:07:30.000+0000,arojas,"In mesos, one can use the flag {{--firewall_rules}} to disable endpoints. Disabled endpoints will return a _403 Forbidden_ response whenever someone tries to access endpoints.

Libprocess support adding one default delegate for endpoints, which is the default process id which handles endpoints if no process id was given. For example, the default id of the master libprocess process is {{master}} which is also set as the delegate for the master system process, so a request to the endpoint {{http://master-address:5050/state.json}} will effectively be resolved by {{http://master-address:5050/master/state.json}}. But if one disables  {{/state.json}} because of how delegates work, it can still access {{/master/state.json}}.

The only workaround is to disabled both enpoints.",Bug,Major,arojas,2015-08-04T10:05:36.000+0000,5,Resolved,Complete,Disable endpoints rule fails to recognize HTTP path delegates,2015-08-04T10:05:36.000+0000,MESOS-3143,2.0,mesos,
marco-mesos,2015-07-24T06:50:27.000+0000,marco-mesos,"When reviewing the code in [r/36425|https://reviews.apache.org/r/36425/] [~benjaminhindman] noticed that there is a better abstraction that is possible to introduce for {{os::shell()}} that will simplify the caller's life.

Instead of having to handle all possible outcomes, we propose to refactor {{os::shell()}} as follows:

{code}
/**
 * Returns the output from running the specified command with the shell.
 */
Try<std::string> shell(const string& command)
{
  // Actually handle the WIFEXITED, WIFSIGNALED here!
}
{code}

where the returned string is {{stdout}} and, should the program be signaled, or exit with a non-zero exit code, we will simply return a {{Failure}} with an error message that will encapsulate both the returned/signaled state, and, possibly {{stderr}}.

And some test driven development:
{code}
EXPECT_ERROR(os::shell(""false""));
EXPECT_SOME(os::shell(""true""));

EXPECT_SOME_EQ(""hello world"", os::shell(""echo hello world""));
{code}

Alternatively, the caller can ask to have {{stderr}} conflated with {{stdout}}:
{code}
Try<string> outAndErr = os::shell(""myCmd --foo 2>&1"");
{code}

However, {{stderr}} will be ignored by default:
{code}
// We don't read standard error by default.
EXPECT_SOME_EQ("""", os::shell(""echo hello world 1>&2""));

// We don't even read stderr if something fails (to return in Try::error).
Try<string> output = os::shell(""echo hello world 1>&2 && false"");
EXPECT_ERROR(output);
EXPECT_FALSE(strings::contains(output.error(), ""hello world""));
{code}

An analysis of existing usage shows that in almost all cases, the caller only cares {{if not error}}; in fact, the actual exit code is read only once, and even then, in a test case.

We believe this will simplify the API to the caller, and will significantly reduce the length and complexity at the calling sites (<6 LOC against the current 20+).",Story,Major,benjaminhindman,2015-08-14T17:42:42.000+0000,5,Resolved,Complete,As a Developer I want a better way to run shell commands,2015-08-14T17:42:42.000+0000,MESOS-3142,2.0,mesos,Mesosphere Sprint 16
haosdent@gmail.com,2015-07-24T01:24:59.000+0000,mcypark,"The purpose of this ticket is to document a very cryptic error message (actually a warning that gets propagated by {{-Werror}}) that gets generated by {{clang-3.5}} from {{gmock}} source code when trying to mock a perfectly innocent-looking function.

h3. Problem

The following code is attempting to mock a {{MesosExecutorDriver}}:

{code}
class MockMesosExecutorDriver : public MesosExecutorDriver {
public:
  MockMesosExecutorDriver(mesos::Executor* executor)
    : MesosExecutorDriver(executor) {}

  MOCK_METHOD1(sendStatusUpdate, Status(const TaskStatus&));
};
{code}

The above code generates the following error message:

{noformat}
In file included from ../3rdparty/libprocess/3rdparty/gmock-1.6.0/include/gmock/gmock.h:58:
In file included from ../3rdparty/libprocess/3rdparty/gmock-1.6.0/include/gmock/gmock-actions.h:46:
../3rdparty/libprocess/3rdparty/gmock-1.6.0/include/gmock/internal/gmock-internal-utils.h:355:10: error: indirection of non-volatile null pointer will be deleted, not trap [-Werror,-Wnull-dereference]
  return *static_cast<typename remove_reference<T>::type*>(__null);
         ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
../3rdparty/libprocess/3rdparty/gmock-1.6.0/include/gmock/gmock-actions.h:78:22: note: in instantiation of function template specialization 'testing::internal::Invalid<mesos::Status>' requested here
    return internal::Invalid<T>();
                     ^
../3rdparty/libprocess/3rdparty/gmock-1.6.0/include/gmock/gmock-actions.h:190:43: note: in instantiation of member function 'testing::internal::BuiltInDefaultValue<mesos::Status>::Get' requested here
        internal::BuiltInDefaultValue<T>::Get() : *value_;
                                          ^
../3rdparty/libprocess/3rdparty/gmock-1.6.0/include/gmock/gmock-spec-builders.h:1435:34: note: in instantiation of member function 'testing::DefaultValue<mesos::Status>::Get' requested here
    return DefaultValue<Result>::Get();
                                 ^
../3rdparty/libprocess/3rdparty/gmock-1.6.0/include/gmock/gmock-spec-builders.h:1334:22: note: in instantiation of member function 'testing::internal::FunctionMockerBase<mesos::Status (const mesos::TaskStatus &)>::PerformDefaultAction' requested here
        func_mocker->PerformDefaultAction(args, call_description));
                     ^
../3rdparty/libprocess/3rdparty/gmock-1.6.0/include/gmock/gmock-spec-builders.h:1448:26: note: in instantiation of function template specialization 'testing::internal::ActionResultHolder<mesos::Status>::PerformDefaultAction<mesos::Status (const mesos::TaskStatus &)>' requested here
    return ResultHolder::PerformDefaultAction(this, args, call_description);
                         ^
../3rdparty/libprocess/3rdparty/gmock-1.6.0/include/gmock/gmock-generated-function-mockers.h:81:7: note: in instantiation of member function 'testing::internal::FunctionMockerBase<mesos::Status (const mesos::TaskStatus &)>::UntypedPerformDefaultAction' requested here
class FunctionMocker<R(A1)> : public
      ^
../3rdparty/libprocess/3rdparty/gmock-1.6.0/include/gmock/internal/gmock-internal-utils.h:355:10: note: consider using __builtin_trap() or qualifying pointer with 'volatile'
  return *static_cast<typename remove_reference<T>::type*>(__null);
         ^
{noformat}

The source of the issue here is that {{Status}} is an {{enum}}. In {{gmock-1.6.0/include/gmock/internal/gmock-internal-utils.h}} you can find the following function:

{code}
template <typename T>
T Invalid() {
  return *static_cast<typename remove_reference<T>::type*>(NULL);
}
{code}

This function gets called with the return type of a mocked function. In our case,  the return type of the mocked function is {{Status}}.

Attempting to compile the following minimal example with {{clang-3.5}} reproduces the error message:

{code}
#include <type_traits>

template <typename T>
T invalid() {
  return *static_cast<typename std::remove_reference<T>::type *>(nullptr);
}

enum E { A, B };

int main() {
  invalid<E>();
}
{code}

* See it online on [GCC Explorer|https://goo.gl/t1FepZ]

Note that if the type is not an {{enum}}, the warning is not generated. This is why existing mocked functions that return non-{{enum}} types such as {{Future<void>}} does not encounter this issue.

h3. Solutions

The simplest solution is to add {{-Wno-null-deference}} to {{mesos_tests_CPPFLAGS}} in {{src/Makefile.am}}.

{code}
mesos_tests_CPPFLAGS = $(MESOS_CPPFLAGS) -Wno-null-dereference
{code}

Another solution is to upgrade {{gmock}} from *1.6* to *1.7* because this problem is solved in the newer versions.

In gmock 1.7
{code}
template <typename T>
inline T Invalid() {
  return const_cast<typename remove_reference<T>::type&>(
      *static_cast<volatile typename remove_reference<T>::type*>(NULL));
}
{code}

Add volatile could avoid this warning. https://goo.gl/opCiLC

",Bug,Major,mcypark,2015-08-07T09:59:28.000+0000,5,Resolved,Complete,Compiler warning when mocking function type has an enum return type.,2015-08-07T14:18:01.000+0000,MESOS-3141,3.0,mesos,
jojy,2015-07-23T20:35:47.000+0000,chenlily,"Given a Docker image name and registry host URL, fetches the image. If necessary, it will download the manifest and layers from the registry host. It will place the layers and image manifest into persistent store.
Done when a Docker image can be successfully stored and retrieved using 'put' and 'get' methods.",Improvement,Major,chenlily,2015-11-19T20:51:52.000+0000,5,Resolved,Complete,Implement Docker remote puller,2015-11-24T01:36:30.000+0000,MESOS-3140,5.0,mesos,Mesosphere Sprint 18
hausdorff,2015-07-23T19:14:47.000+0000,hausdorff,"Right now it's anyone's guess how to build with CMake. If we want people to use it, we should put up documentation. The central challenge is that the CMake instructions will be slightly different for different platforms.

For example, on Linux, the gist of the build is basically the same as autotools; you pull down the system dependencies (like APR, _etc_.), and then:

```
./bootstrap
mkdir build-cmake && cd build-cmake
cmake ..
make
```

But, on Windows, it will be somewhat more complicated. There is no bootstrap step, for example, because Windows doesn't have bash natively. And even when we put that in, you'll still have to build the glog stuff out-of-band because CMake has no way of booting up Visual Studio and calling ""build.""

So practically, we need to figure out:

* What our build story is for different platforms
* Write specific instructions for our ""core"" target platforms.",Task,Major,hausdorff,,10020,Accepted,In Progress,Incorporate CMake into standard documentation,2015-10-30T19:33:35.000+0000,MESOS-3139,13.0,mesos,
jieyu,2015-07-23T18:28:35.000+0000,marco-mesos,"With a clean build ({{make clean}}) running this tests fails:

{code}
GTEST_FILTER=""PersistentVolumeTest.*"" make check
{code}

This is the log:
{noformat}
[==========] Running 7 tests from 1 test case.
[----------] Global test environment set-up.
[----------] 7 tests from PersistentVolumeTest
[ RUN      ] PersistentVolumeTest.SendingCheckpointResourcesMessage
[       OK ] PersistentVolumeTest.SendingCheckpointResourcesMessage (189 ms)
[ RUN      ] PersistentVolumeTest.ResourcesCheckpointing
[       OK ] PersistentVolumeTest.ResourcesCheckpointing (86 ms)
[ RUN      ] PersistentVolumeTest.PreparePersistentVolume
[       OK ] PersistentVolumeTest.PreparePersistentVolume (82 ms)
[ RUN      ] PersistentVolumeTest.MasterFailover
[       OK ] PersistentVolumeTest.MasterFailover (130 ms)
[ RUN      ] PersistentVolumeTest.IncompatibleCheckpointedResources
[       OK ] PersistentVolumeTest.IncompatibleCheckpointedResources (74 ms)
[ RUN      ] PersistentVolumeTest.AccessPersistentVolume
I0723 11:21:40.265787 1955922688 exec.cpp:132] Version: 0.24.0
I0723 11:21:40.268676 174858240 exec.cpp:206] Executor registered on slave 20150723-112140-16777343-61858-2866-S0
E0723 11:21:40.268697 178077696 socket.hpp:173] Shutdown failed on fd=18: Socket is not connected [57]
E0723 11:21:40.273510 178077696 socket.hpp:173] Shutdown failed on fd=18: Socket is not connected [57]
Registered executor on localhost
Starting task 39e32f2b-475e-4754-9e3d-39fd56fb787b
Forked command at 2911
sh -c 'echo abc > path1/file'
E0723 11:21:40.281900 178077696 socket.hpp:173] Shutdown failed on fd=18: Socket is not connected [57]
Command exited with status 0 (pid: 2911)
E0723 11:21:40.389068 178077696 socket.hpp:173] Shutdown failed on fd=18: Socket is not connected [57]
[       OK ] PersistentVolumeTest.AccessPersistentVolume (421 ms)
[ RUN      ] PersistentVolumeTest.SlaveRecovery
I0723 11:21:40.639749 1955922688 exec.cpp:132] Version: 0.24.0
I0723 11:21:40.641904 187400192 exec.cpp:206] Executor registered on slave 20150723-112140-16777343-61858-2866-S0
E0723 11:21:40.641943 191156224 socket.hpp:173] Shutdown failed on fd=18: Socket is not connected [57]
E0723 11:21:40.646507 191156224 socket.hpp:173] Shutdown failed on fd=18: Socket is not connected [57]
Registered executor on localhost
Starting task 809fa50f-bee0-4c9b-a770-434183a9650b
sh -c 'while true; do test -d path1; done'
Forked command at 2941
E0723 11:21:40.655097 191156224 socket.hpp:173] Shutdown failed on fd=18: Socket is not connected [57]
I0723 11:21:40.671840 186863616 exec.cpp:252] Received reconnect request from slave 20150723-112140-16777343-61858-2866-S0
E0723 11:21:40.671953 191156224 socket.hpp:173] Shutdown failed on fd=18: Socket is not connected [57]
I0723 11:21:40.672744 187400192 exec.cpp:229] Executor re-registered on slave 20150723-112140-16777343-61858-2866-S0
E0723 11:21:40.672839 191156224 socket.hpp:173] Shutdown failed on fd=18: Socket is not connected [57]
Re-registered executor on localhost
../../src/tests/persistent_volume_tests.cpp:709: Failure
Value of: status2.get().state()
  Actual: TASK_FAILED
Expected: TASK_KILLED
[  FAILED  ] PersistentVolumeTest.SlaveRecovery (286 ms)
[----------] 7 tests from PersistentVolumeTest (1268 ms total)

[----------] Global test environment tear-down
[==========] 7 tests from 1 test case ran. (1289 ms total)
[  PASSED  ] 6 tests.
[  FAILED  ] 1 test, listed below:
[  FAILED  ] PersistentVolumeTest.SlaveRecovery

 1 FAILED TEST
  YOU HAVE 8 DISABLED TESTS
{noformat}",Bug,Major,marco-mesos,2015-07-24T05:25:16.000+0000,5,Resolved,Complete,PersistentVolumeTest.SlaveRecovery test fails on OSX,2015-07-24T05:25:16.000+0000,MESOS-3138,2.0,mesos,Twitter Mesos Q3 Sprint 2
marco-mesos,2015-07-23T06:55:08.000+0000,marco-mesos,"Following from MESOS-2340, which now allows Master to correctly decode JSON information ({{MasterInfo}}) published to Zookeeper, we can now enable the Master Leader Contender to serialize it too in JSON.",Story,Major,marco-mesos,2015-07-30T00:23:31.000+0000,5,Resolved,Complete,Publish MasterInfo to ZK using JSON,2015-07-30T00:23:31.000+0000,MESOS-3135,2.0,mesos,Mesosphere Sprint 15
hausdorff,2015-07-23T05:33:01.000+0000,hausdorff,"Bootstrap does a lot of significant things, like setting up the git commit hooks. We will want something like bootstrap to run also on systems that don't have bash -- ideally this should just run in CMake itself.",Bug,Major,hausdorff,2015-12-09T23:12:40.000+0000,5,Resolved,Complete,Port bootstrap to CMake,2015-12-09T23:12:40.000+0000,MESOS-3134,5.0,mesos,Mesosphere Sprint 24
karya,2015-07-23T03:54:20.000+0000,karya,"Sometimes the Isolators need to pass on some environment variables for the Executor that is being launched. For example, to successfully launch an executor inside a network namespace, one needs to set LIBPROCESS_IP to point to the container IP, otherwise the executor tries to bind to the Slave IP which may be invalid inside the namespace. Another example is where the file system isolator should be able to specify the WORK_DIR depending on if a new rootfs is used.",Task,Major,karya,2015-07-28T18:19:07.000+0000,5,Resolved,Complete,Isolator::prepare() should return Executor environment vars as well,2015-07-28T18:19:08.000+0000,MESOS-3133,2.0,mesos,Mesosphere Sprint 15
bmahler,2015-07-22T23:44:25.000+0000,bmahler,"The master currently has no install handler for {{ExecutorToFramework}} messages and the slave directly sends these messages to the scheduler driver, bypassing the master entirely.

We need to preserve this behavior for the driver, but HTTP schedulers will not have a libprocess 'pid'. We'll have to ensure that the {{RunTaskMessage}} and {{UpdateFrameworkMessage}} have an optional pid. For now the master will continue to set the pid, but 0.24.0 slaves will know to send messages through the master when the 'pid' is not available.",Task,Major,bmahler,2015-07-24T23:59:24.000+0000,5,Resolved,Complete,Allow slave to forward messages through the master for HTTP schedulers.,2015-07-24T23:59:24.000+0000,MESOS-3132,5.0,mesos,Twitter Mesos Q3 Sprint 2
vinodkone,2015-07-22T22:11:25.000+0000,vinodkone,"In order to deal with network partitions and ensuring network intermediately do not close the persistent subscription connection, master must periodically send heartbeats. 

The expectation with schedulers is that they resubscribe when they do not receive heartbeats for some time.",Task,Major,vinodkone,2015-08-13T23:21:43.000+0000,5,Resolved,Complete,Master should send heartbeats on the subscription connection,2015-08-13T23:21:43.000+0000,MESOS-3131,3.0,mesos,Twitter Mesos Q3 Sprint 3
jieyu,2015-07-22T18:16:02.000+0000,jieyu,"Similar to MESOS-2213, we should not restrict custom isolators to use libprocess Process. We should do a similar refactor as we did for MESOS-2213.",Task,Major,jieyu,2015-07-24T18:02:09.000+0000,5,Resolved,Complete,Custom isolators should implement Isolator instead of IsolatorProcess.,2015-07-24T18:02:09.000+0000,MESOS-3130,3.0,mesos,Twitter Mesos Q3 Sprint 2
gilbert,2015-07-22T17:59:34.000+0000,jieyu,"Currently, some MesosContainerizer specific files are not in the correct location. For example:
{noformat} 
src/slave/containerizer/isolators/*
src/slave/containerizer/provisioner.hpp|cpp
{noformat}

They should be put under src/slave/containerizer/mesos/",Task,Major,jieyu,2015-10-26T18:41:16.000+0000,5,Resolved,Complete,Move all MesosContainerizer related files under src/slave/containerizer/mesos,2015-11-11T23:17:06.000+0000,MESOS-3129,2.0,mesos,Mesosphere Sprint 21
nfnt,2015-07-22T09:37:00.000+0000,nfnt,Include additional information about task reconciliation that explain why the master may not return the states of all tasks immediately and why an explicit task reconciliation algorithm is necessary.,Improvement,Minor,nfnt,2015-07-27T18:37:52.000+0000,5,Resolved,Complete,Improve task reconciliation documentation.,2015-07-27T18:37:52.000+0000,MESOS-3127,1.0,mesos,Mesosphere Sprint 15
,2015-07-22T06:21:36.000+0000,sttts,"With https://reviews.apache.org/r/36282/ no environment variables are available anymore in the docker executors. Hence, setting DOCKER_HOST outside of Mesos stopped working.

Setups which use a remote Docker daemon or tools like Powerstrip stopped working.",Bug,Major,sttts,,10020,Accepted,In Progress,DOCKER_HOST env variable stopped working for executors,2015-07-24T15:54:35.000+0000,MESOS-3125,2.0,mesos,
jieyu,2015-07-22T00:07:14.000+0000,jieyu,"Just realize that while reviewing https://reviews.apache.org/r/34135

Since we don't checkpoint 'resources' in Mesos containerizer, when slave restarts and recovers, the 'resources' in Container struct will be empty, but there are symlinks exists in the sandbox.

We'll end up with trying to create already exist symlinks (and fail). I think we should ignore the creation if it already exists.",Bug,Major,jieyu,2015-07-22T23:02:02.000+0000,5,Resolved,Complete,Updating persistent volumes after slave restart is problematic.,2015-07-22T23:02:02.000+0000,MESOS-3124,3.0,mesos,Twitter Mesos Q3 Sprint 2
jvanremoortere,2015-07-21T20:04:22.000+0000,jvanremoortere,"During the transition to support for windows, it would be great if we had the ability to use a macro that marks functions as un-implemented.
To support being able to find all the unimplemented functions easily at compile time, while also being able to run the tests at the same time, we can add a configuration flag that controls whether this macro aborts or expands to a static assertion.",Improvement,Major,jvanremoortere,2015-07-31T22:49:19.000+0000,5,Resolved,Complete,Add configurable UNIMPLEMENTED macro to stout,2015-07-31T22:49:19.000+0000,MESOS-3122,2.0,mesos,Mesosphere Sprint 15
jvanremoortere,2015-07-21T19:55:56.000+0000,jvanremoortere,"The SSL protocol mismatch tests are failing on Centos7 when matching SSLV2 with SSLV2. Since this version of the protocol is highly discouraged anyway, let's disable it completely unless requested otherwise.",Bug,Major,jvanremoortere,2015-07-27T18:40:56.000+0000,5,Resolved,Complete,Always disable SSLV2,2015-07-27T18:40:56.000+0000,MESOS-3121,2.0,mesos,Mesosphere Sprint 15
karya,2015-07-21T17:45:41.000+0000,karya,"Some isolators need to lookup the executor environment variables to customize their isolation needs. Currently, one has to use the ""prepare()"" call to cache the executor-info to use it later during isolate() call.",Task,Major,karya,2015-08-03T16:10:28.000+0000,5,Resolved,Complete,Pass ExecutorInfo argument into Isolator::isolate().,2015-08-03T16:10:28.000+0000,MESOS-3116,2.0,mesos,Mesosphere Sprint 15
karya,2015-07-21T17:36:37.000+0000,karya,Published RR: https://reviews.apache.org/r/36718/,Task,Major,karya,2015-07-24T03:15:57.000+0000,5,Resolved,Complete,"Convert mesos::slave::{Limitation,ExecutorRunState} into protobufs.",2015-07-24T03:15:57.000+0000,MESOS-3115,1.0,mesos,Mesosphere Sprint 15
karya,2015-07-21T17:33:13.000+0000,karya,"We want to be able to do things like:

{code}
JSON::Value number1 = 25;
JSON::Number number2 = 26;

EXPECT_NE(number1, number2);
EXPECT_EQ(jsonify(12), number1);
EXPECT_EQ(jsonify(12), number2);
{code}",Task,Major,karya,,10020,Accepted,In Progress,"Simplify JSON::* by providing ""jsonify"" along the lines of ""stringify""",2015-08-07T17:24:16.000+0000,MESOS-3114,3.0,mesos,Mesosphere Sprint 15
,2015-07-21T17:18:37.000+0000,nnielsen,"Currently, the containerizer documentation doesn't touch upon the usage() API and how to interpret the collected statistics.",Documentation,Major,nnielsen,,10020,Accepted,In Progress,Add resource usage section to containerizer documentation,2015-11-23T21:26:15.000+0000,MESOS-3113,3.0,mesos,Mesosphere Sprint 21
nfnt,2015-07-21T11:19:28.000+0000,bernd-mesos,"Currently, the fetcher uses a trivial strategy to select eviction victims: it picks the first cache file it finds in linear iteration. This means that potentially a file that has just been used gets evicted the next moment. This performance loss can be avoided by even the simplest enhancement of the selection procedure.

Proposed approach: determine an effective yet relatively uncomplex and quick algorithm and implement it in `FetcherProcess::Cache::selectVictims(const Bytes& requiredSpace)`. Suggestion: approximate MRU-retention somehow.

Unit-test what actually happens!",Improvement,Major,bernd-mesos,2015-08-03T11:25:42.000+0000,5,Resolved,Complete,Fetcher should perform cache eviction based on cache file usage patterns.,2015-08-15T00:26:03.000+0000,MESOS-3112,8.0,mesos,Mesosphere Sprint 15
hausdorff,2015-07-21T01:33:46.000+0000,hausdorff,"Currently the Mesos project has two flavors of dependency: (1) the dependencies we expect are already on the system (_e.g._, apr, libsvn), and (2) the dependencies that are historically bundled with Mesos (_e.g._, glog).

Dependency type (1) requires solid modules that will locate them on any system: Linux, BSD, or Windows. This would come for free if we were using CMake 3.0, but we're using CMake 2.8 so that Ubuntu users can install it out of the box, instead of upgrading CMake first.

This is additionally useful for dependency type (2), where we will expect to have to use these routines when we support both the rebundled dependencies in the `3rdparty/` folder, and system installations of those dependencies.",Task,Major,hausdorff,,10020,Accepted,In Progress,Harden the CMake system-dependency-locating routines,2015-10-06T23:51:58.000+0000,MESOS-3110,3.0,mesos,
hausdorff,2015-07-21T01:28:51.000+0000,hausdorff,"In other tasks in epic MESOS-898, we implement a CMake-based build system that allows us to build process library, the process tests, and the stout tests.

For the CMake build system MVP, it's important that we expand this to build the containerizer, associated modules, and all related tests.",Task,Major,hausdorff,,10020,Accepted,In Progress,Expand CMake build system to support building the containerizer and associated components,2015-10-06T23:51:50.000+0000,MESOS-3109,3.0,mesos,
hausdorff,2015-07-21T01:25:32.000+0000,hausdorff,"In the autoconf-based build system, we there is a notion of building a ""distribution"" of Mesos. Essentially, it is a version of Mesos that is configured for a specific platform (Ubuntu, say); so, if a consumer knows their platform, and there is a Mesos distribution, they need only run `make all` and Mesos builds. This allows the consumer to skip the configure step.

In CMake, it should be possible to do this (should be!), and we should explore making it work after we complete the MVP.",Task,Major,hausdorff,,10020,Accepted,In Progress,Add autotools-style Mesos distributions to the CMake build system,2015-10-06T23:51:43.000+0000,MESOS-3108,3.0,mesos,
hausdorff,2015-07-21T01:21:40.000+0000,hausdorff,"The short story is that it is important to be principled about how the CMake build system is maintained, because there CMake language makes it difficult to statically verify that a configuration is correct. It is not unique in this regard, but (make is arguably even worse) but it is something that's important to make sure we get right.

The longer story is, CMake's language is dynamically scoped and often has somewhat odd defaults for variable values (_e.g._, IIRC, target names passed to ExternalProject_Add default to ""PREFIX"" instead of erroring out). This means that it is rare to get a configuration-time error (_i.e._, CMake usually doesn't say something like ""hey this variable isn't defined""), and in large projects, this can make it very difficult to know where definitions come from, or whether it's important that one config routine runs before another. Dynamic scoping also makes it particularly easy to write spaghetti code, which is clearly undesirable for something as important as a build system.

Thus, it is particularly important that we lay down our expectations for how the CMake system is to be structured. This might include:

* Function naming (_e.g._, making it easy to tell whether a function was defined by us, and where it was defined; so we might say that we want our functions to have an underscore to start, and start with the package the come from, like libprocess, so that we know where to look for the definition.)
* What assertions we want to check variable values against, so that we can replace subtle errors (_e.g._, a library is accidentally named something silly like ""PREFIX.0.0.1"") with an obvious ones (_e.g._, ""You have failed to define your target name, so CMake has defaulted to 'PREFIX'; please check your configuration routines"")
* Decisions of what goes where. (_e.g._, the most complex parts of the CMake MVPs is in the configuration routines, like `MesosConfigure.cmake`; to curb this, we should have strict rules about what goes in that file vs other files, and how we know what is to be run before what. Part of this should probably be prominent comments explaining the structure of the project, so that people aren't confused!)
* And so on.",Task,Major,hausdorff,,10020,Accepted,In Progress,Define CMake style guide,2015-10-06T23:51:36.000+0000,MESOS-3107,3.0,mesos,Mesosphere Sprint 15
hausdorff,2015-07-21T00:53:36.000+0000,hausdorff,"Currently Mesos has third-party dependencies of two types: (1) those that are expected to be on the system (such as APR, libsvn, _etc_.), and (2) those that have been historically bundled as tarballs inside the Mesos repository, and are not expected to be on the system when Mesos is installed (these are located in the `3rdparty/` directory, and includes things like boost and glog).

For type (2), the MVP of the CMake-based build system will always pull down a fresh tarball from an external source, instead of using the bundled tarballs in the `3rdparty/` folder.

However, many CI systems do not have Internet access, so in the long term, we need to provide many options for getting these dependencies.",Task,Major,hausdorff,,10020,Accepted,In Progress,Extend CMake build system to support building against third-party libraries from either the system or the local Mesos rebundling,2015-10-06T23:51:29.000+0000,MESOS-3106,5.0,mesos,Mesosphere Sprint 15
hausdorff,2015-07-21T00:39:13.000+0000,hausdorff,"One major barrier to widespread adoption of the CMake-based build system (other than the fact that we haven't implemented it yet!) is that most of our institutional knowledge of the quirks of how to build Mesos across many platforms is tied up in files like `configure.ac`.

Therefore, a ""good"" CMake-based build system will require us to go through these files systematically and manually port this logic to CMake (as well as testing it).",Task,Major,hausdorff,,10020,Accepted,In Progress,Port flag generation logic from the autotools solution to CMake,2015-10-06T23:51:18.000+0000,MESOS-3105,3.0,mesos,
hausdorff,2015-07-20T23:22:07.000+0000,kaysoky,"This issue tracks changes for all files under {{3rdparty/libprocess/include/}} and {{3rdparty/libprocess/src}}.

The changes will be based on this commit:
https://github.com/hausdorff/mesos/commit/b862f66c6ff58c115a009513621e5128cb734d52#diff-a6d038bad64b154996452bec020cfa7c",Task,Major,kaysoky,,10020,Accepted,In Progress,Separate OS-specific code in the libprocess library,2015-08-12T16:30:39.000+0000,MESOS-3103,5.0,mesos,
kaysoky,2015-07-20T23:17:16.000+0000,kaysoky,"This issue tracks changes for all files under {{3rdparty/libprocess/3rdparty/stout/}}

The changes will be based on this commit:
https://github.com/hausdorff/mesos/commit/b862f66c6ff58c115a009513621e5128cb734d52#diff-a6d038bad64b154996452bec020cfa7c",Task,Major,kaysoky,2015-07-30T23:27:27.000+0000,5,Resolved,Complete,Separate OS-specific code in the stout library,2015-07-30T23:27:27.000+0000,MESOS-3102,5.0,mesos,
kaysoky,2015-07-20T23:09:32.000+0000,kaysoky,"There are 50+ files that must be touched to separate OS-specific code.

First, we will standardize the changes by using stout/abort.hpp as an example.
The review/discussion can be found here:
https://reviews.apache.org/r/36625/",Task,Major,kaysoky,2015-07-30T23:28:17.000+0000,5,Resolved,Complete,Standardize separation of Windows/Linux-specific OS code,2015-08-10T10:11:28.000+0000,MESOS-3101,3.0,mesos,Mesosphere Sprint 15
,2015-07-20T22:51:46.000+0000,chenlily,"Docker layers should be verified against their checksum digests before they are stored to ensure the integrity of the docker layer content. This includes supporting sha256, sha384, sha512 hash algorithms.",Improvement,Major,chenlily,,10020,Accepted,In Progress,Validation of Docker Layers Pulled From Docker Registry,2015-12-18T07:02:20.000+0000,MESOS-3100,3.0,mesos,
gilbert,2015-07-20T22:45:45.000+0000,chenlily,Docker image manifests pulled from remote Docker registries should be verified against their signature digest before they are used. ,Improvement,Major,chenlily,2015-10-16T12:18:16.000+0000,5,Resolved,Complete,Validation of Docker Image Manifests from Docker Registry,2015-11-08T12:51:43.000+0000,MESOS-3099,3.0,mesos,Mesosphere Sprint 20
hausdorff,2015-07-20T22:44:30.000+0000,kaysoky,"The MVP for Windows support is a containerizer that (1) runs on Windows, and (2) runs and passes all the tests that are relevant to the Windows platform (_e.g._, not the tests that involve cgroups). To do this we require at least a `WindowsContainerizer` (to be implemented alongside the `MesosContainerizer`), which provides no meaningful (_e.g._) process namespacing (much like the default unix containerizer). In the long term (hopefully before MesosCon) we want to support also the Windows container API. This will require implementing a separate containerizer, maybe called `WindowsDockerContainerizer`.

Since the Windows container API is actually officially supported through the Docker interface (_i.e._, MSFT actually ported the Docker engine to Windows, and that is the official API), the interfaces (like the fetcher) shouldn't change much. The tests probably will have to change, as we don't have access to any isolation primitives like cgroups for those tests.

Outstanding TODO([~hausdorff]): Flesh out this description when more details are available, regarding:
* The container API for Windows (when we know them)
* The nuances of Windows vs Linux (when we know them)
* etc.",Task,Major,kaysoky,,10020,Accepted,In Progress,Implement WindowsContainerizer and WindowsDockerContainerizer,2015-08-12T16:30:34.000+0000,MESOS-3098,13.0,mesos,
hausdorff,2015-07-20T22:40:54.000+0000,kaysoky,"In the process of adding the Cmake build system, [~hausdorff] noted and stubbed out all OS-specific code.
That sweep (mostly of libprocess and stout) is here:
https://github.com/hausdorff/mesos/commit/b862f66c6ff58c115a009513621e5128cb734d52

Instead of having inline {{#if defined(...)}}, the OS-specific code will be separated into directories.
The Windows code will be stubbed out.",Story,Major,kaysoky,,10020,Accepted,In Progress,OS-specific code touched by the containerizer tests is not Windows compatible,2015-08-12T16:30:35.000+0000,MESOS-3097,13.0,mesos,
jojy,2015-07-20T22:39:52.000+0000,chenlily,"In order to pull Docker images from Docker Hub and private Docker registries, the provisioner must support two primary authentication frameworks to authenticate with the registries, basic authentication and the OAuth2.0 authorization framework, as per the docker registry spec. A Docker registry can also operate in standalone mode and may not require authentication.",Improvement,Major,chenlily,2015-12-22T00:41:15.000+0000,5,Resolved,Complete,Authentication for Communicating with Docker Registry,2015-12-22T00:41:22.000+0000,MESOS-3096,5.0,mesos,
tnachen,2015-07-20T22:39:35.000+0000,tnachen,This is to implement a PoC of the alternative design choices with MESOS-3004,Improvement,Major,tnachen,2015-08-06T20:01:29.000+0000,5,Resolved,Complete,PoC running command executor with image provisioner,2015-08-06T20:01:29.000+0000,MESOS-3095,3.0,mesos,Mesosphere Sprint 15
jojy,2015-07-20T22:25:16.000+0000,chenlily,"In order to pull images from Docker registries, https calls are needed to securely communicate with the registry hosts. Currently, only http requests are supported through libprocess. Now that SSL sockets are available through libprocess, support for https can be added.",Improvement,Major,chenlily,2015-07-29T17:50:03.000+0000,5,Resolved,Complete,Support HTTPS requests in libprocess,2015-07-29T17:50:13.000+0000,MESOS-3093,3.0,mesos,Mesosphere Sprint 15
tnachen,2015-07-20T21:19:40.000+0000,tnachen,Add a jenkin job to run the Docker tests,Improvement,Major,tnachen,,3,In Progress,In Progress,Configure Jenkins to run Docker tests,2015-09-08T20:10:22.000+0000,MESOS-3092,2.0,mesos,Mesosphere Sprint 15
vinodkone,2015-07-20T19:29:19.000+0000,vinodkone,"See MESOS-2913 for context.

From the dev list it looks like users depend on this call for their custom allocator, so we need to support it going forward.",Task,Major,vinodkone,2015-07-23T19:34:14.000+0000,5,Resolved,Complete,Update scheduler library to send REQUEST call,2015-07-23T19:34:14.000+0000,MESOS-3089,2.0,mesos,Twitter Mesos Q3 Sprint 2
vinodkone,2015-07-20T19:26:40.000+0000,vinodkone,See MESOS-2913 for context.,Task,Major,vinodkone,2015-07-22T18:11:30.000+0000,5,Resolved,Complete,Update scheduler driver to send SUBSCRIBE call,2015-07-27T20:53:14.000+0000,MESOS-3088,2.0,mesos,Twitter Mesos Q3 Sprint 2
haosdent@gmail.com,2015-07-20T10:42:06.000+0000,candlerb,"* In docs/oversubscription.md: there are three cases where ""revocable"" is written as ""recovable"", including the name of a JSON field.

{noformat}
$ grep -niR recovable .
./docs/oversubscription.md:51:with revocable resources.  Further more, recovable resources cannot be
./docs/oversubscription.md:95:Launching tasks using recovable resources is done through the existing
./docs/oversubscription.md:96:`launchTasks` API. Revocable resources will have the `recovable` field set. See
{noformat}

* Also in `docs/oversubscription.md`: the last sentence doesn't make sense

{noformat}
To select custom a resource estimator and QoS controller, please refer to the
[modules documentation](modules.md).
{noformat}

Maybe should say ""To select a custom..."" or ""To install a custom...""",Documentation,Trivial,candlerb,2015-07-27T17:23:50.000+0000,5,Resolved,Complete,Typos in oversubscription doc,2015-07-27T17:23:50.000+0000,MESOS-3087,1.0,mesos,
js84,2015-07-20T08:31:20.000+0000,js84,"We have a number of test issues when we cannot remove cgroups (in case there are still related tasks running) in cases where the freezer subsystem is not available. 
In the current code (https://github.com/apache/mesos/blob/0.22.1/src/linux/cgroups.cpp#L1728)  we will fallback to a very simple mechnism of recursivly trying to remove the cgroups which fails if there are still tasks running. 
Therefore we need an additional  (NonFreeze)TasksKiller which doesn't  rely on the freezer subsystem.

This problem caused issues when running 'sudo make check' during 0.23 release testing, where BenH provided already a better error message with b1a23d6a52c31b8c5c840ab01902dbe00cb1feef / https://reviews.apache.org/r/36604.
",Bug,Major,js84,,10006,Reviewable,New,Create cgroups TasksKiller for non freeze subsystems.,2015-12-07T18:16:49.000+0000,MESOS-3086,4.0,mesos,Mesosphere Sprint 15
,2015-07-19T18:11:53.000+0000,benjaminhindman,"The namespace tests attempt to clone a process with all namespaces that are available from the kernel which includes the 'user' namespace in Ubuntu 14.04 which causes the child process to be user 'nobody' instead of user 'root' after invoking 'clone' which is bad because the test requires that the child process is 'root' and so things fail (because of insufficient permissions). For now, we explicitly ignore the 'user' namespace in the tests, but this issue is to track exactly how we might want to manage this going forward.",Bug,Major,benjaminhindman,,10020,Accepted,In Progress,Doing 'clone' on Linux with the CLONE_NEWUSER namespace type can drop root privileges.,2015-07-24T15:47:22.000+0000,MESOS-3083,5.0,mesos,
nfnt,2015-07-19T18:01:05.000+0000,benjaminhindman,"When running the tests on Ubuntu 14.04 the 'cycles' value collected by perf is always 0, meaning certain tests always fail. These lines in the test have been commented out for now and a TODO has been attached which links to this JIRA issue, since the solution is unclear. In particular, 'cycles' might not properly be counted because it is a hardware counter and this particular machine was a virtual machine. Either way, we should determine the best events to collect from perf in either VM or physical settings.",Bug,Major,benjaminhindman,2016-01-18T23:42:00.000+0000,5,Resolved,Complete,Perf related tests rely on 'cycles' which might not always be present.,2016-01-18T23:42:00.000+0000,MESOS-3082,5.0,mesos,Mesosphere Sprint 26
marco-mesos,2015-07-18T22:56:52.000+0000,marco-mesos,"Running tests as root causes a large number of failures.
{noformat}
$ lsb_release -a
LSB Version:    core-2.0-amd64:core-2.0-noarch:core-3.0-amd64:core-3.0-noarch:core-3.1-amd64:core-3.1-noarch:core-3.2-amd64:core-3.2-noarch:core-4.0-amd64:core-4.0-noarch:core-4.1-amd64:core-4.1-noarch:cxx-3.0-amd64:cxx-3.0-noarch:cxx-3.1-amd64:cxx-3.1-noarch:cxx-3.2-amd64:cxx-3.2-noarch:cxx-4.0-amd64:cxx-4.0-noarch:cxx-4.1-amd64:cxx-4.1-noarch:desktop-3.1-amd64:desktop-3.1-noarch:desktop-3.2-amd64:desktop-3.2-noarch:desktop-4.0-amd64:desktop-4.0-noarch:desktop-4.1-amd64:desktop-4.1-noarch:graphics-2.0-amd64:graphics-2.0-noarch:graphics-3.0-amd64:graphics-3.0-noarch:graphics-3.1-amd64:graphics-3.1-noarch:graphics-3.2-amd64:graphics-3.2-noarch:graphics-4.0-amd64:graphics-4.0-noarch:graphics-4.1-amd64:graphics-4.1-noarch:languages-3.2-amd64:languages-3.2-noarch:languages-4.0-amd64:languages-4.0-noarch:languages-4.1-amd64:languages-4.1-noarch:multimedia-3.2-amd64:multimedia-3.2-noarch:multimedia-4.0-amd64:multimedia-4.0-noarch:multimedia-4.1-amd64:multimedia-4.1-noarch:printing-3.2-amd64:printing-3.2-noarch:printing-4.0-amd64:printing-4.0-noarch:printing-4.1-amd64:printing-4.1-noarch:qt4-3.1-amd64:qt4-3.1-noarch:security-4.0-amd64:security-4.0-noarch:security-4.1-amd64:security-4.1-noarch
Distributor ID: Ubuntu
Description:    Ubuntu 14.04.2 LTS
Release:        14.04
Codename:       trusty

$ sudo make -j12 V=0 check

[==========] 712 tests from 116 test cases ran. (318672 ms total)
[  PASSED  ] 676 tests.
[  FAILED  ] 36 tests, listed below:
[  FAILED  ] PerfEventIsolatorTest.ROOT_CGROUPS_Sample
[  FAILED  ] UserCgroupIsolatorTest/2.ROOT_CGROUPS_UserCgroup, where TypeParam = mesos::internal::slave::CgroupsPerfEventIsolatorProcess
[  FAILED  ] SlaveRecoveryTest/0.RecoverSlaveState, where TypeParam = mesos::internal::slave::MesosContainerizer
[  FAILED  ] SlaveRecoveryTest/0.RecoverStatusUpdateManager, where TypeParam = mesos::internal::slave::MesosContainerizer
[  FAILED  ] SlaveRecoveryTest/0.ReconnectExecutor, where TypeParam = mesos::internal::slave::MesosContainerizer
[  FAILED  ] SlaveRecoveryTest/0.RecoverUnregisteredExecutor, where TypeParam = mesos::internal::slave::MesosContainerizer
[  FAILED  ] SlaveRecoveryTest/0.RecoverTerminatedExecutor, where TypeParam = mesos::internal::slave::MesosContainerizer
[  FAILED  ] SlaveRecoveryTest/0.RecoverCompletedExecutor, where TypeParam = mesos::internal::slave::MesosContainerizer
[  FAILED  ] SlaveRecoveryTest/0.CleanupExecutor, where TypeParam = mesos::internal::slave::MesosContainerizer
[  FAILED  ] SlaveRecoveryTest/0.RemoveNonCheckpointingFramework, where TypeParam = mesos::internal::slave::MesosContainerizer
[  FAILED  ] SlaveRecoveryTest/0.NonCheckpointingFramework, where TypeParam = mesos::internal::slave::MesosContainerizer
[  FAILED  ] SlaveRecoveryTest/0.KillTask, where TypeParam = mesos::internal::slave::MesosContainerizer
[  FAILED  ] SlaveRecoveryTest/0.Reboot, where TypeParam = mesos::internal::slave::MesosContainerizer
[  FAILED  ] SlaveRecoveryTest/0.GCExecutor, where TypeParam = mesos::internal::slave::MesosContainerizer
[  FAILED  ] SlaveRecoveryTest/0.ShutdownSlave, where TypeParam = mesos::internal::slave::MesosContainerizer
[  FAILED  ] SlaveRecoveryTest/0.ShutdownSlaveSIGUSR1, where TypeParam = mesos::internal::slave::MesosContainerizer
[  FAILED  ] SlaveRecoveryTest/0.RegisterDisconnectedSlave, where TypeParam = mesos::internal::slave::MesosContainerizer
[  FAILED  ] SlaveRecoveryTest/0.ReconcileKillTask, where TypeParam = mesos::internal::slave::MesosContainerizer
[  FAILED  ] SlaveRecoveryTest/0.ReconcileShutdownFramework, where TypeParam = mesos::internal::slave::MesosContainerizer
[  FAILED  ] SlaveRecoveryTest/0.ReconcileTasksMissingFromSlave, where TypeParam = mesos::internal::slave::MesosContainerizer
[  FAILED  ] SlaveRecoveryTest/0.SchedulerFailover, where TypeParam = mesos::internal::slave::MesosContainerizer
[  FAILED  ] SlaveRecoveryTest/0.PartitionedSlave, where TypeParam = mesos::internal::slave::MesosContainerizer
[  FAILED  ] SlaveRecoveryTest/0.MasterFailover, where TypeParam = mesos::internal::slave::MesosContainerizer
[  FAILED  ] SlaveRecoveryTest/0.MultipleFrameworks, where TypeParam = mesos::internal::slave::MesosContainerizer
[  FAILED  ] SlaveRecoveryTest/0.MultipleSlaves, where TypeParam = mesos::internal::slave::MesosContainerizer
[  FAILED  ] SlaveRecoveryTest/0.RestartBeforeContainerizerLaunch, where TypeParam = mesos::internal::slave::MesosContainerizer
[  FAILED  ] MesosContainerizerSlaveRecoveryTest.ResourceStatistics
[  FAILED  ] MesosContainerizerSlaveRecoveryTest.CGROUPS_ROOT_PerfRollForward
[  FAILED  ] MesosContainerizerSlaveRecoveryTest.CGROUPS_ROOT_PidNamespaceForward
[  FAILED  ] MesosContainerizerSlaveRecoveryTest.CGROUPS_ROOT_PidNamespaceBackward
[  FAILED  ] CgroupsAnyHierarchyWithPerfEventTest.ROOT_CGROUPS_Perf
[  FAILED  ] MemoryPressureMesosTest.CGROUPS_ROOT_Statistics
[  FAILED  ] MemoryPressureMesosTest.CGROUPS_ROOT_SlaveRecovery
[  FAILED  ] NsTest.ROOT_setns
[  FAILED  ] PerfTest.ROOT_Events
[  FAILED  ] PerfTest.ROOT_SamplePid

36 FAILED TESTS
{noformat}

Full log attached.",Bug,Blocker,marco-mesos,2015-08-03T21:30:28.000+0000,5,Resolved,Complete,`sudo make distcheck` fails on Ubuntu 14.04 (and possibly other OSes too),2015-10-05T23:55:58.000+0000,MESOS-3079,2.0,mesos,Mesosphere Sprint 15
,2015-07-17T22:36:02.000+0000,bmahler,"Currently, when resources are recovered, we do not perform an allocation for that slave. Rather, we wait until the next allocation interval.

For small task, high throughput frameworks, this can have a significant impact on overall throughput, see the following thread:
http://markmail.org/thread/y6mzfwzlurv6nik3

We should consider immediately performing a re-allocation for the slave upon resource recovery.",Improvement,Major,bmahler,,10020,Accepted,In Progress,Recovered resources are not re-allocated until the next allocation delay.,2016-02-25T01:42:03.000+0000,MESOS-3078,5.0,mesos,
kaysoky,2015-07-17T21:21:22.000+0000,kaysoky,"Persisted info is fetched from the registry when a master is elected or after failover.  Currently, this process involves 3 steps:
* Fetch the ""registry"".
* Start an operation to add the new master to the fetched registry.
* Check the success of the operation and finish recovering.
These methods can be found in src/master/registrar.cpp {code}RegistrarProcess::recover, ::_recover, ::__recover{code}

Since the maintenance schedule is stored in a separate key, the recover process must also fetch a new ""maintenance"" object.  This object needs to be passed along to the master along with the existing ""registry"" object.

Possible test(s):
* src/tests/registrar_tests.cpp
** Change the ""Recovery"" test to include checks for the new object.",Task,Major,kaysoky,2015-07-30T22:20:08.000+0000,5,Resolved,Complete,Registry recovery does not recover the maintenance object.,2015-09-14T20:15:00.000+0000,MESOS-3077,5.0,mesos,
karya,2015-07-17T19:54:50.000+0000,karya,"This would allow the executors and Slave modules to expose some meta-data to frameworks and Mesos-DNS via state.json.

A typical use case is to allow the containers to expose their IP to framework/Mesos-DNS.",Task,Critical,karya,2015-07-23T03:42:05.000+0000,5,Resolved,Complete,Add Labels to TaskStatus and expose them via state.json,2015-07-23T03:42:05.000+0000,MESOS-3076,2.0,mesos,Mesosphere Sprint 15
alexr,2015-07-17T14:49:09.000+0000,js84,"We need to to validate quota requests in the Mesos Master as outlined in the Design Doc: https://docs.google.com/document/d/16iRNmziasEjVOblYp5bbkeBZ7pnjNlaIzPQqMTHQ-9I

This ticket aims to validate satisfiability (in terms of available resources) of a quota request using a heuristic algorithm in the Mesos Master, rather than validating the syntax of the request.",Improvement,Major,js84,2015-11-16T16:26:37.000+0000,5,Resolved,Complete,Add capacity heuristic for quota requests in Master,2015-11-16T19:04:50.000+0000,MESOS-3074,3.0,mesos,Mesosphere Sprint 15
js84,2015-07-17T14:45:13.000+0000,js84,"We need to implement the HTTP endpoints for Quota as outlined in the Design Doc: (https://docs.google.com/document/d/16iRNmziasEjVOblYp5bbkeBZ7pnjNlaIzPQqMTHQ-9I).
",Epic,Major,js84,2016-01-15T23:28:38.000+0000,5,Resolved,Complete,Introduce HTTP endpoints for Quota,2016-01-15T23:28:39.000+0000,MESOS-3073,3.0,mesos,Mesosphere Sprint 15
arojas,2015-07-17T09:28:59.000+0000,arojas,"h1.Introduction

As it stands right now, default implementations of modularized components are required to have a non parametrized {{create()}} static method. This allows to write tests which can cover default implementations and modules based on these default implementations on a uniform way.

For example, with the interface {{Foo}}:

{code}
class Foo {
public:
  virtual ~Foo() {}

  virtual Future<int> hello() = 0;

protected:
  Foo() {}
};
{code}

With a default implementation:

{code}
class LocalFoo {
public:
  Try<Foo*> create() {
    return new Foo;
  }

  virtual Future<int> hello() {
    return 1;
  }
};
{code}

This allows to create typed tests which look as following:

{code}
typedef ::testing::Types<LocalFoo,
                         tests::Module<Foo, TestLocalFoo>>
  FooTestTypes;

TYPED_TEST_CASE(FooTest, FooTestTypes);

TYPED_TEST(FooTest, ATest)
{
  Try<Foo*> foo = TypeParam::create();
  ASSERT_SOME(foo);

  AWAIT_CHECK_EQUAL(foo.get()->hello(), 1);
}
{code}

The test will be applied to each of types in the template parameters of {{FooTestTypes}}. This allows to test different implementation of an interface. In our code, it tests default implementations and a module which uses the same default implementation.

The class {{tests::Module<typename T, ModuleID N>}} needs a little explanation, it is a wrapper around {{ModuleManager}} which allows the tests to encode information about the requested module in the type itself instead of passing a string to the factory method. The wrapper around create, the real important method looks as follows:

{code}
template<typename T, ModuleID N>
static Try<T*> test::Module<T, N>::create()
{
  Try<std::string> moduleName = getModuleName(N);
  if (moduleName.isError()) {
    return Error(moduleName.error());
  }
  return mesos::modules::ModuleManager::create<T>(moduleName.get());
}
{code}

h1.The Problem

Consider the following implementation of {{Foo}}:

{code}
class ParameterFoo {
public:
  Try<Foo*> create(int i) {
    return new ParameterFoo(i);
  }

  ParameterFoo(int i) : i_(i) {}

  virtual Future<int> hello() {
    return i;
  }

private:
  int i_;
};
{code}

As it can be seen, this implementation cannot be used as a default implementation since its create API does not match the one of {{test::Module<>}}: {{create()}} has a different signature for both types. It is still a common situation to require initialization parameters for objects, however this constraint (keeping both interfaces alike) forces default implementations of modularized components to have default constructors, therefore the tests are forcing the design of the interfaces.

Implementations which are supposed to be used as modules only, i.e. non default implementations are allowed to have constructor parameters, since the actual signature of their factory method is, this factory method's function is to decode the parameters and call the appropriate constructor:

{code}
template<typename T>
T* Module<T>::create(const Parameters& params);
{code}

where parameters is just an array of key-value string pairs whose interpretation is left to the specific module. Sadly, this call is wrapped by 
{{ModuleManager}} which only allows module parameters to be passed from the command line and does not offer a programmatic way to feed construction parameters to modules.

h1.The Ugly Workaround

With the requirement of a default constructor and parameters devoid {{create()}} factory function, a common pattern (see [Authenticator|https://github.com/apache/mesos/blob/9d4ac11ed757aa5869da440dfe5343a61b07199a/include/mesos/authentication/authenticator.hpp]) has been introduced to feed construction parameters into default implementation, this leads to adding an {{initialize()}} call to the public interface, which will have {{Foo}} become:

{code}
class Foo {
public:
  virtual ~Foo() {}

  virtual Try<Nothing> initialize(Option<int> i) = 0;

  virtual Future<int> hello() = 0;

protected:
  Foo() {}
};
{code}

{{ParameterFoo}} will thus look as follows:

{code}
class ParameterFoo {
public:
  Try<Foo*> create() {
    return new ParameterFoo;
  }

  ParameterFoo() : i_(None()) {}

  virtual Try<Nothing> initialize(Option<int> i) {
    if (i.isNone()) {
      return Error(""Need value to initialize"");
    }

    i_ = i;

    return Nothing;
  }

  virtual Future<int> hello() {
    if (i_.isNone()) {
      return Future<int>::failure(""Not initialized"");
    }

    return i_.get();
  }

private:
  Option<int> i_;
};
{code}

Look that this {{initialize()}} method now has to be implemented by all descendants of {{Foo}}, even if there's a {{DatabaseFoo}} which takes is
return value for {{hello()}} from a DB, it will need to support {{int}} as an initialization parameter.

The problem is more severe the more specific the parameter to {{initialize()}} is. For example, if there is a very complex structure implementing ACLs, all implementations of an authorizer will need to import this structure even if they can completely ignore it.

In the {{Foo}} example if {{ParameterFoo}} were to become the default implementation of {{Foo}}, the tests would look as follows:

{code}
typedef ::testing::Types<ParameterFoo,
                         tests::Module<Foo, TestParameterFoo>>
  FooTestTypes;

TYPED_TEST_CASE(FooTest, FooTestTypes);

TYPED_TEST(FooTest, ATest)
{
  Try<Foo*> foo = TypeParam::create();
  ASSERT_SOME(foo);

  int fooValue = 1;
  foo.get()->initialize(fooValue);

  AWAIT_CHECK_EQUAL(foo.get()->hello(), fooValue);
}
{code}",Improvement,Major,arojas,2016-03-07T10:12:12.000+0000,5,Resolved,Complete,Unify initialization of modularized components,2016-03-07T10:12:12.000+0000,MESOS-3072,3.0,mesos,
kaysoky,2015-07-16T18:38:53.000+0000,kaysoky,"In order to modify the maintenance schedule in the replicated registry, we will need Operations (src/master/registrar.hpp).

The operations will likely correspond to the HTTP API:
* UpdateMaintenanceSchedule: Given a blob representing a maintenance schedule, perform some verification on the blob.  Write the blob to the registry.  
* StartMaintenance:  Given a set of machines, verify then transition machines from Draining to Deactivated.
* StopMaintenance:  Given a set of machines, verify then transition machines from Deactivated to Normal.  Remove affected machines from the schedule(s).",Task,Major,kaysoky,2015-08-31T17:27:51.000+0000,5,Resolved,Complete,Registry operations do not exist for manipulating maintanence schedules,2015-09-25T22:46:30.000+0000,MESOS-3069,8.0,mesos,Mesosphere Sprint 16
kaysoky,2015-07-16T18:22:57.000+0000,kaysoky,"This is primarily a refactoring.

The prototype for modifying the registry is currently:
{code}
Try<bool> operator () (
    Registry* registry,
    hashset<SlaveID>* slaveIDs,
    bool strict);
{code}

In order to support Maintenance schedules (possibly Quotas as well), there should be an alternate prototype for Maintenance.  Something like:
{code}
Try<bool> operation () (
    Maintenance* maintenance,
    bool strict);
{code}

The existing RegistrarProcess::update (src/master/registrar.cpp) should be refactored to allow for more than one key.  If necessary, refactor existing operations defined in src/master/master.hpp (AdminSlave, ReadminSlave,  RemoveSlave).",Task,Major,kaysoky,2015-07-30T22:18:03.000+0000,5,Resolved,Complete,Registry operations are hardcoded for a single key (Registry object),2015-09-14T20:15:00.000+0000,MESOS-3068,5.0,mesos,
bmahler,2015-07-16T18:08:06.000+0000,anandmazumdar,"We need a streaming response decoder to de-serialize chunks sent from the master on the events stream.

From the HTTP API design doc:
Master encodes each Event in RecordIO format, i.e. a string representation of length of the event in bytes followed by JSON or binary Protobuf  (possibly compressed) encoded event.

As of now for getting the basic features right , this is being done in the test-cases:

{code}
  auto reader = response.get().reader;
  ASSERT_SOME(reader);

  Future<std::string> eventFuture = reader.get().read();
  AWAIT_READY(eventFuture);

  Event event;
  event.ParseFromString(eventFuture.get());
{code}

Two things need to happen:
- We need master to emit events in RecordIO format i.e. event size followed by the serialized event instead of just the serialized events as is the case now.
- The decoder class should then abstract away the logic of reading the response and de-serializing events from the stream.

Ideally, the decoder should work with both ""json"" and ""protobuf"" responses.
",Task,Major,anandmazumdar,2015-07-24T22:29:32.000+0000,5,Resolved,Complete,Implement a streaming response decoder for events stream,2015-07-24T22:29:32.000+0000,MESOS-3067,3.0,mesos,Twitter Mesos Q3 Sprint 2
kaysoky,2015-07-16T18:05:30.000+0000,kaysoky,"In order to persist maintenance schedules across failovers of the master, the schedule information must be kept in the replicated registry.

This means adding an additional message in the Registry protobuf in src/master/registry.proto.  The status of each individual slave's maintenance will also be persisted in this way.
{code}
message Maintenance {
  message HostStatus {
    required string hostname = 1;

    // True if the slave is deactivated for maintenance.
    // False if the slave is draining in preparation for maintenance.
    required bool is_down = 2;  // Or an enum
  }

  message Schedule {
    // The set of affected slave(s).
    repeated HostStatus hosts = 1;

    // Interval in which this set of slaves is expected to be down for.
    optional Unavailability interval = 2;
  }

  message Schedules {
    repeated Schedule schedules;
  }

  optional Schedules schedules = 1;
}
{code}

Note: There can be multiple SlaveID's attached to a single hostname.",Task,Major,kaysoky,2015-08-31T17:22:29.000+0000,5,Resolved,Complete,Replicated registry needs a representation of maintenance schedules,2015-09-25T22:46:27.000+0000,MESOS-3066,3.0,mesos,Mesosphere Sprint 15
greggomann,2015-07-16T16:52:45.000+0000,mcypark,"This is the third in a series of tickets that adds authorization support to persistent volumes.

When a framework creates a persistent volume, ""create"" ACLs are checked to see if the framework (FrameworkInfo.principal) or the operator (Credential.user) is authorized to create persistent volumes. If not authorized, the create operation is rejected.

When a framework destroys a persistent volume, ""destroy"" ACLs are checked to see if the framework (FrameworkInfo.principal) or the operator (Credential.user) is authorized to destroy the persistent volume created by a framework or operator (Resource.DiskInfo.principal). If not authorized, the destroy operation is rejected.

A separate ticket will use the structures created here to enable authorization of the ""/create"" and ""/destroy"" HTTP endpoints: https://issues.apache.org/jira/browse/MESOS-3903",Task,Major,mcypark,2015-12-19T01:36:06.000+0000,5,Resolved,Complete,Add framework authorization for persistent volume,2015-12-19T01:36:06.000+0000,MESOS-3065,5.0,mesos,Mesosphere Sprint 16
greggomann,2015-07-16T16:32:19.000+0000,mcypark,"In order to support authorization for persistent volumes, we should add the {{principal}} to {{Resource.DiskInfo}}, analogous to {{Resource.ReservationInfo.principal}}.",Task,Major,mcypark,2015-12-07T18:59:56.000+0000,5,Resolved,Complete,Add 'principal' field to 'Resource.DiskInfo.Persistence',2015-12-07T18:59:56.000+0000,MESOS-3064,1.0,mesos,Mesosphere Sprint 16
greggomann,2015-07-16T16:10:09.000+0000,mcypark,"Dynamic reservations should be authorized with the {{principal}} of the reserving entity (framework or master). The idea is to introduce {{Reserve}} and {{Unreserve}} into the ACL.

{code}
  message Reserve {
    // Subjects.
    required Entity principals = 1;

    // Objects.  MVP: Only possible values = ANY, NONE
    required Entity resources = 1;
  }

  message Unreserve {
    // Subjects.
    required Entity principals = 1;

    // Objects.
    required Entity reserver_principals = 2;
  }
{code}

When a framework/operator reserves resources, ""reserve"" ACLs are checked to see if the framework ({{FrameworkInfo.principal}}) or the operator ({{Credential.user}}) is authorized to reserve the specified resources. If not authorized, the reserve operation is rejected.

When a framework/operator unreserves resources, ""unreserve"" ACLs are checked to see if the framework ({{FrameworkInfo.principal}}) or the operator ({{Credential.user}}) is authorized to unreserve the resources reserved by a framework or operator ({{Resource.ReservationInfo.principal}}). If not authorized, the unreserve operation is rejected.",Task,Major,mcypark,2015-12-04T00:53:08.000+0000,5,Resolved,Complete,Add authorization for dynamic reservation,2015-12-04T00:53:08.000+0000,MESOS-3062,2.0,mesos,Mesosphere Sprint 16
karya,2015-07-16T14:49:43.000+0000,karya,"We want to expose docker container IP to Mesos-DNS. One potential solution is to make it available via Master's state.json. We can set a label ""Docker.NetworkSettings.IPAddress"" in TaskStatus message (when it is sent the first time with TASK_RUNNING status).",Task,Critical,karya,2015-07-23T03:40:35.000+0000,5,Resolved,Complete,Expose docker container IP in Master's state.json,2015-07-23T03:42:34.000+0000,MESOS-3061,2.0,mesos,Mesosphere Sprint 15
nfnt,2015-07-16T13:18:24.000+0000,nfnt,"The response code for successful HTTP requests is 200, the response code for successful FTP file transfers is 226. The fetcher currently only checks for a response code of 200 even for FTP URIs. This results in failed fetching even though the resource gets downloaded successfully. This has been found by a dedicated external test using an FTP server.
",Bug,Major,nfnt,2015-07-17T09:24:13.000+0000,5,Resolved,Complete,FTP response code for success not recognized by fetcher.,2015-07-17T20:41:44.000+0000,MESOS-3060,1.0,mesos,
vinodkone,2015-07-15T17:20:39.000+0000,vinodkone,"Master::subscribe() incorrectly handles re-registration. It handles it as a registration request (not ""re-registration"") because of a bug in the if loop (should have been !frameworkInfo.has_id()).

{code}
void Master::subscribe(
    const UPID& from,
    const scheduler::Call::Subscribe& subscribe)
{
  const FrameworkInfo& frameworkInfo = subscribe.framework_info();

  // TODO(vinod): Instead of calling '(re-)registerFramework()' from
  // here refactor those methods to call 'subscribe()'.
  if (frameworkInfo.has_id() || frameworkInfo.id() == """") {
    registerFramework(from, frameworkInfo);
  } else {
    reregisterFramework(from, frameworkInfo, subscribe.force());
  }
}

{code}",Bug,Major,vinodkone,2015-07-16T22:34:28.000+0000,5,Resolved,Complete,Master doesn't properly handle SUBSCRIBE call,2015-07-16T22:34:28.000+0000,MESOS-3055,2.0,mesos,Twitter Mesos Q3 Sprint 1
js84,2015-07-15T05:11:05.000+0000,jamespeach,"Testing in an environment with lots of frameworks (>200), where the frameworks permanently decline resources they don't need. The allocator ends up spending a lot of time figuring out whether offers are refused (the code path through {{HierarchicalAllocatorProcess::isFiltered()}}.

In profiling a synthetic benchmark, it turns out that comparing port ranges is very expensive, involving many temporary allocations. 61% of Resources::contains() run time is in operator -= (Resource). 35% of Resources::contains() run time is in Resources::_contains().

The heaviest call chain through {{Resources::_contains}} is:

{code}
Running Time          Self (ms)         Symbol Name
7237.0ms   35.5%          4.0            mesos::Resources::_contains(mesos::Resource const&) const
7200.0ms   35.3%          1.0             mesos::contains(mesos::Resource const&, mesos::Resource const&)
7133.0ms   35.0%        121.0              mesos::operator<=(mesos::Value_Ranges const&, mesos::Value_Ranges const&)
6319.0ms   31.0%          7.0               mesos::coalesce(mesos::Value_Ranges*, mesos::Value_Ranges const&)
6240.0ms   30.6%        161.0                mesos::coalesce(mesos::Value_Ranges*, mesos::Value_Range const&)
1867.0ms    9.1%         25.0                 mesos::Value_Ranges::add_range()
1694.0ms    8.3%          4.0                 mesos::Value_Ranges::~Value_Ranges()
1495.0ms    7.3%         16.0                 mesos::Value_Ranges::operator=(mesos::Value_Ranges const&)
 445.0ms    2.1%         94.0                 mesos::Value_Range::MergeFrom(mesos::Value_Range const&)
 154.0ms    0.7%         24.0                 mesos::Value_Ranges::range(int) const
 103.0ms    0.5%         24.0                 mesos::Value_Ranges::range_size() const
  95.0ms    0.4%          2.0                 mesos::Value_Range::Value_Range(mesos::Value_Range const&)
  59.0ms    0.2%          4.0                 mesos::Value_Ranges::Value_Ranges()
  50.0ms    0.2%         50.0                 mesos::Value_Range::begin() const
  28.0ms    0.1%         28.0                 mesos::Value_Range::end() const
  26.0ms    0.1%          0.0                 mesos::Value_Range::~Value_Range()
{code}

mesos::coalesce(Value_Ranges) gets done a lot and ends up being really expensive. The heaviest parts of the inverted call chain are:

{code}
Running Time	Self (ms)		Symbol Name
3209.0ms   15.7%	3209.0	 	mesos::Value_Range::~Value_Range()
3209.0ms   15.7%	0.0	 	 google::protobuf::internal::GenericTypeHandler<mesos::Value_Range>::Delete(mesos::Value_Range*)
3209.0ms   15.7%	0.0	 	  void google::protobuf::internal::RepeatedPtrFieldBase::Destroy<google::protobuf::RepeatedPtrField<mesos::Value_Range>::TypeHandler>()
3209.0ms   15.7%	0.0	 	   google::protobuf::RepeatedPtrField<mesos::Value_Range>::~RepeatedPtrField()
3209.0ms   15.7%	0.0	 	    google::protobuf::RepeatedPtrField<mesos::Value_Range>::~RepeatedPtrField()
3209.0ms   15.7%	0.0	 	     mesos::Value_Ranges::~Value_Ranges()
3209.0ms   15.7%	0.0	 	      mesos::Value_Ranges::~Value_Ranges()
2441.0ms   11.9%	0.0	 	       mesos::coalesce(mesos::Value_Ranges*, mesos::Value_Range const&)
 452.0ms    2.2%	0.0	 	       mesos::remove(mesos::Value_Ranges*, mesos::Value_Range const&)
 169.0ms    0.8%	0.0	 	       mesos::operator<=(mesos::Value_Ranges const&, mesos::Value_Ranges const&)
  82.0ms    0.4%	0.0	 	       mesos::operator-=(mesos::Value_Ranges&, mesos::Value_Ranges const&)
  65.0ms    0.3%	0.0	 	       mesos::Value_Ranges::~Value_Ranges()

2541.0ms   12.4%	2541.0	 	google::protobuf::internal::GenericTypeHandler<mesos::Value_Range>::New()
2541.0ms   12.4%	0.0	 	 google::protobuf::RepeatedPtrField<mesos::Value_Range>::TypeHandler::Type* google::protobuf::internal::RepeatedPtrFieldBase::Add<google::protobuf::RepeatedPtrField<mesos::Value_Range>::TypeHandler>()
2305.0ms   11.3%	0.0	 	  google::protobuf::RepeatedPtrField<mesos::Value_Range>::Add()
2305.0ms   11.3%	0.0	 	   mesos::Value_Ranges::add_range()
1962.0ms    9.6%	0.0	 	    mesos::coalesce(mesos::Value_Ranges*, mesos::Value_Range const&)
 343.0ms    1.6%	0.0	 	    mesos::ranges::add(mesos::Value_Ranges*, long long, long long)

236.0ms    1.1%	0.0	 	  void google::protobuf::internal::RepeatedPtrFieldBase::MergeFrom<google::protobuf::RepeatedPtrField<mesos::Value_Range>::TypeHandler>(google::protobuf::internal::RepeatedPtrFieldBase const&)
1471.0ms    7.2%	1471.0	 	google::protobuf::internal::RepeatedPtrFieldBase::Reserve(int)
1333.0ms    6.5%	0.0	 	 google::protobuf::RepeatedPtrField<mesos::Value_Range>::TypeHandler::Type* google::protobuf::internal::RepeatedPtrFieldBase::Add<google::protobuf::RepeatedPtrField<mesos::Value_Range>::TypeHandler>()
1333.0ms    6.5%	0.0	 	  google::protobuf::RepeatedPtrField<mesos::Value_Range>::Add()
1333.0ms    6.5%	0.0	 	   mesos::Value_Ranges::add_range()
1086.0ms    5.3%	0.0	 	    mesos::coalesce(mesos::Value_Ranges*, mesos::Value_Range const&)
 247.0ms    1.2%	0.0	 	    mesos::ranges::add(mesos::Value_Ranges*, long long, long long)

107.0ms    0.5%	0.0	 	 void google::protobuf::internal::RepeatedPtrFieldBase::MergeFrom<google::protobuf::RepeatedPtrField<mesos::Value_Range>::TypeHandler>(google::protobuf::internal::RepeatedPtrFieldBase const&)
107.0ms    0.5%	0.0	 	  google::protobuf::RepeatedPtrField<mesos::Value_Range>::MergeFrom(google::protobuf::RepeatedPtrField<mesos::Value_Range> const&)
107.0ms    0.5%	0.0	 	   mesos::Value_Ranges::MergeFrom(mesos::Value_Ranges const&)
105.0ms    0.5%	0.0	 	    mesos::Value_Ranges::CopyFrom(mesos::Value_Ranges const&)
105.0ms    0.5%	0.0	 	     mesos::Value_Ranges::operator=(mesos::Value_Ranges const&)
104.0ms    0.5%	0.0	 	      mesos::coalesce(mesos::Value_Ranges*, mesos::Value_Range const&)
  1.0ms    0.0%	0.0	 	      mesos::remove(mesos::Value_Ranges*, mesos::Value_Range const&)
  2.0ms    0.0%	0.0	 	    mesos::Resource::MergeFrom(mesos::Resource const&)
  2.0ms    0.0%	0.0	 	     google::protobuf::internal::GenericTypeHandler<mesos::Resource>::Merge(mesos::Resource const&, mesos::Resource*)
  2.0ms    0.0%	0.0	 	      void google::protobuf::internal::RepeatedPtrFieldBase::MergeFrom<google::protobuf::RepeatedPtrField<mesos::Resource>::TypeHandler>(google::protobuf::internal::RepeatedPtrFieldBase const&)
 29.0ms    0.1%	0.0	 	 void google::protobuf::internal::RepeatedPtrFieldBase::MergeFrom<google::protobuf::RepeatedPtrField<mesos::Resource>::TypeHandler>(google::protobuf::internal::RepeatedPtrFieldBase const&)

898.0ms    4.4%	898.0	 	google::protobuf::RepeatedPtrField<mesos::Value_Range>::TypeHandler::Type* google::protobuf::internal::RepeatedPtrFieldBase::Add<google::protobuf::RepeatedPtrField<mesos::Value_Range>::TypeHandler>()
517.0ms    2.5%	0.0	 	 google::protobuf::RepeatedPtrField<mesos::Value_Range>::Add()
517.0ms    2.5%	0.0	 	  mesos::Value_Ranges::add_range()
429.0ms    2.1%	0.0	 	   mesos::coalesce(mesos::Value_Ranges*, mesos::Value_Range const&)
 88.0ms    0.4%	0.0	 	   mesos::ranges::add(mesos::Value_Ranges*, long long, long long)
379.0ms    1.8%	0.0	 	 void google::protobuf::internal::RepeatedPtrFieldBase::MergeFrom<google::protobuf::RepeatedPtrField<mesos::Value_Range>::TypeHandler>(google::protobuf::internal::RepeatedPtrFieldBase const&)
{code}
",Bug,Major,jamespeach,2015-09-24T18:05:33.000+0000,5,Resolved,Complete,performance issues with port ranges comparison,2016-02-27T00:45:15.000+0000,MESOS-3051,8.0,mesos,Mesosphere Sprint 17
marco-mesos,2015-07-15T00:50:20.000+0000,adam-mesos,"Running `sudo make check` on CentOS 7.1 for Mesos 0.23.0-rc3 causes several several failures/errors:

{code}
[ RUN      ] DockerTest.ROOT_DOCKER_CheckPortResource
../../src/tests/docker_tests.cpp:303: Failure
(run).failure(): Container exited on error: exited with status 1
[  FAILED  ] DockerTest.ROOT_DOCKER_CheckPortResource (709 ms)
{code}
...
{code}
[ RUN      ] PerfEventIsolatorTest.ROOT_CGROUPS_Sample
../../src/tests/isolator_tests.cpp:837: Failure
isolator: Failed to create PerfEvent isolator, invalid events: { cycles, task-clock }
[  FAILED  ] PerfEventIsolatorTest.ROOT_CGROUPS_Sample (9 ms)
[----------] 1 test from PerfEventIsolatorTest (9 ms total)
   
[----------] 2 tests from SharedFilesystemIsolatorTest
[ RUN      ] SharedFilesystemIsolatorTest.ROOT_RelativeVolume
+ mount -n --bind /tmp/SharedFilesystemIsolatorTest_ROOT_RelativeVolume_4yTEAC/var/tmp /var/tmp
+ touch /var/tmp/492407e1-5dec-4b34-8f2f-130430f41aac
../../src/tests/isolator_tests.cpp:1001: Failure
Value of: os::exists(file)
  Actual: true
Expected: false
[  FAILED  ] SharedFilesystemIsolatorTest.ROOT_RelativeVolume (92 ms)
[ RUN      ] SharedFilesystemIsolatorTest.ROOT_AbsoluteVolume
+ mount -n --bind /tmp/SharedFilesystemIsolatorTest_ROOT_AbsoluteVolume_OwYrXK /var/tmp
+ touch /var/tmp/7de712aa-52eb-4976-b0f9-32b6a006418d
../../src/tests/isolator_tests.cpp:1086: Failure
Value of: os::exists(path::join(containerPath, filename))
  Actual: true
Expected: false
[  FAILED  ] SharedFilesystemIsolatorTest.ROOT_AbsoluteVolume (100 ms)
{code}
...
{code}
[----------] 1 test from UserCgroupIsolatorTest/0, where TypeParam = mesos::internal::slave::CgroupsMemIsolatorProcess
userdel: user 'mesos.test.unprivileged.user' does not exist
[ RUN      ] UserCgroupIsolatorTest/0.ROOT_CGROUPS_UserCgroup
-bash: /sys/fs/cgroup/blkio/user.slice/cgroup.procs: Permission denied
mkdir: cannot create directory ‘/sys/fs/cgroup/blkio/user.slice/user’: Permission denied
../../src/tests/isolator_tests.cpp:1274: Failure
Value of: os::system( ""su - "" + UNPRIVILEGED_USERNAME + "" -c 'mkdir "" + path::join(flags.cgroups_hierarchy, userCgroup) + ""'"")
  Actual: 256
Expected: 0
-bash: /sys/fs/cgroup/blkio/user.slice/user/cgroup.procs: No such file or directory
../../src/tests/isolator_tests.cpp:1283: Failure
Value of: os::system( ""su - "" + UNPRIVILEGED_USERNAME + "" -c 'echo $$ >"" + path::join(flags.cgroups_hierarchy, userCgroup, ""cgroup.procs"") + ""'"")
  Actual: 256
Expected: 0
-bash: /sys/fs/cgroup/memory/mesos/bbf8c8f0-3d67-40df-a269-b3dc6a9597aa/cgroup.procs: Permission denied
-bash: /sys/fs/cgroup/cpuacct,cpu/user.slice/cgroup.procs: No such file or directory
mkdir: cannot create directory ‘/sys/fs/cgroup/cpuacct,cpu/user.slice/user’: No such file or directory
../../src/tests/isolator_tests.cpp:1274: Failure
Value of: os::system( ""su - "" + UNPRIVILEGED_USERNAME + "" -c 'mkdir "" + path::join(flags.cgroups_hierarchy, userCgroup) + ""'"")
  Actual: 256
Expected: 0
-bash: /sys/fs/cgroup/cpuacct,cpu/user.slice/user/cgroup.procs: No such file or directory
../../src/tests/isolator_tests.cpp:1283: Failure
Value of: os::system( ""su - "" + UNPRIVILEGED_USERNAME + "" -c 'echo $$ >"" + path::join(flags.cgroups_hierarchy, userCgroup, ""cgroup.procs"") + ""'"")
  Actual: 256
Expected: 0
-bash: /sys/fs/cgroup/name=systemd/user.slice/user-2004.slice/session-3865.scope/cgroup.procs: No such file or directory
mkdir: cannot create directory ‘/sys/fs/cgroup/name=systemd/user.slice/user-2004.slice/session-3865.scope/user’: No such file or directory
../../src/tests/isolator_tests.cpp:1274: Failure
Value of: os::system( ""su - "" + UNPRIVILEGED_USERNAME + "" -c 'mkdir "" + path::join(flags.cgroups_hierarchy, userCgroup) + ""'"")
  Actual: 256
Expected: 0
-bash: /sys/fs/cgroup/name=systemd/user.slice/user-2004.slice/session-3865.scope/user/cgroup.procs: No such file or directory
../../src/tests/isolator_tests.cpp:1283: Failure
Value of: os::system( ""su - "" + UNPRIVILEGED_USERNAME + "" -c 'echo $$ >"" + path::join(flags.cgroups_hierarchy, userCgroup, ""cgroup.procs"") + ""'"")
  Actual: 256
Expected: 0
[  FAILED  ] UserCgroupIsolatorTest/0.ROOT_CGROUPS_UserCgroup, where TypeParam = mesos::internal::slave::CgroupsMemIsolatorProcess (1034 ms)
[----------] 1 test from UserCgroupIsolatorTest/0 (1034 ms total)
[----------] 1 test from UserCgroupIsolatorTest/1, where TypeParam = mesos::internal::slave::CgroupsCpushareIsolatorProcess
userdel: user 'mesos.test.unprivileged.user' does not exist
[ RUN      ] UserCgroupIsolatorTest/1.ROOT_CGROUPS_UserCgroup
-bash: /sys/fs/cgroup/blkio/user.slice/cgroup.procs: Permission denied
mkdir: cannot create directory ‘/sys/fs/cgroup/blkio/user.slice/user’: Permission denied
../../src/tests/isolator_tests.cpp:1274: Failure
Value of: os::system( ""su - "" + UNPRIVILEGED_USERNAME + "" -c 'mkdir "" + path::join(flags.cgroups_hierarchy, userCgroup) + ""'"")
  Actual: 256
Expected: 0
-bash: /sys/fs/cgroup/blkio/user.slice/user/cgroup.procs: No such file or directory
../../src/tests/isolator_tests.cpp:1283: Failure
Value of: os::system( ""su - "" + UNPRIVILEGED_USERNAME + "" -c 'echo $$ >"" + path::join(flags.cgroups_hierarchy, userCgroup, ""cgroup.procs"") + ""'"")
  Actual: 256
Expected: 0
-bash: /sys/fs/cgroup/cpuacct,cpu/mesos/eeeb99f0-7c5c-4185-869d-635d51dcc6e1/cgroup.procs: No such file or directory
mkdir: cannot create directory ‘/sys/fs/cgroup/cpuacct,cpu/mesos/eeeb99f0-7c5c-4185-869d-635d51dcc6e1/user’: No such file or directory
../../src/tests/isolator_tests.cpp:1274: Failure
Value of: os::system( ""su - "" + UNPRIVILEGED_USERNAME + "" -c 'mkdir "" + path::join(flags.cgroups_hierarchy, userCgroup) + ""'"")
  Actual: 256
Expected: 0
-bash: /sys/fs/cgroup/cpuacct,cpu/mesos/eeeb99f0-7c5c-4185-869d-635d51dcc6e1/user/cgroup.procs: No such file or directory
../../src/tests/isolator_tests.cpp:1283: Failure
Value of: os::system( ""su - "" + UNPRIVILEGED_USERNAME + "" -c 'echo $$ >"" + path::join(flags.cgroups_hierarchy, userCgroup, ""cgroup.procs"") + ""'"")
  Actual: 256
Expected: 0
-bash: /sys/fs/cgroup/name=systemd/user.slice/user-2004.slice/session-3865.scope/cgroup.procs: No such file or directory
mkdir: cannot create directory ‘/sys/fs/cgroup/name=systemd/user.slice/user-2004.slice/session-3865.scope/user’: No such file or directory
../../src/tests/isolator_tests.cpp:1274: Failure
Value of: os::system( ""su - "" + UNPRIVILEGED_USERNAME + "" -c 'mkdir "" + path::join(flags.cgroups_hierarchy, userCgroup) + ""'"")
  Actual: 256
Expected: 0
-bash: /sys/fs/cgroup/name=systemd/user.slice/user-2004.slice/session-3865.scope/user/cgroup.procs: No such file or directory
../../src/tests/isolator_tests.cpp:1283: Failure
Value of: os::system( ""su - "" + UNPRIVILEGED_USERNAME + "" -c 'echo $$ >"" + path::join(flags.cgroups_hierarchy, userCgroup, ""cgroup.procs"") + ""'"")
  Actual: 256
Expected: 0
[  FAILED  ] UserCgroupIsolatorTest/1.ROOT_CGROUPS_UserCgroup, where TypeParam = mesos::internal::slave::CgroupsCpushareIsolatorProcess (763 ms)
[----------] 1 test from UserCgroupIsolatorTest/1 (763 ms total)
[----------] 1 test from UserCgroupIsolatorTest/2, where TypeParam = mesos::internal::slave::CgroupsPerfEventIsolatorProcess
userdel: user 'mesos.test.unprivileged.user' does not exist
[ RUN      ] UserCgroupIsolatorTest/2.ROOT_CGROUPS_UserCgroup
../../src/tests/isolator_tests.cpp:1200: Failure
isolator: Failed to create PerfEvent isolator, invalid events: { cpu-cycles }
[  FAILED  ] UserCgroupIsolatorTest/2.ROOT_CGROUPS_UserCgroup, where TypeParam = mesos::internal::slave::CgroupsPerfEventIsolatorProcess (6 ms)
[----------] 1 test from UserCgroupIsolatorTest/2 (6 ms total)
{code}",Bug,Blocker,adam-mesos,2015-08-20T17:59:08.000+0000,5,Resolved,Complete,Failing ROOT_ tests on CentOS 7.1,2015-08-20T17:59:08.000+0000,MESOS-3050,5.0,mesos,Mesosphere Sprint 16
klaus1982,2015-07-14T21:59:15.000+0000,bmahler,"Per [~StephanErb] and [~kevints]'s observations on MESOS-2940, stout's UUID abstraction is re-seeding the random generator during each call to {{UUID::random()}}, which is really expensive.

This is confirmed in the perf graph from MESOS-2940.",Bug,Major,bmahler,2015-09-18T16:24:23.000+0000,5,Resolved,Complete,Stout's UUID re-seeds a new random generator during each call to UUID::random.,2016-02-27T00:45:24.000+0000,MESOS-3046,3.0,mesos,
kaysoky,2015-07-14T20:53:12.000+0000,kaysoky,"When a master starts up, or after a master has failed, it must re-populate maintenance information (i.e. from the registry to the local state).

Particularly, {{Master::recover}} in {{src/master/master.cpp}} should be changed to process maintenance information.",Task,Major,kaysoky,2015-08-31T17:23:30.000+0000,5,Resolved,Complete,Maintenance information is not populated in case of failover,2015-09-25T22:46:29.000+0000,MESOS-3045,3.0,mesos,Mesosphere Sprint 17
hartem,2015-07-14T19:02:39.000+0000,kaysoky,"After a maintenance window is reached, the slave should be deactivated to prevent further tasks from utilizing it.


* For slaves that have completely drained, simply deactivate the slave.  See Master::deactivate(Slave*).
* For tasks which have not explicitly declined the InverseOffers (i.e. they've accepted them or do not understand InverseOffers), send kill signals.  See Master::killTask
* If a slave has tasks that have declined the InverseOffers, do not deactivate the slave.

Possible test(s):
* SlaveDrainedTest
** Start master, slave.
** Set maintenance to now.
** Check that slave gets deactivated
* InverseOfferAgnosticTest
** Start master, slave, framework.
** Have a task run on the slave (ignores InverseOffers).
** Set maintenance to now.
** Check that task gets killed.
** Check that slave gets deactivated.
* InverseOfferAcceptanceTest
** Start master, slave, framework.
** Run a task on the slave.
** Set maintenance to future.
** Have task accept InverseOffer.
** Check task gets killed, slave gets deactivated.
* InverseOfferDeclinedTest
** Start master, slave, framework.
** Run task on slave.
** Set maintenance to future.
** Have task decline maintenance with reason.
** Check task lives, slave still active.",Task,Major,kaysoky,2015-07-17T18:21:47.000+0000,5,Resolved,Complete,Slaves are not deactivated upon reaching a maintenance window,2015-09-14T20:15:00.000+0000,MESOS-3044,8.0,mesos,
jvanremoortere,2015-07-14T18:42:43.000+0000,kaysoky,"InverseOffers are similar to Offers in that they are Accepted or Declined based on their OfferID.  

Some additional logic may be neccesary in Master::accept (src/master/master.cpp) to gracefully handle the acceptance of InverseOffers.
* The InverseOffer needs to be removed from the set of pending InverseOffers.
* The InverseOffer should not result any errors/warnings.  

Note: accepted InverseOffers do not preclude further InverseOffers from being sent to the framework.  Instead, an accepted InverseOffer merely signifies that the framework is _currently_ fine with the expected downtime.",Task,Major,kaysoky,2015-09-14T18:05:54.000+0000,5,Resolved,Complete,Master does not handle InverseOffers in the Accept call (Event/Call API),2015-09-25T22:46:30.000+0000,MESOS-3043,3.0,mesos,Mesosphere Sprint 17
jvanremoortere,2015-07-14T18:24:38.000+0000,kaysoky,"Offers are currently sent from master/allocator to framework via ResourceOffersMessage's.  InverseOffers, which are roughly equivalent to negative Offers, can be sent in the same package.
In src/messages/messages.proto
{code}
message ResourceOffersMessage {
  repeated Offer offers = 1;
  repeated string pids = 2;

  // New field with InverseOffers
  repeated InverseOffer inverseOffers = 3;
}
{code}

Sent InverseOffers can be tracked in the master's local state:
i.e. In src/master/master.hpp:
{code}
struct Slave {
  ... // Existing fields.

  // Active InverseOffers on this slave.
  // Similar pattern to the ""offers"" field
  hashset<InverseOffer*> inverseOffers;
}
{code}

One actor (master or allocator) should populate the new InverseOffers field.
* In master (src/master/master.cpp)
** Master::offer is where the ResourceOffersMessage and Offer object is constructed.
** The same method could also check for maintenance and send InverseOffers.
* In the allocator (src/master/allocator/mesos/hierarchical.hpp)
** HierarchicalAllocatorProcess::allocate is where slave resources are aggregated an sent off to the frameworks.
** InverseOffers (i.e. negative resources) allocation could be calculated in this method.
** A change to Master::offer (i.e. the ""offerCallback"") may be necessary to account for the negative resources.

Possible test(s):
* InverseOfferTest
** Start master, slave, framework.
** Accept resource offer, start task.
** Set maintenance schedule to the future.
** Check that InverseOffer(s) are sent to the framework.
** Decline InverseOffer.
** Check that more InverseOffer(s) are sent.
** Accept InverseOffer.
** Check that more InverseOffer(s) are sent.",Task,Major,kaysoky,2015-09-14T18:04:42.000+0000,5,Resolved,Complete,Master/Allocator does not send InverseOffers to resources to be maintained,2015-09-25T22:46:27.000+0000,MESOS-3042,8.0,mesos,Mesosphere Sprint 16
gyliu,2015-07-14T17:34:37.000+0000,kaysoky,"In the Event/Call API, the Decline call is currently used by frameworks to reject resource offers.

In the case of InverseOffers, the framework could give additional information to the operators and/or allocator, as to why the InverseOffer is declined. i.e. Suppose a cluster running some consensus algorithm is given an InverseOffer on one of its nodes.  It may decline saying ""Too few nodes"" (or, more verbosely, ""Specified InverseOffer would lower the number of active nodes below quorum"").

This change requires the following changes:
* include/mesos/scheduler/scheduler.proto:
{code}
message Call {
  ...

  message Decline {
    repeated OfferID offer_ids = 1;
    optional Filters filters = 2;

    // Add this extra string for each OfferID
    // i.e. reasons[i] is for offer_ids[i]
    repeated string reasons = 3;
  }

  ...
}
{code}
* src/master/master.cpp
Change Master::decline to either store the reason, or log it.
* Add a declineOffer overload in the (Mesos)SchedulerDriver with an optional ""reason"".
** Extend the interface in include/mesos/scheduler.hpp
** Add/change the declineOffer method in src/sched/sched.cpp",Task,Major,kaysoky,,10006,Reviewable,New,"Decline call does not include an optional ""reason"", in the Event/Call API",2015-11-09T21:28:40.000+0000,MESOS-3041,3.0,mesos,Mesosphere Sprint 17
karya,2015-07-14T01:26:24.000+0000,karya,"Currently, the Slave will bind either to the loopback IP (127.0.0.1) or to the IP passed via the '--ip' flag. When it launches a containerized executor (e.g, via Mesos Containerizer), the executor inherits the binding IP of the Slave. This is due to the fact that the '--ip' flags sets the environment variable `LIBPROCESS_IP` to the passed IP. The executor then inherits this environment variable and is forced to bind to the Slave IP.

If an executor is running in its own containerized environment, with a separate IP than that of the Slave, currently there is no way of forcing it to bind to its own IP. A potential solution is to use the executor environment decorator hooks to update LIBPROCESS_IP environment variable for the executor.",Bug,Critical,karya,,10020,Accepted,In Progress,Allow executors binding IP to be different than Slave binding IP,2015-08-07T17:26:04.000+0000,MESOS-3039,2.0,mesos,Mesosphere Sprint 14
jvanremoortere,2015-07-14T00:53:32.000+0000,kaysoky,"Given a schedule, defined elsewhere, any resource offers to affected slaves must include an Unavailability field.

The maintenance schedule for a single slave should be held in [persistent storage|MESOS-2075] and locally by the master.  i.e. In src/master/master.hpp:
{code}
struct Slave {
  ... // Existing fields.

  // New field that the master/allocator can access
  Maintenances pendingDowntime;
}
{code}
The new field should be populated via an API call (see [MESOS-2067]).

The Unavailability field can be added to Master::offer (src/master/master.cpp).
{code}
offer->mutable_unavailability()->MergeFrom(slave->pendingDowntime);
{code}

Possible test(s):
* PendingUnavailibilityTest
** Start master, slave.
** Check unavailability of offer == none.
** Set unavailability to the future.
** Check offer has unavailability.
",Task,Major,kaysoky,2015-09-14T18:05:25.000+0000,5,Resolved,Complete,"Resource offers do not contain Unavailability, given a maintenance schedule",2015-09-25T22:46:29.000+0000,MESOS-3038,8.0,mesos,Mesosphere Sprint 17
gyliu,2015-07-13T23:40:27.000+0000,vinodkone,"SUPPRESS call is the complement to the current REVIVE call i.e., it will inform Mesos to stop sending offers to the framework. 

For the scheduler driver to send only Call messages (MESOS-2913), DeactivateFrameworkMessage needs to be converted to Call(s). We can implement this by having the driver send a SUPPRESS call followed by a DECLINE call for outstanding offers.",Improvement,Major,vinodkone,2015-09-18T23:18:12.000+0000,5,Resolved,Complete,Add a SUPPRESS call to the scheduler,2015-09-21T23:33:42.000+0000,MESOS-3037,3.0,mesos,Twitter Mesos Q3 Sprint 2
,2015-07-13T22:16:39.000+0000,marco-mesos,"As part of MESOS-2830 and MESOS-2902 I have been researching the ability to run a {{Subprocess}} and capture the {{stdout / stderr}} along with the exit status code.

{{process::subprocess()}} offers much of the functionality, but in a way that still requires a lot of handiwork on the developer's part; we would like to further abstract away the ability to just pass a string, an optional set of command-line arguments and then collect the output of the command (bonus: without blocking).",Story,Major,marco-mesos,,1,Open,New,As a Developer I would like a standard way to run a Subprocess in libprocess,2016-01-13T02:16:47.000+0000,MESOS-3035,3.0,mesos,Mesosphere Sprint 14
jojy,2015-07-13T18:32:48.000+0000,jojy,"We currently dont have enough documentation for the containerizer component. This task adds documentation for containerizer launch sequence.
The mail goals are:
- Have diagrams (state, sequence, class etc) depicting the containerizer launch process.
- Make the documentation newbie friendly.
- Usable for future design discussions.",Documentation,Minor,jojy,2015-12-18T01:38:56.000+0000,5,Resolved,Complete,Document containerizer launch ,2015-12-21T18:33:54.000+0000,MESOS-3032,3.0,mesos,Mesosphere Sprint 21
,2015-07-09T18:54:46.000+0000,jvanremoortere,"{code}
[ RUN      ] ProcessTest.Cache
../../../3rdparty/libprocess/src/tests/process_tests.cpp:1726: Failure
Value of: response.get().status
  Actual: ""200 OK""
Expected: ""304 Not Modified""
[  FAILED  ] ProcessTest.Cache (1 ms)
{code}
The tests then finish running, but the gtest framework fails to terminate and uses 100% CPU.",Bug,Minor,jvanremoortere,,10020,Accepted,In Progress,ProcessTest.Cache fails and hangs,2015-11-23T09:59:39.000+0000,MESOS-3026,5.0,mesos,
bmahler,2015-07-09T17:58:56.000+0000,bmahler,"In the process of fixing MESOS-2940, we accidentally introduced a non-backwards compatible change:

--> StatusUpdate.uuid was required in 0.22.x and was always set.
--> StatusUpdate.uuid is optional in 0.23.x and the master is not setting it for master-generated updates.

In 0.22.x, the scheduler driver ignores the 'uuid' for master/driver generated updates already. I'd suggest the following fix:

# In 0.23.x, rather than not setting StatusUpdate.uuid, set it to an empty string.
# In 0.23.x, ensure the scheduler driver also ignores empty StatusUpdate.uuids.
# In 0.24.x, stop setting StatusUpdate.uuid.",Bug,Blocker,bmahler,2015-07-09T22:03:07.000+0000,5,Resolved,Complete,0.22.x scheduler driver drops 0.23.x reconciliation status updates due to missing StatusUpdate.uuid.,2015-07-09T22:03:07.000+0000,MESOS-3025,3.0,mesos,Twitter Mesos Q3 Sprint 1
tillt,2015-07-09T06:45:48.000+0000,adam-mesos,"If I set `--credentials` on the master, framework and slave authentication are allowed, but not required. On the other hand, http authentication is now required for authenticated endpoints (currently only `/shutdown`). That means that I cannot enable framework or slave authentication without also enabling http endpoint authentication. This is undesirable.

Framework and slave authentication have separate flags (`\--authenticate` and `\--authenticate_slaves`) to require authentication for each. It would be great if there was also such a flag for http authentication. Or maybe we get rid of these flags altogether and rely on ACLs to determine which unauthenticated principals are even allowed to authenticate for each endpoint/action.",Bug,Major,adam-mesos,2016-01-08T03:26:06.000+0000,5,Resolved,Complete,HTTP endpoint authN is enabled merely by specifying --credentials,2016-02-23T06:08:41.000+0000,MESOS-3024,8.0,mesos,Mesosphere Sprint 21
klaus1982,2015-07-09T05:57:19.000+0000,hartem,"fetcher_test.cpp uses the following code for generating URLs:

string url = ""http://"" + net::getHostname(process.self().address.ip).get() + "":"" + stringify(process.self().address.port) + ""/"" + process.self().id

it would be good to isolate that code in a function, and replace the code above with something like:

string url = ""http://"" + endpoint_url(process, ""uri_test"");
",Task,Minor,hartem,2015-07-30T13:13:35.000+0000,5,Resolved,Complete,Factoring out the pattern for URL generation ,2015-08-03T08:24:26.000+0000,MESOS-3023,1.0,mesos,
tnachen,2015-07-08T22:33:25.000+0000,chenlily,"Create a comprehensive store to look up an image and tag's associated image layer ID. Implement add, remove, save, and update images and their associated tags.",Improvement,Major,chenlily,2015-09-25T16:34:26.000+0000,5,Resolved,Complete,Implement Docker Image Provisioner Reference Store,2015-09-25T16:34:26.000+0000,MESOS-3021,3.0,mesos,Mesosphere Sprint 14
pbrett,2015-07-08T22:03:02.000+0000,pbrett,"Stout version class does not expose version components, preventing computations manipulation of version information.  Solution is to make major, minor and patch public.",Improvement,Major,pbrett,2015-07-09T22:19:05.000+0000,5,Resolved,Complete,"Expose major, minor and patch components from stout Version  ",2015-07-09T22:35:56.000+0000,MESOS-3020,1.0,mesos,Twitter Mesos Q3 Sprint 1
karya,2015-07-08T02:06:33.000+0000,karya,A slave module should be able to send a message to a master module and vice-versa to allow out-of-band communication between master/slave modules.,Task,Major,karya,,10020,Accepted,In Progress,A mechanism for messages between Master modules and Slave modules,2015-07-10T15:21:13.000+0000,MESOS-3018,8.0,mesos,
karya,2015-07-08T02:03:06.000+0000,karya,The task termination hooks are needed for doing task-specific cleanup in Master/Slave.,Task,Major,karya,,10020,Accepted,In Progress,Add task status update hooks for Master/Slave,2015-07-21T18:54:31.000+0000,MESOS-3016,3.0,mesos,
nnielsen,2015-07-08T01:55:20.000+0000,karya,"The hook will be triggered on slave exits. A master hook module can use this to do Slave-specific cleanups.

In our particular use case, the hook would trigger cleanup of IPs assigned to the given Slave (see the [design doc | https://docs.google.com/document/d/17mXtAmdAXcNBwp_JfrxmZcQrs7EO6ancSbejrqjLQ0g/edit#]).",Task,Major,karya,2015-09-23T22:39:09.000+0000,5,Resolved,Complete,Add hooks for Slave exits,2015-09-25T22:46:28.000+0000,MESOS-3015,2.0,mesos,Mesosphere Sprint 15
karya,2015-07-08T01:37:25.000+0000,karya,"As per the [design doc|https://docs.google.com/document/d/17mXtAmdAXcNBwp_JfrxmZcQrs7EO6ancSbejrqjLQ0g], we need to enable frameworks to specify network requirements. The proposed message could be along the lines of:

{code}
/**
 * Collection of network request.
 * TODO(kapil): Add a high-level explanation/motivation.
 */
message NetworkInfo {
  // Specify IPAddress requirement.
  enum Protocol {
    IPv4 = 0,
    IPv6 = 1
  }

  // TODO: Document how to use this field to request an
  // 1) IPv4 address
  // 2) IPv6 address
  // 3) Any of the above
  optional Protocol protocol = 1;

  // Statically assigned IPs provided by the Framework.
  optional string ip_address = 2;

  // A group is the name given to a set of logically-related IPs that are
  // allowed to communicate within themselves. For example, one might want 
  // to create separate groups for dev, testing, qa and prod deployment 
  // environments.  
  repeated string groups = 3;

  // To tag certain metadata to be used by Isolator/IPAM. E.g., rack, pop, etc.
  optional Labels labels = 4;
};

message ContainerInfo {
 …
 repeated NetworkInfo network_infos;
…
};

message ContainerStatus {
   repeated NetworkInfo network_infos;
}

message TaskStatus {
 …
 // TODO: Comment on the fact that this is resolved during container setup.
 optional ContainerStatus container;
…
};
{code}",Task,Major,karya,2015-09-17T01:27:23.000+0000,5,Resolved,Complete,"Extend ContainerInfo to include ""NetworkInfo"" message",2015-09-17T01:27:23.000+0000,MESOS-3013,2.0,mesos,
bmahler,2015-07-08T01:07:06.000+0000,bmahler,"See the thread here:
http://markmail.org/thread/wvapc7vkbv7z6gbx

The scheduler driver currently sends framework messages directly to the slave, when possible:

{noformat}
                  (through master)
    Scheduler  —————> Master  —————>  Slave ————>  Executor
     Driver    ————————————————————>                Driver
                   (skip master)
{noformat}

The slave always sends messages directly to the scheduler driver:
{noformat}
    Scheduler         Master          Slave <————  Executor
     Driver    <————————————————————                Driver
                   (skip master)
{noformat}

In order for the scheduler driver to receive Events from the master, it needs enough information to continue directly sending messages to slaves. This was previously accomplished by sending the slave's pid inside the [offer message|https://github.com/apache/mesos/blob/0.23.0-rc1/src/messages/messages.proto#L168]:

{code}
message ResourceOffersMessage {
  repeated Offer offers = 1;
  repeated string pids = 2;
}
{code}

We could add an 'Address' to the Offer protobuf to provide the scheduler driver with the same information:

{code}
message Address {
  required string ip;
  required string hostname;
  required uint32_t port;

  // All HTTP requests to this address must begin with this prefix.
  required string path_prefix;
}

message Offer {
  required OfferID id = 1;
  required FrameworkID framework_id = 2;
  required SlaveID slave_id = 3;
  required string hostname = 4;   // Deprecated in favor of 'address'.
  optional Address address = 8;  // Obviates 'hostname'.
  ...
}
{code}

The path prefix is required for testing purposes, where we can have multiple slaves within a process (e.g. {{localhost:5051/slave(1)/state.json}} vs. {{localhost:5051/slave(2)/state.json}}).

This provides enough information to allow the scheduler driver to continue to directly send messages to the slaves, which unblocks MESOS-2910.",Task,Major,bmahler,2015-07-17T21:05:48.000+0000,5,Resolved,Complete,Support existing message passing optimization with Event/Call.,2015-07-17T21:05:48.000+0000,MESOS-3012,1.0,mesos,Twitter Mesos Q3 Sprint 1
jvanremoortere,2015-07-07T21:55:38.000+0000,hartem,"It has been noticed before that systemd reorganizes cgroup hierarchy created by mesos slave. Because of this mesos is no longer able to find the cgroup, and there is also a chance of undoing the isolation that mesos slave puts in place. ",Task,Major,hartem,2015-07-20T17:56:32.000+0000,5,Resolved,Complete,Reproduce systemd cgroup behavior ,2015-07-20T17:56:32.000+0000,MESOS-3009,5.0,mesos,Mesosphere Sprint 14
jvanremoortere,2015-07-07T20:40:54.000+0000,jvanremoortere,"we currently disable to epoll in libevent to allow SSL to work.
It would be more scalable if we didn't have to do that.",Improvement,Major,jvanremoortere,,10020,Accepted,In Progress,Libevent SSL doesn't use EPOLL,2015-07-10T15:19:45.000+0000,MESOS-3008,8.0,mesos,
,2015-07-07T20:03:48.000+0000,jojy,"cgroups API current does expose ""stats"" from the memory namespace. Having this API would enable isolators to use its various fields(eg. rss, rss_huge, writeback etc) in use cases like usage metrics.",Task,Major,jojy,,10020,Accepted,In Progress,Add cgroups memory stats API,2016-01-07T17:08:25.000+0000,MESOS-3006,2.0,mesos,
jvanremoortere,2015-07-07T19:38:03.000+0000,jvanremoortere,"Depending on how /etc/hosts is configured, the SSL tests can fail with a bad hostname match for the certificate.
We can avoid this by explicitly matching the hostname for the certificate to the IP that will be used during the test.",Bug,Blocker,jvanremoortere,2015-07-07T22:09:46.000+0000,5,Resolved,Complete,SSL tests can fail depending on hostname configuration,2015-07-07T22:58:31.000+0000,MESOS-3005,3.0,mesos,Mesosphere Sprint 14
tnachen,2015-07-07T19:24:27.000+0000,tnachen,"Mesos Containerizer uses the command executor to actually launch the user defined command, and the command executor then can communicate with the slave about the process lifecycle.
When we provision a new container with the user specified image, we also need to be able to run the command executor in the container to support the same semantics.
One approach is to dynamically mount in a static binary of the command executor with all its dependencies in a special directory so it doesn't interfere with the provisioned root filesystem and configure the mesos containerizer to run the command executor in that directory.",Improvement,Major,tnachen,2015-08-07T01:40:02.000+0000,5,Resolved,Complete,Design support running the command executor with provisioned image for running a task in a container,2015-08-08T00:16:07.000+0000,MESOS-3004,5.0,mesos,Mesosphere Sprint 15
jvanremoortere,2015-07-07T18:44:29.000+0000,pbrett,"Change to Option from get() to getOrElse() breaks network isolator.  Building with '../configure --with-network-isolator' generates the following error:

../../src/slave/containerizer/isolators/network/port_mapping.cpp: In static member function 'static Try<mesos::slave::Isolator*> mesos::internal::slave::PortMappingIsolatorProcess::create(const mesos::internal::slave::Flags&)':
../../src/slave/containerizer/isolators/network/port_mapping.cpp:1103:29: error: no matching function for call to 'Option<std::basic_string<char> >::get(const char [1]) const'
       flags.resources.get(""""),
                             ^
../../src/slave/containerizer/isolators/network/port_mapping.cpp:1103:29: note: candidates are:
In file included from ../../3rdparty/libprocess/3rdparty/stout/include/stout/check.hpp:26:0,
                 from ../../3rdparty/libprocess/include/process/check.hpp:19,
                 from ../../3rdparty/libprocess/include/process/collect.hpp:7,
                 from ../../src/slave/containerizer/isolators/network/port_mapping.cpp:30:
../../3rdparty/libprocess/3rdparty/stout/include/stout/option.hpp:130:12: note: const T& Option<T>::get() const [with T = std::basic_string<char>]
   const T& get() const { assert(isSome()); return t; }
            ^
../../3rdparty/libprocess/3rdparty/stout/include/stout/option.hpp:130:12: note:   candidate expects 0 arguments, 1 provided
../../3rdparty/libprocess/3rdparty/stout/include/stout/option.hpp:131:6: note: T& Option<T>::get() [with T = std::basic_string<char>]
   T& get() { assert(isSome()); return t; }
      ^
../../3rdparty/libprocess/3rdparty/stout/include/stout/option.hpp:131:6: note:   candidate expects 0 arguments, 1 provided
make[2]: *** [slave/containerizer/isolators/network/libmesos_no_3rdparty_la-port_mapping.lo] Error 1
make[2]: Leaving directory `/home/pbrett/sandbox/mesos.master/build/src'
make[1]: *** [check] Error 2
make[1]: Leaving directory `/home/pbrett/sandbox/mesos.master/build/src'
make: *** [check-recursive] Error 1
",Bug,Blocker,pbrett,2015-07-07T21:52:08.000+0000,5,Resolved,Complete,Rename Option<T>::get(const T& _t) to getOrElse() broke network isolator,2015-07-07T21:56:28.000+0000,MESOS-3002,1.0,mesos,Mesosphere Sprint 14
anandmazumdar,2015-07-07T18:20:38.000+0000,marco-mesos,"We want to create a simple ""demo"" HTTP API Client (in Java, Python or Go) that can serve as an ""example framework"" for people who will want to use the new API for their Frameworks.

The scope should be fairly limited (eg, launching a simple Container task?) but sufficient to exercise most of the new API endpoint messages/capabilities.

Scope: TBD

Non-Goals: 

- create a ""best-of-breed"" Framework to deliver any specific functionality;
- create an Integration Test for the HTTP API.",Bug,Major,marco-mesos,2015-08-14T23:34:17.000+0000,5,Resolved,Complete,"Create a ""demo"" HTTP API client",2015-08-14T23:34:17.000+0000,MESOS-3001,8.0,mesos,
jvanremoortere,2015-07-07T02:43:00.000+0000,jvanremoortere,"{code}
[ RUN      ] SSLTest.BasicSameProcess
F0706 18:32:28.465451 238583808 libevent_ssl_socket.cpp:507] Check failed: 'self->bev' Must be non NULL
{code}",Bug,Blocker,jvanremoortere,2015-07-07T20:30:36.000+0000,5,Resolved,Complete,SSL connection failure causes failed CHECK.,2015-07-08T01:31:42.000+0000,MESOS-2997,3.0,mesos,Mesosphere Sprint 14
hausdorff,2015-07-06T17:58:53.000+0000,kaysoky,"As per the discussion in MESOS-2965, the use of the Path object should be standardized:
* Functions which effectively use Paths (as strings) should instead take Paths.
* Functions which modify and return Paths (as strings) should instead return Paths.
* Extraneous uses of Path.value should be removed.",Improvement,Minor,kaysoky,,10020,Accepted,In Progress,Standardize use of Path ,2015-10-22T07:43:40.000+0000,MESOS-2995,3.0,mesos,
pbrett,2015-07-06T17:52:47.000+0000,pbrett,Document new network isolation capabilities in 0.23,Bug,Major,pbrett,2015-07-08T17:52:38.000+0000,5,Resolved,Complete,Document  per container unique egress flow and network queueing statistics,2015-07-08T17:52:54.000+0000,MESOS-2993,3.0,mesos,Twitter Mesos Q3 Sprint 1
mcypark,2015-07-06T13:18:24.000+0000,alex-mesos,"Compiling 0.23.0 (rc1) produces compilation errors on Mac OS 10.10.4 with {{g++}} based on LLVM 3.5. It looks like the issue was introduced in {{a5640ad813e6256b548fca068f04fd9fa3a03eda}}, https://reviews.apache.org/r/32838. In contrast to the commit message, compiling the rc with gcc4.4 on CentOS worked fine for me. 

According to 0.23 release notes and MESOS-2604, we should support clang 3.5. 

{code}
../../../../../3rdparty/libprocess/3rdparty/stout/tests/os_tests.cpp:543:25: error: conversion from 'void ()' to 'const Option<void (*)()>' is ambiguous
                   Fork(dosetsid,          // Great-great-granchild.
                        ^~~~~~~~
../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/option.hpp:40:3: note: candidate constructor
  Option(const T& _t) : state(SOME), t(_t) {}
  ^
../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/option.hpp:42:3: note: candidate constructor
  Option(T&& _t) : state(SOME), t(std::move(_t)) {}
  ^
../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/option.hpp:45:3: note: candidate constructor [with U = void ()]
  Option(const U& u) : state(SOME), t(u) {}
  ^
{code}

Compiler version:
{code}
$ g++ --version
Configured with: --prefix=/Applications/Xcode.app/Contents/Developer/usr --with-gxx-include-dir=/usr/include/c++/4.2.1
Apple LLVM version 6.0 (clang-600.0.54) (based on LLVM 3.5svn)
Target: x86_64-apple-darwin14.4.0
Thread model: posix
{code}
",Bug,Major,alexr,2015-07-07T14:06:43.000+0000,5,Resolved,Complete,Compilation Error on Mac OS 10.10.4 with clang 3.5.0,2015-10-14T10:00:45.000+0000,MESOS-2991,1.0,mesos,Mesosphere Sprint 14
haosdent@gmail.com,2015-07-02T08:56:54.000+0000,ijimenez,"We currently use docker version to get Docker version, in Docker master branch and soon in Docker 1.8 [1] the output for this command changes. The solution for now will be to use the unchanged docker --version output, in the long term we should consider stop using the cli and use the API instead. 

[1] https://github.com/docker/docker/pull/14047",Bug,Major,ijimenez,2015-07-06T18:58:41.000+0000,5,Resolved,Complete,Docker version output is not compatible with Mesos,2015-09-25T00:17:16.000+0000,MESOS-2986,1.0,mesos,Mesosphere Sprint 13
ijimenez,2015-07-02T07:52:47.000+0000,ijimenez,Remove the '.json' extension on endpoints such as `/files/browse.json` so it become `/files/browse`,Improvement,Major,ijimenez,2015-09-14T05:33:52.000+0000,5,Resolved,Complete,Deprecating '.json' extension in files endpoints url,2016-02-27T01:03:09.000+0000,MESOS-2984,1.0,mesos,Mesosphere Sprint 18
ijimenez,2015-07-02T07:48:26.000+0000,ijimenez,Remove the '.json' extension on endpoints such as `/slave/state.json` so it become `/slave/state`,Improvement,Major,ijimenez,2015-09-12T09:23:30.000+0000,5,Resolved,Complete,Deprecating '.json' extension in slave endpoints url,2016-02-27T01:03:08.000+0000,MESOS-2983,1.0,mesos,Mesosphere Sprint 18
gilbert,2015-07-01T18:59:58.000+0000,tnachen,"Image specs also includes execution configuration (e.g: Env, user, ports, etc).
We should support passing those information from the image provisioner back to the containerizer.",Improvement,Major,tnachen,2015-12-18T23:05:20.000+0000,5,Resolved,Complete,Allow runtime configuration to be returned from provisioner,2015-12-18T23:05:20.000+0000,MESOS-2980,5.0,mesos,Mesosphere Sprint 24
,2015-07-01T02:17:34.000+0000,jvanremoortere,"Stout flags don't remember their default values, and so can't have their defaults reset. This makes it hard to reset flags to their defaults between tests.",Bug,Major,jvanremoortere,,1,Open,New,stout flags can't have their defaults reset,2016-02-26T23:47:19.000+0000,MESOS-2974,5.0,mesos,
jvanremoortere,2015-07-01T01:51:24.000+0000,jvanremoortere,"commit bfa89f22e9d6a3f365113b32ee1cac5208a0456f
Author: Joris Van Remoortere <joris.van.remoortere@gmail.com>
Date:   Wed Jul 1 16:16:52 2015 -0700

    MESOS-2973: Allow SSL tests to run using gtest_repeat.
    
    The SSL ctx object carried some settings between reinitialize()
    calls. Re-construct the object to avoid this state transition.
    
    Review: https://reviews.apache.org/r/36074",Bug,Major,jvanremoortere,2015-07-24T01:10:13.000+0000,5,Resolved,Complete,SSL tests don't work with --gtest_repeat,2015-07-24T01:10:13.000+0000,MESOS-2973,3.0,mesos,Mesosphere Sprint 13
gilbert,2015-07-01T00:37:32.000+0000,tnachen,"The Docker image specification defines a schema for the metadata json that it puts into each image. Currently the docker image provisioner needs to be able to parse and understand this metadata json, and we should create a protobuf equivelent schema so we can utilize the json to protobuf conversion to read and validate the metadata.",Improvement,Major,tnachen,2015-10-16T12:17:36.000+0000,5,Resolved,Complete,Serialize Docker image spec as protobuf,2015-11-23T19:36:26.000+0000,MESOS-2972,3.0,mesos,Mesosphere Sprint 20
lins05,2015-07-01T00:36:36.000+0000,tnachen,"Part of the image provisioning process is to call a backend to create a root filesystem based on the image on disk layout.
The problem with the copy backend is that it's both waste of IO and space, and bind only can deal with one layer.
Overlayfs backend allows us to utilize the filesystem to merge multiple filesystems into one efficiently.",Improvement,Major,tnachen,2016-02-29T01:01:23.000+0000,5,Resolved,Complete,Implement OverlayFS based provisioner backend,2016-02-29T01:01:23.000+0000,MESOS-2971,5.0,mesos,Mesosphere Sprint 29
tnachen,2015-07-01T00:27:48.000+0000,tnachen,"Currently Appc and Docker both implemented its own copy backend, but most of the logic is the same where the input is just a image name with its dependencies.
We can refactor both so that we just have one implementation that is shared between both provisioners, so appc and docker can reuse the shared copy backend.",Improvement,Major,tnachen,2015-09-03T01:31:56.000+0000,5,Resolved,Complete,Implement shared copy based provisioner backend,2015-09-03T01:31:56.000+0000,MESOS-2968,3.0,mesos,Mesosphere Sprint 17
kaysoky,2015-06-30T21:18:33.000+0000,hartem,Convert existing comments to doxygen format.  ,Improvement,Major,hartem,2015-07-06T21:31:10.000+0000,5,Resolved,Complete,Missing doxygen documentation for libprocess socket interface ,2015-07-06T21:31:10.000+0000,MESOS-2967,5.0,mesos,Mesosphere Sprint 14
jvanremoortere,2015-06-30T21:09:56.000+0000,hartem,libevent SSL currently uses a secondary FD so we need to virtualize the get() function on socket interface. ,Improvement,Major,hartem,2015-07-02T22:11:55.000+0000,5,Resolved,Complete,socket::peer() and socket::address() might fail with SSL enabled,2015-07-02T22:11:55.000+0000,MESOS-2966,5.0,mesos,Mesosphere Sprint 13
kaysoky,2015-06-30T21:04:09.000+0000,hartem,"For example:

{code}inline Try<Nothing> rm(const std::string& path){code} does not have an overload for {code}inline Try<Nothing> rm(const Path& path){code}

The implementation should be something like: 
{code}
inline Try<Nothing> rm(const Path& path)
{
  rm(path.value);
}
{code}",Improvement,Minor,hartem,2015-07-06T02:40:19.000+0000,5,Resolved,Complete,Add implicit cast to string operator to Path.,2015-07-06T16:51:10.000+0000,MESOS-2965,2.0,mesos,Mesosphere Sprint 13
hartem,2015-06-30T20:55:36.000+0000,hartem,"Finally, I so wish we could just do:

{code}
io::peek(request->socket, 6)
  .then([request](const string& data) {
    // Comment about the rules ...
    if (data.length() < 2) { // Rule 1
    
    } else if (...) { // Rule 2.
    
    } else if (...) { // Rule 3.
    
    }
    
    if (ssl) {
      accept_SSL_callback(request);
    } else {
      ...;
    }
  });
{code}

from:
https://reviews.apache.org/r/31207/",Improvement,Minor,hartem,2015-07-17T20:45:49.000+0000,5,Resolved,Complete,libprocess io does not support peek(),2015-08-28T04:51:13.000+0000,MESOS-2964,3.0,mesos,Mesosphere Sprint 15
marco-mesos,2015-06-30T18:36:49.000+0000,marco-mesos,"If the DNS cannot resolve the hostname-to-IP for a slave node, we correctly return an {{Error}} object, but we then fail with a segfault.

This code adds a more user-friendly message and exits normally (with an {{EXIT_FAILURE}} code).

For example, forcing {{net::getIp()}} to always return an {{Error}}, now causes the slave to exit like this:

{noformat}
$ ./bin/mesos-slave.sh --master=10.10.1.121:5405
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0630 11:31:45.777465 1944417024 process.cpp:899] Could not obtain the IP address for stratos.local; the DNS service may not be able to resolve it: >>> Marco was here!!!

$ echo $?
1
{noformat}",Bug,Major,marco-mesos,2015-07-21T20:00:03.000+0000,5,Resolved,Complete,Slave fails with Abort stacktrace when DNS cannot resolve hostname,2015-07-21T20:00:03.000+0000,MESOS-2962,1.0,mesos,Mesosphere Sprint 13
jojy,2015-06-30T17:14:10.000+0000,jojy,"Current cgroups implementation does not have a cpuacct subsystem implementation. This subsystem reports important metrics like user and system CPU ticks spent by a process. ""cgroups"" namespace has subsystem specific utilities for ""cpu"", ""memory"" etc. It could use other subsystems specific utils (eg. cpuacct).

In the future, we could also view cgroups as a mesos-subsystem with  features like event notifications.

Although refactoring cgroups would be a different epic, listing the possible tasks:
  -  Have hierarchies, subsystems abstracted to represent the domain 
  - Create  ""cgroups service""
  -  ""cgroups service"" listen to update events from the OS on files like stats. This would be an interrupt based system(maybe use linux fsnotify)
  - ""cgroups service"" services events to mesos (containers for example).

",Task,Major,jojy,2015-07-15T01:00:24.000+0000,5,Resolved,Complete,Add cpuacct subsystem utils to cgroups,2015-08-15T00:26:02.000+0000,MESOS-2961,2.0,mesos,Mesosphere Sprint 13
vinodkone,2015-06-29T20:07:37.000+0000,vinodkone,It is better for FrameworkInfo to be only included in 'Subscribe' message (that needs to be added) instead of for every call. Instead the top level Call should contain a FrameworkID to identify the framework making the call.,Improvement,Major,vinodkone,2015-07-13T18:41:12.000+0000,5,Resolved,Complete,Update Call protobuf to move top level FrameworkInfo inside Subscribe,2015-07-13T18:41:12.000+0000,MESOS-2958,3.0,mesos,Twitter Mesos Q2 Sprint 6
marco-mesos,2015-06-29T20:05:40.000+0000,vinodkone,This will help schedulers figure out the version of the master that they are interacting with. See MESOS-2736 for additional context.,Improvement,Major,vinodkone,2015-07-06T18:58:12.000+0000,5,Resolved,Complete,Add version to MasterInfo,2015-07-06T18:58:12.000+0000,MESOS-2957,1.0,mesos,Mesosphere Sprint 13
pbrett,2015-06-29T18:47:47.000+0000,pbrett,"PerfEventIsolatorTest fails with stack trace when run in Linux VM

[----------] 1 test from PerfEventIsolatorTest
[ RUN      ] PerfEventIsolatorTest.ROOT_CGROUPS_Sample
F0629 11:38:17.088412 14114 isolator_tests.cpp:837] CHECK_SOME(isolator): Failed to create PerfEvent isolator, invalid events: { cycles, task-clock } 
*** Check failure stack trace: ***
    @     0x2ab5e5aeeb1a  google::LogMessage::Fail()
    @     0x2ab5e5aeea66  google::LogMessage::SendToLog()
    @     0x2ab5e5aee468  google::LogMessage::Flush()
    @     0x2ab5e5af137c  google::LogMessageFatal::~LogMessageFatal()
    @           0x864b0c  _CheckFatal::~_CheckFatal()
    @           0xc458ed  mesos::internal::tests::PerfEventIsolatorTest_ROOT_CGROUPS_Sample_Test::TestBody()
    @          0x119fb17  testing::internal::HandleSehExceptionsInMethodIfSupported<>()
    @          0x119ac9e  testing::internal::HandleExceptionsInMethodIfSupported<>()
    @          0x118305f  testing::Test::Run()
    @          0x1183782  testing::TestInfo::Run()
    @          0x1183d0a  testing::TestCase::Run()
    @          0x11889d4  testing::internal::UnitTestImpl::RunAllTests()
    @          0x11a09ae  testing::internal::HandleSehExceptionsInMethodIfSupported<>()
    @          0x119b9c3  testing::internal::HandleExceptionsInMethodIfSupported<>()
    @          0x11878e0  testing::UnitTest::Run()
    @           0xcdc8c7  main
    @     0x2ab5e7fdbec5  (unknown)
    @           0x861a89  (unknown)
make[3]: *** [check-local] Aborted (core dumped)

[ RUN      ] UserCgroupIsolatorTest/2.ROOT_CGROUPS_UserCgroup
F0629 11:49:38.763434 18836 isolator_tests.cpp:1200] CHECK_SOME(isolator): Failed to create PerfEvent isolator, invalid events: { cpu-cycles } 
*** Check failure stack trace: ***
    @     0x2ba40eb2db1a  google::LogMessage::Fail()
    @     0x2ba40eb2da66  google::LogMessage::SendToLog()
    @     0x2ba40eb2d468  google::LogMessage::Flush()
    @     0x2ba40eb3037c  google::LogMessageFatal::~LogMessageFatal()
    @           0x864b0c  _CheckFatal::~_CheckFatal()
    @           0xc5ddb1  mesos::internal::tests::UserCgroupIsolatorTest_ROOT_CGROUPS_UserCgroup_Test<>::TestBody()
    @          0x119fc43  testing::internal::HandleSehExceptionsInMethodIfSupported<>()
    @          0x119adca  testing::internal::HandleExceptionsInMethodIfSupported<>()
    @          0x118318b  testing::Test::Run()
    @          0x11838ae  testing::TestInfo::Run()
    @          0x1183e36  testing::TestCase::Run()
    @          0x1188b00  testing::internal::UnitTestImpl::RunAllTests()
    @          0x11a0ada  testing::internal::HandleSehExceptionsInMethodIfSupported<>()
    @          0x119baef  testing::internal::HandleExceptionsInMethodIfSupported<>()
    @          0x1187a0c  testing::UnitTest::Run()
    @           0xcdc9f3  main
    @     0x2ba41101aec5  (unknown)
    @           0x861a89  (unknown)
make[3]: *** [check-local] Aborted (core dumped)

",Bug,Major,pbrett,2015-07-06T18:29:30.000+0000,5,Resolved,Complete,Stack trace in isolator tests on Linux VM,2015-07-06T18:29:30.000+0000,MESOS-2956,1.0,mesos,Twitter Mesos Q2 Sprint 6
jojy,2015-06-26T18:22:14.000+0000,jojy,"docker containerizer currently collects usage statistics by calling os's process statistics (eg ps ). There is scope for making this efficient, say by querying cgroups file system.

",Improvement,Major,jojy,2015-07-15T00:59:34.000+0000,5,Resolved,Complete,Inefficient container usage collection,2015-08-15T00:26:03.000+0000,MESOS-2951,3.0,mesos,Mesosphere Sprint 13
arojas,2015-06-26T14:27:09.000+0000,arojas,"In order to maintain compatibility with existent versions of Mesos, as well as to prove the flexibility of the generalized {{mesos::Authorizer}} design, the current authorization mechanism through ACL definitions needs to run under the updated interface without any changes being noticeable by the current authorization users.",Task,Major,arojas,2016-03-11T21:53:37.000+0000,5,Resolved,Complete,Implement current mesos Authorizer in terms of generalized Authorizer interface,2016-03-11T21:53:38.000+0000,MESOS-2950,8.0,mesos,Mesosphere Sprint 30
arojas,2015-06-26T14:24:04.000+0000,arojas,"As mentioned in MESOS-2948 the current {{mesos::Authorizer}} interface is rather inflexible if new _Actions_ or _Objects_ need to be added.

A new API needs to be designed in a way that allows for arbitrary _Actions_ and _Objects_ to be added to the authorization mechanism without having to recompile mesos.",Task,Major,arojas,2015-11-09T09:35:21.000+0000,5,Resolved,Complete,Draft design for generalized Authorizer interface,2015-11-09T09:42:41.000+0000,MESOS-2949,3.0,mesos,Mesosphere Sprint 15
arojas,2015-06-26T13:00:39.000+0000,tillt,"h4.Motivation
Provide an example authorizer module based on the {{LocalAuthorizer}} implementation. Make sure that such authorizer module can be fully unit- and integration- tested within the mesos test suite.
",Improvement,Major,tillt,2015-08-13T23:48:34.000+0000,5,Resolved,Complete,"Authorizer Module: Implementation, Integration & Tests",2015-08-15T00:26:03.000+0000,MESOS-2947,8.0,mesos,Mesosphere Sprint 15
arojas,2015-06-26T12:55:03.000+0000,tillt,"h4.Motivation
Design an interface covering authorizer modules while staying minimally invasive in regards to changes to the existing {{LocalAuthorizer}} implementation.
",Improvement,Major,tillt,2015-08-13T23:49:37.000+0000,5,Resolved,Complete,Authorizer Module: Interface design,2015-08-15T00:26:02.000+0000,MESOS-2946,2.0,mesos,Mesosphere Sprint 15
js84,2015-06-26T12:18:18.000+0000,js84,"In docker_containerizer_test we have the following pattern.

{code}
    EXPECT_NE(0u, offers.get().size());

    const Offer& offer = offers.get()[0];
{code}

As we rely on the value afterwards we should use ASSERT_NE instead. In that case the test will fail immediately. ",Bug,Minor,js84,,5,Resolved,Complete,Use of EXPECT in test and relying on the checked condition afterwards.,2015-06-29T08:33:15.000+0000,MESOS-2944,1.0,mesos,
jvanremoortere,2015-06-26T03:24:22.000+0000,hartem,"../configure --enable-debug --enable-libevent --enable-ssl && make

produces the following error:

poll.cpp' || echo '../../../3rdparty/libprocess/'`src/libevent_poll.cpp
libtool: compile:  g++ -DPACKAGE_NAME=\""libprocess\"" -DPACKAGE_TARNAME=\""libprocess\"" -DPACKAGE_VERSION=\""0.0.1\"" ""-DPACKAGE_STRING=\""libprocess 0.0.1\"""" -DPACKAGE_BUGREPORT=\""\"" -DPACKAGE_URL=\""\"" -DPACKAGE=\""libprocess\"" -DVERSION=\""0.0.1\"" -DSTDC_HEADERS=1 -DHAVE_SYS_TYPES_H=1 -DHAVE_SYS_STAT_H=1 -DHAVE_STDLIB_H=1 -DHAVE_STRING_H=1 -DHAVE_MEMORY_H=1 -DHAVE_STRINGS_H=1 -DHAVE_INTTYPES_H=1 -DHAVE_STDINT_H=1 -DHAVE_UNISTD_H=1 -DHAVE_DLFCN_H=1 -DLT_OBJDIR=\"".libs/\"" -DHAVE_APR_POOLS_H=1 -DHAVE_LIBAPR_1=1 -DHAVE_SVN_VERSION_H=1 -DHAVE_LIBSVN_SUBR_1=1 -DHAVE_SVN_DELTA_H=1 -DHAVE_LIBSVN_DELTA_1=1 -DHAVE_LIBCURL=1 -DHAVE_EVENT2_EVENT_H=1 -DHAVE_LIBEVENT=1 -DHAVE_EVENT2_THREAD_H=1 -DHAVE_LIBEVENT_PTHREADS=1 -DHAVE_OPENSSL_SSL_H=1 -DHAVE_LIBSSL=1 -DHAVE_LIBCRYPTO=1 -DHAVE_EVENT2_BUFFEREVENT_SSL_H=1 -DHAVE_LIBEVENT_OPENSSL=1 -DUSE_SSL_SOCKET=1 -DHAVE_PTHREAD_PRIO_INHERIT=1 -DHAVE_PTHREAD=1 -DHAVE_LIBZ=1 -DHAVE_LIBDL=1 -I. -I../../../3rdparty/libprocess -I../../../3rdparty/libprocess/include -I../../../3rdparty/libprocess/3rdparty/stout/include -I3rdparty/boost-1.53.0 -I3rdparty/libev-4.15 -I3rdparty/picojson-4f93734 -I3rdparty/glog-0.3.3/src -I3rdparty/ry-http-parser-1c3624a -I/usr/local/opt/openssl/include -I/usr/local/opt/libevent/include -I/usr/local/opt/subversion/include/subversion-1 -I/usr/include/apr-1 -I/usr/include/apr-1.0 -g1 -O0 -std=c++11 -stdlib=libc++ -DGTEST_USE_OWN_TR1_TUPLE=1 -MT libprocess_la-libevent_poll.lo -MD -MP -MF .deps/libprocess_la-libevent_poll.Tpo -c ../../../3rdparty/libprocess/src/libevent_poll.cpp  -fno-common -DPIC -o libprocess_la-libevent_poll.o
mv -f .deps/libprocess_la-socket.Tpo .deps/libprocess_la-socket.Plo
mv -f .deps/libprocess_la-subprocess.Tpo .deps/libprocess_la-subprocess.Plo
mv -f .deps/libprocess_la-libevent.Tpo .deps/libprocess_la-libevent.Plo
mv -f .deps/libprocess_la-metrics.Tpo .deps/libprocess_la-metrics.Plo
In file included from ../../../3rdparty/libprocess/src/libevent_ssl_socket.cpp:11:
In file included from ../../../3rdparty/libprocess/include/process/queue.hpp:9:
../../../3rdparty/libprocess/include/process/future.hpp:849:7: error: no viable conversion from 'const process::Future<const process::Future<process::network::Socket> >' to 'const process::network::Socket'
 set(u);
     ^
../../../3rdparty/libprocess/src/libevent_ssl_socket.cpp:769:10: note: in instantiation of function template specialization 'process::Future<process::network::Socket>::Future<process::Future<const process::Future<process::network::Socket> > >' requested here
 return accept_queue.get()
        ^
../../../3rdparty/libprocess/include/process/socket.hpp:21:7: note: candidate constructor (the implicit move constructor) not viable: no known conversion from 'const process::Future<const process::Future<process::network::Socket> >' to
     'process::network::Socket &&' for 1st argument
class Socket
     ^
../../../3rdparty/libprocess/include/process/socket.hpp:21:7: note: candidate constructor (the implicit copy constructor) not viable: no known conversion from 'const process::Future<const process::Future<process::network::Socket> >' to
     'const process::network::Socket &' for 1st argument
class Socket
     ^
../../../3rdparty/libprocess/include/process/future.hpp:411:21: note: passing argument to parameter '_t' here
 bool set(const T& _t);
                   ^
1 error generated.
make[4]: *** [libprocess_la-libevent_ssl_socket.lo] Error 1
make[4]: *** Waiting for unfinished jobs....
mv -f .deps/libprocess_la-libevent_poll.Tpo .deps/libprocess_la-libevent_poll.Plo
mv -f .deps/libprocess_la-openssl.Tpo .deps/libprocess_la-openssl.Plo
mv -f .deps/libprocess_la-process.Tpo .deps/libprocess_la-process.Plo
make[3]: *** [all-recursive] Error 1
make[2]: *** [all-recursive] Error 1
make[1]: *** [all] Error 2
make: *** [all-recursive] Error 1",Bug,Blocker,hartem,2015-07-07T13:34:28.000+0000,5,Resolved,Complete,mesos fails to compile under mac when libssl and libevent are enabled,2015-07-07T21:56:24.000+0000,MESOS-2943,2.0,mesos,Mesosphere Sprint 14
,2015-06-25T21:36:14.000+0000,bmahler,"Per MESOS-2940, it would be great to have a benchmark for task reconciliation, given large numbers of tasks.

This can guide attempts at improving performance.",Task,Major,bmahler,,10020,Accepted,In Progress,Add a benchmark for task reconciliation.,2015-11-20T02:09:02.000+0000,MESOS-2941,1.0,mesos,Twitter Mesos Q2 Sprint 6
bmahler,2015-06-25T21:33:26.000+0000,bmahler,"We've observed that both implicit and explicit reconciliation are expensive for large numbers of tasks:

{noformat: title=Explicit O(100,000) tasks: 70secs}
I0625 20:55:23.716320 21937 master.cpp:3863] Performing explicit task state reconciliation for N tasks of framework F (NAME) at S@IP:PORT
I0625 20:56:34.812464 21937 master.cpp:5041] Removing task T with resources R of framework F on slave S at slave(1)@IP:PORT (HOST)
{noformat}

{noformat: title=Implicit with O(100,000) tasks: 60secs}
I0625 20:25:22.310601 21936 master.cpp:3802] Performing implicit task state reconciliation for framework F (NAME) at S@IP:PORT
I0625 20:26:23.874528 21921 master.cpp:218] Scheduling shutdown of slave S due to health check timeout
{noformat}

Let's add a benchmark to see if there are any bottlenecks here, and to guide improvements.",Improvement,Critical,bmahler,2015-07-24T01:12:43.000+0000,5,Resolved,Complete,Reconciliation is expensive for large numbers of tasks.,2015-07-24T01:12:43.000+0000,MESOS-2940,3.0,mesos,Twitter Mesos Q2 Sprint 6
marco-mesos,2015-06-25T18:23:34.000+0000,marco-mesos,"This is a simple test story to try out the new workflow.

Unfortunately, testing and getting it to work seems to be something that actually does take up time, so I'm tracking this here.",Story,Major,marco-mesos,2015-07-22T07:24:30.000+0000,5,Resolved,Complete,Testing the new workflow,2015-07-22T07:24:30.000+0000,MESOS-2939,3.0,mesos,Mesosphere Sprint 13
jojy,2015-06-25T17:54:54.000+0000,jojy,"On linux,  when a simple task is being executed on docker container executor, the sandbox stderr shows a backtrace:

*** Aborted at 1435254156 (unix time) try ""date -d @1435254156"" if you are using GNU date ***
PC: @     0x7ffff2b1364d (unknown)
*** SIGSEGV (@0xfffffffffffffff8) received by PID 88424 (TID 0x7fffe88fb700) from PID 18446744073709551608; stack trace: ***
    @     0x7ffff25a4340 (unknown)
    @     0x7ffff2b1364d (unknown)
    @     0x7ffff2b724df (unknown)
    @           0x4a6466 Docker::Container::~Container()
    @     0x7ffff5bfa49a Option<>::~Option()
    @     0x7ffff5c15989 Option<>::operator=()
    @     0x7ffff5c09e9f Try<>::operator=()
    @     0x7ffff5c09ee3 Result<>::operator=()
    @     0x7ffff5c0a938 process::Future<>::set()
    @     0x7ffff5bff412 process::Promise<>::set()
    @     0x7ffff5be53e3 Docker::___inspect()
    @     0x7ffff5be3cf8 _ZZN6Docker9__inspectERKSsRKN7process5OwnedINS2_7PromiseINS_9ContainerEEEEERK6OptionI8DurationENS2_6FutureISsEERKNS2_10SubprocessEENKUlRKSG_E1_clESL_
    @     0x7ffff5be91e9 _ZZNK7process6FutureISsE5onAnyIZN6Docker9__inspectERKSsRKNS_5OwnedINS_7PromiseINS3_9ContainerEEEEERK6OptionI8DurationES1_RKNS_10SubprocessEEUlRKS1_E1_vEESM_OT_NS1_6PreferEENUlSM_E_clESM_
    @     0x7ffff5be9d9d _ZNSt17_Function_handlerIFvRKN7process6FutureISsEEEZNKS2_5onAnyIZN6Docker9__inspectERKSsRKNS0_5OwnedINS0_7PromiseINS7_9ContainerEEEEERK6OptionI8DurationES2_RKNS0_10SubprocessEEUlS4_E1_vEES4_OT_NS2_6PreferEEUlS4_E_E9_M_invokeERKSt9_Any_dataS4_
    @     0x7ffff5c1eadd std::function<>::operator()()
    @     0x7ffff5c15e07 process::Future<>::onAny()
    @     0x7ffff5be93a1 _ZNK7process6FutureISsE5onAnyIZN6Docker9__inspectERKSsRKNS_5OwnedINS_7PromiseINS3_9ContainerEEEEERK6OptionI8DurationES1_RKNS_10SubprocessEEUlRKS1_E1_vEESM_OT_NS1_6PreferE
    @     0x7ffff5be87f6 _ZNK7process6FutureISsE5onAnyIZN6Docker9__inspectERKSsRKNS_5OwnedINS_7PromiseINS3_9ContainerEEEEERK6OptionI8DurationES1_RKNS_10SubprocessEEUlRKS1_E1_EESM_OT_
    @     0x7ffff5be459c Docker::__inspect()
    @     0x7ffff5be337c _ZZN6Docker8_inspectERKSsRKN7process5OwnedINS2_7PromiseINS_9ContainerEEEEERK6OptionI8DurationEENKUlvE_clEv
    @     0x7ffff5be8c5a _ZZNK7process6FutureI6OptionIiEE5onAnyIZN6Docker8_inspectERKSsRKNS_5OwnedINS_7PromiseINS5_9ContainerEEEEERKS1_I8DurationEEUlvE_vEERKS3_OT_NS3_10LessPreferEENUlSL_E_clESL_
    @     0x7ffff5be9b36 _ZNSt17_Function_handlerIFvRKN7process6FutureI6OptionIiEEEEZNKS4_5onAnyIZN6Docker8_inspectERKSsRKNS0_5OwnedINS0_7PromiseINS9_9ContainerEEEEERKS2_I8DurationEEUlvE_vEES6_OT_NS4_10LessPreferEEUlS6_E_E9_M_invokeERKSt9_Any_dataS6_
    @     0x7ffff5c1e9b3 std::function<>::operator()()
    @     0x7ffff6184a1a _ZN7process8internal3runISt8functionIFvRKNS_6FutureI6OptionIiEEEEEJRS6_EEEvRKSt6vectorIT_SaISD_EEDpOT0_
    @     0x7ffff617e64d process::Future<>::set()
    @     0x7ffff6752e46 process::Promise<>::set()
    @     0x7ffff675faec process::internal::cleanup()
    @     0x7ffff6765293 _ZNSt5_BindIFPFvRKN7process6FutureI6OptionIiEEEPNS0_7PromiseIS3_EERKNS0_10SubprocessEESt12_PlaceholderILi1EES9_SA_EE6__callIvIS6_EILm0ELm1ELm2EEEET_OSt5tupleIIDpT0_EESt12_Index_tupleIIXspT1_EEE
    @     0x7ffff6764bcd _ZNSt5_BindIFPFvRKN7process6FutureI6OptionIiEEEPNS0_7PromiseIS3_EERKNS0_10SubprocessEESt12_PlaceholderILi1EES9_SA_EEclIJS6_EvEET0_DpOT_
    @     0x7ffff67642a5 _ZZNK7process6FutureI6OptionIiEE5onAnyISt5_BindIFPFvRKS3_PNS_7PromiseIS2_EERKNS_10SubprocessEESt12_PlaceholderILi1EESA_SB_EEvEES7_OT_NS3_6PreferEENUlS7_E_clES7_
    @     0x7ffff676531d _ZNSt17_Function_handlerIFvRKN7process6FutureI6OptionIiEEEEZNKS4_5onAnyISt5_BindIFPFvS6_PNS0_7PromiseIS3_EERKNS0_10SubprocessEESt12_PlaceholderILi1EESC_SD_EEvEES6_OT_NS4_6PreferEEUlS6_E_E9_M_invokeERKSt9_Any_dataS6_
    @     0x7ffff5c1e9b3 std::function<>::operator()()
(END)
 ",Bug,Major,jojy,2015-12-15T00:33:21.000+0000,5,Resolved,Complete,Linux docker inspect crashes,2015-12-15T00:33:21.000+0000,MESOS-2938,1.0,mesos,
alexr,2015-06-25T17:51:13.000+0000,alex-mesos,Create a design document for the Quota feature support in the built-in Hierarchical DRF allocator to be shared with the Mesos community.,Documentation,Major,alexr,2015-09-02T12:52:01.000+0000,5,Resolved,Complete,Initial design document for Quota support in Allocator.,2015-11-07T00:14:19.000+0000,MESOS-2937,5.0,mesos,Mesosphere Sprint 16
alexr,2015-06-25T17:47:05.000+0000,alex-mesos,"Create a design document for the Quota feature support in Mesos Master (excluding allocator) to be shared with the Mesos community.

Design Doc:
https://docs.google.com/document/d/16iRNmziasEjVOblYp5bbkeBZ7pnjNlaIzPQqMTHQ-9I/edit?usp=sharing",Documentation,Major,alexr,2015-08-02T10:11:43.000+0000,5,Resolved,Complete,Create a design document for Quota support in Master,2015-11-17T06:44:16.000+0000,MESOS-2936,8.0,mesos,Mesosphere Sprint 15
pbrett,2015-06-24T23:47:53.000+0000,pbrett,Update stout to #include headers for symbols we rely on and reorder to comply with the style guide.,Improvement,Major,pbrett,2015-07-24T01:12:58.000+0000,5,Resolved,Complete,Update stout #include headers,2015-07-24T01:12:58.000+0000,MESOS-2928,2.0,mesos,Twitter Mesos Q2 Sprint 6
,2015-06-24T23:45:43.000+0000,pbrett,"cpplint.py provides the capability to enforce the style guide requirements for #including everything you use and ordering files based on type but it does not work for mesos because we do use #include <...> for project files where it expects #include ""..."".  

We should update the style checker to support our include usage and then turn it on by default in the commit hook.",Bug,Major,pbrett,,3,In Progress,In Progress,Extend mesos-style.py/cpplint.py to check #include files,2015-10-13T17:51:34.000+0000,MESOS-2926,1.0,mesos,
pbrett,2015-06-24T19:42:22.000+0000,pbrett,"The C++ specification states:

The macro ATOMIC_FLAG_INIT shall be defined in such a way that it can be used to initialize an object of type atomic_flag to the clear state. The macro can be used in the form: ""atomic_flag guard = ATOMIC_FLAG_INIT; ""It is unspecified whether the macro can be used in other initialization contexts."" 

Clang catches this (although reports it erroneously as a braced scaled init issue) and refuses to compile libprocess.",Bug,Major,pbrett,2015-06-25T20:19:56.000+0000,5,Resolved,Complete,Invalid usage of ATOMIC_FLAG_INIT in member initialization,2016-02-05T00:46:20.000+0000,MESOS-2925,1.0,mesos,Twitter Mesos Q2 Sprint 6
bernd-mesos,2015-06-24T14:12:24.000+0000,tmieszkowski,"Mesos 0.22.0/0.22.1 built and installed from sources accordingly to the instructions given [here|http://mesos.apache.org/gettingstarted/] has some problem with certificates.
Every time I try to deploy something that requires downloading any resource via HTTPS (with URI specified via Marathon), such deployment fails and I get this message in failed app's sandbox:
{code}
E0617 09:58:44.339409 12380 fetcher.cpp:138] Error downloading resource: Problem with the SSL CA cert (path? access rights?)
{code}
Trying to download the same resource on the same slave with {{curl}} or {{wget}} works without problems.
Moreover, when I install exactly the same version of Mesos from Mesosphere's debs on identical machines (i.e., set up by the same Ansible scripts), everything works fine as well.
I guess it must be something related to the way how Mesos is built - maybe some missing switch for {{configure}} or {{make}}..?

Any ideas..?",Bug,Major,tmieszkowski,,10020,Accepted,In Progress,fetcher.cpp - problem with certificates..?,2015-07-20T17:40:54.000+0000,MESOS-2923,2.0,mesos,
,2015-06-24T01:43:24.000+0000,bmahler,"Now that we have C++11, let's add move constructors and move assignment operators for Future, similarly to what was done for Option. There is currently one move constructor for Future<T>, but not for T, U, and no assignment operator.",Improvement,Major,bmahler,,10020,Accepted,In Progress,Add move constructors / assignment to Future.,2015-11-04T09:27:26.000+0000,MESOS-2922,3.0,mesos,
,2015-06-24T01:43:22.000+0000,bmahler,"Now that we have C++11, let's add move constructors and move assignment operators for Result, similarly to what was done for Option.",Improvement,Major,bmahler,,10020,Accepted,In Progress,Add move constructors / assignment to Result.,2015-11-04T09:27:26.000+0000,MESOS-2921,3.0,mesos,
,2015-06-24T01:43:20.000+0000,bmahler,"Now that we have C++11, let's add move constructors and move assignment operators for Try, similarly to what was done for Option.",Improvement,Major,bmahler,,10020,Accepted,In Progress,Add move constructors / assignment to Try.,2015-11-04T09:26:46.000+0000,MESOS-2920,3.0,mesos,
jieyu,2015-06-24T00:01:56.000+0000,jieyu,"This is due to a bug in the hierarchical allocator. Here is the sequence of events:

1) slave uses a fixed resource estimator which advertise 4 revocable cpus
2) a framework A launches a task that uses all the 4 revocable cpus
3) master fails over
4) slave re-registers with the new master, and sends UpdateSlaveMessage with 4 revocable cpus as oversubscribed resources
5) framework A hasn't registered yet, therefore, the slave's available resources will be 4 revocable cpus
6) framework A registered and will receive an additional 4 revocable cpus. So it can launch another task with 4 revocable cpus (that means 8 total!)

The problem is due to the way we calculate 'allocated' resource in allocator when 'updateSlave'. If the framework is not registered, the 'allocation' below is not accurate (check that if block in 'addSlave').

{code}
template <class RoleSorter, class FrameworkSorter>
void
HierarchicalAllocatorProcess<RoleSorter, FrameworkSorter>::updateSlave(
    const SlaveID& slaveId,
    const Resources& oversubscribed)
{
  CHECK(initialized);
  CHECK(slaves.contains(slaveId));

  // Check that all the oversubscribed resources are revocable.
  CHECK_EQ(oversubscribed, oversubscribed.revocable());

  // Update the total resources.

  // First remove the old oversubscribed resources from the total.
  slaves[slaveId].total -= slaves[slaveId].total.revocable();

  // Now add the new estimate of oversubscribed resources.
  slaves[slaveId].total += oversubscribed;

  // Now, update the total resources in the role sorter.
  roleSorter->update(
      slaveId,
      slaves[slaveId].total.unreserved());

  // Calculate the current allocation of oversubscribed resources.
  Resources allocation;
  foreachkey (const std::string& role, roles) {
    allocation += roleSorter->allocation(role, slaveId).revocable();
  }

  // Update the available resources.

  // First remove the old oversubscribed resources from available.
  slaves[slaveId].available -= slaves[slaveId].available.revocable();

  // Now add the new estimate of available oversubscribed resources.
  slaves[slaveId].available += oversubscribed - allocation;

  LOG(INFO) << ""Slave "" << slaveId << "" ("" << slaves[slaveId].hostname
            << "") updated with oversubscribed resources "" << oversubscribed
            << "" (total: "" << slaves[slaveId].total
            << "", available: "" << slaves[slaveId].available << "")"";

  allocate(slaveId);
}

template <class RoleSorter, class FrameworkSorter>
void
HierarchicalAllocatorProcess<RoleSorter, FrameworkSorter>::addSlave(
    const SlaveID& slaveId,
    const SlaveInfo& slaveInfo,
    const Resources& total,
    const hashmap<FrameworkID, Resources>& used)
{
  CHECK(initialized);
  CHECK(!slaves.contains(slaveId));

  roleSorter->add(slaveId, total.unreserved());

  foreachpair (const FrameworkID& frameworkId,
               const Resources& allocated,
               used) {
    if (frameworks.contains(frameworkId)) {
      const std::string& role = frameworks[frameworkId].role;

      // TODO(bmahler): Validate that the reserved resources have the
      // framework's role.

      roleSorter->allocated(role, slaveId, allocated.unreserved());
      frameworkSorters[role]->add(slaveId, allocated);
      frameworkSorters[role]->allocated(
          frameworkId.value(), slaveId, allocated);
    }
  }
  ...
}
{code}",Bug,Critical,jieyu,2015-06-25T17:37:59.000+0000,5,Resolved,Complete,Framework can overcommit oversubscribable resources during master failover.,2015-06-25T17:37:59.000+0000,MESOS-2919,3.0,mesos,Twitter Mesos Q2 Sprint 6
karya,2015-06-23T02:27:49.000+0000,karya,"Currently configure.ac lists 3.2.24 as the required libnl version. However, https://reviews.apache.org/r/31503 caused the minimum required version to be bumped to 3.2.26. The configure check thus fails to error out during execution and the dependency is captured only during the build step.",Bug,Blocker,karya,2015-06-23T23:47:03.000+0000,5,Resolved,Complete,Specify correct libnl version for configure check,2015-06-23T23:47:03.000+0000,MESOS-2917,1.0,mesos,Mesosphere Sprint 13
jieyu,2015-06-22T21:12:51.000+0000,jieyu,"Otherwise, the icmp/arp filter on host eth0 might be removed as a result of _cleanup if 'infos' is empty, causing subsequent '_cleanup' to fail on both known/unknown orphan containers.

{noformat}
I0612 17:46:51.518501 16308 containerizer.cpp:314] Recovering containerizer
I0612 17:46:51.520612 16308 port_mapping.cpp:1567] Discovered network namespace handle symlink ddcb8397-3552-44f9-bc99-b5b69aa72944 -> 31607
I0612 17:46:51.521183 16308 port_mapping.cpp:1567] Discovered network namespace handle symlink d8c48a4a-fdfb-47dd-b8d8-07188c21600d -> 41020
I0612 17:46:51.521883 16308 port_mapping.cpp:1567] Discovered network namespace handle symlink 8953fc7f-9fca-4931-b0cb-2f4959ddee74 -> 3302
I0612 17:46:51.522542 16308 port_mapping.cpp:1567] Discovered network namespace handle symlink 50f9986f-ebbc-440d-86a7-9fa1a7c55a75 -> 19805
I0612 17:46:51.523643 16308 port_mapping.cpp:2597] Removing IP packet filters with ports [33792,34815] for container with pid 52304
I0612 17:46:51.525063 16308 port_mapping.cpp:2616] Freed ephemeral ports [33792,34816) for container with pid 52304
I0612 17:46:51.547696 16308 port_mapping.cpp:2762] Successfully performed cleanup for pid 52304
I0612 17:46:51.550027 16308 port_mapping.cpp:1698] Network isolator recovery complete
I0612 17:46:51.550946 16329 containerizer.cpp:449] Removing orphan container 111ea69c-6184-4da1-a0e9-c34e8c6deb30
I0612 17:46:51.552686 16329 containerizer.cpp:449] Removing orphan container ddcb8397-3552-44f9-bc99-b5b69aa72944
I0612 17:46:51.552734 16309 cgroups.cpp:2377] Freezing cgroup /sys/fs/cgroup/freezer/mesos/111ea69c-6184-4da1-a0e9-c34e8c6deb30
I0612 17:46:51.554932 16329 containerizer.cpp:449] Removing orphan container 8953fc7f-9fca-4931-b0cb-2f4959ddee74
I0612 17:46:51.555032 16309 cgroups.cpp:2377] Freezing cgroup /sys/fs/cgroup/freezer/mesos/ddcb8397-3552-44f9-bc99-b5b69aa72944
I0612 17:46:51.555629 16308 cgroups.cpp:1420] Successfully froze cgroup /sys/fs/cgroup/freezer/mesos/111ea69c-6184-4da1-a0e9-c34e8c6deb30 after 1.730304ms
I0612 17:46:51.557507 16329 containerizer.cpp:449] Removing orphan container 50f9986f-ebbc-440d-86a7-9fa1a7c55a75
I0612 17:46:51.557611 16309 cgroups.cpp:2377] Freezing cgroup /sys/fs/cgroup/freezer/mesos/8953fc7f-9fca-4931-b0cb-2f4959ddee74
I0612 17:46:51.557896 16313 cgroups.cpp:1420] Successfully froze cgroup /sys/fs/cgroup/freezer/mesos/ddcb8397-3552-44f9-bc99-b5b69aa72944 after 1.685248ms
I0612 17:46:51.559412 16310 cgroups.cpp:2394] Thawing cgroup /sys/fs/cgroup/freezer/mesos/111ea69c-6184-4da1-a0e9-c34e8c6deb30
I0612 17:46:51.561564 16329 containerizer.cpp:449] Removing orphan container d8c48a4a-fdfb-47dd-b8d8-07188c21600d
I0612 17:46:51.562489 16315 cgroups.cpp:2377] Freezing cgroup /sys/fs/cgroup/freezer/mesos/50f9986f-ebbc-440d-86a7-9fa1a7c55a75
I0612 17:46:51.562988 16313 cgroups.cpp:2394] Thawing cgroup /sys/fs/cgroup/freezer/mesos/ddcb8397-3552-44f9-bc99-b5b69aa72944
I0612 17:46:51.563303 16310 cgroups.cpp:1449] Successfullly thawed cgroup /sys/fs/cgroup/freezer/mesos/111ea69c-6184-4da1-a0e9-c34e8c6deb30 after 2.076928ms
I0612 17:46:51.566052 16308 cgroups.cpp:2377] Freezing cgroup /sys/fs/cgroup/freezer/mesos/d8c48a4a-fdfb-47dd-b8d8-07188c21600d
I0612 17:46:51.566102 16313 slave.cpp:3911] Finished recovery
W0612 17:46:51.566432 16323 disk.cpp:299] Ignoring cleanup for unknown container 111ea69c-6184-4da1-a0e9-c34e8c6deb30
I0612 17:46:51.566651 16317 cgroups.cpp:1449] Successfullly thawed cgroup /sys/fs/cgroup/freezer/mesos/ddcb8397-3552-44f9-bc99-b5b69aa72944 after 2.12096ms
I0612 17:46:51.566987 16313 slave.cpp:3944] Garbage collecting old slave 20150319-213133-2080910346-5050-57551-S3314
I0612 17:46:51.567777 16318 cgroups.cpp:1420] Successfully froze cgroup /sys/fs/cgroup/freezer/mesos/d8c48a4a-fdfb-47dd-b8d8-07188c21600d after 1.323008ms
W0612 17:46:51.568042 16323 port_mapping.cpp:2544] Ignoring cleanup for unknown container 111ea69c-6184-4da1-a0e9-c34e8c6deb30
I0612 17:46:51.569522 16311 gc.cpp:56] Scheduling '/var/lib/mesos/slaves/20150319-213133-2080910346-5050-57551-S3314' for gc 6.99999341503407days in the future
W0612 17:46:51.569725 16329 disk.cpp:299] Ignoring cleanup for unknown container ddcb8397-3552-44f9-bc99-b5b69aa72944
I0612 17:46:51.570911 16325 cgroups.cpp:2394] Thawing cgroup /sys/fs/cgroup/freezer/mesos/d8c48a4a-fdfb-47dd-b8d8-07188c21600d
I0612 17:46:51.573581 16316 port_mapping.cpp:2597] Removing IP packet filters with ports [35840,36863] for container with pid 31607
I0612 17:46:51.575127 16316 port_mapping.cpp:2616] Freed ephemeral ports [35840,36864) for container with pid 31607
I0612 17:46:51.588284 16330 cgroups.cpp:1449] Successfullly thawed cgroup /sys/fs/cgroup/freezer/mesos/d8c48a4a-fdfb-47dd-b8d8-07188c21600d after 14.503936ms
E0612 17:46:51.622140 16310 containerizer.cpp:480] Failed to clean up an isolator when destroying orphan container ddcb8397-3552-44f9-bc99-b5b69aa72944: The ICMP packet filter on host eth0 does not exist, The ARP packet filter on host eth0 does not exist
W0612 17:46:51.773123 16313 disk.cpp:299] Ignoring cleanup for unknown container d8c48a4a-fdfb-47dd-b8d8-07188c21600d
I0612 17:46:51.774153 16325 port_mapping.cpp:2597] Removing IP packet filters with ports [32768,33791] for container with pid 41020
I0612 17:46:51.775167 16325 port_mapping.cpp:2616] Freed ephemeral ports [32768,33792) for container with pid 41020
E0612 17:46:51.817221 16323 containerizer.cpp:480] Failed to clean up an isolator when destroying orphan container d8c48a4a-fdfb-47dd-b8d8-07188c21600d: The ICMP packet filter on host eth0 does not exist, The ARP packet filter on host eth0 does not exist
I0612 17:46:51.872231 16314 cgroups.cpp:1420] Successfully froze cgroup /sys/fs/cgroup/freezer/mesos/50f9986f-ebbc-440d-86a7-9fa1a7c55a75 after 308.33792ms
I0612 17:46:51.874572 16314 cgroups.cpp:2394] Thawing cgroup /sys/fs/cgroup/freezer/mesos/50f9986f-ebbc-440d-86a7-9fa1a7c55a75
I0612 17:46:51.876566 16314 cgroups.cpp:1449] Successfullly thawed cgroup /sys/fs/cgroup/freezer/mesos/50f9986f-ebbc-440d-86a7-9fa1a7c55a75 after 1.593344ms
2015-06-12 17:46:54,833:16307(0x7f172eb07940):ZOO_INFO@auth_completion_func@1286: Authentication scheme digest succeeded
I0612 17:46:54.835737 16321 group.cpp:385] Trying to create path '/home/mesos/prod/master' in ZooKeeper
I0612 17:46:54.839110 16321 detector.cpp:138] Detected a new leader: (id='1')
I0612 17:46:54.840276 16330 group.cpp:659] Trying to get '/home/mesos/prod/master/info_0000000001' in ZooKeeper
I0612 17:46:54.842350 16330 detector.cpp:452] A new leading master (UPID=master@10.44.14.132:5050) is detected
I0612 17:46:54.843297 16330 slave.cpp:653] New master detected at master@10.44.14.132:5050
I0612 17:46:54.843298 16312 status_update_manager.cpp:171] Pausing sending status updates
I0612 17:46:54.844091 16330 slave.cpp:678] No credentials provided. Attempting to register without authentication
I0612 17:46:54.845087 16330 slave.cpp:689] Detecting new master
I0612 17:47:01.561920 16309 cgroups.cpp:2394] Thawing cgroup /sys/fs/cgroup/freezer/mesos/8953fc7f-9fca-4931-b0cb-2f4959ddee74
I0612 17:47:01.564687 16309 cgroups.cpp:1449] Successfullly thawed cgroup /sys/fs/cgroup/freezer/mesos/8953fc7f-9fca-4931-b0cb-2f4959ddee74 after 1.924096ms
I0612 17:47:01.565467 16309 cgroups.cpp:2377] Freezing cgroup /sys/fs/cgroup/freezer/mesos/8953fc7f-9fca-4931-b0cb-2f4959ddee74
W0612 17:47:09.978946 16326 disk.cpp:299] Ignoring cleanup for unknown container 50f9986f-ebbc-440d-86a7-9fa1a7c55a75
I0612 17:47:09.979818 16327 port_mapping.cpp:2597] Removing IP packet filters with ports [34816,35839] for container with pid 19805
I0612 17:47:09.981474 16327 port_mapping.cpp:2616] Freed ephemeral ports [34816,35840) for container with pid 19805
E0612 17:47:10.278715 16325 containerizer.cpp:480] Failed to clean up an isolator when destroying orphan container 50f9986f-ebbc-440d-86a7-9fa1a7c55a75: The ICMP packet filter on host eth0 does not exist, The ARP packet filter on host eth0 does not exist
I0612 17:47:11.568151 16326 cgroups.cpp:2394] Thawing cgroup /sys/fs/cgroup/freezer/mesos/8953fc7f-9fca-4931-b0cb-2f4959ddee74
I0612 17:47:11.570915 16326 cgroups.cpp:1449] Successfullly thawed cgroup /sys/fs/cgroup/freezer/mesos/8953fc7f-9fca-4931-b0cb-2f4959ddee74 after 1.987072ms
I0612 17:47:11.571728 16326 cgroups.cpp:2377] Freezing cgroup /sys/fs/cgroup/freezer/mesos/8953fc7f-9fca-4931-b0cb-2f4959ddee74
I0612 17:47:14.549536 16316 slave.cpp:821] Registered with master master@10.44.14.132:5050; given slave ID 20150602-190100-2215521290-5050-39399-S23257
I0612 17:47:14.550220 16318 status_update_manager.cpp:178] Resuming sending status updates
I0612 17:47:21.574513 16319 cgroups.cpp:2394] Thawing cgroup /sys/fs/cgroup/freezer/mesos/8953fc7f-9fca-4931-b0cb-2f4959ddee74
I0612 17:47:21.576817 16319 cgroups.cpp:1449] Successfullly thawed cgroup /sys/fs/cgroup/freezer/mesos/8953fc7f-9fca-4931-b0cb-2f4959ddee74 after 1.587968ms
I0612 17:47:21.577466 16319 cgroups.cpp:2377] Freezing cgroup /sys/fs/cgroup/freezer/mesos/8953fc7f-9fca-4931-b0cb-2f4959ddee74
I0612 17:47:31.580281 16310 cgroups.cpp:2394] Thawing cgroup /sys/fs/cgroup/freezer/mesos/8953fc7f-9fca-4931-b0cb-2f4959ddee74
I0612 17:47:31.582365 16310 cgroups.cpp:1449] Successfullly thawed cgroup /sys/fs/cgroup/freezer/mesos/8953fc7f-9fca-4931-b0cb-2f4959ddee74 after 1.410048ms
I0612 17:47:31.582895 16310 cgroups.cpp:2377] Freezing cgroup /sys/fs/cgroup/freezer/mesos/8953fc7f-9fca-4931-b0cb-2f4959ddee74
I0612 17:47:41.585619 16322 cgroups.cpp:2394] Thawing cgroup /sys/fs/cgroup/freezer/mesos/8953fc7f-9fca-4931-b0cb-2f4959ddee74
I0612 17:47:41.587703 16322 cgroups.cpp:1449] Successfullly thawed cgroup /sys/fs/cgroup/freezer/mesos/8953fc7f-9fca-4931-b0cb-2f4959ddee74 after 1.418752ms
I0612 17:47:41.588436 16322 cgroups.cpp:2377] Freezing cgroup /sys/fs/cgroup/freezer/mesos/8953fc7f-9fca-4931-b0cb-2f4959ddee74
I0612 17:47:51.515689 16330 slave.cpp:3733] Current disk usage 11.43%. Max allowed age: 5.499938588861215days
E0612 17:47:51.557831 16330 containerizer.cpp:468] Failed to destroy orphan container 8953fc7f-9fca-4931-b0cb-2f4959ddee74: Timed out after 1mins
{noformat}",Bug,Major,jieyu,2015-06-22T23:56:44.000+0000,5,Resolved,Complete,Port mapping isolator should cleanup unknown orphan containers after all known orphan containers are recovered during recovery.,2015-06-23T03:22:45.000+0000,MESOS-2914,3.0,mesos,Twitter Mesos Q2 Sprint 6
vinodkone,2015-06-22T20:22:24.000+0000,vinodkone,"To vet the new Call protobufs, it is prudent to have the scheduler driver (sched.cpp) send Call messages to the master (similar to what we are doing with the scheduler library).",Task,Major,vinodkone,2015-07-20T19:29:48.000+0000,5,Resolved,Complete,Scheduler driver should send Call messages to the master,2015-09-06T09:26:41.000+0000,MESOS-2913,8.0,mesos,Twitter Mesos Q2 Sprint 6
marco-mesos,2015-06-22T20:18:54.000+0000,vinodkone,"When schedulers start interacting with Mesos master via HTTP endpoints, they need a way to detect masters. 

Mesos should provide a master detection Python library to make this easy for frameworks.",Task,Major,vinodkone,2015-08-14T23:35:48.000+0000,5,Resolved,Complete,Provide a Python library for master detection,2015-09-14T20:15:00.000+0000,MESOS-2912,5.0,mesos,
anandmazumdar,2015-06-22T20:15:13.000+0000,vinodkone,"Adding this handler lets master send Event messages to the library.

See MESOS-2909 for additional context.

This ticket only tracks the installation of the handler and maybe handling of a single event for testing. Additional events handling will be captured in a different ticket(s).",Task,Major,vinodkone,2015-07-21T00:42:59.000+0000,5,Resolved,Complete,Add an Event message handler to scheduler library,2015-07-21T00:42:59.000+0000,MESOS-2911,3.0,mesos,Twitter Mesos Q2 Sprint 6
bmahler,2015-06-22T20:14:41.000+0000,vinodkone,"Adding this handler lets master send Event messages to the driver.

See MESOS-2909 for additional context.",Task,Major,vinodkone,2015-07-17T21:03:49.000+0000,5,Resolved,Complete,Add an Event message handler to scheduler driver,2015-07-27T21:41:43.000+0000,MESOS-2910,8.0,mesos,Twitter Mesos Q2 Sprint 6
bmahler,2015-06-22T20:08:32.000+0000,vinodkone,"In the same way we added 'version' field to RegisterSlaveMessage and ReregisterSlaveMessage, we should do it framework (re-)registration messages. This would help master determine which version of scheduler driver it is talking to.

We want this so that master can start sending Event messages to the scheduler driver (and scheduler library). In the long term, master will send a streaming response to the libraries, but in the meantime we can test the event protobufs by sending Event messages.",Task,Major,vinodkone,2015-07-17T23:10:08.000+0000,5,Resolved,Complete,Add version field to RegisterFrameworkMessage and ReregisterFrameworkMessage,2015-07-17T23:10:08.000+0000,MESOS-2909,3.0,mesos,Twitter Mesos Q2 Sprint 6
anandmazumdar,2015-06-22T18:36:24.000+0000,anandmazumdar,"This is the first basic step in ensuring the basic /call functionality: 

- Set up the route on the agent for ""api/v1/executor"" endpoint.
- The endpoint should perform basic header/protobuf validation and return {{501 NotImplemented}} for now.
- Introduce initial tests in executor_api_tests.cpp that just verify the status code.
",Task,Major,anandmazumdar,2015-09-21T22:06:15.000+0000,5,Resolved,Complete,Agent : Create Basic Functionality to handle /call endpoint,2016-02-27T00:05:04.000+0000,MESOS-2907,5.0,mesos,Mesosphere Sprint 19
ijimenez,2015-06-22T18:29:33.000+0000,anandmazumdar,"/call endpoint on the slave will return a 202 accepted code but has to do some basic validations before. In case of invalidation it will return a {{BadRequest}} back to the client.

- We need to create the required infrastructure to validate the request and then process it similar to {{src/master/validation.cpp}} in the {{namespace scheduler}} i.e. check if the protobuf is properly initialized, has the required attributes set pertaining to the call message etc.",Task,Major,anandmazumdar,2015-10-27T19:31:10.000+0000,5,Resolved,Complete,Slave : Synchronous Validation for Calls,2016-02-27T00:05:06.000+0000,MESOS-2906,3.0,mesos,Mesosphere Sprint 19
pbrett,2015-06-20T00:04:00.000+0000,pbrett,"We have seen circumstances where a machine has been consistently unable to launch containers due to an inconsistent state (for example, unexpected network configuration).   Adding a metric to track container launch failures will allow us to detect and alert on slaves in such a state.",Bug,Major,pbrett,2015-06-23T21:31:54.000+0000,5,Resolved,Complete,Add slave metric to count container launch failures,2015-06-23T21:31:54.000+0000,MESOS-2904,1.0,mesos,Twitter Mesos Q2 Sprint 6
pbrett,2015-06-19T23:21:39.000+0000,pbrett,"Network isolator has multiple instances of the following pattern:

{noformat}
  Try<bool> something = ....::create();                                  
  if (something.isError()) {                                                   
    ++metrics.something_errors;                                      
    return Failure(""Failed to create something ..."")
  } else if (!icmpVethToEth0.get()) {                                               
    ++metrics.adding_veth_icmp_filters_already_exist;                               
    return Failure(""Something already exists"");
  }                                                                                 
{noformat}

These failures have occurred in operation due to the failure to recover or delete an orphan, causing the slave to remain on line but unable to create new resources.    We should convert the second failure message in this pattern to an information message since the final state of the system is the state that we requested.",Bug,Critical,pbrett,2015-06-22T21:29:11.000+0000,5,Resolved,Complete,Network isolator should not fail when target state already exists,2015-06-22T21:29:11.000+0000,MESOS-2903,3.0,mesos,Twitter Mesos Q2 Sprint 6
marco-mesos,2015-06-19T21:36:40.000+0000,cmaloney,"Currently Mesos tries to guess the IP, HOSTNAME by doing a reverse DNS lookup. This doesn't work on a lot of clouds as we want things like public IPs (which aren't the default DNS), there aren't FQDN names (Azure), or the correct way to figure it out is to call some cloud-specific endpoint.

If Mesos / Libprocess could load a mesos-module (Or run a script) which is provided per-cloud, we can figure out perfectly the IP / Hostname for the given environment. It also means we can ship one identical set of files to all hosts in a given provider which doesn't happen to have the DNS scheme + hostnames that libprocess/Mesos expects. Currently we have to generate host-specific config files which Mesos uses to guess.

The host-specific files break / fall apart if machines change IP / hostname without being reinstalled.",Improvement,Critical,cmaloney,2015-07-24T20:47:53.000+0000,5,Resolved,Complete,"Enable Mesos to use arbitrary script / module to figure out IP, HOSTNAME",2015-09-04T18:14:48.000+0000,MESOS-2902,5.0,mesos,Mesosphere Sprint 15
marco-mesos,2015-06-19T15:58:53.000+0000,marco-mesos,"Follow up from MESOS-2340, need to ensure this does not break the ZooKeeper discovery functionality.",Task,Major,marco-mesos,2015-06-25T16:33:35.000+0000,5,Resolved,Complete,Write tests for new JSON (ZooKeeper) functionality,2015-06-26T01:43:52.000+0000,MESOS-2898,2.0,mesos,Mesosphere Sprint 13
bmahler,2015-06-18T21:51:26.000+0000,bmahler,"In light of the performance regression in MESOS-2891, we'd like to have visibility into the queue size of the allocator. This will enable alerting on performance problems.

We currently have no metrics in the allocator.

I will also look into MESOS-1286 now that we have gcc 4.8, current queue size gauges require a trip through the Process' queue.",Task,Critical,bmahler,2015-06-18T23:50:30.000+0000,5,Resolved,Complete,Add queue size metrics for the allocator.,2015-06-18T23:50:30.000+0000,MESOS-2893,1.0,mesos,Twitter Mesos Q2 Sprint 5
jieyu,2015-06-18T21:42:30.000+0000,bmahler,"In light of the performance regression in MESOS-2891, we'd like to have a synthetic benchmark of the allocator code, in order to analyze and direct improvements.",Task,Critical,bmahler,2015-06-19T02:20:06.000+0000,5,Resolved,Complete,Add benchmark for hierarchical allocator.,2015-07-02T06:51:17.000+0000,MESOS-2892,3.0,mesos,Twitter Mesos Q2 Sprint 5
jieyu,2015-06-18T21:34:29.000+0000,bmahler,"For large clusters, the 0.23.0 allocator cannot keep up with the volume of slaves. After the following slave was re-registered, it took the allocator a long time to work through the backlog of slaves to add:

{noformat:title=45 minute delay}
I0618 18:55:40.738399 10172 master.cpp:3419] Re-registered slave 20150422-211121-2148346890-5050-3253-S4695
I0618 19:40:14.960636 10164 hierarchical.hpp:496] Added slave 20150422-211121-2148346890-5050-3253-S4695
{noformat}

Empirically, [addSlave|https://github.com/apache/mesos/blob/dda49e688c7ece603ac7a04a977fc7085c713dd1/src/master/allocator/mesos/hierarchical.hpp#L462] and [updateSlave|https://github.com/apache/mesos/blob/dda49e688c7ece603ac7a04a977fc7085c713dd1/src/master/allocator/mesos/hierarchical.hpp#L533] have become expensive.

Some timings from a production cluster reveal that the allocator spending in the low tens of milliseconds for each call to {{addSlave}} and {{updateSlave}}, when there are tens of thousands of slaves this amounts to the large delay seen above.

We also saw a slow steady increase in memory consumption, hinting further at a queue backup in the allocator.

A synthetic benchmark like we did for the registrar would be prudent here, along with visibility into the allocator's queue size.",Bug,Blocker,bmahler,2015-06-21T18:52:36.000+0000,5,Resolved,Complete,Performance regression in hierarchical allocator.,2015-06-21T18:52:36.000+0000,MESOS-2891,3.0,mesos,Twitter Mesos Q2 Sprint 5
jvanremoortere,2015-06-18T20:03:42.000+0000,jvanremoortere,"The links to the sandbox in the web ui don't work when ssl is enabled. 
This can happen if the certificate for the master and the slave do not match. This is a consequence of the redirection that happens when serving files.
The resolution to this is currently to set up your certificates to serve the hostnames of the master and slaves.",Bug,Critical,jvanremoortere,2015-07-06T18:56:54.000+0000,5,Resolved,Complete,Sandbox URL doesn't work in web-ui when using SSL,2015-07-06T18:56:54.000+0000,MESOS-2890,3.0,mesos,Mesosphere Sprint 13
hartem,2015-06-18T20:01:24.000+0000,jvanremoortere,The python egg requires explicit dependencies for SSL. Add these to the python configuration if ssl is enabled.,Bug,Major,jvanremoortere,2015-07-06T18:56:26.000+0000,5,Resolved,Complete,Add SSL switch to python configuration,2015-07-06T18:56:26.000+0000,MESOS-2889,3.0,mesos,Mesosphere Sprint 13
jvanremoortere,2015-06-18T19:58:08.000+0000,jvanremoortere,"commit beac384c77d4a9c235a813e9286716f4509bdd55
Author: Joris Van Remoortere <joris.van.remoortere@gmail.com>
Date:   Fri Jun 26 18:30:12 2015 -0700

    Add SSL tests.
    
    Review: https://reviews.apache.org/r/35889",Improvement,Major,jvanremoortere,2015-07-06T18:54:16.000+0000,5,Resolved,Complete,Add SSL socket tests,2015-07-06T18:54:16.000+0000,MESOS-2888,5.0,mesos,Mesosphere Sprint 13
alexr,2015-06-18T15:49:39.000+0000,alex-mesos,In Mesos tests we use some tricks and patterns to express certain expectations. These are not always obvious and not documented. The intent of the ticket is to kick-start the document with the description of those tricks for posterity.,Documentation,Minor,alexr,2015-06-18T17:34:09.000+0000,5,Resolved,Complete,Capture some testing patterns we use in a doc,2015-10-14T10:00:39.000+0000,MESOS-2886,1.0,mesos,Mesosphere Sprint 13
karya,2015-06-18T00:52:46.000+0000,karya,"Currently, the LinuxLauncher looks into SlaveFlags to compute the namespaces that should be enabled when launching the executor. This means that a custom Isolator module doesn't have any way to specify dependency on a set of namespaces.

The proposed solution is to extend the Isolator interface to also export the namespaces dependency. This way the MesosContainerizer can directly query all loaded Isolators (inbuilt and custom modules) to compute the set of namespaces required by the executor. This set of namespaces is then passed on to the LinuxLauncher.
",Task,Major,karya,2015-06-25T17:50:27.000+0000,5,Resolved,Complete,Allow isolators to specify required namespaces,2015-07-02T07:21:46.000+0000,MESOS-2884,5.0,mesos,Mesosphere Sprint 13
karya,2015-06-17T21:05:41.000+0000,karya,"Hooks modules allow us to provide decorators during various aspects of a task lifecycle such as label decorator, environment decorator, etc.  Often the call into such a decorator hooks results in a new copy of labels, environment, etc., being returned to the call site. This is an unnecessary overhead if there are no hooks installed.

The proper way would be to call decorators via the hook manager only if there are some hooks installed. This would prevent unnecessary copying overhead if no hooks are available.",Improvement,Major,karya,2015-06-17T21:22:56.000+0000,5,Resolved,Complete,Do not call hook manager if no hooks installed,2015-06-18T00:31:25.000+0000,MESOS-2883,2.0,mesos,Mesosphere Sprint 12
greggomann,2015-06-17T16:09:22.000+0000,arojas,"While running make check on OS X, from time to time {{recursive_mutex}} errors appear after running all the test successfully. Just one of the experience messages actually stops {{make check}} reporting an error.

The following error messages have been experienced:

{code}
libc++abi.dylib: libc++abi.dylib: libc++abi.dylib: libc++abi.dylib: libc++abi.dylib: libc++abi.dylib: terminating with uncaught exception of type std::__1::system_error: recursive_mutex lock failed: Invalid argumentterminating with uncaught exception of type std::__1::system_error: recursive_mutex lock failed: Invalid argumentterminating with uncaught exception of type std::__1::system_error: recursive_mutex lock failed: Invalid argumentterminating with uncaught exception of type std::__1::system_error: recursive_mutex lock failed: Invalid argumentterminating with uncaught exception of type std::__1::system_error: recursive_mutex lock failed: Invalid argumentterminating with uncaught exception of type std::__1::system_error: recursive_mutex lock failed: Invalid argument


*** Aborted at 1434553937 (unix time) try ""date -d @1434553937"" if you are using GNU date ***
{code}

{code}
libc++abi.dylib: terminating with uncaught exception of type std::__1::system_error: recursive_mutex lock failed: Invalid argument
*** Aborted at 1434557001 (unix time) try ""date -d @1434557001"" if you are using GNU date ***
libc++abi.dylib: PC: @     0x7fff93855286 __pthread_kill
libc++abi.dylib: *** SIGABRT (@0x7fff93855286) received by PID 88060 (TID 0x10fc40000) stack trace: ***
    @     0x7fff8e1d6f1a _sigtramp
libc++abi.dylib:     @        0x10fc3f1a8 (unknown)
libc++abi.dylib:     @     0x7fff979deb53 abort
libc++abi.dylib: libc++abi.dylib: libc++abi.dylib: terminating with uncaught exception of type std::__1::system_error: recursive_mutex lock failed: Invalid argumentterminating with uncaught exception of type std::__1::system_error: recursive_mutex lock failed: Invalid argumentterminating with uncaught exception of type std::__1::system_error: recursive_mutex lock failed: Invalid argumentterminating with uncaught exception of type std::__1::system_error: recursive_mutex lock failed: Invalid argumentterminating with uncaught exception of type std::__1::system_error: recursive_mutex lock failed: Invalid argumentterminating with uncaught exception of type std::__1::system_error: recursive_mutex lock failed: Invalid argumentMaking check in include
{code}

{code}
Assertion failed: (e == 0), function ~recursive_mutex, file /SourceCache/libcxx/libcxx-120/src/mutex.cpp, line 82.
*** Aborted at 1434555685 (unix time) try ""date -d @1434555685"" if you are using GNU date ***
PC: @     0x7fff93855286 __pthread_kill
*** SIGABRT (@0x7fff93855286) received by PID 60235 (TID 0x7fff7ebdc300) stack trace: ***
    @     0x7fff8e1d6f1a _sigtramp
    @        0x10b512350 google::CheckNotNull<>()
    @     0x7fff979deb53 abort
    @     0x7fff979a6c39 __assert_rtn
    @     0x7fff9bffdcc9 std::__1::recursive_mutex::~recursive_mutex()
    @        0x10b881928 process::ProcessManager::~ProcessManager()
    @        0x10b874445 process::ProcessManager::~ProcessManager()
    @        0x10b874418 process::finalize()
    @        0x10b2f7aec main
    @     0x7fff98edc5c9 start
make[5]: *** [check-local] Abort trap: 6
make[4]: *** [check-am] Error 2
make[3]: *** [check-recursive] Error 1
make[2]: *** [check-recursive] Error 1
make[1]: *** [check] Error 2
make: *** [check-recursive] Error 1
{code}",Bug,Major,arojas,2015-10-01T19:46:09.000+0000,5,Resolved,Complete,Random recursive_mutex errors in when running make check,2015-10-01T19:46:09.000+0000,MESOS-2879,1.0,mesos,Mesosphere Sprint 15
pbrett,2015-06-16T16:00:36.000+0000,pbrett,"Simplify PortMappingStatistics by using JSON::Protocol and protobuf::parse to convert ResourceStatistics to/from line format.

This change will simplify the implementation of MESOS-2332.",Bug,Major,pbrett,2015-06-17T20:50:38.000+0000,5,Resolved,Complete,Convert PortMappingStatistics to use automatic JSON encoding/decoding,2015-06-18T18:18:16.000+0000,MESOS-2874,2.0,mesos,Twitter Mesos Q2 Sprint 5
arojas,2015-06-16T11:16:41.000+0000,arojas,"According to the original [markdown specification|http://daringfireball.net/projects/markdown/syntax#p] and to the most [recent standarization|http://spec.commonmark.org/0.20/#hard-line-breaks] effort, two spaces at the end of a line create a hard line break (it breaks the line without starting a new paragraph), similar to the html code {{<br/>}}. 

However, there's a hook in mesos which prevent files with trailing whitespace to be committed.",Bug,Trivial,arojas,2015-06-20T10:34:01.000+0000,5,Resolved,Complete,style hook prevent's valid markdown files from getting committed,2015-06-22T15:41:50.000+0000,MESOS-2873,1.0,mesos,Mesosphere Sprint 12
jieyu,2015-06-13T14:17:32.000+0000,mcypark,"Came up in https://reviews.apache.org/r/35395/

{code}
[ RUN      ] OversubscriptionTest.FixedResourceEstimator
I0613 13:41:02.604904 19367 exec.cpp:132] Version: 0.23.0
I0613 13:41:02.610995 19398 exec.cpp:206] Executor registered on slave 20150613-134102-3142697795-48295-13678-S0
Registered executor on pomona.apache.org
Starting task 7d78a3ef-2de9-46c9-811c-b2c0e2d50578
Forked command at 19410
sh -c 'sleep 1000'
../../src/tests/oversubscription_tests.cpp:579: Failure
Mock function called more times than expected - returning directly.
    Function call: statusUpdate(0x7ffffbc0c4e0, @0x2ade2bffa910 96-byte object <50-3E D7-22 DE-2A 00-00 00-00 00-00 00-00 00-00 D0-C4 00-48 DE-2A 00-00 50-71 AC-01 00-00 00-00 01-00 00-00 02-00 00-00 50-71 AC-01 00-00 00-00 B0-66 00-48 DE-2A 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-2A 00-00 E7-17 A8-BB 0C-5F D5-41 10-31 01-48 DE-2A 00-00 00-00 00-00 4B-03 00-00>)
         Expected: to be called once
           Actual: called twice - over-saturated and active
[  FAILED  ] OversubscriptionTest.FixedResourceEstimator (714 ms)
{code}",Bug,Major,mcypark,2015-06-15T19:34:17.000+0000,5,Resolved,Complete,OversubscriptionTest.FixedResourceEstimator is flaky,2015-06-15T19:34:17.000+0000,MESOS-2869,1.0,mesos,Twitter Mesos Q2 Sprint 5
bmahler,2015-06-12T22:48:57.000+0000,bmahler,"After a master failover, if the total amount of oversubscribed resources does not change, then the slave will not send the UpdateSlave message to the new master. The slave needs to send the information to the new master regardless of this.",Bug,Critical,bmahler,2015-06-15T21:50:06.000+0000,5,Resolved,Complete,Slave should send oversubscribed resource information after master failover.,2015-06-15T21:50:06.000+0000,MESOS-2866,3.0,mesos,Twitter Mesos Q2 Sprint 5
hartem,2015-06-11T22:50:14.000+0000,cmaloney,"Discovered while running mesos with marathon on top. If I launch a marathon task with a URI which is "" http://apache.osuosl.org/mesos/0.22.1/mesos-0.22.1.tar.gz"" mesos will log to stderr:

{code}
I0611 22:39:22.815636 35673 logging.cpp:177] Logging to STDERR
I0611 22:39:25.643889 35673 fetcher.cpp:214] Fetching URI ' http://apache.osuosl.org/mesos/0.22.1/mesos-0.22.1.tar.gz'
I0611 22:39:25.648111 35673 fetcher.cpp:94] Hadoop Client not available, skipping fetch with Hadoop Client
Failed to fetch:  http://apache.osuosl.org/mesos/0.22.1/mesos-0.22.1.tar.gz
Failed to synchronize with slave (it's probably exited)
{code}

It would be nice if mesos trimmed leading whitespace before doing protocol detection so that simple mistakes are just fixed. ",Bug,Minor,cmaloney,2015-07-18T21:23:56.000+0000,5,Resolved,Complete,"mesos-fetcher won't fetch uris which begin with a "" """,2015-07-18T21:23:56.000+0000,MESOS-2862,2.0,mesos,Mesosphere Sprint 14
ijimenez,2015-06-11T20:50:45.000+0000,marco-mesos,"This is the first basic step in ensuring the basic {{/call}} functionality: processing a 
{noformat}
POST /call
{noformat}
and returning:

- {{202}} if all goes well;
- {{401}} if not authorized; and
- {{403}} if the request is malformed.

We'll get more sophisticated as the work progressed (eg, supporting {{415}} if the content-type is not of the right kind).",Story,Major,marco-mesos,2015-08-12T17:31:41.000+0000,5,Resolved,Complete,Create the basic infrastructure to handle /scheduler endpoint,2015-08-15T00:22:41.000+0000,MESOS-2860,3.0,mesos,Mesosphere Sprint 15
bbannier,2015-06-11T03:18:06.000+0000,bmahler,"From jenkins:

{noformat}
[ RUN      ] FetcherCacheTest.LocalCachedExtract
Using temporary directory '/tmp/FetcherCacheTest_LocalCachedExtract_Cwdcdj'
I0610 20:04:48.591573 24561 leveldb.cpp:176] Opened db in 3.512525ms
I0610 20:04:48.592456 24561 leveldb.cpp:183] Compacted db in 828630ns
I0610 20:04:48.592512 24561 leveldb.cpp:198] Created db iterator in 32992ns
I0610 20:04:48.592531 24561 leveldb.cpp:204] Seeked to beginning of db in 8967ns
I0610 20:04:48.592545 24561 leveldb.cpp:273] Iterated through 0 keys in the db in 7762ns
I0610 20:04:48.592604 24561 replica.cpp:744] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned
I0610 20:04:48.593438 24587 recover.cpp:449] Starting replica recovery
I0610 20:04:48.593698 24587 recover.cpp:475] Replica is in EMPTY status
I0610 20:04:48.595641 24580 replica.cpp:641] Replica in EMPTY status received a broadcasted recover request
I0610 20:04:48.596086 24590 recover.cpp:195] Received a recover response from a replica in EMPTY status
I0610 20:04:48.596607 24590 recover.cpp:566] Updating replica status to STARTING
I0610 20:04:48.597507 24590 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 717888ns
I0610 20:04:48.597535 24590 replica.cpp:323] Persisted replica status to STARTING
I0610 20:04:48.597697 24590 recover.cpp:475] Replica is in STARTING status
I0610 20:04:48.599165 24584 replica.cpp:641] Replica in STARTING status received a broadcasted recover request
I0610 20:04:48.599434 24584 recover.cpp:195] Received a recover response from a replica in STARTING status
I0610 20:04:48.599915 24590 recover.cpp:566] Updating replica status to VOTING
I0610 20:04:48.600545 24590 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 432335ns
I0610 20:04:48.600574 24590 replica.cpp:323] Persisted replica status to VOTING
I0610 20:04:48.600659 24590 recover.cpp:580] Successfully joined the Paxos group
I0610 20:04:48.600797 24590 recover.cpp:464] Recover process terminated
I0610 20:04:48.602905 24594 master.cpp:363] Master 20150610-200448-3875541420-32907-24561 (dbade881e927) started on 172.17.0.231:32907
I0610 20:04:48.602957 24594 master.cpp:365] Flags at startup: --acls="""" --allocation_interval=""1secs"" --allocator=""HierarchicalDRF"" --authenticate=""true"" --authenticate_slaves=""true"" --authenticators=""crammd5"" --credentials=""/tmp/FetcherCacheTest_LocalCachedExtract_Cwdcdj/credentials"" --framework_sorter=""drf"" --help=""false"" --initialize_driver_logging=""true"" --log_auto_initialize=""true"" --logbufsecs=""0"" --logging_level=""INFO"" --quiet=""false"" --recovery_slave_removal_limit=""100%"" --registry=""replicated_log"" --registry_fetch_timeout=""1mins"" --registry_store_timeout=""25secs"" --registry_strict=""true"" --root_submissions=""true"" --slave_reregister_timeout=""10mins"" --user_sorter=""drf"" --version=""false"" --webui_dir=""/mesos/mesos-0.23.0/_inst/share/mesos/webui"" --work_dir=""/tmp/FetcherCacheTest_LocalCachedExtract_Cwdcdj/master"" --zk_session_timeout=""10secs""
I0610 20:04:48.603374 24594 master.cpp:410] Master only allowing authenticated frameworks to register
I0610 20:04:48.603392 24594 master.cpp:415] Master only allowing authenticated slaves to register
I0610 20:04:48.603404 24594 credentials.hpp:37] Loading credentials for authentication from '/tmp/FetcherCacheTest_LocalCachedExtract_Cwdcdj/credentials'
I0610 20:04:48.603751 24594 master.cpp:454] Using default 'crammd5' authenticator
I0610 20:04:48.604928 24594 master.cpp:491] Authorization enabled
I0610 20:04:48.606034 24593 hierarchical.hpp:309] Initialized hierarchical allocator process
I0610 20:04:48.606106 24593 whitelist_watcher.cpp:79] No whitelist given
I0610 20:04:48.607430 24594 master.cpp:1476] The newly elected leader is master@172.17.0.231:32907 with id 20150610-200448-3875541420-32907-24561
I0610 20:04:48.607466 24594 master.cpp:1489] Elected as the leading master!
I0610 20:04:48.607481 24594 master.cpp:1259] Recovering from registrar
I0610 20:04:48.607712 24594 registrar.cpp:313] Recovering registrar
I0610 20:04:48.608543 24588 log.cpp:661] Attempting to start the writer
I0610 20:04:48.610231 24588 replica.cpp:477] Replica received implicit promise request with proposal 1
I0610 20:04:48.611335 24588 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 1.086439ms
I0610 20:04:48.611382 24588 replica.cpp:345] Persisted promised to 1
I0610 20:04:48.612303 24588 coordinator.cpp:230] Coordinator attemping to fill missing position
I0610 20:04:48.613883 24593 replica.cpp:378] Replica received explicit promise request for position 0 with proposal 2
I0610 20:04:48.619205 24593 leveldb.cpp:343] Persisting action (8 bytes) to leveldb took 5.228235ms
I0610 20:04:48.619257 24593 replica.cpp:679] Persisted action at 0
I0610 20:04:48.621919 24593 replica.cpp:511] Replica received write request for position 0
I0610 20:04:48.621987 24593 leveldb.cpp:438] Reading position from leveldb took 49394ns
I0610 20:04:48.622689 24593 leveldb.cpp:343] Persisting action (14 bytes) to leveldb took 668412ns
I0610 20:04:48.622716 24593 replica.cpp:679] Persisted action at 0
I0610 20:04:48.623507 24584 replica.cpp:658] Replica received learned notice for position 0
I0610 20:04:48.624155 24584 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 612283ns
I0610 20:04:48.624186 24584 replica.cpp:679] Persisted action at 0
I0610 20:04:48.624215 24584 replica.cpp:664] Replica learned NOP action at position 0
I0610 20:04:48.625144 24593 log.cpp:677] Writer started with ending position 0
I0610 20:04:48.626724 24589 leveldb.cpp:438] Reading position from leveldb took 72013ns
I0610 20:04:48.629276 24591 registrar.cpp:346] Successfully fetched the registry (0B) in 21.520128ms
I0610 20:04:48.629663 24591 registrar.cpp:445] Applied 1 operations in 129587ns; attempting to update the 'registry'
I0610 20:04:48.632237 24579 log.cpp:685] Attempting to append 131 bytes to the log
I0610 20:04:48.632624 24579 coordinator.cpp:340] Coordinator attempting to write APPEND action at position 1
I0610 20:04:48.633739 24579 replica.cpp:511] Replica received write request for position 1
I0610 20:04:48.634351 24579 leveldb.cpp:343] Persisting action (150 bytes) to leveldb took 583937ns
I0610 20:04:48.634382 24579 replica.cpp:679] Persisted action at 1
I0610 20:04:48.635073 24583 replica.cpp:658] Replica received learned notice for position 1
I0610 20:04:48.635442 24583 leveldb.cpp:343] Persisting action (152 bytes) to leveldb took 357122ns
I0610 20:04:48.635469 24583 replica.cpp:679] Persisted action at 1
I0610 20:04:48.635494 24583 replica.cpp:664] Replica learned APPEND action at position 1
I0610 20:04:48.636337 24583 registrar.cpp:490] Successfully updated the 'registry' in 6.534144ms
I0610 20:04:48.636725 24594 log.cpp:704] Attempting to truncate the log to 1
I0610 20:04:48.636858 24583 registrar.cpp:376] Successfully recovered registrar
I0610 20:04:48.637073 24594 coordinator.cpp:340] Coordinator attempting to write TRUNCATE action at position 2
I0610 20:04:48.637789 24594 master.cpp:1286] Recovered 0 slaves from the Registry (95B) ; allowing 10mins for slaves to re-register
I0610 20:04:48.638630 24583 replica.cpp:511] Replica received write request for position 2
I0610 20:04:48.639127 24583 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 396272ns
I0610 20:04:48.639153 24583 replica.cpp:679] Persisted action at 2
I0610 20:04:48.639804 24583 replica.cpp:658] Replica received learned notice for position 2
I0610 20:04:48.640965 24583 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 1.147322ms
I0610 20:04:48.641054 24583 leveldb.cpp:401] Deleting ~1 keys from leveldb took 72395ns
I0610 20:04:48.641197 24583 replica.cpp:679] Persisted action at 2
I0610 20:04:48.641345 24583 replica.cpp:664] Replica learned TRUNCATE action at position 2
I0610 20:04:48.652274 24561 containerizer.cpp:111] Using isolation: posix/cpu,posix/mem
I0610 20:04:48.658994 24590 slave.cpp:188] Slave started on 42)@172.17.0.231:32907
I0610 20:04:48.659049 24590 slave.cpp:189] Flags at startup: --authenticatee=""crammd5"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --credential=""/tmp/FetcherCacheTest_LocalCachedExtract_LCHuuM/credential"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_kill_orphans=""true"" --docker_remove_delay=""6hrs"" --docker_sandbox_directory=""/mnt/mesos/sandbox"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/tmp/FetcherCacheTest_LocalCachedExtract_LCHuuM/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --initialize_driver_logging=""true"" --isolation=""posix/cpu,posix/mem"" --launcher_dir=""/mesos/mesos-0.23.0/_build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""10ms"" --resource_monitoring_interval=""1secs"" --resources=""cpus(*):1000; mem(*):1000"" --revocable_cpu_low_priority=""true"" --strict=""true"" --switch_user=""true"" --version=""false"" --work_dir=""/tmp/FetcherCacheTest_LocalCachedExtract_LCHuuM""
I0610 20:04:48.659570 24590 credentials.hpp:85] Loading credential for authentication from '/tmp/FetcherCacheTest_LocalCachedExtract_LCHuuM/credential'
I0610 20:04:48.659803 24590 slave.cpp:319] Slave using credential for: test-principal
I0610 20:04:48.660441 24590 slave.cpp:352] Slave resources: cpus(*):1000; mem(*):1000; disk(*):3.70122e+06; ports(*):[31000-32000]
I0610 20:04:48.660555 24590 slave.cpp:382] Slave hostname: dbade881e927
I0610 20:04:48.660578 24590 slave.cpp:387] Slave checkpoint: true
I0610 20:04:48.661550 24588 state.cpp:35] Recovering state from '/tmp/FetcherCacheTest_LocalCachedExtract_LCHuuM/meta'
I0610 20:04:48.661913 24590 status_update_manager.cpp:201] Recovering status update manager
I0610 20:04:48.662253 24590 containerizer.cpp:312] Recovering containerizer
I0610 20:04:48.663207 24581 slave.cpp:3950] Finished recovery
I0610 20:04:48.663761 24581 slave.cpp:4104] Querying resource estimator for oversubscribable resources
I0610 20:04:48.664077 24581 slave.cpp:678] New master detected at master@172.17.0.231:32907
I0610 20:04:48.664088 24586 status_update_manager.cpp:175] Pausing sending status updates
I0610 20:04:48.664245 24581 slave.cpp:741] Authenticating with master master@172.17.0.231:32907
I0610 20:04:48.664388 24581 slave.cpp:746] Using default CRAM-MD5 authenticatee
I0610 20:04:48.664611 24581 slave.cpp:714] Detecting new master
I0610 20:04:48.664647 24594 authenticatee.hpp:139] Creating new client SASL connection
I0610 20:04:48.664813 24581 slave.cpp:4125] Received oversubscribable resources  from the resource estimator
I0610 20:04:48.665060 24581 slave.cpp:4129] No master detected. Re-querying resource estimator after 15secs
I0610 20:04:48.665096 24594 master.cpp:4181] Authenticating slave(42)@172.17.0.231:32907
I0610 20:04:48.665247 24581 authenticator.cpp:406] Starting authentication session for crammd5_authenticatee(130)@172.17.0.231:32907
I0610 20:04:48.665657 24581 authenticator.cpp:92] Creating new server SASL connection
I0610 20:04:48.666013 24581 authenticatee.hpp:230] Received SASL authentication mechanisms: CRAM-MD5
I0610 20:04:48.666159 24581 authenticatee.hpp:256] Attempting to authenticate with mechanism 'CRAM-MD5'
I0610 20:04:48.666443 24592 authenticator.cpp:197] Received SASL authentication start
I0610 20:04:48.666591 24592 authenticator.cpp:319] Authentication requires more steps
I0610 20:04:48.666779 24592 authenticatee.hpp:276] Received SASL authentication step
I0610 20:04:48.667007 24585 authenticator.cpp:225] Received SASL authentication step
I0610 20:04:48.667043 24585 auxprop.cpp:101] Request to lookup properties for user: 'test-principal' realm: 'dbade881e927' server FQDN: 'dbade881e927' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0610 20:04:48.667058 24585 auxprop.cpp:173] Looking up auxiliary property '*userPassword'
I0610 20:04:48.667110 24585 auxprop.cpp:173] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0610 20:04:48.667142 24585 auxprop.cpp:101] Request to lookup properties for user: 'test-principal' realm: 'dbade881e927' server FQDN: 'dbade881e927' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0610 20:04:48.667155 24585 auxprop.cpp:123] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0610 20:04:48.667163 24585 auxprop.cpp:123] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0610 20:04:48.667181 24585 authenticator.cpp:311] Authentication success
I0610 20:04:48.667331 24585 authenticatee.hpp:316] Authentication success
I0610 20:04:48.667414 24585 master.cpp:4211] Successfully authenticated principal 'test-principal' at slave(42)@172.17.0.231:32907
I0610 20:04:48.667505 24585 authenticator.cpp:424] Authentication session cleanup for crammd5_authenticatee(130)@172.17.0.231:32907
I0610 20:04:48.667809 24585 slave.cpp:812] Successfully authenticated with master master@172.17.0.231:32907
I0610 20:04:48.667982 24585 slave.cpp:1146] Will retry registration in 7.257154ms if necessary
I0610 20:04:48.668226 24585 master.cpp:3157] Registering slave at slave(42)@172.17.0.231:32907 (dbade881e927) with id 20150610-200448-3875541420-32907-24561-S0
I0610 20:04:48.668737 24585 registrar.cpp:445] Applied 1 operations in 90255ns; attempting to update the 'registry'
I0610 20:04:48.672297 24585 log.cpp:685] Attempting to append 305 bytes to the log
I0610 20:04:48.672541 24585 coordinator.cpp:340] Coordinator attempting to write APPEND action at position 3
I0610 20:04:48.673528 24593 replica.cpp:511] Replica received write request for position 3
I0610 20:04:48.674321 24593 leveldb.cpp:343] Persisting action (324 bytes) to leveldb took 766804ns
I0610 20:04:48.674355 24593 replica.cpp:679] Persisted action at 3
I0610 20:04:48.675138 24587 replica.cpp:658] Replica received learned notice for position 3
I0610 20:04:48.675866 24587 leveldb.cpp:343] Persisting action (326 bytes) to leveldb took 714643ns
I0610 20:04:48.675897 24587 replica.cpp:679] Persisted action at 3
I0610 20:04:48.675922 24587 replica.cpp:664] Replica learned APPEND action at position 3
I0610 20:04:48.677471 24587 registrar.cpp:490] Successfully updated the 'registry' in 8.656128ms
I0610 20:04:48.677759 24587 log.cpp:704] Attempting to truncate the log to 3
I0610 20:04:48.678423 24593 coordinator.cpp:340] Coordinator attempting to write TRUNCATE action at position 4
I0610 20:04:48.678621 24587 master.cpp:3214] Registered slave 20150610-200448-3875541420-32907-24561-S0 at slave(42)@172.17.0.231:32907 (dbade881e927) with cpus(*):1000; mem(*):1000; disk(*):3.70122e+06; ports(*):[31000-32000]
I0610 20:04:48.678959 24593 hierarchical.hpp:496] Added slave 20150610-200448-3875541420-32907-24561-S0 (dbade881e927) with cpus(*):1000; mem(*):1000; disk(*):3.70122e+06; ports(*):[31000-32000] (and cpus(*):1000; mem(*):1000; disk(*):3.70122e+06; ports(*):[31000-32000] available)
I0610 20:04:48.679157 24593 hierarchical.hpp:933] No resources available to allocate!
I0610 20:04:48.679183 24593 hierarchical.hpp:852] Performed allocation for slave 20150610-200448-3875541420-32907-24561-S0 in 175519ns
I0610 20:04:48.679805 24593 replica.cpp:511] Replica received write request for position 4
I0610 20:04:48.684160 24587 slave.cpp:846] Registered with master master@172.17.0.231:32907; given slave ID 20150610-200448-3875541420-32907-24561-S0
I0610 20:04:48.684229 24587 fetcher.cpp:77] Clearing fetcher cache
I0610 20:04:48.684666 24587 slave.cpp:869] Checkpointing SlaveInfo to '/tmp/FetcherCacheTest_LocalCachedExtract_LCHuuM/meta/slaves/20150610-200448-3875541420-32907-24561-S0/slave.info'
I0610 20:04:48.687366 24587 slave.cpp:2895] Received ping from slave-observer(42)@172.17.0.231:32907
I0610 20:04:48.687453 24584 status_update_manager.cpp:182] Resuming sending status updates
I0610 20:04:48.690901 24593 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 3.385583ms
I0610 20:04:48.690975 24593 replica.cpp:679] Persisted action at 4
I0610 20:04:48.692137 24593 replica.cpp:658] Replica received learned notice for position 4
I0610 20:04:48.692603 24593 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 449838ns
I0610 20:04:48.692674 24593 leveldb.cpp:401] Deleting ~2 keys from leveldb took 52471ns
I0610 20:04:48.692699 24593 replica.cpp:679] Persisted action at 4
I0610 20:04:48.692726 24593 replica.cpp:664] Replica learned TRUNCATE action at position 4
I0610 20:04:48.693544 24561 sched.cpp:157] Version: 0.23.0
I0610 20:04:48.695550 24590 sched.cpp:254] New master detected at master@172.17.0.231:32907
I0610 20:04:48.697090 24590 sched.cpp:310] Authenticating with master master@172.17.0.231:32907
I0610 20:04:48.697136 24590 sched.cpp:317] Using default CRAM-MD5 authenticatee
I0610 20:04:48.697511 24586 authenticatee.hpp:139] Creating new client SASL connection
I0610 20:04:48.697937 24586 master.cpp:4181] Authenticating scheduler-51f5c1b5-bb50-4118-bde8-4dcdfd69205d@172.17.0.231:32907
I0610 20:04:48.698185 24584 authenticator.cpp:406] Starting authentication session for crammd5_authenticatee(131)@172.17.0.231:32907
I0610 20:04:48.698575 24584 authenticator.cpp:92] Creating new server SASL connection
I0610 20:04:48.698807 24584 authenticatee.hpp:230] Received SASL authentication mechanisms: CRAM-MD5
I0610 20:04:48.699898 24584 authenticatee.hpp:256] Attempting to authenticate with mechanism 'CRAM-MD5'
I0610 20:04:48.700040 24584 authenticator.cpp:197] Received SASL authentication start
I0610 20:04:48.700119 24584 authenticator.cpp:319] Authentication requires more steps
I0610 20:04:48.700193 24584 authenticatee.hpp:276] Received SASL authentication step
I0610 20:04:48.700287 24584 authenticator.cpp:225] Received SASL authentication step
I0610 20:04:48.700320 24584 auxprop.cpp:101] Request to lookup properties for user: 'test-principal' realm: 'dbade881e927' server FQDN: 'dbade881e927' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0610 20:04:48.700333 24584 auxprop.cpp:173] Looking up auxiliary property '*userPassword'
I0610 20:04:48.700392 24584 auxprop.cpp:173] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0610 20:04:48.700425 24584 auxprop.cpp:101] Request to lookup properties for user: 'test-principal' realm: 'dbade881e927' server FQDN: 'dbade881e927' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0610 20:04:48.700439 24584 auxprop.cpp:123] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0610 20:04:48.700448 24584 auxprop.cpp:123] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0610 20:04:48.700467 24584 authenticator.cpp:311] Authentication success
I0610 20:04:48.700640 24584 authenticatee.hpp:316] Authentication success
I0610 20:04:48.700742 24584 authenticator.cpp:424] Authentication session cleanup for crammd5_authenticatee(131)@172.17.0.231:32907
I0610 20:04:48.701282 24590 sched.cpp:398] Successfully authenticated with master master@172.17.0.231:32907
I0610 20:04:48.701315 24590 sched.cpp:521] Sending registration request to master@172.17.0.231:32907
I0610 20:04:48.701386 24590 sched.cpp:554] Will retry registration in 1.128089605secs if necessary
I0610 20:04:48.701676 24586 master.cpp:4211] Successfully authenticated principal 'test-principal' at scheduler-51f5c1b5-bb50-4118-bde8-4dcdfd69205d@172.17.0.231:32907
I0610 20:04:48.702863 24586 master.cpp:1716] Received registration request for framework 'default' at scheduler-51f5c1b5-bb50-4118-bde8-4dcdfd69205d@172.17.0.231:32907
W0610 20:04:48.702924 24586 master.cpp:1539] Framework at scheduler-51f5c1b5-bb50-4118-bde8-4dcdfd69205d@172.17.0.231:32907 (authenticated as 'test-principal') does not specify principal in its FrameworkInfo
I0610 20:04:48.702957 24586 master.cpp:1555] Authorizing framework principal '' to receive offers for role '*'
I0610 20:04:48.703580 24586 master.cpp:1783] Registering framework 20150610-200448-3875541420-32907-24561-0000 (default) at scheduler-51f5c1b5-bb50-4118-bde8-4dcdfd69205d@172.17.0.231:32907 with checkpointing enabled and capabilities [  ]
I0610 20:04:48.705044 24590 hierarchical.hpp:354] Added framework 20150610-200448-3875541420-32907-24561-0000
I0610 20:04:48.705657 24590 hierarchical.hpp:834] Performed allocation for 1 slaves in 583520ns
I0610 20:04:48.707613 24586 master.cpp:4100] Sending 1 offers to framework 20150610-200448-3875541420-32907-24561-0000 (default) at scheduler-51f5c1b5-bb50-4118-bde8-4dcdfd69205d@172.17.0.231:32907
I0610 20:04:48.709035 24590 sched.cpp:448] Framework registered with 20150610-200448-3875541420-32907-24561-0000
I0610 20:04:48.709113 24590 sched.cpp:462] Scheduler::registered took 33214ns

GMOCK WARNING:
Uninteresting mock function call - returning directly.
    Function call: resourceOffers(0x3bd7fb0, @0x7fe5bb7af898 { 128-byte object <90-8D 30-CD E5-7F 00-00 00-00 00-00 00-00 00-00 30-E8 00-80 E5-7F 00-00 D0-E8 00-80 E5-7F 00-00 70-E9 00-80 E5-7F 00-00 10-EA 00-80 E5-7F 00-00 F0-58 00-80 E5-7F 00-00 04-00 00-00 04-00 00-00 04-00 00-00 65-45 76-65 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 E5-7F 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 0F-00 00-00> })
Stack trace:
I0610 20:04:48.709631 24590 sched.cpp:611] Scheduler::resourceOffers took 189034ns
I0610 20:04:49.607378 24589 hierarchical.hpp:933] No resources available to allocate!
I0610 20:04:49.607435 24589 hierarchical.hpp:834] Performed allocation for 1 slaves in 474600ns
I0610 20:04:50.608489 24582 hierarchical.hpp:933] No resources available to allocate!
I0610 20:04:50.608551 24582 hierarchical.hpp:834] Performed allocation for 1 slaves in 517133ns
I0610 20:04:51.609849 24589 hierarchical.hpp:933] No resources available to allocate!
I0610 20:04:51.609908 24589 hierarchical.hpp:834] Performed allocation for 1 slaves in 440523ns
I0610 20:04:52.611188 24584 hierarchical.hpp:933] No resources available to allocate!
I0610 20:04:52.611250 24584 hierarchical.hpp:834] Performed allocation for 1 slaves in 471882ns
I0610 20:04:53.612911 24581 hierarchical.hpp:933] No resources available to allocate!
I0610 20:04:53.612962 24581 hierarchical.hpp:834] Performed allocation for 1 slaves in 411941ns
I0610 20:04:54.614280 24582 hierarchical.hpp:933] No resources available to allocate!
I0610 20:04:54.614336 24582 hierarchical.hpp:834] Performed allocation for 1 slaves in 448103ns
I0610 20:04:55.615985 24583 hierarchical.hpp:933] No resources available to allocate!
I0610 20:04:55.616046 24583 hierarchical.hpp:834] Performed allocation for 1 slaves in 494677ns
I0610 20:04:56.616896 24580 hierarchical.hpp:933] No resources available to allocate!
I0610 20:04:56.616952 24580 hierarchical.hpp:834] Performed allocation for 1 slaves in 461555ns
I0610 20:04:57.618814 24587 hierarchical.hpp:933] No resources available to allocate!
I0610 20:04:57.618885 24587 hierarchical.hpp:834] Performed allocation for 1 slaves in 491478ns
I0610 20:04:58.620564 24589 hierarchical.hpp:933] No resources available to allocate!
I0610 20:04:58.620621 24589 hierarchical.hpp:834] Performed allocation for 1 slaves in 434384ns
I0610 20:04:59.621649 24584 hierarchical.hpp:933] No resources available to allocate!
I0610 20:04:59.621706 24584 hierarchical.hpp:834] Performed allocation for 1 slaves in 453279ns
I0610 20:05:00.623241 24593 hierarchical.hpp:933] No resources available to allocate!
I0610 20:05:00.623299 24593 hierarchical.hpp:834] Performed allocation for 1 slaves in 423551ns
I0610 20:05:01.624984 24590 hierarchical.hpp:933] No resources available to allocate!
I0610 20:05:01.625041 24590 hierarchical.hpp:834] Performed allocation for 1 slaves in 458545ns
I0610 20:05:02.626266 24591 hierarchical.hpp:933] No resources available to allocate!
I0610 20:05:02.626327 24591 hierarchical.hpp:834] Performed allocation for 1 slaves in 490068ns
I0610 20:05:03.627702 24593 hierarchical.hpp:933] No resources available to allocate!
I0610 20:05:03.627766 24593 hierarchical.hpp:834] Performed allocation for 1 slaves in 473279ns
I0610 20:05:03.666060 24581 slave.cpp:4104] Querying resource estimator for oversubscribable resources
I0610 20:05:03.666353 24581 slave.cpp:4125] Received oversubscribable resources  from the resource estimator
I0610 20:05:03.680258 24588 slave.cpp:2895] Received ping from slave-observer(42)@172.17.0.231:32907
F0610 20:05:03.725155 24561 fetcher_cache_tests.cpp:354] CHECK_READY(offers): is PENDING Failed to wait for resource offers
*** Check failure stack trace: ***
    @     0x7fe5cc5a2a0d  google::LogMessage::Fail()
    @     0x7fe5cc5a1dee  google::LogMessage::SendToLog()
    @     0x7fe5cc5a26cd  google::LogMessage::Flush()
    @     0x7fe5cc5a5b38  google::LogMessageFatal::~LogMessageFatal()
    @           0x8947c7  _CheckFatal::~_CheckFatal()
    @           0xadf458  mesos::internal::tests::FetcherCacheTest::launchTask()
    @           0xae5ea4  mesos::internal::tests::FetcherCacheTest_LocalCachedExtract_Test::TestBody()
    @          0x128fb83  testing::internal::HandleSehExceptionsInMethodIfSupported<>()
    @          0x127a7e7  testing::internal::HandleExceptionsInMethodIfSupported<>()
    @          0x1261c45  testing::Test::Run()
    @          0x126287b  testing::TestInfo::Run()
    @          0x1262ec7  testing::TestCase::Run()
    @          0x126854a  testing::internal::UnitTestImpl::RunAllTests()
    @          0x128c163  testing::internal::HandleSehExceptionsInMethodIfSupported<>()
    @          0x127c817  testing::internal::HandleExceptionsInMethodIfSupported<>()
    @          0x1268247  testing::UnitTest::Run()
    @           0xca398e  main
    @     0x7fe5c8436ec5  (unknown)
    @           0x749e8c  (unknown)
{noformat}

[~bernd-mesos] not sure if there's a ticket capturing this already, sorry if this is a duplicate.",Bug,Major,bmahler,2016-01-04T08:44:50.000+0000,5,Resolved,Complete,FetcherCacheTest.LocalCachedExtract is flaky.,2016-01-04T08:44:50.000+0000,MESOS-2857,1.0,mesos,Mesosphere Sprint 23
xujyan,2015-06-10T21:39:14.000+0000,xujyan,"So code like this doesn't raise Error.
{code}
Resources::parse(""foo(role1):1;foo(role2):[0-1]"")
{code}

Doesn't look like allowing this adds value and this complicates resource maths/validation/reporting.

We should disallow this.",Bug,Major,xujyan,2015-06-10T23:31:41.000+0000,5,Resolved,Complete,Resources::parse(...) allows different resources of the same name to have different types.,2015-06-10T23:31:41.000+0000,MESOS-2854,2.0,mesos,Twitter Mesos Q2 Sprint 5
,2015-06-10T21:30:07.000+0000,pbrett,Export in statistics.json the fq_codel flow statistics for each container.,Improvement,Major,pbrett,,10020,Accepted,In Progress,Report per-container metrics from host egress filter,2015-10-13T17:50:53.000+0000,MESOS-2853,1.0,mesos,Twitter Mesos Q2 Sprint 5
tnachen,2015-06-10T20:48:06.000+0000,tnachen,"Provisions a Docker image (provisions all its dependent layers), fetch an image from persistent store, and also destroy an image. 

Done when tested for local discovery and copy backend. ",Improvement,Major,tnachen,2015-09-25T16:37:08.000+0000,5,Resolved,Complete,Implement Docker image provisioner,2015-09-25T16:37:08.000+0000,MESOS-2850,3.0,mesos,Mesosphere Sprint 18
tnachen,2015-06-10T20:47:34.000+0000,tnachen,"Given a local Docker image name and path to the image or image tarball, fetches the image's dependent layers, untarring if necessary. It will also parse the image layers' configuration json and place the layers and image into persistent store.

Done when a Docker image can be successfully stored and retrieved using 'put' and 'get' methods. ",Improvement,Major,tnachen,2015-09-25T16:35:20.000+0000,5,Resolved,Complete,Implement Docker local image store,2015-09-25T16:35:20.000+0000,MESOS-2849,5.0,mesos,Mesosphere Sprint 15
chenlily,2015-06-10T20:47:19.000+0000,tnachen,"Given a docker image name and the local directory where images can be found, creates a URI with a path to the corresponding image.

Done when system successfully checks for the image, untars the image if necessary, and returns the proper URI to the image.",Improvement,Major,tnachen,2015-07-20T17:35:46.000+0000,5,Resolved,Complete,Local filesystem docker image discovery,2015-07-20T17:35:46.000+0000,MESOS-2848,2.0,mesos,Mesosphere Sprint 14
,2015-06-10T18:31:38.000+0000,nnielsen,"Add and document new labels field to framework info:

{code}
message FrameworkInfo {
  // Used to determine the Unix user that an executor or task should
  // be launched as. If the user field is set to an empty string Mesos
  // will automagically set it to the current user.
  required string user = 1;

  // Name of the framework that shows up in the Mesos Web UI.
  required string name = 2;

  // Note that 'id' is only available after a framework has
  // registered, however, it is included here in order to facilitate
  // scheduler failover (i.e., if it is set then the
  // MesosSchedulerDriver expects the scheduler is performing
  // failover).
  optional FrameworkID id = 3;

  ...

  // This field allows a framework to advertise its set of
  // capabilities (e.g., ability to receive offers for revocable
  // resources).
  repeated Capability capabilities = 10;

  optional Labels labels = 11;
}
{code}",Improvement,Major,nnielsen,2015-09-09T21:42:10.000+0000,5,Resolved,Complete,Add and document new labels field to framework info,2015-09-09T21:42:10.000+0000,MESOS-2844,1.0,mesos,
jdef,2015-06-10T14:58:03.000+0000,jdef,"A framework instance may offer specific capabilities to the cluster: storage, smartly-balanced request handling across deployed tasks, access to 3rd party services outside of the cluster, etc. These capabilities may or may not be utilized by all, or even most mesos clusters. However, it should be possible for processes running in the cluster to discover capabilities or features of frameworks in order to achieve a higher level of functionality and a more seamless integration experience across the cluster.

A rich discovery API attached to the FrameworkInfo could result in some form of early lock-in: there are probably many ways to realize cross-framework integration and external services integration that we haven't considered yet. Rather than over-specify a discovery info message type at the framework level I think FrameworkInfo should expose a **very generic** way to supply metadata for interested consumers (other processes, tasks, etc).

Adding a Labels field to FrameworkInfo reuses an existing message type and seems to fit well with the overall intent: attaching generic metadata to a framework instance. These labels should be visible when querying a mesos master's state.json endpoint.",Improvement,Major,jdef,2015-08-14T13:37:48.000+0000,5,Resolved,Complete,"FrameworkInfo should include a Labels field to support arbitrary, lightweight metadata",2015-09-10T12:11:27.000+0000,MESOS-2841,8.0,mesos,
xujyan,2015-06-09T22:11:56.000+0000,xujyan,"As shown here: https://github.com/apache/mesos/blob/8559d7b7356ec91795e564767588c6f4519653a5/src/common/http.cpp#L50

So if there are two ""cpus"" of different roles, whichever comes later will overwrite the previous.

We should instead aggregate different resources of the same name.

However, in the presence of revocable resources, in order to maintain backwards compatibility we should exclude revocable resources.",Bug,Major,xujyan,2015-06-12T00:46:28.000+0000,5,Resolved,Complete,In Resources JSON model() resources of the same name overwrite each other.,2015-07-22T17:21:19.000+0000,MESOS-2838,2.0,mesos,Twitter Mesos Q2 Sprint 5
pbrett,2015-06-09T21:54:26.000+0000,pbrett,Decode network statistics from mesos-network-helper and output to slave statistics.json,Improvement,Major,pbrett,2015-06-17T23:33:46.000+0000,5,Resolved,Complete,Decode network statistics from mesos-network-helper,2015-06-18T18:20:26.000+0000,MESOS-2837,1.0,mesos,Twitter Mesos Q2 Sprint 5
pbrett,2015-06-09T21:52:23.000+0000,pbrett,Report per-container metrics for network bandwidth throttling to the slave in the output of mesos-network-helper.,Improvement,Major,pbrett,2015-06-17T23:33:17.000+0000,5,Resolved,Complete,Report per-container metrics for network bandwidth throttling to the slave,2015-06-18T18:21:13.000+0000,MESOS-2836,1.0,mesos,Twitter Mesos Q2 Sprint 5
pbrett,2015-06-09T04:32:10.000+0000,idownes,"The output format of perf changes in 3.14 (inserting an additional field) and in again in 4.1 (appending additional) fields. See kernel commits:
410136f5dd96b6013fe6d1011b523b1c247e1ccb
d73515c03c6a2706e088094ff6095a3abefd398b

Update the perf::parse() function to understand all these formats.",Improvement,Major,idownes,2015-09-21T16:45:10.000+0000,5,Resolved,Complete,Support different perf output formats,2016-03-04T06:59:36.000+0000,MESOS-2834,3.0,mesos,Twitter Mesos Q2 Sprint 6
benjaminhindman,2015-06-08T20:21:18.000+0000,cmaloney,"Currently if mesos is configured with environment variables (MESOS_MODULES), those show up in every task which is launched unless the executor explicitly cleans them up. 

If the task being launched happens to be something libprocess / mesos based, this can often prevent the task from starting up (A scheduler has issues loading a module intended for the slave).

There are also cases where it would be nice to be able to change what the PATH is that tasks launch with (the host may have more in the path than tasks are supposed to / allowed to depend upon).",Wish,Critical,cmaloney,2015-06-25T00:37:26.000+0000,5,Resolved,Complete,Enable configuring Mesos with environment variables without having them leak to tasks launched,2016-01-07T01:15:02.000+0000,MESOS-2832,8.0,mesos,
bernd-mesos,2015-06-08T19:37:51.000+0000,vinodkone,"Saw this when reviewbot was testing an unrelated review https://reviews.apache.org/r/35119/

{code}
[ RUN      ] FetcherCacheTest.SimpleEviction

GMOCK WARNING:
Uninteresting mock function call - returning directly.
    Function call: resourceOffers(0x5365320, @0x2b7bef9f1b20 { 128-byte object <B0-C0 36-E6 7B-2B 00-00 00-00 00-00 00-00 00-00 20-75 00-18 7C-2B 00-00 C0-75 00-18 7C-2B 00-00 60-76 00-18 7C-2B 00-00 00-77 00-18 7C-2B 00-00 40-3A 00-18 7C-2B 00-00 04-00 00-00 04-00 00-00 04-00 00-00 7C-2B 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 0F-00 00-00> })
Stack trace:
F0607 21:19:23.181392  4246 fetcher_cache_tests.cpp:354] CHECK_READY(offers): is PENDING Failed to wait for resource offers
*** Check failure stack trace: ***
    @     0x2b7be56c5972  google::LogMessage::Fail()
    @     0x2b7be56c58be  google::LogMessage::SendToLog()
    @     0x2b7be56c52c0  google::LogMessage::Flush()
    @     0x2b7be56c81d4  google::LogMessageFatal::~LogMessageFatal()
    @           0x97d182  _CheckFatal::~_CheckFatal()
    @           0xb58a28  mesos::internal::tests::FetcherCacheTest::launchTask()
    @           0xb65b50  mesos::internal::tests::FetcherCacheTest_SimpleEviction_Test::TestBody()
    @          0x11923b7  testing::internal::HandleSehExceptionsInMethodIfSupported<>()
    @          0x118d5b4  testing::internal::HandleExceptionsInMethodIfSupported<>()
    @          0x1175975  testing::Test::Run()
    @          0x1176098  testing::TestInfo::Run()
    @          0x1176620  testing::TestCase::Run()
    @          0x117b2ea  testing::internal::UnitTestImpl::RunAllTests()
    @          0x1193229  testing::internal::HandleSehExceptionsInMethodIfSupported<>()
    @          0x118e2a5  testing::internal::HandleExceptionsInMethodIfSupported<>()
    @          0x117a1f6  testing::UnitTest::Run()
    @           0xcc832b  main
    @     0x2b7be7d46ec5  (unknown)
    @           0x872379  (unknown)
{code}",Bug,Major,vinodkone,2015-06-22T14:59:32.000+0000,5,Resolved,Complete,FetcherCacheTest.SimpleEviction is flaky,2015-11-09T23:01:47.000+0000,MESOS-2831,0.0,mesos,
marco-mesos,2015-06-08T19:10:11.000+0000,cmaloney,"As a System Administrator often times I need to run a organization-mandated task on every machine in the cluster. Ideally I could do this within the framework of mesos resources if it is a ""cleanup"" or auditing task, but sometimes I just have to run something, and run it now, regardless if a machine has un-accounted resources  (Ex: Adding/removing a user).

Currently to do this I have to completely bypass Mesos and SSH to the box. Ideally I could tell a mesos slave (With proper authentication) to run a container with the limited special permissions needed to get the task done.",Wish,Minor,cmaloney,2015-07-20T21:19:58.000+0000,5,Resolved,Complete,Add an endpoint to slaves to allow launching system administration tasks,2015-11-18T17:28:22.000+0000,MESOS-2830,8.0,mesos,Mesosphere Sprint 15
bernd-mesos,2015-06-08T17:13:31.000+0000,vinodkone,"Observed this on internal CI

{code}
DEBUG: [ RUN      ] FetcherCacheTest.CachedFallback
DEBUG: Using temporary directory '/tmp/FetcherCacheTest_CachedFallback_dmr8bH'
DEBUG: I0608 08:32:39.091538 56580 leveldb.cpp:176] Opened db in 5.078136ms
DEBUG: I0608 08:32:39.093271 56580 leveldb.cpp:183] Compacted db in 1.696938ms
DEBUG: I0608 08:32:39.093312 56580 leveldb.cpp:198] Created db iterator in 5708ns
DEBUG: I0608 08:32:39.093325 56580 leveldb.cpp:204] Seeked to beginning of db in 940ns
DEBUG: I0608 08:32:39.093333 56580 leveldb.cpp:273] Iterated through 0 keys in the db in 270ns
DEBUG: I0608 08:32:39.093353 56580 replica.cpp:744] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned
DEBUG: I0608 08:32:39.093597 56613 recover.cpp:449] Starting replica recovery
DEBUG: I0608 08:32:39.093706 56597 recover.cpp:475] Replica is in EMPTY status
DEBUG: I0608 08:32:39.094511 56607 replica.cpp:641] Replica in EMPTY status received a broadcasted recover request
DEBUG: I0608 08:32:39.094952 56606 recover.cpp:195] Received a recover response from a replica in EMPTY status
DEBUG: I0608 08:32:39.095067 56606 recover.cpp:566] Updating replica status to STARTING
DEBUG: I0608 08:32:39.095487 56598 master.cpp:363] Master 20150608-083239-1787367596-39187-56580 (<***redacted***>) started on <***redacted***>
DEBUG: I0608 08:32:39.095510 56598 master.cpp:365] Flags at startup: --acls="""" --allocation_interval=""1secs"" --allocator=""HierarchicalDRF"" --authenticate=""true"" --authenticate_slaves=""true"" --authenticators=""crammd5"" --credentials=""/tmp/FetcherCacheTest_CachedFallback_dmr8bH/credentials"" --framework_sorter=""drf"" --help=""false"" --initialize_driver_logging=""true"" --log_auto_initialize=""true"" --logbufsecs=""0"" --logging_level=""INFO"" --quiet=""false"" --recovery_slave_removal_limit=""100%"" --registry=""replicated_log"" --registry_fetch_timeout=""1mins"" --registry_store_timeout=""25secs"" --registry_strict=""true"" --root_submissions=""true"" --slave_reregister_timeout=""10mins"" --user_sorter=""drf"" --version=""false"" --webui_dir=""/usr/local/share/mesos/webui"" --work_dir=""/tmp/FetcherCacheTest_CachedFallback_dmr8bH/master"" --zk_session_timeout=""10secs""
DEBUG: I0608 08:32:39.095630 56598 master.cpp:410] Master only allowing authenticated frameworks to register
DEBUG: I0608 08:32:39.095636 56598 master.cpp:415] Master only allowing authenticated slaves to register
DEBUG: I0608 08:32:39.095643 56598 credentials.hpp:37] Loading credentials for authentication from '/tmp/FetcherCacheTest_CachedFallback_dmr8bH/credentials'
DEBUG: I0608 08:32:39.095743 56598 master.cpp:454] Using default 'crammd5' authenticator
DEBUG: I0608 08:32:39.095782 56598 master.cpp:491] Authorization enabled
DEBUG: I0608 08:32:39.096765 56606 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 1.654474ms
DEBUG: I0608 08:32:39.096788 56606 replica.cpp:323] Persisted replica status to STARTING
DEBUG: I0608 08:32:39.096925 56612 master.cpp:1474] The newly elected leader is master@<***redacted***> with id 20150608-083239-1787367596-39187-56580
DEBUG: I0608 08:32:39.096940 56612 master.cpp:1487] Elected as the leading master!
DEBUG: I0608 08:32:39.096946 56612 master.cpp:1257] Recovering from registrar
DEBUG: I0608 08:32:39.097033 56608 registrar.cpp:313] Recovering registrar
DEBUG: I0608 08:32:39.097095 56616 recover.cpp:475] Replica is in STARTING status
DEBUG: I0608 08:32:39.097718 56607 replica.cpp:641] Replica in STARTING status received a broadcasted recover request
DEBUG: I0608 08:32:39.097949 56602 recover.cpp:195] Received a recover response from a replica in STARTING status
DEBUG: I0608 08:32:39.098423 56609 recover.cpp:566] Updating replica status to VOTING
DEBUG: I0608 08:32:39.099462 56609 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 962629ns
DEBUG: I0608 08:32:39.099488 56609 replica.cpp:323] Persisted replica status to VOTING
DEBUG: I0608 08:32:39.099527 56609 recover.cpp:580] Successfully joined the Paxos group
DEBUG: I0608 08:32:39.099577 56609 recover.cpp:464] Recover process terminated
DEBUG: I0608 08:32:39.099874 56601 log.cpp:661] Attempting to start the writer
DEBUG: I0608 08:32:39.101229 56611 replica.cpp:477] Replica received implicit promise request with proposal 1
DEBUG: I0608 08:32:39.102149 56611 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 894488ns
DEBUG: I0608 08:32:39.102175 56611 replica.cpp:345] Persisted promised to 1
DEBUG: I0608 08:32:39.102978 56612 coordinator.cpp:230] Coordinator attemping to fill missing position
DEBUG: I0608 08:32:39.104015 56612 replica.cpp:378] Replica received explicit promise request for position 0 with proposal 2
DEBUG: I0608 08:32:39.105124 56612 leveldb.cpp:343] Persisting action (8 bytes) to leveldb took 1.08594ms
DEBUG: I0608 08:32:39.105156 56612 replica.cpp:679] Persisted action at 0
DEBUG: I0608 08:32:39.106197 56596 replica.cpp:511] Replica received write request for position 0
DEBUG: I0608 08:32:39.106230 56596 leveldb.cpp:438] Reading position from leveldb took 16312ns
DEBUG: I0608 08:32:39.107139 56596 leveldb.cpp:343] Persisting action (14 bytes) to leveldb took 891559ns
DEBUG: I0608 08:32:39.107167 56596 replica.cpp:679] Persisted action at 0
DEBUG: I0608 08:32:39.107656 56601 replica.cpp:658] Replica received learned notice for position 0
DEBUG: I0608 08:32:39.109146 56601 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 1.459405ms
DEBUG: I0608 08:32:39.109180 56601 replica.cpp:679] Persisted action at 0
DEBUG: I0608 08:32:39.109194 56601 replica.cpp:664] Replica learned NOP action at position 0
DEBUG: I0608 08:32:39.109606 56604 log.cpp:677] Writer started with ending position 0
DEBUG: I0608 08:32:39.109839 56613 leveldb.cpp:438] Reading position from leveldb took 17622ns
DEBUG: I0608 08:32:39.111635 56600 registrar.cpp:346] Successfully fetched the registry (0B) in 14.553088ms
DEBUG: I0608 08:32:39.111667 56600 registrar.cpp:445] Applied 1 operations in 2865ns; attempting to update the 'registry'
DEBUG: I0608 08:32:39.112975 56601 log.cpp:685] Attempting to append 157 bytes to the log
DEBUG: I0608 08:32:39.113034 56608 coordinator.cpp:340] Coordinator attempting to write APPEND action at position 1
DEBUG: I0608 08:32:39.113862 56595 replica.cpp:511] Replica received write request for position 1
DEBUG: I0608 08:32:39.114780 56595 leveldb.cpp:343] Persisting action (176 bytes) to leveldb took 885199ns
DEBUG: I0608 08:32:39.114806 56595 replica.cpp:679] Persisted action at 1
DEBUG: I0608 08:32:39.115378 56610 replica.cpp:658] Replica received learned notice for position 1
DEBUG: I0608 08:32:39.116317 56610 leveldb.cpp:343] Persisting action (178 bytes) to leveldb took 915397ns
DEBUG: I0608 08:32:39.116343 56610 replica.cpp:679] Persisted action at 1
DEBUG: I0608 08:32:39.116356 56610 replica.cpp:664] Replica learned APPEND action at position 1
DEBUG: I0608 08:32:39.117009 56615 registrar.cpp:490] Successfully updated the 'registry' in 4.941824ms
DEBUG: I0608 08:32:39.117075 56615 registrar.cpp:376] Successfully recovered registrar
DEBUG: I0608 08:32:39.117147 56615 master.cpp:1284] Recovered 0 slaves from the Registry (119B) ; allowing 10mins for slaves to re-register
DEBUG: I0608 08:32:39.117447 56614 log.cpp:704] Attempting to truncate the log to 1
DEBUG: I0608 08:32:39.117504 56613 coordinator.cpp:340] Coordinator attempting to write TRUNCATE action at position 2
DEBUG: I0608 08:32:39.118638 56608 replica.cpp:511] Replica received write request for position 2
DEBUG: I0608 08:32:39.119679 56608 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 1.017833ms
DEBUG: I0608 08:32:39.119707 56608 replica.cpp:679] Persisted action at 2
DEBUG: I0608 08:32:39.120102 56608 replica.cpp:658] Replica received learned notice for position 2
DEBUG: I0608 08:32:39.121323 56608 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 1.201996ms
DEBUG: I0608 08:32:39.121378 56608 leveldb.cpp:401] Deleting ~1 keys from leveldb took 22912ns
DEBUG: I0608 08:32:39.121395 56608 replica.cpp:679] Persisted action at 2
DEBUG: I0608 08:32:39.121404 56608 replica.cpp:664] Replica learned TRUNCATE action at position 2
DEBUG: I0608 08:32:39.129438 56580 containerizer.cpp:111] Using isolation: posix/cpu,posix/mem
DEBUG: I0608 08:32:39.132251 56615 slave.cpp:188] Slave started on 40)@<***redacted***>
DEBUG: I0608 08:32:39.132272 56615 slave.cpp:189] Flags at startup: --authenticatee=""crammd5"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --credential=""/tmp/FetcherCacheTest_CachedFallback_P03rUd/credential"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_kill_orphans=""true"" --docker_remove_delay=""6hrs"" --docker_sandbox_directory=""/mnt/mesos/sandbox"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --egress_unique_flow_per_container=""false"" --enforce_container_disk_quota=""false"" --ephemeral_ports_per_container=""1024"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/tmp/FetcherCacheTest_CachedFallback_P03rUd/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --initialize_driver_logging=""true"" --isolation=""posix/cpu,posix/mem"" --launcher_dir=""/builddir/build/BUILD/mesos-0.23.0/src"" --logbufsecs=""0"" --logging_level=""INFO"" --network_enable_socket_statistics_details=""false"" --network_enable_socket_statistics_summary=""false"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""10ms"" --resource_monitoring_interval=""1secs"" --resources=""cpus(*):1000; mem(*):1000"" --revocable_cpu_low_priority=""true"" --strict=""true"" --switch_user=""true"" --version=""false"" --work_dir=""/tmp/FetcherCacheTest_CachedFallback_P03rUd""
DEBUG: I0608 08:32:39.132558 56615 credentials.hpp:85] Loading credential for authentication from '/tmp/FetcherCacheTest_CachedFallback_P03rUd/credential'
DEBUG: I0608 08:32:39.132689 56615 slave.cpp:319] Slave using credential for: test-principal
DEBUG: I0608 08:32:39.132869 56615 slave.cpp:352] Slave resources: cpus(*):1000; mem(*):1000; disk(*):464332; ports(*):[31000-32000]
DEBUG: I0608 08:32:39.133673 56615 slave.cpp:382] Slave hostname: <***redacted***>
DEBUG: I0608 08:32:39.133689 56615 slave.cpp:387] Slave checkpoint: true
DEBUG: I0608 08:32:39.133949 56596 state.cpp:35] Recovering state from '/tmp/FetcherCacheTest_CachedFallback_P03rUd/meta'
DEBUG: I0608 08:32:39.134446 56595 status_update_manager.cpp:201] Recovering status update manager
DEBUG: I0608 08:32:39.134539 56609 containerizer.cpp:312] Recovering containerizer
DEBUG: I0608 08:32:39.135972 56616 slave.cpp:3950] Finished recovery
DEBUG: I0608 08:32:39.136243 56616 slave.cpp:4104] Querying resource estimator for oversubscribable resources
DEBUG: I0608 08:32:39.136576 56599 status_update_manager.cpp:175] Pausing sending status updates
DEBUG: I0608 08:32:39.136595 56602 slave.cpp:678] New master detected at master@<***redacted***>
DEBUG: I0608 08:32:39.136620 56602 slave.cpp:741] Authenticating with master master@<***redacted***>
DEBUG: I0608 08:32:39.136628 56602 slave.cpp:746] Using default CRAM-MD5 authenticatee
DEBUG: I0608 08:32:39.136670 56602 slave.cpp:714] Detecting new master
DEBUG: I0608 08:32:39.136698 56602 slave.cpp:4125] Received oversubscribable resources  from the resource estimator
DEBUG: I0608 08:32:39.136703 56602 slave.cpp:4129] No master detected. Re-querying resource estimator after 15secs
DEBUG: I0608 08:32:39.136778 56618 authenticatee.hpp:139] Creating new client SASL connection
DEBUG: I0608 08:32:39.137074 56618 master.cpp:4167] Authenticating slave(40)@<***redacted***>
DEBUG: I0608 08:32:39.137186 56611 authenticator.cpp:92] Creating new server SASL connection
DEBUG: I0608 08:32:39.137485 56605 authenticatee.hpp:230] Received SASL authentication mechanisms: CRAM-MD5
DEBUG: I0608 08:32:39.137503 56605 authenticatee.hpp:256] Attempting to authenticate with mechanism 'CRAM-MD5'
DEBUG: I0608 08:32:39.137540 56605 authenticator.cpp:197] Received SASL authentication start
DEBUG: I0608 08:32:39.137615 56605 authenticator.cpp:319] Authentication requires more steps
DEBUG: I0608 08:32:39.137645 56605 authenticatee.hpp:276] Received SASL authentication step
DEBUG: I0608 08:32:39.137817 56609 authenticator.cpp:225] Received SASL authentication step
DEBUG: I0608 08:32:39.137866 56609 authenticator.cpp:311] Authentication success
DEBUG: I0608 08:32:39.137989 56609 master.cpp:4197] Successfully authenticated principal 'test-principal' at slave(40)@<***redacted***>
DEBUG: I0608 08:32:39.138123 56601 authenticatee.hpp:316] Authentication success
DEBUG: I0608 08:32:39.138363 56601 slave.cpp:812] Successfully authenticated with master master@<***redacted***>
DEBUG: I0608 08:32:39.138468 56596 master.cpp:3149] Registering slave at slave(40)@<***redacted***> (<***redacted***>) with id 20150608-083239-1787367596-39187-56580-S0
DEBUG: I0608 08:32:39.138628 56614 registrar.cpp:445] Applied 1 operations in 11844ns; attempting to update the 'registry'
DEBUG: I0608 08:32:39.140049 56596 log.cpp:685] Attempting to append 351 bytes to the log
DEBUG: I0608 08:32:39.140110 56600 coordinator.cpp:340] Coordinator attempting to write APPEND action at position 3
DEBUG: I0608 08:32:39.141154 56596 replica.cpp:511] Replica received write request for position 3
DEBUG: I0608 08:32:39.142328 56596 leveldb.cpp:343] Persisting action (370 bytes) to leveldb took 1.151559ms
DEBUG: I0608 08:32:39.142359 56596 replica.cpp:679] Persisted action at 3
DEBUG: I0608 08:32:39.142838 56607 replica.cpp:658] Replica received learned notice for position 3
DEBUG: I0608 08:32:39.144026 56607 leveldb.cpp:343] Persisting action (372 bytes) to leveldb took 1.167494ms
DEBUG: I0608 08:32:39.144060 56607 replica.cpp:679] Persisted action at 3
DEBUG: I0608 08:32:39.144073 56607 replica.cpp:664] Replica learned APPEND action at position 3
DEBUG: I0608 08:32:39.144342 56595 registrar.cpp:490] Successfully updated the 'registry' in 5696us
DEBUG: I0608 08:32:39.144366 56607 log.cpp:704] Attempting to truncate the log to 3
DEBUG: I0608 08:32:39.144423 56614 coordinator.cpp:340] Coordinator attempting to write TRUNCATE action at position 4
DEBUG: I0608 08:32:39.144631 56607 master.cpp:3206] Registered slave 20150608-083239-1787367596-39187-56580-S0 at slave(40)@<***redacted***> (<***redacted***>) with cpus(*):1000; mem(*):1000; disk(*):464332; ports(*):[31000-32000]
DEBUG: I0608 08:32:39.144726 56617 hierarchical.hpp:496] Added slave 20150608-083239-1787367596-39187-56580-S0 (<***redacted***>) with cpus(*):1000; mem(*):1000; disk(*):464332; ports(*):[31000-32000] (and cpus(*):1000; mem(*):1000; disk(*):464332; ports(*):[31000-32000] available)
DEBUG: I0608 08:32:39.144745 56609 slave.cpp:846] Registered with master master@<***redacted***>; given slave ID 20150608-083239-1787367596-39187-56580-S0
DEBUG: I0608 08:32:39.144872 56612 status_update_manager.cpp:182] Resuming sending status updates
DEBUG: I0608 08:32:39.145023 56602 replica.cpp:511] Replica received write request for position 4
DEBUG: GMOCK WARNING:
DEBUG: Uninteresting mock function call - returning directly.
DEBUG:     Function call: resourceOffers(0x3754860, @0x7fad6e321b30 { 128-byte object <50-DB E0-80 AD-7F 00-00 00-00 00-00 00-00 00-00 D0-78 00-04 AD-7F 00-00 70-79 00-04 AD-7F 00-00 10-7A 00-04 AD-7F 00-00 B0-7A 00-04 AD-7F 00-00 A0-6B 00-04 AD-7F 00-00 04-00 00-00 04-00 00-00 04-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 0F-00 00-00> })
DEBUG: Stack trace:
DEBUG: I0608 08:32:39.146750 56580 sched.cpp:157] Version: 0.23.0-rc0
DEBUG: I0608 08:32:39.147104 56613 sched.cpp:254] New master detected at master@<***redacted***>
DEBUG: I0608 08:32:39.147121 56613 sched.cpp:310] Authenticating with master master@<***redacted***>
DEBUG: I0608 08:32:39.147128 56613 sched.cpp:317] Using default CRAM-MD5 authenticatee
DEBUG: I0608 08:32:39.147186 56613 authenticatee.hpp:139] Creating new client SASL connection
DEBUG: I0608 08:32:39.147374 56595 master.cpp:4167] Authenticating scheduler-4b62df2f-7cdc-427c-8301-0e282d99bc06@<***redacted***>
DEBUG: I0608 08:32:39.147596 56595 authenticator.cpp:92] Creating new server SASL connection
DEBUG: I0608 08:32:39.147666 56595 authenticatee.hpp:230] Received SASL authentication mechanisms: CRAM-MD5
DEBUG: I0608 08:32:39.147678 56595 authenticatee.hpp:256] Attempting to authenticate with mechanism 'CRAM-MD5'
DEBUG: I0608 08:32:39.147701 56595 authenticator.cpp:197] Received SASL authentication start
DEBUG: I0608 08:32:39.147738 56595 authenticator.cpp:319] Authentication requires more steps
DEBUG: I0608 08:32:39.147789 56615 authenticatee.hpp:276] Received SASL authentication step
DEBUG: I0608 08:32:39.147853 56615 authenticator.cpp:225] Received SASL authentication step
DEBUG: I0608 08:32:39.147881 56615 authenticator.cpp:311] Authentication success
DEBUG: I0608 08:32:39.147923 56615 authenticatee.hpp:316] Authentication success
DEBUG: I0608 08:32:39.148066 56617 master.cpp:4197] Successfully authenticated principal 'test-principal' at scheduler-4b62df2f-7cdc-427c-8301-0e282d99bc06@<***redacted***>
DEBUG: I0608 08:32:39.148905 56598 sched.cpp:398] Successfully authenticated with master master@<***redacted***>
DEBUG: I0608 08:32:39.149000 56608 master.cpp:1714] Received registration request for framework 'default' at scheduler-4b62df2f-7cdc-427c-8301-0e282d99bc06@<***redacted***>
DEBUG: W0608 08:32:39.149026 56608 master.cpp:1537] Framework at scheduler-4b62df2f-7cdc-427c-8301-0e282d99bc06@<***redacted***> (authenticated as 'test-principal') does not specify principal in its FrameworkInfo
DEBUG: I0608 08:32:39.149039 56608 master.cpp:1553] Authorizing framework principal '' to receive offers for role '*'
DEBUG: I0608 08:32:39.149502 56618 master.cpp:1781] Registering framework 20150608-083239-1787367596-39187-56580-0000 (default) at scheduler-4b62df2f-7cdc-427c-8301-0e282d99bc06@<***redacted***>
DEBUG: I0608 08:32:39.149587 56618 hierarchical.hpp:354] Added framework 20150608-083239-1787367596-39187-56580-0000
DEBUG: I0608 08:32:39.149668 56614 sched.cpp:448] Framework registered with 20150608-083239-1787367596-39187-56580-0000
DEBUG: I0608 08:32:39.149765 56615 master.cpp:4086] Sending 1 offers to framework 20150608-083239-1787367596-39187-56580-0000 (default) at scheduler-4b62df2f-7cdc-427c-8301-0e282d99bc06@<***redacted***>
DEBUG: I0608 08:32:39.153530 56602 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 8.487561ms
DEBUG: I0608 08:32:39.153561 56602 replica.cpp:679] Persisted action at 4
DEBUG: I0608 08:32:39.154438 56603 replica.cpp:658] Replica received learned notice for position 4
DEBUG: I0608 08:32:39.155531 56603 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 1.074345ms
DEBUG: I0608 08:32:39.155571 56603 leveldb.cpp:401] Deleting ~2 keys from leveldb took 16036ns
DEBUG: I0608 08:32:39.155580 56603 replica.cpp:679] Persisted action at 4
DEBUG: I0608 08:32:39.155587 56603 replica.cpp:664] Replica learned TRUNCATE action at position 4
DEBUG: I0608 08:32:54.136912 56596 slave.cpp:4104] Querying resource estimator for oversubscribable resources
DEBUG: I0608 08:32:54.137251 56611 slave.cpp:4125] Received oversubscribable resources  from the resource estimator
DEBUG: F0608 08:32:54.154590 56580 fetcher_cache_tests.cpp:354] CHECK_READY(offers): is PENDING Failed to wait for resource offers
DEBUG: *** Check failure stack trace: ***
DEBUG:     @     0x7fad806b9f1d  google::LogMessage::Fail()
DEBUG:     @     0x7fad806bbd5d  google::LogMessage::SendToLog()
DEBUG:     @     0x7fad806b9b0c  google::LogMessage::Flush()
DEBUG:     @     0x7fad806bc659  google::LogMessageFatal::~LogMessageFatal()
DEBUG:     @           0x53d858  _CheckFatal::~_CheckFatal()
DEBUG:     @           0x66c14f  mesos::internal::tests::FetcherCacheTest::launchTask()
DEBUG:     @           0x66f9e9  mesos::internal::tests::FetcherCacheTest_CachedFallback_Test::TestBody()
DEBUG:     @           0xba34d3  testing::internal::HandleExceptionsInMethodIfSupported<>()
DEBUG:     @           0xb9a777  testing::Test::Run()
DEBUG:     @           0xb9a81e  testing::TestInfo::Run()
DEBUG:     @           0xb9a925  testing::TestCase::Run()
DEBUG:     @           0xb9abc8  testing::internal::UnitTestImpl::RunAllTests()
DEBUG:     @           0xb9ae67  testing::UnitTest::Run()
DEBUG:     @           0x4a1e73  main
DEBUG:     @     0x7fad7e4e2d5d  __libc_start_main
DEBUG:     @           0x4acf79  (unknown)
DEBUG: make[3]: *** [check-local] Aborted (core dumped)
{code}",Bug,Major,vinodkone,2015-06-22T15:00:44.000+0000,5,Resolved,Complete,FetcherCacheTest.CachedFallback test is flaky,2015-06-22T15:00:44.000+0000,MESOS-2829,0.0,mesos,
gyliu,2015-06-07T05:33:58.000+0000,idownes,"Default container images can be specified with the --default_container_info flag to the slave. This may be a large image that will take a long time to initially fetch/hash/extract when the first container is provisioned. Add optional support to start fetching the image when the slave starts and consider not registering until the fetch is complete.

To extend that, we should support an operator endpoint so that operators can specify images to pre-fetch.",Improvement,Minor,idownes,,10020,Accepted,In Progress,Support pre-fetching images,2016-03-22T13:49:40.000+0000,MESOS-2824,5.0,mesos,Twitter Mesos Q2 Sprint 5
Bartek Plotka,2015-06-05T23:36:22.000+0000,Bartek Plotka,"We need to allow QoS Controller to call 'ResourceMonitor::usages()'. We will pass it in a lambda.
",Task,Major,Bartek Plotka,2015-06-12T20:49:54.000+0000,5,Resolved,Complete,Pass callback to the QoS Controller to retrieve ResourceUsage from Resource Monitor on demand.,2015-06-12T20:49:55.000+0000,MESOS-2823,2.0,mesos,
,2015-06-05T21:28:37.000+0000,xujyan,"We already have {{EXPECT_NO_FUTURE_MESSAGES}}, {{EXPECT_NO_FUTURE_DISPATCHES}} should be done the same way.

We already have a use case for it: https://github.com/apache/mesos/blob/master/src/tests/master_contender_detector_tests.cpp#L251",Improvement,Minor,xujyan,,1,Open,New,Add `EXPECT_NO_FUTURE_DISPATCHES` macro for tests.,2015-06-05T21:28:37.000+0000,MESOS-2822,1.0,mesos,
pbrett,2015-06-05T20:29:39.000+0000,pbrett,The structure of traffic control qdiscs and filters in non-trivial with the knowledge of which handles are the parents of which filters or qdiscs are in the create and recovery functions and will be needed to collect statistics on the links.  Lets pull out the constants and document them.,Improvement,Major,pbrett,2015-06-08T20:02:49.000+0000,5,Resolved,Complete,Document and consolidate qdisc handles,2015-07-02T18:43:32.000+0000,MESOS-2821,1.0,mesos,Twitter Mesos Q2 Sprint 5
jieyu,2015-06-05T00:47:42.000+0000,jieyu,"Resource estimator obviously need this information to calculate, say the usage slack. Now the question is how. There are two approaches:

1) Pass in the allocated resources for each executor through the 'oversubscribable()' interface.

2) Let containerizer return total resources allocated for each container when 'usages()' are invoked.

I would suggest to take route (1) for several reasons:

1) Eventually, we'll need to pass in slave's total resources to the resource estimator (so that RE can calculate allocation slack). There is no way that we can get that from containerizer. The slave's total resources keep changing due to dynamic reservation. So we cannot pass in the slave total resources during initialization.

2) The current implementation of usages() might skip some containers if it fails to get statistics for that container (not an error). This will cause in-complete information to the RE.

3) We may want to calculate 'unallocated = total - allocated' so that we can send allocation slack as well. Getting 'total' and 'allocated' from two different components might result in inconsistent value. Remember that 'total' keeps changing due to dynamic reservation.",Task,Major,jieyu,2015-06-11T18:38:02.000+0000,5,Resolved,Complete,Pass 'allocated' resources for each executor to the resource estimator.,2015-06-15T22:35:40.000+0000,MESOS-2818,3.0,mesos,Twitter Q2 Sprint 3
idownes,2015-06-04T19:16:00.000+0000,idownes,"MESOS-2652 provided preliminary support for revocable cpu resources only when specified in the initial resources for a container. Improve this to support updates to/from revocable cpu. Note, *any* revocable cpu will result in the entire container's cpu being treated as revocable at the cpu isolator level. Higher level logic is responsible for adding/removing based on some policy.",Improvement,Major,idownes,2015-06-08T18:23:05.000+0000,5,Resolved,Complete,Support revocable/non-revocable CPU updates in Mesos containerizer,2015-06-08T18:23:05.000+0000,MESOS-2817,3.0,mesos,Twitter Q2 Sprint 3
bernd-mesos,2015-06-04T14:11:55.000+0000,bernd-mesos,"FetcherCacheHttpTest.HttpCachedSerialized has been observed to fail (once  so far), but normally works fine. Here is the failure output:

[ RUN      ] FetcherCacheHttpTest.HttpCachedSerialized

GMOCK WARNING:
Uninteresting mock function call - returning directly.
    Function call: resourceOffers(0x3cca8e0, @0x2b1053422b20 { 128-byte object <D0-E1 59-49 10-2B 00-00 00-00 00-00 00-00 00-00 10-A1 00-68 10-2B 00-00 A0-DE 00-68 10-2B 00-00 20-DF 00-68 10-2B 00-00 40-9C 00-68 10-2B 00-00 90-2D 00-68 10-2B 00-00 04-00 00-00 04-00 00-00 04-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 10-2B 00-00 00-00 00-00 0F-00 00-00> })
Stack trace:
F0604 13:08:16.377907  6813 fetcher_cache_tests.cpp:354] CHECK_READY(offers): is PENDING Failed to wait for resource offers
*** Check failure stack trace: ***
    @     0x2b10488ff6c0  google::LogMessage::Fail()
    @     0x2b10488ff60c  google::LogMessage::SendToLog()
    @     0x2b10488ff00e  google::LogMessage::Flush()
    @     0x2b1048901f22  google::LogMessageFatal::~LogMessageFatal()
    @           0x9721e4  _CheckFatal::~_CheckFatal()
    @           0xb4da86  mesos::internal::tests::FetcherCacheTest::launchTask()
    @           0xb53f8d  mesos::internal::tests::FetcherCacheHttpTest_HttpCachedSerialized_Test::TestBody()
    @          0x116ac21  testing::internal::HandleSehExceptionsInMethodIfSupported<>()
    @          0x1165e1e  testing::internal::HandleExceptionsInMethodIfSupported<>()
    @          0x114e1df  testing::Test::Run()
    @          0x114e902  testing::TestInfo::Run()
    @          0x114ee8a  testing::TestCase::Run()
    @          0x1153b54  testing::internal::UnitTestImpl::RunAllTests()
    @          0x116ba93  testing::internal::HandleSehExceptionsInMethodIfSupported<>()
    @          0x1166b0f  testing::internal::HandleExceptionsInMethodIfSupported<>()
    @          0x1152a60  testing::UnitTest::Run()
    @           0xcbc50f  main
    @     0x2b104af78ec5  (unknown)
    @           0x867559  (unknown)
make[4]: *** [check-local] Aborted
make[4]: Leaving directory `/home/jenkins/jenkins-slave/workspace/mesos-reviewbot/mesos-0.23.0/_build/src'
make[3]: *** [check-am] Error 2
make[3]: Leaving directory `/home/jenkins/jenkins-slave/workspace/mesos-reviewbot/mesos-0.23.0/_build/src'
make[2]: *** [check] Error 2
make[2]: Leaving directory `/home/jenkins/jenkins-slave/workspace/mesos-reviewbot/mesos-0.23.0/_build/src'
make[1]: *** [check-recursive] Error 1
make[1]: Leaving directory `/home/jenkins/jenkins-slave/workspace/mesos-reviewbot/mesos-0.23.0/_build'
make: *** [distcheck] Error 1
",Bug,Minor,bernd-mesos,2015-06-22T15:01:50.000+0000,5,Resolved,Complete,Flaky test: FetcherCacheHttpTest.HttpCachedSerialized,2015-11-23T12:52:27.000+0000,MESOS-2815,2.0,mesos,
ijimenez,2015-06-04T02:48:43.000+0000,cmaloney,"In master there are currently three implementations of the function:
 https://github.com/apache/mesos/blob/master/3rdparty/libprocess/3rdparty/stout/include/stout/os/read.hpp#L42
https://github.com/apache/mesos/blob/master/3rdparty/libprocess/3rdparty/stout/include/stout/os/read.hpp#L82
https://github.com/apache/mesos/blob/master/3rdparty/libprocess/3rdparty/stout/include/stout/os/read.hpp#L42

All of them have fairly radically different implementations (One uses C read(), one uses c++ ifstream, one uses c fopen)

The read() based one does an excess / unnecessary copy / buffer allocation (it is going to read into one temporary buffer, then copy into the result string. Would be more efficient to do a .reserve() on the result string, and then fill the result buffer).

The ifstream/ifstreambuf_iterator ignores that you can have an error partially through reading a file / doesn't find the error or propagate it up.

The fopen() variant reads one newline separated line at a time. This could produce interesting / unexpected reading in the context of a binary file. It also causes glibc to insert null bytes at the end of the buffer it reads (excess computation). result isn't pre-allocated to be the right length, meaning that most of the continually read lines will result in realloc() and a lot of memory copies which will be inefficient on large files.",Improvement,Major,cmaloney,,10020,Accepted,In Progress,os::read should have one implementation,2016-02-18T08:20:31.000+0000,MESOS-2814,3.0,mesos,
vinodkone,2015-06-03T20:59:30.000+0000,vinodkone,"Currently, the polling of resource estimator is decoupled from the loop in the slave that forwards oversubscribed resources.

Now that the slave only sends updates when there is a change from the previous estimate, it can just poll the resource estimator whenever it wants to send an estimate. One advantage with this is that if the estimator is slow to respond, the slave doesn't keep forwarding estimates with the stale 'oversubscribable' value causing more revocable tasks to be unintentionally launched.",Bug,Major,vinodkone,2015-06-04T00:45:23.000+0000,5,Resolved,Complete,Slave should call into resource estimator whenever it wants to forward oversubscribed resources,2015-06-04T00:45:23.000+0000,MESOS-2808,3.0,mesos,Twitter Q2 Sprint 3
marco-mesos,2015-06-03T19:09:50.000+0000,marco-mesos,"As a preliminary to MESOS-2340, this requires the implementation of a simple (de)serialization mechanism to JSON from/to {{MasterInfo}} protobuf.",Task,Major,marco-mesos,2015-06-19T18:26:11.000+0000,5,Resolved,Complete,As a developer I need an easy way to convert MasterInfo protobuf to/from JSON,2015-06-19T18:26:11.000+0000,MESOS-2807,3.0,mesos,Mesosphere Sprint 11
marco-mesos,2015-06-03T18:27:57.000+0000,marco-mesos,"See attached screenshot - the story is in the {{Accepted}} state, so it should now have a {{Start Progress}} button, but it has a {{Stop Progress}} one instead.

Also, when in the {{In Progress}} it has an {{Accept}} button (I think) or something similar; also other states appear inconsistent.

This Story is about first looking at the workflow; ensuring the stories and their status(es) are consistent; that button in the UI are consistently applied and then correct any issues that may have been identified.

The assumption here is that the workflow is:
{noformat}
Open >> Accepted >> Progress >> Reviewable >> Resolved >> Closed
   Accept       Start      Ready         Resolve      Close
{noformat}
and, at each stage, it can be moved ""back by one"" ({{Unaccept}}, {{Stop Progress}}, {{Unresolve}}) and that, at any stage, it can be moved to {{Closed}} (for whatever reason).",Task,Major,marco-mesos,2015-06-18T17:40:49.000+0000,5,Resolved,Complete,Jira workflow appears inconsistent,2015-07-21T19:57:10.000+0000,MESOS-2806,2.0,mesos,
jvanremoortere,2015-06-03T17:10:57.000+0000,jvanremoortere,"Re-organize Synchronized to allow {{synchronized(m)}} to work on:
  1. {{std::mutex}}
  2. {{std::recursive_mutex}}
  3. {{std::atomic_flag}}

Move synchronized.hpp into stout, so that developers don't think it's part of the utility suite for actors in libprocess.

Remove references to internal.hpp and replace them with {{std::atomic_flag}} synchronization.",Improvement,Major,jvanremoortere,2015-06-19T18:17:07.000+0000,5,Resolved,Complete,Make synchronized as primary form of synchronization.,2015-06-19T18:17:08.000+0000,MESOS-2805,8.0,mesos,Mesosphere Sprint 11
bmahler,2015-06-03T00:43:31.000+0000,bmahler,"Now that {{Capabilities}} has been added to FrameworkInfo, we should log these in the master when a framework (re-)registers (i.e. which capabilities are enabled and disabled). This would make debugging easier for framework developers.

Ideally, folding in the old {{checkpoint}} capability and logging that as well. In the past, the fact that {{checkpoint}} defaults to false has tripped up a lot of developers.",Improvement,Minor,bmahler,2015-06-10T00:41:03.000+0000,5,Resolved,Complete,Log framework capabilities in the master.,2015-06-10T00:41:03.000+0000,MESOS-2804,1.0,mesos,Twitter Mesos Q2 Sprint 5
jvanremoortere,2015-06-02T08:02:47.000+0000,jvanremoortere,Remove the dynamic allocation of `T*` inside `Future::Data`,Improvement,Major,jvanremoortere,2015-06-19T18:54:29.000+0000,5,Resolved,Complete,Remove dynamic allocation from Future<T>,2015-06-19T18:54:29.000+0000,MESOS-2801,3.0,mesos,Mesosphere Sprint 11
balamark,2015-06-02T06:11:08.000+0000,balamark,"As suggested, if we want to change the name then we should refactor the original function as opposed to having 2 copies. 
If we did have 2 versions of the same function, would it make more sense to delegate one of them to the other.

As of today, there is only one file need to be refactor: 3rdparty/libprocess/3rdparty/stout/include/stout/os/osx.hpp at line 151, 161",Improvement,Minor,balamark,2015-07-06T20:44:29.000+0000,5,Resolved,Complete,Rename Option<T>::get(const T& _t) to getOrElse() and refactor the original function,2015-07-07T19:56:39.000+0000,MESOS-2800,3.0,mesos,Mesosphere Sprint 14
xujyan,2015-06-01T21:28:39.000+0000,idownes,Implement a filesystem provisioner that can provision container images compliant with the Application Container Image (aci) [specification|https://github.com/appc/spec].,Improvement,Major,idownes,2015-09-11T22:48:18.000+0000,5,Resolved,Complete,Implement AppC image provisioner.,2015-11-17T00:05:00.000+0000,MESOS-2796,5.0,mesos,Twitter Mesos Q3 Sprint 4
idownes,2015-06-01T21:25:55.000+0000,idownes,"Optional filesystem provisioner component for the Mesos containerizer that can provision per-container filesystems.

This is different to a filesystem isolators because it just provisions a root filesystem for a container and doesn't actually do any isolation (e.g., through a mount namespace + pivot or chroot).",Improvement,Major,idownes,2015-07-28T18:42:15.000+0000,5,Resolved,Complete,Introduce filesystem provisioner abstraction,2015-08-03T19:05:45.000+0000,MESOS-2795,5.0,mesos,Twitter Q2 Sprint 3
jieyu,2015-06-01T21:23:33.000+0000,idownes,"Move persistent volume support from Mesos containerizer to separate filesystem isolators, including support for container rootfs, where possible.

Use symlinks for posix systems without container rootfs. Use bind mounts for Linux with/without container rootfs.",Improvement,Major,idownes,2015-08-13T00:30:06.000+0000,5,Resolved,Complete,Implement filesystem isolators,2015-08-13T00:30:06.000+0000,MESOS-2794,13.0,mesos,Twitter Q2 Sprint 3
idownes,2015-06-01T21:18:59.000+0000,idownes,Mesos containers can have a different rootfs to the host. Update Isolator interface to pass rootfs during Isolator::prepare(). Update Isolators where  necessary.,Improvement,Major,idownes,2015-06-22T18:19:49.000+0000,5,Resolved,Complete,Add support for container rootfs to Mesos isolators,2015-06-22T18:20:02.000+0000,MESOS-2793,1.0,mesos,Twitter Mesos Q2 Sprint 6
pbrett,2015-06-01T19:05:07.000+0000,pbrett,"fq_codel and ingress queueing disciplines include multiple uses of the string literals ""ingress"" and ""fq_codel"".  Any mismatch in these would cause runtime errors which can be prevented at compile time.",Bug,Major,pbrett,2015-06-03T17:11:01.000+0000,5,Resolved,Complete,Remove duplicate literals in ingress & fq_codel queueing disciplines,2015-06-03T17:11:01.000+0000,MESOS-2792,1.0,mesos,Twitter Q2 Sprint 3
jieyu,2015-06-01T18:52:18.000+0000,jieyu,"This will be useful for testing oversubscription in a real environment. Also, it will be useful for people who has a prior knowledge about the amount of resources that can be safely oversubscribed on each slave.",Task,Major,jieyu,2015-06-11T18:38:26.000+0000,5,Resolved,Complete,Create a FixedResourceEstimator to return fixed amount of oversubscribable resources.,2015-06-11T18:38:27.000+0000,MESOS-2791,5.0,mesos,Twitter Q2 Sprint 3
pbrett,2015-05-29T23:05:30.000+0000,pbrett,constexpr is currently used to eliminate initialization dependency issues for non-POD objects.  We should add it to the whitelist of acceptable c++11 features in the style guide.,Improvement,Major,pbrett,2015-07-24T01:13:33.000+0000,5,Resolved,Complete,Added constexpr to C++11 whitelist.,2015-07-24T01:13:33.000+0000,MESOS-2784,1.0,mesos,Twitter Q2 Sprint 3
bernd-mesos,2015-05-29T16:12:16.000+0000,air,"For framework developers specifically, Mesos provides a fetcher to move binaries. This needs MVP documentation.

- What is it
- How does it help
- What protocols or schemas are supported
- Can it be extended

This is important to get framework developers over the hump of learning to code against Mesos and grow the ecosystem.",Documentation,Major,air,2015-11-10T10:16:47.000+0000,5,Resolved,Complete,document the fetcher,2015-11-10T10:16:47.000+0000,MESOS-2783,5.0,mesos,Mesosphere Sprint 11
pbrett,2015-05-28T22:24:46.000+0000,pbrett,The getQdisc function ignores the passed link parameter and returns the first qdisc of the required type from any available interface.,Bug,Major,pbrett,2015-06-03T17:09:52.000+0000,5,Resolved,Complete,getQdisc function in routing::queueing::internal.cpp returns incorrect qdisc,2015-06-03T17:09:52.000+0000,MESOS-2781,1.0,mesos,Twitter Q2 Sprint 3
pbrett,2015-05-28T19:54:51.000+0000,pbrett,"We declare const non-POD static variables for the following:

fq_codel::HANDLE
ingress::ROOT
ingress::HANDLE

We can eliminate the risk of indeterminate initialization by converting to C++11 constexpr",Bug,Major,pbrett,2015-05-29T21:22:19.000+0000,5,Resolved,Complete,Non-POD static variables used in fq_codel and ingress.,2015-05-29T21:22:19.000+0000,MESOS-2778,1.0,mesos,Twitter Q2 Sprint 3
xujyan,2015-05-27T19:20:34.000+0000,vinodkone,"metrics/snapshot should expose metrics on oversubscribed resources (allocated and available).
",Task,Major,vinodkone,2015-06-09T22:15:16.000+0000,5,Resolved,Complete,Master should expose metrics about oversubscribed resources,2015-07-02T21:59:33.000+0000,MESOS-2776,5.0,mesos,Twitter Mesos Q2 Sprint 5
bmahler,2015-05-27T19:16:34.000+0000,vinodkone,metrics/snapshot should expose metrics on oversubscribed resources (allocated and available). ,Task,Major,vinodkone,2015-06-10T22:10:43.000+0000,5,Resolved,Complete,Slave should expose metrics about oversubscribed resources,2015-06-10T22:10:43.000+0000,MESOS-2775,2.0,mesos,Twitter Mesos Q2 Sprint 5
Bartek Plotka,2015-05-27T01:05:14.000+0000,Bartek Plotka,We need to expose ResourceMonitor::Usage so that module writers can access it. We will define a protobuf message for that.,Task,Major,Bartek Plotka,2015-06-02T23:01:09.000+0000,5,Resolved,Complete,Define protobuf for ResourceMonitor::Usage.,2015-06-02T23:01:09.000+0000,MESOS-2772,1.0,mesos,
nnielsen,2015-05-26T23:03:06.000+0000,xujyan,"Observed in production.

{noformat:title=slave log}
I0523 17:03:59.830229 56587 port_mapping.cpp:2616] Freed ephemeral ports [33792,34816) for container with pid 47791
I0523 17:03:59.849773 56587 port_mapping.cpp:2764] Successfully performed cleanup for pid 47791
*** Aborted at 1432400641 (unix time) try ""date -d @1432400641"" if you are using GNU date ***
PC: @     0x7f100fcbfd85 _ZNSt17_Function_handlerIFvRKSsEZNK7process6FutureIN5mesos8internal5slave15ResourceMonitor5UsageEE8onFailedIZNS7_22ResourceMonitorProcess5usageENS5_11ContainerIDEEUlS1_E_vEERKSA_OT_NSA_6PreferEEUlS1_E_E9_M_invokeERKSt9_Any_dataS1_
I0523 17:03:59.898959 56587 slave.cpp:3246] Executor 'thermos-1432400210944-mesos-test-exhaust_diskspace-5-4744d0fb-e0a1-4e40-bb22-56bd5cbd9524' of framework 201103282247-0000000019-0000 terminated with signal Killed
I0523 17:04:03.419869 56587 slave.cpp:2547] Handling status update TASK_FAILED (UUID: 3be19404-f737-4a70-a330-d1d924a85dbb) for task 1432400210944-mesos-test-exhaust_diskspace-5-4744d0fb-e0a1-4e40-bb22-56bd5cbd9524 of framework 201103282247-0000000019-0000 from @0.0.0.0:0
I0523 17:04:03.773061 56587 slave.cpp:4077] Received a new estimation of the oversubscribable resources 
I0523 17:04:03.773907 56587 slave.cpp:4077] Received a new estimation of the oversubscribable resources 
I0523 17:04:03.774683 56587 slave.cpp:4077] Received a new estimation of the oversubscribable resources 
I0523 17:04:03.776345 56587 slave.cpp:4077] Received a new estimation of the oversubscribable resources 
*** SIGSEGV (@0x0) received by PID 56573 (TID 0x7f100a190940) from PID 0; stack trace: ***
    @     0x7f100f181ca0 (unknown)
    @     0x7f100fcbfd85 _ZNSt17_Function_handlerIFvRKSsEZNK7process6FutureIN5mesos8internal5slave15ResourceMonitor5UsageEE8onFailedIZNS7_22ResourceMonitorProcess5usageENS5_11ContainerIDEEUlS1_E_vEERKSA_OT_NSA_6PreferEEUlS1_E_E9_M_invokeERKSt9_Any_dataS1_
    @     0x7f100fb01506 process::internal::run<>()
    @     0x7f100fcc701b process::Future<>::fail()
    @     0x7f100fccfbde process::internal::thenf<>()
    @     0x7f100fd64bee _ZN7process8internal3runISt8functionIFvRKNS_6FutureIN5mesos18ResourceStatisticsEEEEEJRS6_EEEvRKSt6vectorIT_SaISD_EEDpOT0_
    @     0x7f100fd656dd process::Future<>::fail()
    @     0x7f100fd6c332 process::Promise<>::associate()
    @     0x7f100fe2777e _ZNSt17_Function_handlerIFvPN7process11ProcessBaseEEZNS0_8dispatchIN5mesos18ResourceStatisticsENS5_8internal5slave25MesosContainerizerProcessERKNS5_11ContainerIDESA_EENS0_6FutureIT_EERKNS0_3PIDIT0_EEMSH_FSF_T1_ET2_EUlS2_E_E9_M_invokeERKSt9_Any_dataS2_
    @     0x7f101015561a process::ProcessManager::resume()
    @     0x7f10101558dc process::schedule()
    @     0x7f100f17983d start_thread
    @     0x7f100e96bfcd clone
/usr/local/bin/mesos-slave.sh: line 102: 56573 Segmentation fault      (core dumped) $debug /usr/local/sbin/mesos-slave ""${MESOS_FLAGS[@]}""
Slave Exit Status: 139
{noformat}

{noformat:title=gdb core dump}
Thread 20 (Thread 0x7f100a190940 (LWP 56574)):
#0  _M_data (__functor=Unhandled dwarf expression opcode 0xf3
) at /opt/rh/devtoolset-2/root/usr/include/c++/4.8.2/bits/basic_string.h:293
#1  _M_rep (__functor=Unhandled dwarf expression opcode 0xf3
) at /opt/rh/devtoolset-2/root/usr/include/c++/4.8.2/bits/basic_string.h:301
#2  size (__functor=Unhandled dwarf expression opcode 0xf3
) at /opt/rh/devtoolset-2/root/usr/include/c++/4.8.2/bits/basic_string.h:716
#3  operator<< <char, std::char_traits<char>, std::allocator<char> > (__functor=Unhandled dwarf expression opcode 0xf3
) at /opt/rh/devtoolset-2/root/usr/include/c++/4.8.2/bits/basic_string.h:2758
#4  operator<< (__functor=Unhandled dwarf expression opcode 0xf3
) at ../include/mesos/type_utils.hpp:267
#5  operator() (__functor=Unhandled dwarf expression opcode 0xf3
) at slave/monitor.cpp:129
#6  operator() (__functor=Unhandled dwarf expression opcode 0xf3
) at ../3rdparty/libprocess/include/process/future.hpp:220
#7  std::_Function_handler<void(const std::basic_string<char, std::char_traits<char>, std::allocator<char> >&), process::Future<T>::onFailed(F&&, process::Future<T>::Prefer) const [with F = mesos::internal::slave::ResourceMonitorProcess::usage(mesos::ContainerID)::__lambda180; <template-parameter-2-2> = void; T = mesos::internal::slave::ResourceMonitor::Usage]::__lambda2>::_M_invoke(const std::_Any_data &, const std::basic_string<char, std::char_traits<char>, std::allocator<char> > &) (__functor=Unhandled dwarf expression opcode 0xf3
) at /opt/rh/devtoolset-2/root/usr/include/c++/4.8.2/functional:2071
#8  0x00007f100fb01506 in process::internal::run<std::function<void(const std::basic_string<char>&)>, std::basic_string<char, std::char_traits<char>, std::allocator<char> >&>(const std::vector<std::function<void(const std::basic_string<char, std::char_traits<char>, std::allocator<char> >&)>, std::allocator<std::function<void(const std::basic_string<char, std::char_traits<char>, std::allocator<char> >&)> > > &) (callbacks=std::vector of length 1, capacity 1 = {...})
    at ../3rdparty/libprocess/include/process/future.hpp:420
#9  0x00007f100fcc701b in process::Future<mesos::internal::slave::ResourceMonitor::Usage>::fail (this=0x7f0ffc185ca8, _message=""Unknown container: c0ab6cd3-fe4f-49bd-8dd6-32b388fcfab2"")
    at ../3rdparty/libprocess/include/process/future.hpp:1406
#10 0x00007f100fccfbde in fail (f=Unhandled dwarf expression opcode 0xf3
) at ../3rdparty/libprocess/include/process/future.hpp:649
#11 process::internal::thenf<mesos::ResourceStatistics, mesos::internal::slave::ResourceMonitor::Usage>(const std::function<process::Future<mesos::internal::slave::ResourceMonitor::Usage>(const mesos::ResourceStatistics&)> &, const std::shared_ptr<process::Promise<mesos::internal::slave::ResourceMonitor::Usage> > &, const process::Future<mesos::ResourceStatistics> &) (f=Unhandled dwarf expression opcode 0xf3
) at ../3rdparty/libprocess/include/process/future.hpp:1193
#12 0x00007f100fd64bee in operator() (callbacks=std::vector of length 1, capacity 1 = {...}) at /opt/rh/devtoolset-2/root/usr/include/c++/4.8.2/functional:2464
#13 process::internal::run<std::function<void(const process::Future<mesos::ResourceStatistics>&)>, process::Future<mesos::ResourceStatistics>&>(const std::vector<std::function<void(const process::Future<mesos::ResourceStatistics>&)>, std::allocator<std::function<void(const process::Future<mesos::ResourceStatistics>&)> > > &) (callbacks=std::vector of length 1, capacity 1 = {...}) at ../3rdparty/libprocess/include/process/future.hpp:420
#14 0x00007f100fd656dd in process::Future<mesos::ResourceStatistics>::fail (this=0x7f0ff8046230, _message=""Unknown container: c0ab6cd3-fe4f-49bd-8dd6-32b388fcfab2"") at ../3rdparty/libprocess/include/process/future.hpp:1407
#15 0x00007f100fd6c332 in onFailed (this=Unhandled dwarf expression opcode 0xf3
) at ../3rdparty/libprocess/include/process/future.hpp:1121
#16 onFailed<std::_Bind<std::_Mem_fn<bool (process::Future<mesos::ResourceStatistics>::*)(const std::basic_string<char>&)>(process::Future<mesos::ResourceStatistics>, std::_Placeholder<1>)>, bool> (this=Unhandled dwarf expression opcode 0xf3
)
    at ../3rdparty/libprocess/include/process/future.hpp:221
#17 onFailed<std::_Bind<std::_Mem_fn<bool (process::Future<mesos::ResourceStatistics>::*)(const std::basic_string<char>&)>(process::Future<mesos::ResourceStatistics>, std::_Placeholder<1>)> > (this=Unhandled dwarf expression opcode 0xf3
)
    at ../3rdparty/libprocess/include/process/future.hpp:270
#18 process::Promise<mesos::ResourceStatistics>::associate (this=Unhandled dwarf expression opcode 0xf3
) at ../3rdparty/libprocess/include/process/future.hpp:635
#19 0x00007f100fe2777e in operator() (__functor=Unhandled dwarf expression opcode 0xf3
) at ../3rdparty/libprocess/include/process/dispatch.hpp:239
#20 std::_Function_handler<void(process::ProcessBase*), process::dispatch(const process::PID<T>&, process::Future<T> (T::*)(P0), A0) [with R = mesos::ResourceStatistics; T = mesos::internal::slave::MesosContainerizerProcess; P0 = const mesos::ContainerID&; A0 = mesos::ContainerID]::__lambda21>::_M_invoke(const std::_Any_data &, process::ProcessBase *) (__functor=Unhandled dwarf expression opcode 0xf3
) at /opt/rh/devtoolset-2/root/usr/include/c++/4.8.2/functional:2071
#21 0x00007f101015561a in process::ProcessManager::resume (this=0xc24d20, process=0x7f0ffc0169b0) at src/process.cpp:2172
#22 0x00007f10101558dc in process::schedule (arg=Unhandled dwarf expression opcode 0xf3
) at src/process.cpp:602
#23 0x00007f100f17983d in start_thread () from /lib64/libpthread.so.0
#24 0x00007f100e96bfcd in clone () from /lib64/libc.so.6
{noformat}",Bug,Major,xujyan,2015-05-28T19:52:22.000+0000,5,Resolved,Complete,SIGSEGV received during ResourceMonitorProcess::usage(),2015-05-28T19:52:22.000+0000,MESOS-2771,1.0,mesos,
vinodkone,2015-05-26T22:33:48.000+0000,vinodkone,"In addition to the unallocated oversubscribed resources, the slave should also send the oversubscribed resources that are already allocated.

This is needed by the master/allocator to accurately calculate the available oversubscribed resources to offer.",Task,Major,vinodkone,2015-05-29T01:04:02.000+0000,5,Resolved,Complete,Slave should forward total amount of oversubscribed resources to the master,2015-05-29T01:04:02.000+0000,MESOS-2770,3.0,mesos,Twitter Q2 Sprint 3
wangcong,2015-05-26T22:01:31.000+0000,idownes,"The metric will provide statistics on the scheduling latency for processes/threads in a container, i.e., statistics on the delay before application code can run. This will be the aggregate effect of the normal scheduling period, contention from other threads/processes, both in the container and on the system, and any effects from the CFS bandwidth control (if enabled) or other CPU isolation strategies.",Improvement,Major,idownes,,10006,Reviewable,New,Metric for cpu scheduling latency from all components,2016-03-16T01:56:46.000+0000,MESOS-2769,8.0,mesos,Twitter Q2 Sprint 3
anandmazumdar,2015-05-26T16:16:58.000+0000,marco-mesos,"In every ""launcher"" file (ie, those containing some variation on {{main()}}) there is a minor variation on:
{code}
  if (flags.help) {
    cout << flags.usage() << endl;
    // arguably this is not an error: the user asked for help,
    // and she got it: // the program execution ought to be
    // considered successful.
    return EXIT_SUCCESS;
  }
{code}

As this is default behavior, and we've added support for the {{--help}} flag in the {{FlagsBase}} class, we should add this too there and remove it from everywhere else.

Additionally, a recurring behavior is checking for the presence of a {{required}} flag:
{code}
if (flags.master.isNone()) {
  EXIT(EXIT_FAILURE) << flags.usage(""--master is required"");
}
{code}

or some variation thereof: we should add automatic validation for required flags during parsing.

This follows the DRY principle.",Bug,Minor,marco-mesos,,10020,Accepted,In Progress,Add validation behavior to FlagsBase,2015-06-21T05:44:52.000+0000,MESOS-2766,1.0,mesos,
Bartek Plotka,2015-05-22T21:37:02.000+0000,jieyu,"This includes two things:
1) We need to expose ResourceMonitor::Usage so that module writers can access it. We could define a protobuf message for that.
2) We need to allow ResourceEstimator to call 'ResourceMonitor::usages()'. We could either expose the ResourceMonitor, or pass in a lambda to the resources estimator.",Bug,Major,jieyu,2015-06-05T17:23:08.000+0000,5,Resolved,Complete,Allow Resource Estimator to get Resource Usage information.,2015-06-05T17:23:08.000+0000,MESOS-2764,5.0,mesos,
,2015-05-22T10:20:12.000+0000,bernd-mesos,"stout/net.hpp and process/http.hpp offer overlapping functionality that could be consolidated in one place, presumably the latter, since it is more elaborate to begin with. This would also remove the dependency of the former on libcurl.

While we are at it, we could then turn net::contentLength() into a generalized, asynchronous process::http::head() call.

(Prerequisite: MESOS-2247, with the suggestion to enhance process::http, not stout, see a comment in that JIRA.)
",Improvement,Minor,bernd-mesos,,10020,Accepted,In Progress,Consolidate functionality in stout/net and process/http,2015-05-22T15:42:07.000+0000,MESOS-2763,8.0,mesos,
js84,2015-05-21T20:31:44.000+0000,js84,"As of right now the styleguide does not allow explicitly defaulted functions (being a c++ 11 feature).
They enhance readability, are supported by all relevant compiler (GCC 4.4+ and Clang 3.0+), and are introduced by some patches (e.g. https://reviews.apache.org/r/34277/). 
Therefore we should officially whitelist them in the styleguide.",Task,Major,js84,2015-06-02T10:27:39.000+0000,5,Resolved,Complete,Explicitly-defaulted functions are not allowed by styleguide,2015-06-02T10:27:39.000+0000,MESOS-2762,1.0,mesos,
js84,2015-05-21T20:22:54.000+0000,js84,"As of right now the styleguide does not allow delegating constructors (being a c++ 11 feature).
They are already used in the code base (e.g. stout/option.hpp), are supported by all relevant compiler (GCC 4.7+ and Clang 3.0+), and enhance readability. 
Therefore we should officially whitelist them in the styleguide.",Task,Major,js84,2015-06-02T09:36:25.000+0000,5,Resolved,Complete,Delegating constructors are not allowed by styleguide,2015-06-02T09:36:25.000+0000,MESOS-2761,1.0,mesos,
Bartek Plotka,2015-05-21T20:15:53.000+0000,skonefal,"The QoS controller informs the slave about correcting actions (kill, resize, throttle best-effort containers, tasks, and so forth) through a protobuf message, called a QoSCorrection. This ticket tracks designing and creating this message.

For example:
{code}
message QoSCorrection {
  // NOTE: In future we can define more actions like
  // resize or freeze, but for now we have:
  // 1) kill - terminate the executor or task
  enum Type {
    KILL = 1;
  }
  //Kill action which will be performed on an executor
  message Kill {
    optional ExecutorID executor_id = 1;
  }

  required Type action = 1;
  optional string reason = 2;
  optional double timestamp = 3;
  optional Kill kill = 4;
}
{code}",Task,Major,skonefal,2015-05-27T15:32:11.000+0000,5,Resolved,Complete,Add correction message to inform slave about QoS Controller actions,2015-05-27T15:32:11.000+0000,MESOS-2760,1.0,mesos,
tnachen,2015-05-21T07:41:50.000+0000,js84,The isolator flags are only relevant when using the Mesos Containerizer. We should reflect this in the flag description to avoid confusion.,Documentation,Major,js84,2015-06-03T17:31:14.000+0000,5,Resolved,Complete,Reflect in documentation that isolator flags are only relevant for Mesos Containerizer,2015-06-03T17:31:15.000+0000,MESOS-2758,1.0,mesos,
bmahler,2015-05-21T00:31:52.000+0000,jvanremoortere,"Let's add operator overloads to Option<T> to allow access to the underlying T using the `->` operator.
",Improvement,Major,jvanremoortere,2015-08-25T20:09:32.000+0000,5,Resolved,Complete,"Add -> operator for Option<T>, Try<T>, Result<T>, Future<T>.",2015-09-25T22:46:28.000+0000,MESOS-2757,3.0,mesos,Twitter Mesos Q3 Sprint 2
jvanremoortere,2015-05-20T23:43:29.000+0000,jvanremoortere,"In order to improve the safety of our code base, let's augment the style guide to:
""Disallow public construction of base classes""
so that we can avoid the object slicing problem. This is a good pattern to follow in general as it prevents subtle semantic bugs like the following:
{code:title=ObjectSlicing.cpp|borderStyle=solid}
#include <stdio.h>
#include <vector>

class Base {
  public:
  Base(int _v) : v(_v) {}
  virtual int get() const { return v; }
  protected:
  int v;
};

class Derived : public Base {
  public:
  Derived(int _v) : Base(_v) {}
  virtual int get() const { return v + 1; }
};

int main() {
  Base b(5);
  Derived d(5);
  std::vector<Base> vec;
  vec.push_back(b);
  vec.push_back(d);
  for (const auto& v : vec) {
    printf(""[%d]\n"", v.get());
  }
}
{code}",Improvement,Major,jvanremoortere,,10006,Reviewable,New,Update style guide: Avoid object slicing,2015-08-15T06:16:49.000+0000,MESOS-2756,1.0,mesos,
pbrett,2015-05-20T21:51:37.000+0000,pbrett,"We have several instances of string literals (e.g. ""mesos-containerizer"", ""net_tcp""rtt_microseconds_p50"") being used in multiple locations where mismatches would result in correctness issues.  We should replace these with a single definition to reduce the risk.",Bug,Major,pbrett,2015-05-27T21:00:04.000+0000,5,Resolved,Complete,Reduce multiple use of string literals,2015-05-27T21:00:05.000+0000,MESOS-2754,1.0,mesos,Twitter Q2 Sprint 3
vinodkone,2015-05-19T22:48:13.000+0000,idownes,"Current implementation out for [review|https://reviews.apache.org/r/34310] only supports setting the priority of containers with revocable CPU if it's specified in the initial executor info resources. This should be enforced at the master.

Also master should make sure that oversubscribed resources used by the task are valid.",Task,Major,idownes,2015-06-11T00:29:27.000+0000,5,Resolved,Complete,Master should validate tasks using oversubscribed resources,2015-06-11T00:29:27.000+0000,MESOS-2753,3.0,mesos,Twitter Q2 Sprint 3
pbrett,2015-05-19T22:03:19.000+0000,pbrett,Network isolator uses a Hierarchical Token Bucket (HTB) traffic control discipline on the egress filter inside each container as the root for adding traffic filters.  A HTB wrapper is needed to access the network statistics for this interface.,Improvement,Major,pbrett,2015-06-09T20:57:54.000+0000,5,Resolved,Complete,Add HTB queueing discipline wrapper class,2015-06-18T18:22:57.000+0000,MESOS-2752,3.0,mesos,Twitter Mesos Q2 Sprint 5
pbrett,2015-05-19T18:47:03.000+0000,pbrett,Export Traffic Control statistics in queueing library to enable reporting out impact of network bandwidth statistics.,Bug,Major,pbrett,2015-06-04T00:39:54.000+0000,5,Resolved,Complete,Extend queueing discipline wrappers to expose network isolator statistics,2015-06-18T18:23:36.000+0000,MESOS-2750,3.0,mesos,Twitter Q2 Sprint 3
haosdent@gmail.com,2015-05-18T23:36:30.000+0000,marco-mesos,"As reported by Michael Lunøe <mlunoe@mesosphere.io> (see also MESOS-329 and MESOS-913 for background):

{quote}
In {{mesos/3rdparty/libprocess/src/help.cpp}} a markdown file is created, which is then converted to html through a javascript library 

All endpoints point to {{/help/...}}, they need to work dynamically for reverse proxy to do its thing. {{/mesos/help}} works, and displays the endpoints, but they each need to go to their respective {{/help/...}} endpoint. 

Note that this needs to work both for master, and for slaves. I think the route to slaves help is something like this: {{/mesos/slaves/20150518-210216-1695027628-5050-1366-S0/help}}, but please double check this.
{quote}

The fix appears to be not too complex (as it would require to simply manipulate the generated URL) but a quick skim of the code would suggest that something more substantial may be desirable too.",Bug,Minor,marco-mesos,2015-06-02T09:09:48.000+0000,5,Resolved,Complete,/help generated links point to wrong URLs,2015-06-09T07:33:18.000+0000,MESOS-2748,2.0,mesos,
jvanremoortere,2015-05-18T18:50:35.000+0000,marco-mesos,"The information exposed by the Framework via the {{WebUIUrl}} does not always resolves to a routable endpoint (eg, when the {{hostname}} is not publicly resolvable, or resolvable at all).

In order to facilitate service discovery (via, eg, Marathon UI) we want to add the information in {{FrameworksPid}} via the {{/state-summary}} endpoint.",Story,Major,marco-mesos,2015-05-18T21:13:33.000+0000,5,Resolved,Complete,As a Framework User I want to be able to discover my Task's IP,2015-05-18T21:13:33.000+0000,MESOS-2746,3.0,mesos,Mesosphere Sprint 10
haosdent@gmail.com,2015-05-15T22:13:33.000+0000,adam-mesos,"The slave/state.json already reports executorInfos:
https://github.com/apache/mesos/blob/0.22.1/src/slave/http.cpp#L215-219

Would be great to see this in the master/state.json as well, so external tools don't have to query each slave to find out executor resources, sandbox directories, etc.",Improvement,Major,adam-mesos,2015-06-20T10:05:04.000+0000,5,Resolved,Complete,Include ExecutorInfos for custom executors in master/state.json,2015-09-14T16:13:19.000+0000,MESOS-2743,3.0,mesos,Mesosphere Sprint 12
haosdent@gmail.com,2015-05-15T19:38:07.000+0000,jieyu,"Right now, the resource monitor returns a Usage which contains ContainerId, ExecutorInfo and ResourceStatistics. In order for resource estimator/qos controller to calculate usage slack, or tell if a container is using revokable resources or not, we need to expose the Resources that are currently assigned to the container.

This requires us the change the containerizer interface to get the Resources as well while calling 'usage()'.",Bug,Major,jieyu,2015-08-16T18:01:52.000+0000,5,Resolved,Complete,Exposing Resources along with ResourceStatistics from resource monitor,2015-08-16T18:01:52.000+0000,MESOS-2741,5.0,mesos,
,2015-05-15T00:24:21.000+0000,bernd-mesos,"[~rcorral] recently observed that according to the master's and the slave's state.json summing up the resources allocated to tasks from different frameworks on a slave does not always match the total that is reported for the slave. The latter number is sometimes higher.

It would be desirable for tools that display allocation statistics to find balanced tallies.
",Improvement,Major,bernd-mesos,,10020,Accepted,In Progress,Reported used resources for tasks in frameworks do not match slave tally,2015-05-22T15:37:24.000+0000,MESOS-2738,3.0,mesos,
bmahler,2015-05-14T21:44:29.000+0000,bmahler,"In order to scale the number of committers in the project, we proposed the concept of maintainers here:

http://markmail.org/thread/cjmdn3d7qfzbxhpm

To follow up on that proposal, we'll need some documentation to capture the concept of maintainers. Both how contributors can benefit from maintainer feedback and the expectations of ""maintainer-ship"".

In order to not enforce an excessive amount of process, maintainers will initially only serve as an encouraged means to help contributors find reviewers and get meaningful feedback.",Documentation,Major,bmahler,2015-05-20T00:00:10.000+0000,5,Resolved,Complete,Add documentation for maintainers.,2015-05-20T00:00:10.000+0000,MESOS-2737,3.0,mesos,Twitter Q2 Sprint 3 - 5/11
marco-mesos,2015-05-14T21:34:17.000+0000,marco-mesos,"Currently, the {{MasterInfo}} PB only supports an {{ip}} field as an {{int32}}.

Beyond making it harder (and opaque; open to subtle bugs) for languages other than C/C++ to decode into an IPv4 octets, this does not allow Mesos to support IPv6 Master nodes.

We should consider ways to upgrade it in ways that permit us to support both IPv4 / IPv6 nodes, and, possibly, in a way that makes it easy for languages such as Java/Python that already have PB support, so could easily deserialize this information.

See also MESOS-2709 for more info.",Improvement,Major,marco-mesos,2015-07-25T00:51:47.000+0000,5,Resolved,Complete,Upgrade the design of MasterInfo,2015-08-15T00:26:02.000+0000,MESOS-2736,3.0,mesos,Mesosphere Sprint 15
jieyu,2015-05-14T21:01:33.000+0000,jieyu,"This will make the semantics more clear. The resource estimator can control the speed of sending resources estimation to the slave.

To avoid cyclic dependency, slave will register a callback with the resource estimator and the resource estimator will simply invoke that callback when there's a new estimation ready. The callback will be a defer to the slave's main event queue.",Bug,Major,jieyu,2015-05-21T23:00:25.000+0000,5,Resolved,Complete,Change the interaction between the slave and the resource estimator from polling to pushing ,2015-06-01T19:36:58.000+0000,MESOS-2735,3.0,mesos,Twitter Q2 Sprint 3 - 5/11
vinodkone,2015-05-14T19:11:52.000+0000,vinodkone,"The simplest way to add support for oversubscribed resources to the allocator is to simply add them to the already existing 'Slave.total' and 'Slave.available' variables. It is easy to distinguish the revocable resources by doing a .revocable() filter.


 ",Task,Major,vinodkone,2015-05-23T00:34:45.000+0000,5,Resolved,Complete,Update allocator to allocate revocable resources,2015-05-23T00:34:45.000+0000,MESOS-2734,5.0,mesos,Twitter Q2 Sprint 3 - 5/11
vinodkone,2015-05-14T19:01:31.000+0000,vinodkone,"Whenever the master gets a new oversubscribed resources estimate from the slave, it should rescind any outstanding revocable offers (with oversubscribed resources) from that slave. It should then call the allocator to update the oversubscribed resources.",Task,Major,vinodkone,2015-05-29T01:04:25.000+0000,5,Resolved,Complete,Update master to handle oversubscribed resource estimate from the slave,2015-05-29T01:04:25.000+0000,MESOS-2733,3.0,mesos,Twitter Q2 Sprint 3
vinodkone,2015-05-14T18:57:37.000+0000,vinodkone,"This tracks just the work of adding the API call to the allocator interface.

Master makes this call on the allocator whenever it gets a new oversubscribed resources estimate from the slave.",Task,Major,vinodkone,2015-05-23T00:18:43.000+0000,5,Resolved,Complete,Add a new API call to the allocator to update oversubscribed resources,2015-05-23T00:18:43.000+0000,MESOS-2730,2.0,mesos,Twitter Q2 Sprint 3 - 5/11
vinodkone,2015-05-14T18:54:16.000+0000,vinodkone,"DRF sorter currently keeps track of allocated resources and total resources, but there is no way to update the total resources. For oversubscription, we need the ability to update total resources because total oversubscribed resources change overtime.",Improvement,Major,vinodkone,2015-05-23T00:18:18.000+0000,5,Resolved,Complete,Update DRF sorter to update total resources,2015-05-23T00:18:18.000+0000,MESOS-2729,2.0,mesos,Twitter Q2 Sprint 3 - 5/11
karya,2015-05-13T22:21:53.000+0000,nnielsen,"Following the discussion Kapil started, it is currently not possible to enable the linux network namespace for a container without enabling the network isolator (which requires certain kernel capabilities and dependencies).
Following the pattern of enabling pid namespaces (--isolation=""namespaces/pid""). One possible solution could be to add another one for network i.e. ""namespaces/network"".

",Task,Major,nnielsen,2015-06-23T00:20:27.000+0000,5,Resolved,Complete,Add support for enabling network namespace without enabling the network isolator,2015-07-02T18:43:33.000+0000,MESOS-2726,13.0,mesos,
,2015-05-12T23:19:15.000+0000,bernd-mesos,"See ""src/state/state.hpp"" and ""src/java/src/org/apache/mesos/state/*.java"" for what the ""state abstraction"" is.

With the new HTTP API (see MESOS-2288, MESOS-2289), there will be no need to link to libmesos to a framework for it to communicate with a Mesos master. However, if a framework uses the Mesos ""state abstraction"", either directly in C++ or through other language bindings (e.g., Java), it still needs to link with libmesos. So, in order to achieve libmesos-free frameworks that can leverage all APIs Mesos has to offer, we need a different way to access the ""state abstraction"". 

---

One approach is to provide an HTTP API for state queries that get routed through the Mesos master, which relays them by making calls into libmesos. Details TBD, including how separate this will be from the general HTTP API.
",Improvement,Major,bernd-mesos,2015-09-22T16:56:06.000+0000,5,Resolved,Complete,"Create access to the Mesos ""state abstraction"" that does not require linking with libmesos",2015-09-22T16:56:12.000+0000,MESOS-2722,13.0,mesos,
karya,2015-05-12T17:14:56.000+0000,nnielsen,"There are many ways in which we can go around wiring up per-container IPs in Mesos.

As there are multiple underlying mechanisms and systems for keeping track of IP pools, we probably need to aim for a very flexible architecture, similar to the oversubscription project.

There are a couple of folks, companies and vendors interested in getting this capability into Mesos asap to provide a stronger networking story (https://www.mail-archive.com/dev@mesos.apache.org/msg32353.html). So let's start discussing and architecting this.",Task,Major,nnielsen,,3,In Progress,In Progress,"Architecture document for per-container IP assignment, enforcement and isolation",2015-07-21T19:33:40.000+0000,MESOS-2721,13.0,mesos,Mesosphere Sprint 13
,2015-05-12T16:55:15.000+0000,ijimenez,"We should define the schema of both requests and responses to the operator endpoints.
",Improvement,Major,ijimenez,,10020,Accepted,In Progress,Publish the schema for operator endpoints,2016-02-26T23:17:18.000+0000,MESOS-2720,2.0,mesos,
ijimenez,2015-05-12T16:46:23.000+0000,ijimenez,Add an endpoint for each master endpoint with a '.json' extension such as `/master/stats.json` so it becomes `/master/stats` after a deprecation cycle.,Improvement,Major,ijimenez,2015-09-12T09:24:22.000+0000,5,Resolved,Complete,Deprecating '.json' extension in master endpoints urls,2016-02-27T01:03:08.000+0000,MESOS-2719,1.0,mesos,Mesosphere Sprint 18
marco-mesos,2015-05-08T18:51:49.000+0000,marco-mesos,"When building clients that do not bind to {{libmesos}} and only use the HTTP API (via ""pure"" language bindings - eg, Java-only) there is no simple way to discover the Master's IP address to connect to.

Rather than relying on 'out-of-band' configuration mechanisms, we would like to enable the ability of interrogating the ZooKeeper ensemble to discover the Master's IP address (and, possibly, other information) to which the HTTP API requests can be addressed to.",Improvement,Major,marco-mesos,2015-06-03T19:04:42.000+0000,5,Resolved,Complete,Design Master discovery functionality for HTTP-only clients,2015-07-02T07:21:46.000+0000,MESOS-2709,3.0,mesos,Mesosphere Sprint 10
anandmazumdar,2015-05-08T17:28:52.000+0000,arojas,"This tracks the design of the Executor HTTP API.
",Bug,Major,arojas,2015-11-02T08:10:44.000+0000,5,Resolved,Complete,Design doc for the Executor HTTP API,2016-02-27T00:05:09.000+0000,MESOS-2708,2.0,mesos,Mesosphere Sprint 17
marco-mesos,2015-05-08T12:06:40.000+0000,oliverpp,"I have 4 slave nodes with the same hardware, operating system and mesos configuration. 

Few minutes ago, all 4 nodes were functioning well. I tried to change the config of *master* from _10.172.230.69:5050_ to _zh://10.172.230.69:2181/mesos_ and restarted them in turn. The other three had started normally but the last one got a segmentation fault as you can see below.

{code}
[root@iZ25to7d407Z ~]# mesos-slave --master=zh://10.172.230.69:2181/mesos --hostname=123.57.42.237 --containerizers=docker,mesos --quiet &
[1] 1216
[root@iZ25to7d407Z ~]# *** Aborted at 1431085131 (unix time) try ""date -d @1431085131"" if you are using GNU date ***
PC: @       0x3aede7b53c (unknown)
*** SIGSEGV (@0x0) received by PID 1216 (TID 0x7f12f984b820) from PID 0; stack trace: ***
    @       0x3aee20f710 (unknown)
    @       0x3aede7b53c (unknown)
    @       0x3aedecf630 (unknown)
    @     0x7f12fce1593f net::getIP()
    @     0x7f12fce507ae process::operator>>()
    @     0x7f12fce50107 process::UPID::UPID()
    @     0x7f12fc52af71 mesos::internal::MasterDetector::create()
    @           0x4b1290 main
    @       0x3aede1ed5d (unknown)
    @           0x4b00b9 (unknown)

[1]+  Segmentation fault      mesos-slave --master=zh://10.172.230.69:2181/mesos --hostname=123.57.42.237 --containerizers=docker,mesos --quiet
{code}",Bug,Major,oliverpp,2015-05-13T00:59:17.000+0000,5,Resolved,Complete,Incorrect zh:// URI scheme causes Slave to SegFault,2015-05-13T00:59:18.000+0000,MESOS-2707,2.0,mesos,
arojas,2015-05-08T03:14:34.000+0000,arojas,"The general rule to format templates is to declare them as:

{code}
template <typename T> // notice the space between template and <
class Foo {
  …
};
{code}

However, the style is not documented anywhere nor it is inherited from the Google style guide.",Documentation,Major,arojas,2015-07-07T08:47:25.000+0000,5,Resolved,Complete,Add correct format template declarations to the styleguide,2015-07-07T16:52:36.000+0000,MESOS-2705,1.0,mesos,
nnielsen,2015-05-07T21:19:06.000+0000,nnielsen,Modularize the QoS controller to enable custom correction policies,Task,Major,nnielsen,2015-06-11T19:20:49.000+0000,5,Resolved,Complete,Modularize the QoS Controller,2015-06-11T19:20:49.000+0000,MESOS-2703,3.0,mesos,
,2015-05-06T17:33:53.000+0000,idownes,Investigate if a flat hierarchy is sufficient for oversubscription of CPU or if a two-way split is necessary/preferred.,Task,Major,idownes,2015-05-18T17:03:30.000+0000,5,Resolved,Complete,Compare split/flattened cgroup hierarchy for CPU oversubscription,2015-05-18T17:03:30.000+0000,MESOS-2702,3.0,mesos,
,2015-05-06T17:24:37.000+0000,idownes,"See this [ticket|https://issues.apache.org/jira/browse/MESOS-2652] for context.

# Configurable bias
# Change cgroup layout
** Implement roll-forward migration path in isolator recover
** Document roll-back migration path",Task,Major,idownes,2015-05-18T17:03:22.000+0000,5,Resolved,Complete,Implement bi-level cpu.shares subtrees in cgroups/cpu isolator.,2015-05-18T17:03:22.000+0000,MESOS-2701,8.0,mesos,
,2015-05-06T17:20:33.000+0000,idownes,"See this [ticket|https://issues.apache.org/jira/browse/MESOS-2652] for context.

* Understand the relationship between cpu.shares and CFS quota.
* Determine range of possible bias splits
* Determine how to achieve bias, e.g., should 20:1 be 20480:1024 or ~1024:50
* Rigorous testing of behavior with varying loads, particularly the combination of latency sensitive loads for high biased tasks (non-revokable), and cpu intensive loads for the low biased tasks (revokable).
* Discover any performance edge cases?",Task,Major,idownes,2015-05-18T17:03:13.000+0000,5,Resolved,Complete,Determine CFS behavior with biased cpu.shares subtrees,2015-05-18T17:03:13.000+0000,MESOS-2700,13.0,mesos,
vinodkone,2015-05-05T22:57:18.000+0000,vinodkone,"We plan to rename ""/shutdown"" endpoint to ""/teardown"" to be compatible with the new API. ""/shutdown"" will be deprecated in 0.23.0 or later.",Task,Major,vinodkone,2015-05-11T23:39:37.000+0000,5,Resolved,Complete,Add a /teardown endpoint on master to teardown a framework,2015-10-06T08:24:19.000+0000,MESOS-2697,2.0,mesos,Twitter Q2 Sprint 2
wangcong,2015-05-05T22:51:01.000+0000,clambert,Exploratory work.  Additional tickets to follow.,Task,Major,clambert,2015-08-17T18:33:44.000+0000,5,Resolved,Complete,Explore exposing stats from kernel,2015-08-17T18:33:44.000+0000,MESOS-2696,5.0,mesos,Twitter Q2 Sprint 2
,2015-05-05T22:01:15.000+0000,vinodkone,"This flag lets an operator control cluster level oversubscription. 

The master should send revocable offers to framework if this flag is enabled and the framework opts in to receive them.

Master should ignore revocable resources from slaves if the flag is disabled.

Need tests for all these scenarios.",Task,Major,vinodkone,2016-01-19T19:17:28.000+0000,5,Resolved,Complete,Add master flag to enable/disable oversubscription,2016-01-19T19:17:28.000+0000,MESOS-2695,5.0,mesos,
wickman,2015-05-05T19:34:01.000+0000,vinodkone,"While new fields like DiskInfo and ReservationInfo have been added to Resource protobuf, the output stream operator hasn't been updated to show these. This is valuable information to have in the logs during debugging.",Improvement,Major,vinodkone,2015-06-01T23:30:19.000+0000,5,Resolved,Complete,"Printing a resource should show information about reservation, disk etc",2015-06-09T01:07:34.000+0000,MESOS-2693,1.0,mesos,Twitter Q2 Sprint 3 - 5/11
vinodkone,2015-05-04T23:36:46.000+0000,vinodkone,"Need to update Resource message with a new subtype that indicates that the resource is revocable. It might also need to specify ""why"" it is revocable (e.g., oversubscribed).

Also need to make sure all the operations on Resource(s) takes this new message into account.",Task,Major,vinodkone,2015-05-16T00:15:23.000+0000,5,Resolved,Complete,Update Resource message to include revocable resources,2015-05-16T00:15:23.000+0000,MESOS-2691,3.0,mesos,Twitter Q2 Sprint 2
jieyu,2015-05-04T19:06:21.000+0000,vinodkone,"Slave simply forwards resource estimates from ResourceEstimator to the master.

Use a new message and handler on the master. 

A slave flag for the interval between the messages.
",Task,Major,vinodkone,2015-05-14T02:21:46.000+0000,5,Resolved,Complete,Slave should forward oversubscribable resources to the master,2015-05-14T02:21:46.000+0000,MESOS-2689,5.0,mesos,Twitter Q2 Sprint 2
jieyu,2015-05-04T18:56:32.000+0000,vinodkone,"If oversubscription for allocation is disabled on a restarted slave (that had it previously enabled), it should kill usage slack revocable tasks.

Slave knows this information from the Resources of a container that it checkpoints and recovers.

Add a new reason OVERSUBSCRIPTION_DISABLED.",Task,Major,vinodkone,,10020,Accepted,In Progress,Slave should kill usage slack revocable tasks if oversubscription is disabled,2016-01-30T09:23:01.000+0000,MESOS-2688,3.0,mesos,
,2015-05-04T18:49:24.000+0000,vinodkone,Slave sends oversubscribable resources to master only when the flag is enabled.,Task,Major,vinodkone,2015-06-15T22:39:13.000+0000,5,Resolved,Complete,Add a slave flag to enable oversubscription,2015-07-02T18:43:31.000+0000,MESOS-2687,2.0,mesos,
alexr,2015-04-30T08:43:24.000+0000,alex-mesos,"Modules doc states the possibility of using hooks, but doesn't refer to necessary flags and usage example.",Improvement,Major,alexr,2015-05-12T06:53:41.000+0000,5,Resolved,Complete,Update modules doc with hook usage example,2015-10-14T10:00:44.000+0000,MESOS-2680,1.0,mesos,Mesosphere Q1 Sprint 9 - 5/15
js84,2015-04-29T01:46:05.000+0000,js84,"The header include order for Mesos actually follows the Google Styleguide but omits step 1 without mentioning this exception in the Mesos styleguide. This proposal suggests to adapt to the include order explained in the Google Styleguide i.e. include the direct headers first in the .cpp files implementing them.

A gist of the proposal can be found here: 
https://gist.github.com/joerg84/65cb9611d24b2e35b69b

The corresponding Review Board review can be found here:
https://reviews.apache.org/r/33646/ ",Improvement,Minor,js84,,10020,Accepted,In Progress,Follow Google Style Guide for header file include order completely.,2015-08-12T16:30:35.000+0000,MESOS-2673,5.0,mesos,Mesosphere Sprint 10
chzhcn,2015-04-29T01:21:04.000+0000,chzhcn,"{noformat}
I0429 00:58:35.267629  2086 slave.cpp:3210] Executor 'default' of framework 20150429-005830-16777343-5432-2023-0000 terminated with signal Aborted
I0429 00:58:35.270761  2086 slave.cpp:2512] Handling status update TASK_LOST (UUID: f969e350-6f91-4fa9-980e-1852554bd704) for task 1 of framework 201
50429-005830-16777343-5432-2023-0000 from @0.0.0.0:0
I0429 00:58:35.270983  2086 slave.cpp:4604] Terminating task 1
W0429 00:58:35.271574  2080 containerizer.cpp:903] Ignoring update for unknown container: 1298549a-a3d2-46ff-aad0-9dbc777affcc
I0429 00:58:35.272541  2074 status_update_manager.cpp:317] Received status update TASK_LOST (UUID: f969e350-6f91-4fa9-980e-1852554bd704) for task 1 o
f framework 20150429-005830-16777343-5432-2023-0000
I0429 00:58:35.272624  2074 status_update_manager.cpp:494] Creating StatusUpdate stream for task 1 of framework 20150429-005830-16777343-5432-2023-00
00
I0429 00:58:35.273217  2053 master.cpp:3493] Executor default of framework 20150429-005830-16777343-5432-2023-0000 on slave 20150429-005830-16777343-
5432-2023-S0 at slave(1)@10.35.12.124:5051 (smfd-aki-27-sr1.devel.twitter.com): terminated with signal Aborted

{noformat}

which is from

{code}
 60    // We use mlock and memset here to make sure that the memory                                                                                  
 61    // actually gets paged in and thus accounted for.                                                                                             
 62    if (mlock(buffer, chunk) != 0) {                                                                                                              
 63      perror(""Failed to lock memory, mlock"");                                                                                                     
 64      abort();                                                                                                                                    
 65    }                                                                                                                                             
 66                                                                                                                                                  
 67    if (memset(buffer, 1, chunk) != buffer) {                                                                                                     
 68      perror(""Failed to fill memory, memset"");                                                                                                    
 69      abort();                                                                                                                                    
 70    }  
{code}

This is the same as MESOS-2660: I've confirmed that swapping them fixed it.

",Bug,Major,chzhcn,2015-05-08T17:55:59.000+0000,5,Resolved,Complete,ContainerizerTest.ROOT_CGROUPS_BalloonFramework flaky,2015-05-08T18:01:46.000+0000,MESOS-2672,1.0,mesos,Twitter Q2 Sprint 2
jieyu,2015-04-28T17:52:18.000+0000,jieyu,"There is a bug in the code. If there are namespaces created by other party (say ip netns), the slave recovery will abort.",Bug,Major,jieyu,2015-04-28T18:42:48.000+0000,5,Resolved,Complete,Port mapping isolator causes SIGABRT during slave recovery.,2015-04-28T18:42:48.000+0000,MESOS-2671,1.0,mesos,Twitter Q2 Sprint 2
pbrett,2015-04-27T16:32:07.000+0000,pbrett,"qdisc search function is dependent on matching a single hard coded handle and does not correctly test for interface, making the implementation fragile.  Additionally, the current setup scripts (using dynamically created shell commands) do not match the hard coded handles.  ",Bug,Critical,pbrett,2015-06-04T00:40:34.000+0000,5,Resolved,Complete,Fix queuing discipline wrapper in linux/routing/queueing ,2015-06-04T00:40:34.000+0000,MESOS-2665,5.0,mesos,Twitter Q2 Sprint 2
hartem,2015-04-24T22:05:31.000+0000,jieyu,"[==========] Running 1 test from 1 test case.
[----------] Global test environment set-up.
[----------] 1 test from CgroupsAnyHierarchyWithCpuMemoryTest
[ RUN      ] CgroupsAnyHierarchyWithCpuMemoryTest.ROOT_CGROUPS_Listen
Failed to allocate RSS memory: Failed to lock memory, mlock: Resource temporarily unavailable../../../mesos/src/tests/cgroups_tests.cpp:571: Failure
Failed to wait 15secs for future
[  FAILED  ] CgroupsAnyHierarchyWithCpuMemoryTest.ROOT_CGROUPS_Listen (15121 ms)
[----------] 1 test from CgroupsAnyHierarchyWithCpuMemoryTest (15121 ms total)

[----------] Global test environment tear-down
[==========] 1 test from 1 test case ran. (15174 ms total)
[  PASSED  ] 0 tests.
[  FAILED  ] 1 test, listed below:
[  FAILED  ] CgroupsAnyHierarchyWithCpuMemoryTest.ROOT_CGROUPS_Listen",Bug,Major,jieyu,2015-04-29T17:18:07.000+0000,5,Resolved,Complete,ROOT_CGROUPS_Listen and ROOT_IncreaseRSS tests are flaky,2015-08-07T17:21:06.000+0000,MESOS-2660,3.0,mesos,Twitter Q2 Sprint 2
bmahler,2015-04-22T22:29:38.000+0000,vinodkone,"Ideally this would be an example framework (or stand alone binary like load generator framework) that helps us evaluate oversubscription in a real cluster.

We need to come up with metrics that need to be exposed by this framework for evaluation (e.g., how many revocable offers, rescinds, preemptions etc).",Task,Major,vinodkone,2015-06-06T01:35:11.000+0000,5,Resolved,Complete,Implement a stand alone test framework that uses revocable cpu resources,2015-06-06T01:35:11.000+0000,MESOS-2655,5.0,mesos,Twitter Q2 Sprint 3
vinodkone,2015-04-22T22:27:49.000+0000,vinodkone,Add a new field to FrameworkInfo that lets the frameworks explicitly choose revocable offers  (for backwards compatibility).,Improvement,Major,vinodkone,2015-05-21T21:14:07.000+0000,5,Resolved,Complete,Update FrameworkInfo to opt in to revocable resources,2015-05-21T21:14:07.000+0000,MESOS-2654,1.0,mesos,Twitter Q2 Sprint 3 - 5/11
nnielsen,2015-04-22T22:26:36.000+0000,vinodkone,"Slave might want to kill revocable tasks based on correction events from the QoS controller.

The QoS controller communicates corrections through a stream (or process::Queue) to the slave which corrections it needs to carry out, in order to mitigate interference with production tasks.

The correction is communicated through a message:
[code]
message QoSCorrection {
     enum CorrectionType {
         KillExecutor = 1
         // KillTask = 2
         // Resize, throttle task
     }
optional string reason = X;
optional ExecutorID executor_id = X;
// optional TaskID task_id = X;
}
[/code]

And the slave will setup a handler to process these events. Initially, only executor termination is supported and cause the slave to issue 'containerizer->destroy()'.",Improvement,Major,vinodkone,2015-06-17T00:44:39.000+0000,5,Resolved,Complete,Slave should act on correction events from QoS controller,2015-07-02T06:51:16.000+0000,MESOS-2653,8.0,mesos,Mesosphere Sprint 12
idownes,2015-04-22T22:23:00.000+0000,vinodkone,"The CPU isolator needs to properly set limits for revocable and non-revocable containers.

The proposed strategy is to use a two-way split of the cpu cgroup hierarchy -- normal (non-revocable) and low priority (revocable) subtrees -- and to use a biased split of CFS cpu.shares across the subtrees, e.g., a 20:1 split (TBD). Containers would be present in only one of the subtrees. CFS quotas will *not* be set on subtree roots, only cpu.shares. Each container would set CFS quota and shares as done currently.
",Task,Major,vinodkone,2015-05-29T21:35:33.000+0000,5,Resolved,Complete,Update Mesos containerizer to understand revocable cpu resources,2015-07-24T18:03:40.000+0000,MESOS-2652,5.0,mesos,Twitter Q2 Sprint 3 - 5/11
nnielsen,2015-04-22T22:21:18.000+0000,vinodkone,"This is a component of the slave that informs the slave about the possible ""corrections"" that need to be performed (e.g., shutdown container using recoverable resources).

This needs to be integrated with the resource monitor.

Need to figure out the metrics used for sending corrections (e.g., scheduling latency, usage, informed by executor/scheduler)

We also need to figure out the feedback loop between the QoS controller and the Resource Estimator.

{code}
class QoSController {
public:
  QoSController(ResourceMonitor* monitor);

  process::Queue<QoSCorrection> correction();
};
{code}

",Story,Major,vinodkone,2015-06-05T00:10:04.000+0000,5,Resolved,Complete,Implement QoS controller,2015-06-05T00:10:05.000+0000,MESOS-2651,3.0,mesos,Mesosphere Sprint 10
Bartek Plotka,2015-04-22T22:17:42.000+0000,vinodkone,"Modularizing the resource estimator opens up the door for org specific implementations.

Test the estimator module.",Task,Major,vinodkone,2015-06-05T17:23:32.000+0000,5,Resolved,Complete,Modularize the Resource Estimator,2015-06-05T17:23:33.000+0000,MESOS-2650,3.0,mesos,Mesosphere Sprint 10
jieyu,2015-04-22T22:16:49.000+0000,vinodkone,"Resource estimator is the component in the slave that estimates the amount of oversubscribable resources.

This needs to be integrated with the slave and resource monitor.",Task,Major,vinodkone,2015-05-22T21:38:40.000+0000,5,Resolved,Complete,Implement Resource Estimator,2015-05-22T21:38:40.000+0000,MESOS-2649,5.0,mesos,Twitter Q2 Sprint 2
nnielsen,2015-04-22T22:07:51.000+0000,vinodkone,Add usage() API call to return usage of all containers,Improvement,Major,vinodkone,2015-05-13T23:55:09.000+0000,5,Resolved,Complete,Update Resource Monitor to return resource usage,2015-05-13T23:55:09.000+0000,MESOS-2648,3.0,mesos,
gyliu,2015-04-22T22:06:12.000+0000,vinodkone,"The latest oversubscribed resource estimate might render a revocable task launch invalid. Slave should check this and send TASK_LOST with appropriate REASON.

We need to add a new REASON for this (REASON_RESOURCE_OVERSUBSCRIBED?).",Task,Major,vinodkone,,10006,Reviewable,New,Slave should validate tasks using oversubscribed resources,2016-01-20T09:36:00.000+0000,MESOS-2647,5.0,mesos,
gradywang,2015-04-22T22:05:12.000+0000,vinodkone,"Master will send separate offers for revocable and non-revocable/regular resources. This allows master to rescind revocable offers (e.g, when a new oversubscribed resources estimate comes from the slave) without impacting regular offers.",Improvement,Major,vinodkone,,10006,Reviewable,New,Update Master to send revocable resources in separate offers,2016-02-16T08:29:57.000+0000,MESOS-2646,3.0,mesos,
lackita,2015-04-20T20:50:28.000+0000,nnielsen,"We are using 'foo', 'bar', ... string constants and pairs in src/tests/master_tests.cpp, src/tests/slave_tests.cpp, src/tests/hook_tests.cpp and src/examples/test_hook_module.cpp for label and hooks tests. These values should be stored in local variables to avoid the possibility of assignment getting out of sync with checking for that same value.",Bug,Major,nnielsen,,10020,Accepted,In Progress,"Consolidate 'foo', 'bar', ... string constants in test and example code",2015-06-24T20:30:55.000+0000,MESOS-2637,2.0,mesos,
chzhcn,2015-04-18T00:11:42.000+0000,chzhcn,"We saw a segfault in production. Attaching the coredump, we see:

Core was generated by `/usr/local/sbin/mesos-slave --port=5051 --resources=cpus:23;mem:70298;ports:[31'.
Program terminated with signal 11, Segmentation fault.
#0  0x00007f639867c77e in free () from /lib64/libc.so.6
(gdb) bt
#0  0x00007f639867c77e in free () from /lib64/libc.so.6
#1  0x00007f63986c25d0 in freeaddrinfo () from /lib64/libc.so.6
#2  0x00007f6399deeafa in net::getIP (hostname=""<redacted>"", family=2) at ./3rdparty/stout/include/stout/net.hpp:201
#3  0x00007f6399e1f273 in process::initialize (delegate=Unhandled dwarf expression opcode 0xf3
) at src/process.cpp:837
#4  0x000000000042342f in main ()",Bug,Major,chzhcn,2015-04-22T18:27:14.000+0000,5,Resolved,Complete,"Segfault in inline Try<IP> getIP(const std::string& hostname, int family)",2015-06-30T18:37:21.000+0000,MESOS-2636,1.0,mesos,Twitter Q2 Sprint 1 - 4/13
ijimenez,2015-04-16T22:04:40.000+0000,jvanremoortere,"To help reduce compile time and keep the header easy to read, let's move the implementations of the Framework struct functions out of master.hpp",Task,Trivial,jvanremoortere,,10020,Accepted,In Progress,Move implementations of Framework struct functions out of master.hpp,2016-02-02T20:26:58.000+0000,MESOS-2633,1.0,mesos,Mesosphere Q2 Sprint 8 - 5/1
jvanremoortere,2015-04-16T21:46:41.000+0000,jvanremoortere,"We modify the style guide to disallow constant references to temporaries as a whole. This means disallowing both (1) and (2) below.

h3. Background
1. Constant references to simple expression temporaries do extend the lifetime of the temporary till end of function scope:
* Temporary returned by function:
  {code}
  // See full example below.
  T f(const char* s) { return T(s); }

  {
    const T& good = f(""Ok"");
    // use of good is ok.
  }
  {code}
* Temporary constructed as simple expression:
  {code}
  // See full example below.
  {
    const T& good = T(""Ok"");
    // use of good is ok.
  }
  {code}

2. Constant references to expressions that result in a reference to a temporary do not extend the lifetime of the temporary:
  * Temporary returned by function:
  {code}
  // See full example below.
  T f(const char* s) { return T(s); }

  {
    const T& bad = f(""Bad!"").Member();
    // use of bad is invalid.
  }
  {code}
  * Temporary constructed as simple expression:
  {code}
  // See full example below.
  {
    const T& bad = T(""Bad!"").Member();
    // use of bad is invalid.
  }
  {code}

h3. Mesos Case
  - In Mesos we use Future<T> a lot. Many of our functions return Futures by value:
  {code}
  class Socket {
    Future<Socket> accept();
    Future<size_t> recv(char* data, size_t size);
    ...
  }
  {code}
  - Sometimes we capture these Futures:
  {code}
  {
    const Future<Socket>& accepted = socket.accept(); // Valid c++, propose we disallow.
  }
  {code}
  - Sometimes we chain these Futures:
  {code}
  {
    socket.accept().then(lambda::bind(_accepted)); // Temporary will be valid during 'then' expression evaluation.
  }
  {code}
  - Sometimes we do both:
  {code}
  {
    const Future<Socket>& accepted = socket.accept().then(lambda::bind(_accepted)); // Dangerous! 'accepted' lifetime will not be valid till end of scope. Disallow!
  }
  {code}

h3. Reasoning
- Although (1) is ok, and considered a [feature|http://herbsutter.com/2008/01/01/gotw-88-a-candidate-for-the-most-important-const/], (2) is extremely dangerous and leads to hard to track bugs.
- If we explicitly allow (1), but disallow (2), then my worry is that someone coming along to maintain the code later on may accidentally turn (1) into (2), without recognizing the severity of this mistake. For example:
{code}
// Original code:
const T& val = T();
std::cout << val << std::endl;
// New code:
const T& val = T().removeWhiteSpace();
std::cout << val << std::endl; // val could be corrupted since the destructor has been invoked and T's memory freed.
{code}
- If we disallow both cases: it will be easier to catch these mistakes early on in code reviews (and avoid these painful bugs), at the same cost of introducing a new style guide rule.

h3. Performance Implications
- BenH suggests c++ developers are commonly taught to capture by constant reference to hint to the compiler that the copy can be elided.
- Modern compilers use a Data Flow Graph to make optimizations such as
  - *In-place-construction*: leveraged by RVO and NRVO to construct the object in place on the stack. Similar to ""*Placement new*"": http://en.wikipedia.org/wiki/Placement_syntax
  - *RVO* (Return Value Optimization): http://en.wikipedia.org/wiki/Return_value_optimization
  - *NRVO* (Named Return Value Optimization): https://msdn.microsoft.com/en-us/library/ms364057%28v=vs.80%29.aspx
- Since modern compilers perform these optimizations, we no longer need to 'hint' to the compiler that the copies can be elided.

h3. Example program
{code}
#include <stdio.h>

class T {
public:
  T(const char* str) : Str(str) {
    printf(""+ T(%s)\n"", Str);
  }
  ~T() {
    printf(""- T(%s)\n"", Str);
  }
  const T& Member() const
  {
    return *this;
  }
private:
  const char* Str;
};

T f(const char* s) { return T(s); }

int main() {
  const T& good = T(""Ok"");
  const T& good_f = f(""Ok function"");

  const T& bad = T(""Bad!"").Member();
  const T& bad_f = T(""Bad function!"").Member();

  printf(""End of function scope...\n"");
}
{code}
Output:
{code}
+ T(Ok)
+ T(Ok function)
+ T(Bad!)
- T(Bad!)
+ T(Bad function!)
- T(Bad function!)
End of function scope...
- T(Ok function)
- T(Ok)
{code}",Task,Major,jvanremoortere,2015-06-02T10:28:21.000+0000,5,Resolved,Complete,Update style guide to disallow capture by reference of temporaries,2015-06-02T10:28:21.000+0000,MESOS-2629,1.0,mesos,Mesosphere Q2 Sprint 8 - 5/1
jieyu,2015-04-16T18:35:43.000+0000,cmaloney,"This just failed for the first time on our OS X Bot (Far less frequent flaky than the other ExamplesTest, but still flaky) while compiling master at commit f6620f851f635b3346c6ebf878152f38b3932ad9. There weren't any commits which touched / changed anything in the test in the set.

{code}
[ RUN      ] ExamplesTest.PersistentVolumeFramework ../../src/tests/script.cpp:83: Failure Failed persistent_volume_framework_test.sh terminated with signal Abort trap: 6 
[  FAILED  ] ExamplesTest.PersistentVolumeFramework (7865 ms)
{code}",Bug,Major,cmaloney,2015-06-15T23:29:35.000+0000,5,Resolved,Complete,ExamplesTest.PersistentVolumeFramework is flaky,2015-06-15T23:29:35.000+0000,MESOS-2627,1.0,mesos,Twitter Mesos Q2 Sprint 5
nnielsen,2015-04-15T23:36:50.000+0000,nnielsen,"In order to enable decorator modules to _remove_ metadata (environment variables or labels), we changed the meaning of the return value for decorator hooks.

The Result<T> return values means:

||State||Before||After||
|Error|Error is propagated to the call-site|No change|
|None|The result of the decorator is not applied|No change|
|Some|The result of the decorator is *appended*|The result of the decorator *overwrites* the final labels/environment object|",Documentation,Major,nnielsen,2015-05-19T01:18:15.000+0000,5,Resolved,Complete,Document the semantic change in decorator return values,2015-07-02T06:51:17.000+0000,MESOS-2622,1.0,mesos,
jvanremoortere,2015-04-14T02:22:39.000+0000,jvanremoortere,"Pipe the 'updateFramework' call from the master through the allocator, as described in the design doc in the epic: MESOS-703",Task,Major,jvanremoortere,2015-06-05T22:52:49.000+0000,5,Resolved,Complete,Pipe 'updateFramework' path from master to Allocator to support framework re-registration,2015-07-02T06:51:15.000+0000,MESOS-2615,1.0,mesos,Mesosphere Q1 Sprint 7 - 4/17
greggomann,2015-04-13T18:34:26.000+0000,mesosmike,"Right now it seems Mesos is using „docker rm –f ID“ to delete containers so bind mounts are not deleted. This means thousands of dirs in /var/lib/docker/vfs/dir   I would like to have the option to change it to „docker rm –f –v ID“ This deletes bind mounts but not persistant volumes.

Best,

Mike",Improvement,Minor,mesosmike,2015-10-13T07:47:06.000+0000,5,Resolved,Complete,Change docker rm command,2015-11-08T12:51:43.000+0000,MESOS-2613,2.0,mesos,Mesosphere Sprint 20
mcypark,2015-04-08T18:50:56.000+0000,mcypark,Enable operators to manage dynamic reservations by Introducing the {{/reserve}} and {{/unreserve}} HTTP endpoints on the master.,Task,Critical,mcypark,2015-09-09T23:14:27.000+0000,5,Resolved,Complete,Add /reserve and /unreserve endpoints on the master for dynamic reservation,2015-09-09T23:14:27.000+0000,MESOS-2600,5.0,mesos,Mesosphere Sprint 10
bschang,2015-04-07T16:30:26.000+0000,matthias@mesosphere.io,"queued_tasks.executor_id is expected to be a string and not a complete json object. It should have the very same format as the tasks array on the same level.

Example, directly taken from slave

{noformat}
         ....
""queued_tasks"": [
{
  ""data"": """",
  ""executor_id"": {
    ""command"": {
      ""argv"": [],
      ""uris"": [
        {
          ""executable"": false,
          ""value"": ""http://downloads.foo.io/orchestra/storm-mesos/0.9.2-incubating-47-ovh.bb373df1c/storm-mesos-0.9.2-incubating.tgz""
        }
      ],
      ""value"": ""cd storm-mesos* && python bin/storm supervisor storm.mesos.MesosSupervisor""
    },
    ""data"": ""{\""assignmentid\"":\""srv4.hw.ca1.foo.com\"",\""supervisorid\"":\""srv4.hw.ca1.foo.com-stage-ingestion-stats-slave-111-1428421145\""}"",
    ""executor_id"": ""stage-ingestion-stats-slave-111-1428421145"",
    ""framework_id"": ""20150401-160104-251662508-5050-2197-0002"",
    ""name"": """",
    ""resources"": {
      ""cpus"": 0.5,
      ""disk"": 0,
      ""mem"": 1000
    }
  },
  ""id"": ""srv4.hw.ca1.foo.com-31708"",
  ""name"": ""worker srv4.hw.ca1.foo.com:31708"",
  ""resources"": {
    ""cpus"": 1,
    ""disk"": 0,
    ""mem"": 5120,
    ""ports"": ""[31708-31708]""
  },
  ""slave_id"": ""20150327-025553-218108076-5050-4122-S0""
},
...
]


{noformat}",Bug,Minor,matthias@mesosphere.io,2015-06-20T17:19:05.000+0000,5,Resolved,Complete,Slave state.json frameworks.executors.queued_tasks wrong format?,2015-06-20T17:19:05.000+0000,MESOS-2598,3.0,mesos,Mesosphere Sprint 12
alexr,2015-04-07T13:53:43.000+0000,alex-mesos,"Once Allocator interface changes, so does the way of writing new allocators. This should be reflected in Mesos docs. The modules doc should mention how to write and use allocator modules. Configuration doc should mention the new {{--allocator}} flag.",Task,Major,alexr,2015-05-29T16:51:32.000+0000,5,Resolved,Complete,Update allocator docs,2015-10-14T10:00:43.000+0000,MESOS-2596,2.0,mesos,Mesosphere Sprint 10
tnachen,2015-04-07T00:34:31.000+0000,tnachen,"Currently we're reusing the command executor to wait on the progress of the docker executor, but has the following drawback:

- We need to launch a seperate docker log process just to forward logs, where we can just simply reattach stdout/stderr if we create a specific executor for docker
- In general, Mesos slave is assuming that the executor is the one starting the actual task. But the current docker containerizer, the containerizer is actually starting the docker container first then launches the command executor to wait on it. This can cause problems if the container failed before the command executor was able to launch, as slave will try to update the limits of the containerizer on executor registration but then the docker containerizer will fail to do so since the container failed. 

Overall it's much simpler to tie the container lifecycle with the executor and simplfies logic and log management.",Improvement,Major,tnachen,2015-05-25T17:45:51.000+0000,5,Resolved,Complete,Create docker executor,2015-07-02T07:50:18.000+0000,MESOS-2595,8.0,mesos,
pbrett,2015-04-03T22:35:31.000+0000,pbrett,Refactor launchHelper and statisticsHelper in port_mapping_tests to allow reuse,Improvement,Minor,pbrett,2015-05-11T18:15:51.000+0000,5,Resolved,Complete,Refactor launchHelper and statisticsHelper in port_mapping_tests to allow reuse,2015-06-18T22:51:57.000+0000,MESOS-2591,2.0,mesos,Twitter Mesos Q1 Sprint 6
jieyu,2015-04-02T18:13:52.000+0000,jieyu,"Right now, we use a sleep command to control the duration of perf sampling:
{noformat}
sudo perf stat -a -x, --log-fd 1 --pid 10940 -- sleep 10
{noformat}

This causes an additional process (i.e., the sleep process) to be forked and causes troubles for us to terminate the perf sampler once the slave exits (See MESOS-2462).

Seems that the additional sleep process is not necessary. The slave can just monitor the duration and send a SIGINT to the perf process when duration elapsed. This will cause the perf process to output the stats and terminate.",Improvement,Major,jieyu,2015-04-03T21:38:05.000+0000,5,Resolved,Complete,Let the slave control the duration of the perf sampler instead of relying on a sleep command.,2015-04-03T21:38:05.000+0000,MESOS-2590,3.0,mesos,Twitter Mesos Q1 Sprint 6
adam-mesos,2015-03-31T21:40:55.000+0000,nnielsen,One of the build artifacts for a release is the python package `mesos.interface`. That needs to be uploaded to PyPi along with a release to allow for users of python frameworks to use that version of mesos.,Documentation,Major,nnielsen,2015-07-24T10:22:57.000+0000,5,Resolved,Complete,Create optional release step: update PyPi repositories,2015-07-24T10:22:57.000+0000,MESOS-2582,2.0,mesos,Mesosphere Q1 Sprint 7 - 4/17
bmahler,2015-03-31T18:51:15.000+0000,bmahler,"We currently have a [""Committers Guide""|https://github.com/apache/mesos/blob/0.22.0/docs/committers-guide.md], however most of this information is relevant to all contributors looking to be participating in the code review process.

I'm proposing we extract much of this information into a more general ""Code Reviewing"" document, and include additional tips, best practices, lessons learned from members of the community.

This would be a great pre-requisite for on-boarding more committers and adding [MAINTAINERS|http://mail-archives.apache.org/mod_mbox/mesos-dev/201502.mbox/%3CCA+8RcoReugMVqoOpsnB8WGYBELa5fHwPA=J=YHJE22iwZvsbeQ@mail.gmail.com%3E].

The committers guide can be more specific to our expectations of committers, so we may want to make this into a ""committership"" document to help set expectations for contributors looking to become committers.",Improvement,Major,bmahler,2015-04-23T23:09:23.000+0000,5,Resolved,Complete,"Document tips, best practices, guidelines for doing code reviews.",2015-04-23T23:09:23.000+0000,MESOS-2581,3.0,mesos,Twitter Mesos Q1 Sprint 6
jvanz,2015-03-31T16:26:37.000+0000,nnielsen,"Similar to MESOS-2577; another common style mistake is to not move curly braces on a newline for function and class declarations:

{code}
class Foo {
  void bar() {
  ...
  }
};
{code}

vs

{code}
class Foo
{
  void bar()
  {
  ...
  }
};
{code}

This should be easy to check with our style checker too.",Improvement,Trivial,nnielsen,,10020,Accepted,In Progress,Add '{' on newline for function declarations in style checker,2016-04-18T22:19:51.000+0000,MESOS-2578,1.0,mesos,
chzhcn,2015-03-30T22:51:15.000+0000,jieyu,"Consider putting symlinks under /var/run/messo/netns. This is because 'ip' command assumes all files under /var/run/netns are valid namespaces without duplication and it has command like:

ip -all netns exec ip link

to list all links for each network namespace.",Bug,Major,jieyu,2015-04-03T17:23:06.000+0000,5,Resolved,Complete,Namespace handle symlinks in port_mapping isolator should not be under /var/run/netns,2015-04-03T17:23:06.000+0000,MESOS-2574,3.0,mesos,Twitter Mesos Q1 Sprint 6
vinodkone,2015-03-26T20:31:10.000+0000,karya,"The main feature of this release is going to be v1 (beta) release of the HTTP scheduler API (part of MESOS-2288 epic).

Unresolved issues tracker: https://issues.apache.org/jira/issues/?jql=project%20%3D%20MESOS%20AND%20status%20!%3D%20Resolved%20AND%20%22Target%20Version%2Fs%22%20%3D%200.24.0%20ORDER%20BY%20status%20DESC",Task,Major,karya,2015-09-05T00:09:38.000+0000,5,Resolved,Complete,0.24.0 release,2016-02-26T21:08:19.000+0000,MESOS-2562,5.0,mesos,Twitter Mesos Q3 Sprint 3
karya,2015-03-26T19:21:30.000+0000,karya,"Assume that FrameworkInfo.id is always set and so need to read/set RunTaskMessage.framework_id.

This should land after https://issues.apache.org/jira/browse/MESOS-2558 has been shipped.",Bug,Major,karya,2015-07-30T17:42:14.000+0000,5,Resolved,Complete,Do not use RunTaskMessage.framework_id.,2015-07-30T17:42:14.000+0000,MESOS-2559,1.0,mesos,
js84,2015-03-26T15:29:45.000+0000,js84,As the problem encountered in MESOS-2419 is a common problem with the default systemd configuration it would make sense to document this in the upgrade guide or somewhere else in the documentation.,Documentation,Critical,js84,2015-07-06T04:28:34.000+0000,5,Resolved,Complete,Document issue with slave recovery when using systemd.,2015-07-07T20:14:03.000+0000,MESOS-2555,1.0,mesos,Mesosphere Sprint 13
anandmazumdar,2015-03-25T22:58:14.000+0000,vinodkone,"Once the scheduler library sends Call messages, we should update it to send Calls as HTTP requests to ""/call"" endpoint on master.",Bug,Major,vinodkone,2015-08-14T06:14:40.000+0000,5,Resolved,Complete,C++ Scheduler library should send HTTP Calls to master,2015-08-14T06:14:40.000+0000,MESOS-2552,3.0,mesos,Mesosphere Sprint 16
vinodkone,2015-03-25T22:56:40.000+0000,vinodkone,"Currently, the C++ library sends different messages to Master instead of a single Call message. To vet the new Call API it should send Call messages. Master should be updated to handle all types of Calls.",Story,Major,vinodkone,2015-07-13T18:41:11.000+0000,5,Resolved,Complete,C++ Scheduler library should send Call messages to Master,2015-07-13T18:41:11.000+0000,MESOS-2551,8.0,mesos,Twitter Mesos Q2 Sprint 6
jieyu,2015-03-25T19:05:40.000+0000,cmaloney,"After the commits:
{code}
Change #21

Category  None
Changed by  Jie Yu <yujie.jay@gmail.com>
Changed at  Wed 25 Mar 2015 00:12:14
Repository  https://git-wip-us.apache.org/repos/asf/mesos.git
Branch  master
Revision  6c6473febac40be1e01c9ab005cca20ad2a48e18
Comments

Disallowed multiple cgroups base hierarchies in tests.
Review: https://reviews.apache.org/r/32452
Changed files

src/tests/mesos.cpp
Change #22

Category  None
Changed by  Jie Yu <yujie.jay@gmail.com>
Changed at  Wed 25 Mar 2015 00:15:37
Repository  https://git-wip-us.apache.org/repos/asf/mesos.git
Branch  master
Revision  212b88c4d20a89dcd9f319b3be984f5646a47499
Comments

Allowed MesosContainerizer to take empty isolation flag.
Review: https://reviews.apache.org/r/32467
{code}

Numerous tests inside our internal CI started failing:
{code}
[ RUN      ] SlaveRecoveryTest/0.RecoverSlaveState
../../src/tests/mesos.cpp:555: Failure
Value of: _baseHierarchy
  Actual: ""/sys/fs/cgroup/cpu,""
Expected: baseHierarchy
Which is: ""/sys/fs/cgroup/""
-------------------------------------------------------------
Multiple cgroups base hierarchies detected:
  '/sys/fs/cgroup/'
  '/sys/fs/cgroup/cpu,'
Mesos does not support multiple cgroups base hierarchies.
Please unmount the corresponding (or all) subsystems.
-------------------------------------------------------------
[  FAILED  ] SlaveRecoveryTest/0.RecoverSlaveState, where TypeParam = mesos::internal::slave::MesosContainerizer (26 ms)
[ RUN      ] SlaveRecoveryTest/0.RecoverStatusUpdateManager
../../src/tests/mesos.cpp:555: Failure
Value of: _baseHierarchy
  Actual: ""/sys/fs/cgroup/cpu,""
Expected: baseHierarchy
Which is: ""/sys/fs/cgroup/""
-------------------------------------------------------------
Multiple cgroups base hierarchies detected:
  '/sys/fs/cgroup/'
  '/sys/fs/cgroup/cpu,'
Mesos does not support multiple cgroups base hierarchies.
Please unmount the corresponding (or all) subsystems.
-------------------------------------------------------------
[  FAILED  ] SlaveRecoveryTest/0.RecoverStatusUpdateManager, where TypeParam = mesos::internal::slave::MesosContainerizer (26 ms)
[ RUN      ] SlaveRecoveryTest/0.ReconnectExecutor
../../src/tests/mesos.cpp:555: Failure
Value of: _baseHierarchy
  Actual: ""/sys/fs/cgroup/cpu,""
Expected: baseHierarchy
Which is: ""/sys/fs/cgroup/""
-------------------------------------------------------------
Multiple cgroups base hierarchies detected:
  '/sys/fs/cgroup/'
  '/sys/fs/cgroup/cpu,'
Mesos does not support multiple cgroups base hierarchies.
Please unmount the corresponding (or all) subsystems.
-------------------------------------------------------------
[  FAILED  ] SlaveRecoveryTest/0.ReconnectExecutor, where TypeParam = mesos::internal::slave::MesosContainerizer (26 ms)
[ RUN      ] SlaveRecoveryTest/0.RecoverUnregisteredExecutor
../../src/tests/mesos.cpp:555: Failure
Value of: _baseHierarchy
  Actual: ""/sys/fs/cgroup/cpu,""
Expected: baseHierarchy
Which is: ""/sys/fs/cgroup/""
-------------------------------------------------------------
Multiple cgroups base hierarchies detected:
  '/sys/fs/cgroup/'
  '/sys/fs/cgroup/cpu,'
Mesos does not support multiple cgroups base hierarchies.
Please unmount the corresponding (or all) subsystems.
-------------------------------------------------------------
[  FAILED  ] SlaveRecoveryTest/0.RecoverUnregisteredExecutor, where TypeParam = mesos::internal::slave::MesosContainerizer (24 ms)
[ RUN      ] SlaveRecoveryTest/0.RecoverTerminatedExecutor
../../src/tests/mesos.cpp:555: Failure
Value of: _baseHierarchy
  Actual: ""/sys/fs/cgroup/cpu,""
Expected: baseHierarchy
Which is: ""/sys/fs/cgroup/""
-------------------------------------------------------------
Multiple cgroups base hierarchies detected:
  '/sys/fs/cgroup/'
  '/sys/fs/cgroup/cpu,'
Mesos does not support multiple cgroups base hierarchies.
Please unmount the corresponding (or all) subsystems.
-------------------------------------------------------------
[  FAILED  ] SlaveRecoveryTest/0.RecoverTerminatedExecutor, where TypeParam = mesos::internal::slave::MesosContainerizer (24 ms)
[ RUN      ] SlaveRecoveryTest/0.RecoverCompletedExecutor
../../src/tests/mesos.cpp:555: Failure
Value of: _baseHierarchy
  Actual: ""/sys/fs/cgroup/cpu,""
Expected: baseHierarchy
Which is: ""/sys/fs/cgroup/""
-------------------------------------------------------------
Multiple cgroups base hierarchies detected:
  '/sys/fs/cgroup/'
  '/sys/fs/cgroup/cpu,'
Mesos does not support multiple cgroups base hierarchies.
Please unmount the corresponding (or all) subsystems.
-------------------------------------------------------------
[  FAILED  ] SlaveRecoveryTest/0.RecoverCompletedExecutor, where TypeParam = mesos::internal::slave::MesosContainerizer (23 ms)
[ RUN      ] SlaveRecoveryTest/0.CleanupExecutor
../../src/tests/mesos.cpp:555: Failure
Value of: _baseHierarchy
  Actual: ""/sys/fs/cgroup/cpu,""
Expected: baseHierarchy
Which is: ""/sys/fs/cgroup/""
-------------------------------------------------------------
Multiple cgroups base hierarchies detected:
  '/sys/fs/cgroup/'
  '/sys/fs/cgroup/cpu,'
Mesos does not support multiple cgroups base hierarchies.
Please unmount the corresponding (or all) subsystems.
-------------------------------------------------------------
[  FAILED  ] SlaveRecoveryTest/0.CleanupExecutor, where TypeParam = mesos::internal::slave::MesosContainerizer (24 ms)
[ RUN      ] SlaveRecoveryTest/0.RemoveNonCheckpointingFramework
../../src/tests/mesos.cpp:555: Failure
Value of: _baseHierarchy
  Actual: ""/sys/fs/cgroup/cpu,""
Expected: baseHierarchy
Which is: ""/sys/fs/cgroup/""
-------------------------------------------------------------
Multiple cgroups base hierarchies detected:
  '/sys/fs/cgroup/'
  '/sys/fs/cgroup/cpu,'
Mesos does not support multiple cgroups base hierarchies.
Please unmount the corresponding (or all) subsystems.
-------------------------------------------------------------
[  FAILED  ] SlaveRecoveryTest/0.RemoveNonCheckpointingFramework, where TypeParam = mesos::internal::slave::MesosContainerizer (25 ms)
[ RUN      ] SlaveRecoveryTest/0.NonCheckpointingFramework
../../src/tests/mesos.cpp:555: Failure
Value of: _baseHierarchy
  Actual: ""/sys/fs/cgroup/cpu,""
Expected: baseHierarchy
Which is: ""/sys/fs/cgroup/""
-------------------------------------------------------------
Multiple cgroups base hierarchies detected:
  '/sys/fs/cgroup/'
  '/sys/fs/cgroup/cpu,'
Mesos does not support multiple cgroups base hierarchies.
Please unmount the corresponding (or all) subsystems.
-------------------------------------------------------------
[  FAILED  ] SlaveRecoveryTest/0.NonCheckpointingFramework, where TypeParam = mesos::internal::slave::MesosContainerizer (24 ms)
[ RUN      ] SlaveRecoveryTest/0.KillTask
../../src/tests/mesos.cpp:555: Failure
Value of: _baseHierarchy
  Actual: ""/sys/fs/cgroup/cpu,""
Expected: baseHierarchy
Which is: ""/sys/fs/cgroup/""
-------------------------------------------------------------
Multiple cgroups base hierarchies detected:
  '/sys/fs/cgroup/'
  '/sys/fs/cgroup/cpu,'
Mesos does not support multiple cgroups base hierarchies.
Please unmount the corresponding (or all) subsystems.
-------------------------------------------------------------
[  FAILED  ] SlaveRecoveryTest/0.KillTask, where TypeParam = mesos::internal::slave::MesosContainerizer (24 ms)
[ RUN      ] SlaveRecoveryTest/0.Reboot
2015-03-25 00:32:56,830:40596(0x7f7cbf4f4700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:32810] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
../../src/tests/mesos.cpp:555: Failure
Value of: _baseHierarchy
  Actual: ""/sys/fs/cgroup/cpu,""
Expected: baseHierarchy
Which is: ""/sys/fs/cgroup/""
-------------------------------------------------------------
Multiple cgroups base hierarchies detected:
  '/sys/fs/cgroup/'
  '/sys/fs/cgroup/cpu,'
Mesos does not support multiple cgroups base hierarchies.
Please unmount the corresponding (or all) subsystems.
-------------------------------------------------------------
[  FAILED  ] SlaveRecoveryTest/0.Reboot, where TypeParam = mesos::internal::slave::MesosContainerizer (24 ms)
[ RUN      ] SlaveRecoveryTest/0.GCExecutor
../../src/tests/mesos.cpp:555: Failure
Value of: _baseHierarchy
  Actual: ""/sys/fs/cgroup/cpu,""
Expected: baseHierarchy
Which is: ""/sys/fs/cgroup/""
-------------------------------------------------------------
Multiple cgroups base hierarchies detected:
  '/sys/fs/cgroup/'
  '/sys/fs/cgroup/cpu,'
Mesos does not support multiple cgroups base hierarchies.
Please unmount the corresponding (or all) subsystems.
-------------------------------------------------------------
[  FAILED  ] SlaveRecoveryTest/0.GCExecutor, where TypeParam = mesos::internal::slave::MesosContainerizer (24 ms)
[ RUN      ] SlaveRecoveryTest/0.ShutdownSlave
../../src/tests/mesos.cpp:555: Failure
Value of: _baseHierarchy
  Actual: ""/sys/fs/cgroup/cpu,""
Expected: baseHierarchy
Which is: ""/sys/fs/cgroup/""
-------------------------------------------------------------
Multiple cgroups base hierarchies detected:
  '/sys/fs/cgroup/'
  '/sys/fs/cgroup/cpu,'
Mesos does not support multiple cgroups base hierarchies.
Please unmount the corresponding (or all) subsystems.
-------------------------------------------------------------
[  FAILED  ] SlaveRecoveryTest/0.ShutdownSlave, where TypeParam = mesos::internal::slave::MesosContainerizer (24 ms)
[ RUN      ] SlaveRecoveryTest/0.ShutdownSlaveSIGUSR1
../../src/tests/mesos.cpp:555: Failure
Value of: _baseHierarchy
  Actual: ""/sys/fs/cgroup/cpu,""
Expected: baseHierarchy
Which is: ""/sys/fs/cgroup/""
-------------------------------------------------------------
Multiple cgroups base hierarchies detected:
  '/sys/fs/cgroup/'
  '/sys/fs/cgroup/cpu,'
Mesos does not support multiple cgroups base hierarchies.
Please unmount the corresponding (or all) subsystems.
-------------------------------------------------------------
[  FAILED  ] SlaveRecoveryTest/0.ShutdownSlaveSIGUSR1, where TypeParam = mesos::internal::slave::MesosContainerizer (24 ms)
[ RUN      ] SlaveRecoveryTest/0.RegisterDisconnectedSlave
../../src/tests/mesos.cpp:555: Failure
Value of: _baseHierarchy
  Actual: ""/sys/fs/cgroup/cpu,""
Expected: baseHierarchy
Which is: ""/sys/fs/cgroup/""
-------------------------------------------------------------
Multiple cgroups base hierarchies detected:
  '/sys/fs/cgroup/'
  '/sys/fs/cgroup/cpu,'
Mesos does not support multiple cgroups base hierarchies.
Please unmount the corresponding (or all) subsystems.
-------------------------------------------------------------
[  FAILED  ] SlaveRecoveryTest/0.RegisterDisconnectedSlave, where TypeParam = mesos::internal::slave::MesosContainerizer (25 ms)
[ RUN      ] SlaveRecoveryTest/0.ReconcileKillTask
../../src/tests/mesos.cpp:555: Failure
Value of: _baseHierarchy
  Actual: ""/sys/fs/cgroup/cpu,""
Expected: baseHierarchy
Which is: ""/sys/fs/cgroup/""
-------------------------------------------------------------
Multiple cgroups base hierarchies detected:
  '/sys/fs/cgroup/'
  '/sys/fs/cgroup/cpu,'
Mesos does not support multiple cgroups base hierarchies.
Please unmount the corresponding (or all) subsystems.
-------------------------------------------------------------
[  FAILED  ] SlaveRecoveryTest/0.ReconcileKillTask, where TypeParam = mesos::internal::slave::MesosContainerizer (24 ms)
[ RUN      ] SlaveRecoveryTest/0.ReconcileShutdownFramework
../../src/tests/mesos.cpp:555: Failure
Value of: _baseHierarchy
  Actual: ""/sys/fs/cgroup/cpu,""
Expected: baseHierarchy
Which is: ""/sys/fs/cgroup/""
-------------------------------------------------------------
Multiple cgroups base hierarchies detected:
  '/sys/fs/cgroup/'
  '/sys/fs/cgroup/cpu,'
Mesos does not support multiple cgroups base hierarchies.
Please unmount the corresponding (or all) subsystems.
-------------------------------------------------------------
[  FAILED  ] SlaveRecoveryTest/0.ReconcileShutdownFramework, where TypeParam = mesos::internal::slave::MesosContainerizer (23 ms)
[ RUN      ] SlaveRecoveryTest/0.ReconcileTasksMissingFromSlave
../../src/tests/mesos.cpp:555: Failure
Value of: _baseHierarchy
  Actual: ""/sys/fs/cgroup/cpu,""
Expected: baseHierarchy
Which is: ""/sys/fs/cgroup/""
-------------------------------------------------------------
Multiple cgroups base hierarchies detected:
  '/sys/fs/cgroup/'
  '/sys/fs/cgroup/cpu,'
Mesos does not support multiple cgroups base hierarchies.
Please unmount the corresponding (or all) subsystems.
-------------------------------------------------------------
[  FAILED  ] SlaveRecoveryTest/0.ReconcileTasksMissingFromSlave, where TypeParam = mesos::internal::slave::MesosContainerizer (25 ms)
[ RUN      ] SlaveRecoveryTest/0.SchedulerFailover
../../src/tests/mesos.cpp:555: Failure
Value of: _baseHierarchy
  Actual: ""/sys/fs/cgroup/cpu,""
Expected: baseHierarchy
Which is: ""/sys/fs/cgroup/""
-------------------------------------------------------------
Multiple cgroups base hierarchies detected:
  '/sys/fs/cgroup/'
  '/sys/fs/cgroup/cpu,'
Mesos does not support multiple cgroups base hierarchies.
Please unmount the corresponding (or all) subsystems.
-------------------------------------------------------------
[  FAILED  ] SlaveRecoveryTest/0.SchedulerFailover, where TypeParam = mesos::internal::slave::MesosContainerizer (26 ms)
[ RUN      ] SlaveRecoveryTest/0.PartitionedSlave
../../src/tests/mesos.cpp:555: Failure
Value of: _baseHierarchy
  Actual: ""/sys/fs/cgroup/cpu,""
Expected: baseHierarchy
Which is: ""/sys/fs/cgroup/""
-------------------------------------------------------------
Multiple cgroups base hierarchies detected:
  '/sys/fs/cgroup/'
  '/sys/fs/cgroup/cpu,'
Mesos does not support multiple cgroups base hierarchies.
Please unmount the corresponding (or all) subsystems.
-------------------------------------------------------------
[  FAILED  ] SlaveRecoveryTest/0.PartitionedSlave, where TypeParam = mesos::internal::slave::MesosContainerizer (26 ms)
[ RUN      ] SlaveRecoveryTest/0.MasterFailover
../../src/tests/mesos.cpp:555: Failure
Value of: _baseHierarchy
  Actual: ""/sys/fs/cgroup/cpu,""
Expected: baseHierarchy
Which is: ""/sys/fs/cgroup/""
-------------------------------------------------------------
Multiple cgroups base hierarchies detected:
  '/sys/fs/cgroup/'
  '/sys/fs/cgroup/cpu,'
Mesos does not support multiple cgroups base hierarchies.
Please unmount the corresponding (or all) subsystems.
-------------------------------------------------------------
[  FAILED  ] SlaveRecoveryTest/0.MasterFailover, where TypeParam = mesos::internal::slave::MesosContainerizer (26 ms)
[ RUN      ] SlaveRecoveryTest/0.MultipleFrameworks
../../src/tests/mesos.cpp:555: Failure
Value of: _baseHierarchy
  Actual: ""/sys/fs/cgroup/cpu,""
Expected: baseHierarchy
Which is: ""/sys/fs/cgroup/""
-------------------------------------------------------------
Multiple cgroups base hierarchies detected:
  '/sys/fs/cgroup/'
  '/sys/fs/cgroup/cpu,'
Mesos does not support multiple cgroups base hierarchies.
Please unmount the corresponding (or all) subsystems.
-------------------------------------------------------------
[  FAILED  ] SlaveRecoveryTest/0.MultipleFrameworks, where TypeParam = mesos::internal::slave::MesosContainerizer (26 ms)
[ RUN      ] SlaveRecoveryTest/0.MultipleSlaves
../../src/tests/mesos.cpp:555: Failure
Value of: _baseHierarchy
  Actual: ""/sys/fs/cgroup/cpu,""
Expected: baseHierarchy
Which is: ""/sys/fs/cgroup/""
-------------------------------------------------------------
Multiple cgroups base hierarchies detected:
  '/sys/fs/cgroup/'
  '/sys/fs/cgroup/cpu,'
Mesos does not support multiple cgroups base hierarchies.
Please unmount the corresponding (or all) subsystems.
-------------------------------------------------------------
[  FAILED  ] SlaveRecoveryTest/0.MultipleSlaves, where TypeParam = mesos::internal::slave::MesosContainerizer (26 ms)
[ RUN      ] SlaveRecoveryTest/0.RestartBeforeContainerizerLaunch
../../src/tests/mesos.cpp:555: Failure
Value of: _baseHierarchy
  Actual: ""/sys/fs/cgroup/cpu,""
Expected: baseHierarchy
Which is: ""/sys/fs/cgroup/""
-------------------------------------------------------------
Multiple cgroups base hierarchies detected:
  '/sys/fs/cgroup/'
  '/sys/fs/cgroup/cpu,'
Mesos does not support multiple cgroups base hierarchies.
Please unmount the corresponding (or all) subsystems.
-------------------------------------------------------------
[  FAILED  ] SlaveRecoveryTest/0.RestartBeforeContainerizerLaunch, where TypeParam = mesos::internal::slave::MesosContainerizer (26 ms)
[----------] 24 tests from SlaveRecoveryTest/0 (596 ms total)

[----------] 4 tests from MesosContainerizerSlaveRecoveryTest
[ RUN      ] MesosContainerizerSlaveRecoveryTest.ResourceStatistics
../../src/tests/mesos.cpp:555: Failure
Value of: _baseHierarchy
  Actual: ""/sys/fs/cgroup/cpu,""
Expected: baseHierarchy
Which is: ""/sys/fs/cgroup/""
-------------------------------------------------------------
Multiple cgroups base hierarchies detected:
  '/sys/fs/cgroup/'
  '/sys/fs/cgroup/cpu,'
Mesos does not support multiple cgroups base hierarchies.
Please unmount the corresponding (or all) subsystems.
-------------------------------------------------------------
[  FAILED  ] MesosContainerizerSlaveRecoveryTest.ResourceStatistics (25 ms)
[ RUN      ] MesosContainerizerSlaveRecoveryTest.CGROUPS_ROOT_PerfRollForward
../../src/tests/mesos.cpp:555: Failure
Value of: _baseHierarchy
  Actual: ""/sys/fs/cgroup/cpu,""
Expected: baseHierarchy
Which is: ""/sys/fs/cgroup/""
-------------------------------------------------------------
Multiple cgroups base hierarchies detected:
  '/sys/fs/cgroup/'
  '/sys/fs/cgroup/cpu,'
Mesos does not support multiple cgroups base hierarchies.
Please unmount the corresponding (or all) subsystems.
-------------------------------------------------------------
[  FAILED  ] MesosContainerizerSlaveRecoveryTest.CGROUPS_ROOT_PerfRollForward (24 ms)
[ RUN      ] MesosContainerizerSlaveRecoveryTest.CGROUPS_ROOT_PidNamespaceForward
../../src/tests/mesos.cpp:555: Failure
Value of: _baseHierarchy
  Actual: ""/sys/fs/cgroup/cpu,""
Expected: baseHierarchy
Which is: ""/sys/fs/cgroup/""
-------------------------------------------------------------
Multiple cgroups base hierarchies detected:
  '/sys/fs/cgroup/'
  '/sys/fs/cgroup/cpu,'
Mesos does not support multiple cgroups base hierarchies.
Please unmount the corresponding (or all) subsystems.
-------------------------------------------------------------
[  FAILED  ] MesosContainerizerSlaveRecoveryTest.CGROUPS_ROOT_PidNamespaceForward (25 ms)
[ RUN      ] MesosContainerizerSlaveRecoveryTest.CGROUPS_ROOT_PidNamespaceBackward
../../src/tests/mesos.cpp:555: Failure
Value of: _baseHierarchy
  Actual: ""/sys/fs/cgroup/cpu,""
Expected: baseHierarchy
Which is: ""/sys/fs/cgroup/""
-------------------------------------------------------------
Multiple cgroups base hierarchies detected:
  '/sys/fs/cgroup/'
  '/sys/fs/cgroup/cpu,'
Mesos does not support multiple cgroups base hierarchies.
Please unmount the corresponding (or all) subsystems.
-------------------------------------------------------------
[  FAILED  ] MesosContainerizerSlaveRecoveryTest.CGROUPS_ROOT_PidNamespaceBackward (24 ms)
[----------] 4 tests from MesosContainerizerSlaveRecoveryTest (98 ms total)
{code}

{code}
[  FAILED  ] 28 tests, listed below:
[  FAILED  ] SlaveRecoveryTest/0.RecoverSlaveState, where TypeParam = mesos::internal::slave::MesosContainerizer
[  FAILED  ] SlaveRecoveryTest/0.RecoverStatusUpdateManager, where TypeParam = mesos::internal::slave::MesosContainerizer
[  FAILED  ] SlaveRecoveryTest/0.ReconnectExecutor, where TypeParam = mesos::internal::slave::MesosContainerizer
[  FAILED  ] SlaveRecoveryTest/0.RecoverUnregisteredExecutor, where TypeParam = mesos::internal::slave::MesosContainerizer
[  FAILED  ] SlaveRecoveryTest/0.RecoverTerminatedExecutor, where TypeParam = mesos::internal::slave::MesosContainerizer
[  FAILED  ] SlaveRecoveryTest/0.RecoverCompletedExecutor, where TypeParam = mesos::internal::slave::MesosContainerizer
[  FAILED  ] SlaveRecoveryTest/0.CleanupExecutor, where TypeParam = mesos::internal::slave::MesosContainerizer
[  FAILED  ] SlaveRecoveryTest/0.RemoveNonCheckpointingFramework, where TypeParam = mesos::internal::slave::MesosContainerizer
[  FAILED  ] SlaveRecoveryTest/0.NonCheckpointingFramework, where TypeParam = mesos::internal::slave::MesosContainerizer
[  FAILED  ] SlaveRecoveryTest/0.KillTask, where TypeParam = mesos::internal::slave::MesosContainerizer
[  FAILED  ] SlaveRecoveryTest/0.Reboot, where TypeParam = mesos::internal::slave::MesosContainerizer
[  FAILED  ] SlaveRecoveryTest/0.GCExecutor, where TypeParam = mesos::internal::slave::MesosContainerizer
[  FAILED  ] SlaveRecoveryTest/0.ShutdownSlave, where TypeParam = mesos::internal::slave::MesosContainerizer
[  FAILED  ] SlaveRecoveryTest/0.ShutdownSlaveSIGUSR1, where TypeParam = mesos::internal::slave::MesosContainerizer
[  FAILED  ] SlaveRecoveryTest/0.RegisterDisconnectedSlave, where TypeParam = mesos::internal::slave::MesosContainerizer
[  FAILED  ] SlaveRecoveryTest/0.ReconcileKillTask, where TypeParam = mesos::internal::slave::MesosContainerizer
[  FAILED  ] SlaveRecoveryTest/0.ReconcileShutdownFramework, where TypeParam = mesos::internal::slave::MesosContainerizer
[  FAILED  ] SlaveRecoveryTest/0.ReconcileTasksMissingFromSlave, where TypeParam = mesos::internal::slave::MesosContainerizer
[  FAILED  ] SlaveRecoveryTest/0.SchedulerFailover, where TypeParam = mesos::internal::slave::MesosContainerizer
[  FAILED  ] SlaveRecoveryTest/0.PartitionedSlave, where TypeParam = mesos::internal::slave::MesosContainerizer
[  FAILED  ] SlaveRecoveryTest/0.MasterFailover, where TypeParam = mesos::internal::slave::MesosContainerizer
[  FAILED  ] SlaveRecoveryTest/0.MultipleFrameworks, where TypeParam = mesos::internal::slave::MesosContainerizer
[  FAILED  ] SlaveRecoveryTest/0.MultipleSlaves, where TypeParam = mesos::internal::slave::MesosContainerizer
[  FAILED  ] SlaveRecoveryTest/0.RestartBeforeContainerizerLaunch, where TypeParam = mesos::internal::slave::MesosContainerizer
[  FAILED  ] MesosContainerizerSlaveRecoveryTest.ResourceStatistics
[  FAILED  ] MesosContainerizerSlaveRecoveryTest.CGROUPS_ROOT_PerfRollForward
[  FAILED  ] MesosContainerizerSlaveRecoveryTest.CGROUPS_ROOT_PidNamespaceForward
[  FAILED  ] MesosContainerizerSlaveRecoveryTest.CGROUPS_ROOT_PidNamespaceBackward
{code}

Docker Info:
Docker is run with `--privileged` and `-v /sys/fs/cgroup:/sys/fs/cgroup:RW`

Dockerfile
{code}
FROM centos:centos7
RUN yum install -y epel-release gcc python-devel
RUN yum install -y python-pip
RUN pip install buildbot-slave
RUN pip install supervisor
RUN yum install -y rpm-build redhat-rpm-config autoconf make gcc gcc-c++ patch libtool git python-devel ruby-devel java-1.7.0-openjdk-devel zlib-devel libcurl-devel openssl-devel cyrus-sasl-devel rubygems apr-devel apr-util-devel subversion-devel maven libselinux-python cyrus-sasl-md5 perf
RUN gem install fpm
RUN buildslave create-slave -r -n slave build.mesosphere.com docker Rwtz49vW1XtM/uUik0qO3o9sP
ENV JAVA_HOME /usr/lib/jvm/java-1.7.0
ENV GTEST_FILTER -CgroupsAnyHierarchyTest.*:CgroupsAnyHierarchyWithCpuMemoryTest.*:CgroupsAnyHierarchyWithCpuAcctMemoryTest.ROOT_CGROUPS_Stat:UserCgroupIsolatorTest*:NsTest.ROOT_setns:ContainerizerTest.ROOT_CGROUPS_BalloonFramework
COPY supervisord.conf /etc/supervisord.conf
CMD supervisord -c /etc/supervisord.conf
{code}

Comands it runs:
{code}
./bootstrap
./configure --enable-optimize --prefix=/usr
make distcheck
{code}",Bug,Blocker,cmaloney,2015-03-25T22:02:34.000+0000,5,Resolved,Complete,new `make distcheck` failures inside a docker container,2015-04-03T22:55:51.000+0000,MESOS-2548,1.0,mesos,Twitter Mesos Q1 Sprint 5
jieyu,2015-03-25T17:48:13.000+0000,jieyu,"Leaked bind mount under /var/run/netns for port mapping isolator is a known issue. There are many ways it can get leaked. For example, if the slave crashes after creating the bind mount but before creating the veth, the bind mount will be leaked. Also, if the detached unmount does not finish in time and the subsequent os::rm fails, the bind mount will be leaked as well.

Since leaked bind mount is inevitable, we need to clean them up during startup (slave recovery).",Improvement,Major,jieyu,2015-03-27T00:31:26.000+0000,5,Resolved,Complete,Cleanup stale bind mounts for port mapping isolator during slave recovery.,2015-03-27T00:31:26.000+0000,MESOS-2547,2.0,mesos,Twitter Mesos Q1 Sprint 5
js84,2015-03-25T12:33:06.000+0000,bernd-mesos,"Create a developer guide for libprocess that explains the philosophy behind it and explains the most important features as well as the prevalent use patterns in Mesos with examples. 

This could be similar to stout/README.md.
",Documentation,Major,bernd-mesos,2015-06-23T13:50:22.000+0000,5,Resolved,Complete,Developer guide for libprocess,2015-06-24T15:49:17.000+0000,MESOS-2545,2.0,mesos,Mesosphere Sprint 13
jieyu,2015-03-24T18:36:15.000+0000,js84,"As all the explicitly set flags are defaults, we can remove them and simplify the code.  MESOS-2375 removed other occurrences of these default flags.",Bug,Minor,js84,2015-03-26T00:36:49.000+0000,5,Resolved,Complete,Remove unnecessary default flags from PortMappingMesosTest.,2015-03-26T00:36:49.000+0000,MESOS-2538,1.0,mesos,Twitter Mesos Q1 Sprint 5
idownes,2015-03-23T23:28:52.000+0000,bmahler,"From MESOS-2300 as well, it looks like this test is not reliable:

{code}
[ RUN      ] PerfTest.ROOT_SampleInit
../../src/tests/perf_tests.cpp:147: Failure
Expected: (0u) < (statistics.get().cycles()), actual: 0 vs 0
../../src/tests/perf_tests.cpp:150: Failure
Expected: (0.0) < (statistics.get().task_clock()),
{code}

It looks like this test samples PID 1, which is either {{init}} or {{systemd}}. Per a chat with [~idownes] this should probably sample something that is guaranteed to be consuming cycles.",Bug,Major,bmahler,2015-03-26T22:41:48.000+0000,5,Resolved,Complete,PerfTest.ROOT_SampleInit test fails.,2015-04-03T22:55:16.000+0000,MESOS-2534,2.0,mesos,Twitter Mesos Q1 Sprint 5
jieyu,2015-03-20T23:58:43.000+0000,jieyu,"This serves two purposes:
1) Allows us to enter the network namespace using container ID (instead of pid): ""ip netns exec <ContainerID> [commands] [args]"".
2) Allows us to get container ID for orphan containers during recovery. This will be helpful for solving MESOS-2367.

The challenge here is to solve it in a backward compatible way. I propose to create symlinks under /var/run/netns. For example:
/var/run/netns/containeridxxxx --> /var/run/netns/12345
(12345 is the pid)

The old code will only remove the bind mounts and leave the symlinks, which I think is fine since containerid is globally unique (uuid).",Improvement,Major,jieyu,2015-03-27T00:31:47.000+0000,5,Resolved,Complete,Symlink the namespace handle with ContainerID for the port mapping isolator.,2015-03-27T00:31:47.000+0000,MESOS-2528,3.0,mesos,Twitter Mesos Q1 Sprint 5
bmahler,2015-03-19T19:22:32.000+0000,drobinson,"Querying /master/state.json is an expensive operation when a cluster is large, and it's possible to DOS the master via frequent and repeated queries (which is a separate problem). Querying the endpoint results in a log entry being written, but the entry lacks useful information, such as an IP address, response code and response size. These details are useful for tracking down who/what is querying the endpoint. Consider adding these details to the log entry, or even writing a separate [access|https://httpd.apache.org/docs/trunk/logs.html#accesslog] [log|https://httpd.apache.org/docs/trunk/logs.html#common]. Also consider writing log entries for _all_ HTTP requests (/metrics/snapshot produces no log entries).

{noformat:title=sample log entry}
I0319 18:06:18.824846 10521 http.cpp:478] HTTP request for '/master/state.json'
{noformat}",Improvement,Minor,drobinson,2015-06-15T22:44:27.000+0000,5,Resolved,Complete,Log IP addresses from HTTP requests,2015-06-15T23:19:39.000+0000,MESOS-2519,3.0,mesos,Twitter Q2 Sprint 2
wangcong,2015-03-18T21:35:29.000+0000,wangcong,"When we enable bandwidth cap, htb is used on egress side inside containers, however, the default leaf qdisc for a htb class is still pfifo_fast, which is known to have buffer bloat. Change the default leaf qdisc to fq_codel too:

`tc qd add dev eth0 parent 1:1 fq_codel`

I can no longer see packet drops after this change.",Bug,Major,wangcong,2015-03-23T18:36:53.000+0000,5,Resolved,Complete,Change the default leaf qdisc to fq_codel inside containers,2015-06-18T18:15:31.000+0000,MESOS-2514,1.0,mesos,Twitter Mesos Q1 Sprint 5
bernd-mesos,2015-03-17T19:20:43.000+0000,vinodkone,"Observed in our internal CI.

{code}
[ RUN      ] FetcherTest.ExtractNotExecutable
Using temporary directory '/tmp/FetcherTest_ExtractNotExecutable_R5R7Cn'
tar: Removing leading `/' from member names
I0316 18:55:48.509306 14678 fetcher.cpp:155] Starting to fetch URIs for container: de1e5165-82b4-434b-9149-8667cf652c64, directory: /tmp/FetcherTest_ExtractNotExecutable_R5R7Cn
I0316 18:55:48.509845 14678 fetcher.cpp:238] Fetching URIs using command '/var/jenkins/workspace/mesos-fedora-20-gcc/src/mesos-fetcher'
I0316 18:55:48.568611 15028 logging.cpp:177] Logging to STDERR
I0316 18:55:48.574928 15028 fetcher.cpp:214] Fetching URI '/tmp/DIjmjV.tar.gz'
I0316 18:55:48.575166 15028 fetcher.cpp:194] Copying resource from '/tmp/DIjmjV.tar.gz' to '/tmp/FetcherTest_ExtractNotExecutable_R5R7Cn'
tar: This does not look like a tar archive
tar: Exiting with failure status due to previous errors
Failed to extract /tmp/FetcherTest_ExtractNotExecutable_R5R7Cn/DIjmjV.tar.gz:Failed to extract: command tar -C '/tmp/FetcherTest_ExtractNotExecutable_R5R7Cn' -xf '/tmp/FetcherTest_ExtractNotExecutable_R5R7Cn/DIjmjV.tar.gz' exited with status: 512
tests/fetcher_tests.cpp:686: Failure
(fetch).failure(): Failed to fetch URIs for container 'de1e5165-82b4-434b-9149-8667cf652c64'with exit status: 256
[  FAILED  ] FetcherTest.ExtractNotExecutable (208 ms)
{code}",Bug,Major,vinodkone,2015-11-30T09:21:11.000+0000,5,Resolved,Complete,FetcherTest.ExtractNotExecutable is flaky,2015-11-30T09:21:11.000+0000,MESOS-2512,2.0,mesos,
bmahler,2015-03-17T01:13:50.000+0000,bmahler,"For large clusters, when a lot of slaves are registering, the master gets backlogged processing registration requests. {{perf}} revealed the following:

{code}
Events: 14K cycles
 25.44%  libmesos-0.22.0-x.so  [.] mesos::internal::master::Master::registerSlave(process::UPID const&, mesos::SlaveInfo const&, std::vector<mesos::Resource, std::allocator<mesos::Resource> > cons
 11.18%  libmesos-0.22.0-x.so  [.] pipecb
  5.88%  libc-2.5.so             [.] malloc_consolidate
  5.33%  libc-2.5.so             [.] _int_free
  5.25%  libc-2.5.so             [.] malloc
  5.23%  libc-2.5.so             [.] _int_malloc
  4.11%  libstdc++.so.6.0.8      [.] std::string::assign(std::string const&)
  3.22%  libmesos-0.22.0-x.so  [.] mesos::Resource::SharedDtor()
  3.10%  [kernel]                [k] _raw_spin_lock
  1.97%  libmesos-0.22.0-x.so  [.] mesos::Attribute::SharedDtor()
  1.28%  libc-2.5.so             [.] memcmp
  1.08%  libc-2.5.so             [.] free
{code}

This is likely because we loop over all the slaves for each registration:

{code}
void Master::registerSlave(
    const UPID& from,
    const SlaveInfo& slaveInfo,
    const vector<Resource>& checkpointedResources,
    const string& version)
{
  // ...

  // Check if this slave is already registered (because it retries).
  foreachvalue (Slave* slave, slaves.registered) {
    if (slave->pid == from) {
      // ...
    }
  }
  // ...
}
{code}",Improvement,Major,bmahler,2015-05-19T19:25:49.000+0000,5,Resolved,Complete,Performance issue in the master when a large number of slaves are registering.,2015-05-19T19:25:49.000+0000,MESOS-2507,5.0,mesos,Twitter Q2 Sprint 3 - 5/11
js84,2015-03-16T15:49:57.000+0000,bernd-mesos,"Create a description of the Doxygen style to use for libprocess documentation. 

It is expected that this will later also become the Doxygen style for stout and Mesos, but we are working on libprocess only for now.

Possible outcome: a file named docs/doxygen-style.md

We hope for much input and expect a lot of discussion!
",Documentation,Major,bernd-mesos,2015-06-23T09:48:27.000+0000,5,Resolved,Complete,Doxygen style for libprocess,2015-06-23T10:44:57.000+0000,MESOS-2501,1.0,mesos,Mesosphere Sprint 13
js84,2015-03-16T15:45:33.000+0000,bernd-mesos,"Goals: 
- Initial doxygen setup. 
- Enable interested developers to generate already available doxygen content locally in their workspace and view it.
- Form the basis for future contributions of more doxygen content.

1. Devise a way to use Doxygen with Mesos source code. (For example, solve this by adding optional brew/apt-get installation to the ""Getting Started"" doc.)
2. Create a make target for libprocess documentation that can be manually triggered.
3. Create initial library top level documentation.
4. Enhance one header file with Doxygen. Make sure the generated output has all necessary links to navigate from the lib to the file and back, etc.
",Documentation,Major,bernd-mesos,2015-06-16T09:17:36.000+0000,5,Resolved,Complete,Doxygen setup for libprocess,2015-06-16T09:17:36.000+0000,MESOS-2500,2.0,mesos,Mesosphere Q1 Sprint 5 - 3/20
ijimenez,2015-03-13T19:03:32.000+0000,ijimenez,/call endpoint will return a 202 accepted code but has to do some basic validations before. In case of invalidation it will return a 4xx code. We have to create a mechanism that will validate the 'request' and send back the appropriate code.,Bug,Major,ijimenez,2015-08-13T19:09:42.000+0000,5,Resolved,Complete,Create synchronous validations for Calls,2015-08-13T19:09:42.000+0000,MESOS-2497,8.0,mesos,Mesosphere Sprint 15
mcypark,2015-03-13T08:17:28.000+0000,mcypark,"h3. Goal

The goal for this task is to persist the reservation state stored on the master on the corresponding slave. The {{needCheckpointing}} predicate is used to capture the condition for which a resource needs to be checkpointed. Currently the only condition is {{isPersistentVolume}}. We'll update this to include dynamically reserved resources.

h3. Expected Outcome

* The dynamically reserved resources will be persisted on the slave.",Task,Major,mcypark,2015-05-13T21:09:22.000+0000,5,Resolved,Complete,Persist the reservation state on the slave,2015-07-02T06:51:13.000+0000,MESOS-2491,5.0,mesos,Mesosphere Q1 Sprint 6 - 4/3
mcypark,2015-03-13T07:55:55.000+0000,mcypark,"h3. Goal

This is the first step to supporting dynamic reservations. The goal of this task is to enable a framework to reply to a resource offer with *Reserve* and *Unreserve* offer operations as defined by {{Offer::Operation}} in {{mesos.proto}}.

h3. Overview

It's divided into a few subtasks so that it's clear what the small chunks to be addressed are. In summary, we need to introduce the {{Resource::ReservationInfo}} protobuf message to encapsulate the reservation information, enable the C++ {{Resources}} class to handle it then enable the master to handle reservation operations.

h3. Expected Outcome

* The framework will be able to send back reservation operations to (un)reserve resources.
* The reservations are kept only in the master since we don't send the {{CheckpointResources}} message to checkpoint the reservations on the slave yet.
* The reservations are considered to be reserved for the framework's role.",Task,Major,mcypark,2015-05-12T20:40:40.000+0000,5,Resolved,Complete,Enable a framework to perform reservation operations.,2015-07-02T20:41:12.000+0000,MESOS-2489,4.0,mesos,Mesosphere Q1 Sprint 6 - 4/3
bmahler,2015-03-12T19:44:00.000+0000,bmahler,"Currently we only expose a single removal metric ({{""master/slave_removals""}}) which makes it difficult to distinguish between removal reasons in the alerting.

Currently, a slave can be removed for the following reasons:

# Health checks failed.
# Slave unregistered.
# Slave was replaced by a new slave (on the same endpoint).

In the case of (2), we expect this to be due to maintenance and don't want to be notified as strongly as with health check failures.",Improvement,Major,bmahler,2015-05-14T00:01:31.000+0000,5,Resolved,Complete,Add ability to distinguish slave removals metrics by reason.,2015-05-21T18:31:11.000+0000,MESOS-2485,3.0,mesos,Twitter Mesos Q1 Sprint 6
mcypark,2015-03-11T04:22:01.000+0000,mcypark,{{Resources::apply}} currently only handles {{Create}} and {{Destroy}} operations which exist for persistent volumes. We need to handle the {{Reserve}} and {{Unreserve}} operations for dynamic reservations as well.,Task,Major,mcypark,2015-05-12T20:41:27.000+0000,5,Resolved,Complete,Enable Resources::apply to handle reservation operations.,2015-07-02T20:41:12.000+0000,MESOS-2477,3.0,mesos,
mcypark,2015-03-11T04:15:12.000+0000,mcypark,"After [MESOS-2475|https://issues.apache.org/jira/browse/MESOS-2475], our C++ {{Resources}} class needs to know how to handle {{Resource}} protobuf messages that have the {{reservation}} field set.",Task,Major,mcypark,2015-05-12T20:41:18.000+0000,5,Resolved,Complete,Enable Resources to handle Resource::ReservationInfo,2015-07-02T20:41:12.000+0000,MESOS-2476,2.0,mesos,Mesosphere Q1 Sprint 5 - 3/20
mcypark,2015-03-11T03:48:36.000+0000,mcypark,"The {{Resource::ReservationInfo}} protobuf message encapsulates information needed to keep track of reservations. It's named {{ReservationInfo}} rather than {{Reservation}} to keep consistency with {{Resource::DiskInfo}}.

Here's what it will look like:

{code}
message ReservationInfo {
  // Indicates the principal of the operator or framework that created the
  // reservation. This is used to determine whether this resource can be 
  // unreserved by an operator or a framework by checking the
  // ""unreserve"" ACL.
  required string principal;
}

// If this is set, this resource was dynamically reserved by an
// operator or a framework. Otherwise, this resource was
// statically configured by an operator via the --resources flag.
optional ReservationInfo reservation;
{code}",Task,Major,mcypark,2015-05-12T20:41:07.000+0000,5,Resolved,Complete,Add the Resource::ReservationInfo protobuf message,2015-07-02T20:41:12.000+0000,MESOS-2475,2.0,mesos,Mesosphere Q1 Sprint 5 - 3/20
vinodkone,2015-03-09T23:39:59.000+0000,vinodkone,"With the current refactoring to IP it looks like master and slave can no longer bind to 127.0.0.1 even if explicitly requested via ""--ip"" flag. 

Among other things, this breaks the balloon framework test which uses this flag.",Bug,Major,vinodkone,2015-03-10T17:35:10.000+0000,5,Resolved,Complete,Mesos master/slave should be able to bind to 127.0.0.1 if explicitly requested,2015-03-10T17:35:10.000+0000,MESOS-2469,1.0,mesos,Twitter Mesos Q1 Sprint 4
greggomann,2015-03-09T16:36:44.000+0000,jieyu,"Currently, we used a customized format for --resources flag. As we introduce more and more stuffs (e.g., persistence, reservation) in Resource object, we need a more generic way to specify --resources.

For backward compatibility, we can scan the first character. If it is '[', then we invoke the JSON parser. Otherwise, we use the existing parser.",Improvement,Major,jieyu,2015-11-09T18:29:52.000+0000,5,Resolved,Complete,Allow --resources flag to take JSON.,2015-11-09T18:29:53.000+0000,MESOS-2467,3.0,mesos,Mesosphere Sprint 20
greggomann,2015-03-09T10:43:56.000+0000,arojas,"libprocess uses a set of environment variables to modify its behaviour; however, these variables are not documented anywhere, nor it is defined where the documentation should be.

What would be needed is a decision whether the environment variables should be documented (a new doc file or reusing an existing one), and then add the documentation there.

After searching in the code, these are the variables which need to be documented:

# {{LIBPROCESS_IP}}
# {{LIBPROCESS_PORT}}
# {{LIBPROCESS_ADVERTISE_IP}}
# {{LIBPROCESS_ADVERTISE_PORT}}",Documentation,Major,arojas,2015-09-04T17:03:42.000+0000,5,Resolved,Complete,Write documentation for all the LIBPROCESS_* environment variables.,2015-09-04T17:03:51.000+0000,MESOS-2466,2.0,mesos,Mesosphere Sprint 17
tillt,2015-03-09T00:44:53.000+0000,tillt,"When slave authentication fails, the following attempt to transmit a {{UnregisterSlaveMessage}} may cause a crash within the slave.

{noformat}
E0309 01:08:34.819758 336699392 slave.cpp:740] Master master@192.168.178.20:5050 refused authentication
I0309 01:08:34.819787 336699392 slave.cpp:538] Master refused authentication; unregistering and shutting down

[libprotobuf FATAL google/protobuf/message_lite.cc:273] CHECK failed: IsInitialized(): Can't serialize message of type ""mesos.internal.UnregisterSlaveMessage"" because it is missing required fields: slave_id.value
libprocess: slave(1)@192.168.178.20:5051 terminating due to CHECK failed: IsInitialized(): Can't serialize message of type ""mesos.internal.UnregisterSlaveMessage"" because it is missing required fields: slave_id.value
{noformat}

The problem here is the following code:

{noformat}
      UnregisterSlaveMessage message_;
      message_.mutable_slave_id()->MergeFrom(info.id());
{noformat}

Authentication happens before registration. {{info.id}} is an optional member (of {{SlaveInfo}}) and not known yet. It is set later, while registering. So {{slave_id}} will remain unset.",Bug,Major,tillt,2015-06-21T08:47:33.000+0000,5,Resolved,Complete,Authentication failure may lead to slave crash,2015-06-21T08:47:33.000+0000,MESOS-2464,1.0,mesos,Mesosphere Sprint 12
jieyu,2015-03-06T22:25:45.000+0000,idownes,"Currently, children forked by the slave, including those through Subprocess, will continue running if the slave exits. For some processes, including helper processes like the fetcher, du, or perf, we'd like them to be terminated when the slave exits.

Add support to Subprocess to optionally set a DEATHSIG for the child, e.g., setting SIGTERM would mean the child would get SIGTERM when the slave terminates.

This can be done (*after forking*) with PR_SET_DEATHSIG. See ""man prctl"". It is preserved through an exec call.",Improvement,Minor,idownes,2015-04-03T21:38:42.000+0000,5,Resolved,Complete,Add option for Subprocess to set a death signal for the forked child,2015-04-28T01:16:23.000+0000,MESOS-2462,3.0,mesos,Twitter Mesos Q1 Sprint 6
jieyu,2015-03-06T21:27:40.000+0000,idownes,"The slave can optionally be put into its own cgroups for a list of subsystems, e.g., for monitoring of memory and cpu. See the slave flag: --slave_subsystems

It currently refuses to start if there are any processes in its cgroups - this could be another slave or some subprocess started by a previous slave - and only logs the pids of those processes.

Improve this to log details about the processes: suggest at least the process command, uid running it, and perhaps its start time.",Improvement,Minor,idownes,2015-04-02T19:06:28.000+0000,5,Resolved,Complete,Slave should provide details on processes running in its cgroups,2015-04-29T07:46:41.000+0000,MESOS-2461,1.0,mesos,Twitter Mesos Q1 Sprint 6
neilc,2015-03-05T23:38:32.000+0000,jieyu,"Persistent volumes will not be released automatically.

So we probably need an endpoint for operators to forcefully release persistent volumes. We probably need to add principal to Persistence struct and use ACLs to control who can release what.

Additionally, it would be useful to have an endpoint for operators to create persistent volumes.",Task,Critical,jieyu,2015-11-27T01:49:18.000+0000,5,Resolved,Complete,Add operator endpoints to create/destroy persistent volumes.,2015-11-27T01:49:18.000+0000,MESOS-2455,3.0,mesos,Mesosphere Sprint 16
idownes,2015-03-05T20:21:01.000+0000,idownes,"/proc/self/mountinfo provides mount information specific to the calling process. This includes information on optional fields describing mount propagation, e.g., shared/slave mounts. 

Initially, add this to linux/fs then perhaps move existing users of MountTable to use the mountinfo, deprecating and removing the mostly (but not entirely) redundant code.",Improvement,Major,idownes,2015-03-12T16:16:34.000+0000,5,Resolved,Complete,Add support for /proc/self/mountinfo on Linux,2015-03-12T16:16:34.000+0000,MESOS-2454,3.0,mesos,Twitter Mesos Q1 Sprint 4
jieyu,2015-03-05T01:28:08.000+0000,jieyu,"The bug was introduced in this review:
https://reviews.apache.org/r/29687 

RunState.directory points to the metadata directory.

This would cause the PosixDiskIsolator to report incorrect disk usages after slave recovery.

We also need a test to test the slave recovery path for the PosixDiskIsolator.",Bug,Critical,jieyu,2015-03-05T22:43:08.000+0000,5,Resolved,Complete,The recovered executor directory points to the meta directory.,2015-03-05T22:43:08.000+0000,MESOS-2452,2.0,mesos,Twitter Mesos Q1 Sprint 4
jieyu,2015-03-04T01:27:43.000+0000,jieyu,"This is a regression introduced during the internal namespace refactor.

0.21.0 master:
{noformat}
I0224 02:43:29.806895 50982 replica.cpp:661] Replica learned APPEND action at position 1655
{noformat}

0.22.0 master:
{noformat}
I0303 21:45:39.406929  1302 replica.cpp:664] Replica learned 2 action at position 2079
{noformat}",Bug,Major,jieyu,2015-03-04T19:52:12.000+0000,5,Resolved,Complete,Mesos replicated log does not log the Action type name.,2015-03-04T19:52:12.000+0000,MESOS-2447,1.0,mesos,Twitter Mesos Q1 Sprint 4
bmahler,2015-03-03T22:25:53.000+0000,bmahler,"Currently libprocess' HTTP::Response supports a PIPE construct for doing streaming responses:

{code}
struct Response
{
  ...

  // Either provide a ""body"", an absolute ""path"" to a file, or a
  // ""pipe"" for streaming a response. Distinguish between the cases
  // using 'type' below.
  //
  // BODY: Uses 'body' as the body of the response. These may be
  // encoded using gzip for efficiency, if 'Content-Encoding' is not
  // already specified.
  //
  // PATH: Attempts to perform a 'sendfile' operation on the file
  // found at 'path'.
  //
  // PIPE: Splices data from 'pipe' using 'Transfer-Encoding=chunked'.
  // Note that the read end of the pipe will be closed by libprocess
  // either after the write end has been closed or if the socket the
  // data is being spliced to has been closed (i.e., nobody is
  // listening any longer). This can cause writes to the pipe to
  // generate a SIGPIPE (which will terminate your program unless you
  // explicitly ignore them or handle them).
  //
  // In all cases (BODY, PATH, PIPE), you are expected to properly
  // specify the 'Content-Type' header, but the 'Content-Length' and
  // or 'Transfer-Encoding' headers will be filled in for you.
  enum {
    NONE,
    BODY,
    PATH,
    PIPE
  } type;

  ...
};
{code}

This interface is too low level and difficult to program against:

* Connection closure is signaled with SIGPIPE, which is difficult for callers to deal with (must suppress SIGPIPE locally or globally in order to get EPIPE instead).
* Pipes are generally for inter-process communication, and the pipe has finite size. With a blocking pipe the caller must deal with blocking when the pipe's buffer limit is exceeded. With a non-blocking pipe, the caller must deal with retrying the write.

We'll want to consider a few use cases:
# Sending an HTTP::Response with streaming data.
# Making a request with http::get and http::post in which the data is returned in a streaming manner.
# Making a request in which the request content is streaming.

This ticket will focus on 1 as it is required for the HTTP API.",Improvement,Major,bmahler,2015-03-31T00:15:12.000+0000,5,Resolved,Complete,Improve support for streaming HTTP Responses in libprocess.,2015-11-04T22:53:04.000+0000,MESOS-2438,8.0,mesos,Twitter Mesos Q1 Sprint 4
jieyu,2015-03-02T22:52:42.000+0000,jieyu,We introduced the new acceptOffers API in C++ driver. We need to provide Python binding for this API as well.,Task,Major,jieyu,2015-03-12T01:23:11.000+0000,5,Resolved,Complete,Add Python bindings for the acceptOffers API.,2015-03-12T01:23:11.000+0000,MESOS-2428,2.0,mesos,Twitter Mesos Q1 Sprint 4
jieyu,2015-03-02T22:51:28.000+0000,jieyu,We introduced the new acceptOffers API in C++ driver. We need to provide Java binding for this API as well.,Task,Major,jieyu,2015-03-12T01:23:28.000+0000,5,Resolved,Complete,Add Java binding for the acceptOffers API.,2015-03-12T01:23:28.000+0000,MESOS-2427,2.0,mesos,Twitter Mesos Q1 Sprint 4
neilc,2015-02-26T00:53:46.000+0000,jieyu,"At present, destroying a persistent volume does not cleanup any filesystem space that was used by the volume (it just removes the Mesos-level metadata about the volume). This effectively leads to a storage leak, which is bad. For task sandboxes, we do ""garbage collection"" to remove the sandbox at a later time to facilitate debugging failed tasks; for volumes, because they are explicitly deleted and are not tied to the lifecycle of a task, removing the associated storage immediately seems best.

To implement this safely, we'll either need to ensure that libprocess messages are delivered in-order, or else add some extra safe-guards to ensure that out-of-order {{CheckpointResources}} messages don't lead to accidental data loss.",Task,Major,jieyu,2016-03-28T20:01:08.000+0000,5,Resolved,Complete,Slave should reclaim storage for destroyed persistent volumes.,2016-03-28T20:01:09.000+0000,MESOS-2408,5.0,mesos,Twitter Mesos Q3 Sprint 1
jieyu,2015-02-26T00:47:59.000+0000,jieyu,"This serves two purposes:

1) testing the new persistence feature
2) served as an example for others to use the new feature",Task,Major,jieyu,2015-04-10T21:57:29.000+0000,5,Resolved,Complete,Add an example framework to test persistent volumes.,2016-03-25T20:59:49.000+0000,MESOS-2404,3.0,mesos,Twitter Mesos Q1 Sprint 6
vinodkone,2015-02-25T20:06:36.000+0000,vinodkone,"{code}
[ RUN      ] MasterAllocatorTest/0.FrameworkReregistersFirst
Using temporary directory '/tmp/MasterAllocatorTest_0_FrameworkReregistersFirst_Vy5Nml'
I0224 23:22:31.681670 30589 leveldb.cpp:176] Opened db in 2.943518ms
I0224 23:22:31.682152 30619 process.cpp:2117] Dropped / Lost event for PID: slave(65)@67.195.81.187:38391
I0224 23:22:31.682732 30589 leveldb.cpp:183] Compacted db in 1.029469ms
I0224 23:22:31.682777 30589 leveldb.cpp:198] Created db iterator in 15460ns
I0224 23:22:31.682792 30589 leveldb.cpp:204] Seeked to beginning of db in 1832ns
I0224 23:22:31.682802 30589 leveldb.cpp:273] Iterated through 0 keys in the db in 319ns
I0224 23:22:31.682833 30589 replica.cpp:744] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned
I0224 23:22:31.683228 30605 recover.cpp:449] Starting replica recovery
I0224 23:22:31.683537 30605 recover.cpp:475] Replica is in 4 status
I0224 23:22:31.684624 30615 replica.cpp:641] Replica in 4 status received a broadcasted recover request
I0224 23:22:31.684978 30616 recover.cpp:195] Received a recover response from a replica in 4 status
I0224 23:22:31.685405 30610 recover.cpp:566] Updating replica status to 3
I0224 23:22:31.686249 30609 master.cpp:349] Master 20150224-232231-3142697795-38391-30589 (pomona.apache.org) started on 67.195.81.187:38391
I0224 23:22:31.686265 30617 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 717897ns
I0224 23:22:31.686319 30617 replica.cpp:323] Persisted replica status to 3
I0224 23:22:31.686336 30609 master.cpp:395] Master only allowing authenticated frameworks to register
I0224 23:22:31.686357 30609 master.cpp:400] Master only allowing authenticated slaves to register
I0224 23:22:31.686390 30609 credentials.hpp:37] Loading credentials for authentication from '/tmp/MasterAllocatorTest_0_FrameworkReregistersFirst_Vy5Nml/credentials'
I0224 23:22:31.686511 30606 recover.cpp:475] Replica is in 3 status
I0224 23:22:31.686563 30609 master.cpp:442] Authorization enabled
I0224 23:22:31.686929 30607 whitelist_watcher.cpp:79] No whitelist given
I0224 23:22:31.686954 30603 hierarchical.hpp:287] Initialized hierarchical allocator process
I0224 23:22:31.687134 30605 replica.cpp:641] Replica in 3 status received a broadcasted recover request
I0224 23:22:31.687731 30609 master.cpp:1356] The newly elected leader is master@67.195.81.187:38391 with id 20150224-232231-3142697795-38391-30589
I0224 23:22:31.839818 30609 master.cpp:1369] Elected as the leading master!
I0224 23:22:31.839834 30609 master.cpp:1187] Recovering from registrar
I0224 23:22:31.839926 30605 registrar.cpp:313] Recovering registrar
I0224 23:22:31.840000 30613 recover.cpp:195] Received a recover response from a replica in 3 status
I0224 23:22:31.840504 30606 recover.cpp:566] Updating replica status to 1
I0224 23:22:31.841599 30611 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 990330ns
I0224 23:22:31.841627 30611 replica.cpp:323] Persisted replica status to 1
I0224 23:22:31.841743 30611 recover.cpp:580] Successfully joined the Paxos group
I0224 23:22:31.841904 30611 recover.cpp:464] Recover process terminated
I0224 23:22:31.842366 30608 log.cpp:660] Attempting to start the writer
I0224 23:22:31.843557 30607 replica.cpp:477] Replica received implicit promise request with proposal 1
I0224 23:22:31.844312 30607 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 722368ns
I0224 23:22:31.844337 30607 replica.cpp:345] Persisted promised to 1
I0224 23:22:31.844889 30615 coordinator.cpp:230] Coordinator attemping to fill missing position
I0224 23:22:31.846043 30614 replica.cpp:378] Replica received explicit promise request for position 0 with proposal 2
I0224 23:22:31.846729 30614 leveldb.cpp:343] Persisting action (8 bytes) to leveldb took 660024ns
I0224 23:22:31.846746 30614 replica.cpp:679] Persisted action at 0
I0224 23:22:31.847671 30611 replica.cpp:511] Replica received write request for position 0
I0224 23:22:31.847723 30611 leveldb.cpp:438] Reading position from leveldb took 27349ns
I0224 23:22:31.848429 30611 leveldb.cpp:343] Persisting action (14 bytes) to leveldb took 671461ns
I0224 23:22:31.848454 30611 replica.cpp:679] Persisted action at 0
I0224 23:22:31.849041 30615 replica.cpp:658] Replica received learned notice for position 0
I0224 23:22:31.849762 30615 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 690386ns
I0224 23:22:31.849787 30615 replica.cpp:679] Persisted action at 0
I0224 23:22:31.849808 30615 replica.cpp:664] Replica learned 1 action at position 0
I0224 23:22:31.850416 30612 log.cpp:676] Writer started with ending position 0
I0224 23:22:31.851490 30615 leveldb.cpp:438] Reading position from leveldb took 30659ns
I0224 23:22:31.854452 30610 registrar.cpp:346] Successfully fetched the registry (0B) in 14.491136ms
I0224 23:22:31.854543 30610 registrar.cpp:445] Applied 1 operations in 18024ns; attempting to update the 'registry'
I0224 23:22:31.857095 30604 log.cpp:684] Attempting to append 139 bytes to the log
I0224 23:22:31.857208 30608 coordinator.cpp:340] Coordinator attempting to write 2 action at position 1
I0224 23:22:31.858073 30609 replica.cpp:511] Replica received write request for position 1
I0224 23:22:31.858808 30609 leveldb.cpp:343] Persisting action (158 bytes) to leveldb took 701708ns
I0224 23:22:31.858835 30609 replica.cpp:679] Persisted action at 1
I0224 23:22:31.859508 30618 replica.cpp:658] Replica received learned notice for position 1
I0224 23:22:31.860267 30618 leveldb.cpp:343] Persisting action (160 bytes) to leveldb took 731035ns
I0224 23:22:31.860309 30618 replica.cpp:679] Persisted action at 1
I0224 23:22:31.860332 30618 replica.cpp:664] Replica learned 2 action at position 1
I0224 23:22:31.860983 30609 registrar.cpp:490] Successfully updated the 'registry' in 6.39616ms
I0224 23:22:31.861071 30609 registrar.cpp:376] Successfully recovered registrar
I0224 23:22:31.861126 30608 log.cpp:703] Attempting to truncate the log to 1
I0224 23:22:31.861249 30603 coordinator.cpp:340] Coordinator attempting to write 3 action at position 2
I0224 23:22:31.861248 30617 master.cpp:1214] Recovered 0 slaves from the Registry (101B) ; allowing 10mins for slaves to re-register
I0224 23:22:31.861831 30613 replica.cpp:511] Replica received write request for position 2
I0224 23:22:31.862504 30613 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 648125ns
I0224 23:22:31.862531 30613 replica.cpp:679] Persisted action at 2
I0224 23:22:31.863067 30603 replica.cpp:658] Replica received learned notice for position 2
I0224 23:22:31.863689 30603 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 602784ns
I0224 23:22:31.863737 30603 leveldb.cpp:401] Deleting ~1 keys from leveldb took 28697ns
I0224 23:22:31.863751 30603 replica.cpp:679] Persisted action at 2
I0224 23:22:31.863767 30603 replica.cpp:664] Replica learned 3 action at position 2
I0224 23:22:31.875962 30610 slave.cpp:174] Slave started on 66)@67.195.81.187:38391
I0224 23:22:31.876008 30610 credentials.hpp:85] Loading credential for authentication from '/tmp/MasterAllocatorTest_0_FrameworkReregistersFirst_ikVXQM/credential'
I0224 23:22:31.876144 30610 slave.cpp:281] Slave using credential for: test-principal
I0224 23:22:31.876404 30610 slave.cpp:299] Slave resources: cpus(*):2; mem(*):1024; disk(*):3.70122e+06; ports(*):[31000-32000]
I0224 23:22:31.876489 30610 slave.cpp:328] Slave hostname: pomona.apache.org
I0224 23:22:31.876502 30610 slave.cpp:329] Slave checkpoint: false
W0224 23:22:31.876507 30610 slave.cpp:331] Disabling checkpointing is deprecated and the --checkpoint flag will be removed in a future release. Please avoid using this flag
I0224 23:22:31.877014 30603 state.cpp:35] Recovering state from '/tmp/MasterAllocatorTest_0_FrameworkReregistersFirst_ikVXQM/meta'
I0224 23:22:31.877230 30610 status_update_manager.cpp:197] Recovering status update manager
I0224 23:22:31.877495 30609 slave.cpp:3776] Finished recovery
I0224 23:22:31.877879 30607 status_update_manager.cpp:171] Pausing sending status updates
I0224 23:22:31.877879 30604 slave.cpp:624] New master detected at master@67.195.81.187:38391
I0224 23:22:31.877959 30604 slave.cpp:687] Authenticating with master master@67.195.81.187:38391
I0224 23:22:31.877975 30604 slave.cpp:692] Using default CRAM-MD5 authenticatee
I0224 23:22:31.878069 30604 slave.cpp:660] Detecting new master
I0224 23:22:31.878093 30608 authenticatee.hpp:139] Creating new client SASL connection
I0224 23:22:31.878223 30604 master.cpp:3813] Authenticating slave(66)@67.195.81.187:38391
I0224 23:22:31.878244 30604 master.cpp:3824] Using default CRAM-MD5 authenticator
I0224 23:22:31.878412 30613 authenticator.hpp:170] Creating new server SASL connection
I0224 23:22:31.878525 30603 authenticatee.hpp:230] Received SASL authentication mechanisms: CRAM-MD5
I0224 23:22:31.878551 30603 authenticatee.hpp:256] Attempting to authenticate with mechanism 'CRAM-MD5'
I0224 23:22:31.878625 30617 authenticator.hpp:276] Received SASL authentication start
I0224 23:22:31.878662 30617 authenticator.hpp:398] Authentication requires more steps
I0224 23:22:31.878727 30603 authenticatee.hpp:276] Received SASL authentication step
I0224 23:22:31.878815 30617 authenticator.hpp:304] Received SASL authentication step
I0224 23:22:31.878839 30617 auxprop.cpp:99] Request to lookup properties for user: 'test-principal' realm: 'pomona.apache.org' server FQDN: 'pomona.apache.org' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0224 23:22:31.878847 30617 auxprop.cpp:171] Looking up auxiliary property '*userPassword'
I0224 23:22:31.878875 30617 auxprop.cpp:171] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0224 23:22:31.878891 30617 auxprop.cpp:99] Request to lookup properties for user: 'test-principal' realm: 'pomona.apache.org' server FQDN: 'pomona.apache.org' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0224 23:22:31.878900 30617 auxprop.cpp:121] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0224 23:22:31.878906 30617 auxprop.cpp:121] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0224 23:22:31.878916 30617 authenticator.hpp:390] Authentication success
I0224 23:22:31.880717 30589 sched.cpp:157] Version: 0.23.0
I0224 23:22:32.017823 30611 authenticatee.hpp:316] Authentication success
I0224 23:22:32.017901 30618 master.cpp:3871] Successfully authenticated principal 'test-principal' at slave(66)@67.195.81.187:38391
I0224 23:22:32.018156 30615 sched.cpp:254] New master detected at master@67.195.81.187:38391
I0224 23:22:32.018240 30615 sched.cpp:310] Authenticating with master master@67.195.81.187:38391
I0224 23:22:32.018263 30615 sched.cpp:317] Using default CRAM-MD5 authenticatee
I0224 23:22:32.018496 30613 slave.cpp:758] Successfully authenticated with master master@67.195.81.187:38391
I0224 23:22:32.018579 30611 authenticatee.hpp:139] Creating new client SASL connection
I0224 23:22:32.018620 30613 slave.cpp:1090] Will retry registration in 363167ns if necessary
I0224 23:22:32.018811 30615 master.cpp:2938] Registering slave at slave(66)@67.195.81.187:38391 (pomona.apache.org) with id 20150224-232231-3142697795-38391-30589-S0
I0224 23:22:32.019122 30615 master.cpp:3813] Authenticating scheduler-9a3224cc-aef0-49a7-a240-4b85b913ff44@67.195.81.187:38391
I0224 23:22:32.019156 30615 master.cpp:3824] Using default CRAM-MD5 authenticator
I0224 23:22:32.019232 30612 registrar.cpp:445] Applied 1 operations in 57599ns; attempting to update the 'registry'
I0224 23:22:32.019394 30603 authenticator.hpp:170] Creating new server SASL connection
I0224 23:22:32.019541 30611 authenticatee.hpp:230] Received SASL authentication mechanisms: CRAM-MD5
I0224 23:22:32.019568 30611 authenticatee.hpp:256] Attempting to authenticate with mechanism 'CRAM-MD5'
I0224 23:22:32.019666 30605 authenticator.hpp:276] Received SASL authentication start
I0224 23:22:32.019717 30605 authenticator.hpp:398] Authentication requires more steps
I0224 23:22:32.019805 30615 authenticatee.hpp:276] Received SASL authentication step
I0224 23:22:32.019942 30605 authenticator.hpp:304] Received SASL authentication step
I0224 23:22:32.019979 30605 auxprop.cpp:99] Request to lookup properties for user: 'test-principal' realm: 'pomona.apache.org' server FQDN: 'pomona.apache.org' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0224 23:22:32.019994 30605 auxprop.cpp:171] Looking up auxiliary property '*userPassword'
I0224 23:22:32.020025 30605 auxprop.cpp:171] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0224 23:22:32.020036 30610 slave.cpp:1090] Will retry registration in 10.850555ms if necessary
I0224 23:22:32.020053 30605 auxprop.cpp:99] Request to lookup properties for user: 'test-principal' realm: 'pomona.apache.org' server FQDN: 'pomona.apache.org' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0224 23:22:32.020102 30605 auxprop.cpp:121] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0224 23:22:32.020117 30605 auxprop.cpp:121] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0224 23:22:32.020133 30605 authenticator.hpp:390] Authentication success
I0224 23:22:32.020151 30611 master.cpp:2926] Ignoring register slave message from slave(66)@67.195.81.187:38391 (pomona.apache.org) as admission is already in progress
I0224 23:22:32.020226 30603 authenticatee.hpp:316] Authentication success
I0224 23:22:32.020256 30611 master.cpp:3871] Successfully authenticated principal 'test-principal' at scheduler-9a3224cc-aef0-49a7-a240-4b85b913ff44@67.195.81.187:38391
I0224 23:22:32.020534 30615 sched.cpp:398] Successfully authenticated with master master@67.195.81.187:38391
I0224 23:22:32.020561 30615 sched.cpp:521] Sending registration request to master@67.195.81.187:38391
I0224 23:22:32.020635 30615 sched.cpp:554] Will retry registration in 490.035142ms if necessary
I0224 23:22:32.020720 30613 master.cpp:1574] Received registration request for framework 'default' at scheduler-9a3224cc-aef0-49a7-a240-4b85b913ff44@67.195.81.187:38391
I0224 23:22:32.020787 30613 master.cpp:1435] Authorizing framework principal 'test-principal' to receive offers for role '*'
I0224 23:22:32.021122 30607 master.cpp:1638] Registering framework 20150224-232231-3142697795-38391-30589-0000 (default) at scheduler-9a3224cc-aef0-49a7-a240-4b85b913ff44@67.195.81.187:38391
I0224 23:22:32.021502 30611 hierarchical.hpp:321] Added framework 20150224-232231-3142697795-38391-30589-0000
I0224 23:22:32.021531 30611 hierarchical.hpp:834] No resources available to allocate!
I0224 23:22:32.021543 30611 hierarchical.hpp:741] Performed allocation for 0 slaves in 18915ns
I0224 23:22:32.021618 30609 sched.cpp:448] Framework registered with 20150224-232231-3142697795-38391-30589-0000
I0224 23:22:32.021673 30609 sched.cpp:462] Scheduler::registered took 26310ns
I0224 23:22:32.022400 30613 log.cpp:684] Attempting to append 316 bytes to the log
I0224 23:22:32.022523 30608 coordinator.cpp:340] Coordinator attempting to write 2 action at position 3
I0224 23:22:32.023232 30607 replica.cpp:511] Replica received write request for position 3
I0224 23:22:32.024055 30607 leveldb.cpp:343] Persisting action (335 bytes) to leveldb took 798548ns
I0224 23:22:32.024073 30607 replica.cpp:679] Persisted action at 3
I0224 23:22:32.024651 30610 replica.cpp:658] Replica received learned notice for position 3
I0224 23:22:32.025252 30610 leveldb.cpp:343] Persisting action (337 bytes) to leveldb took 580525ns
I0224 23:22:32.025271 30610 replica.cpp:679] Persisted action at 3
I0224 23:22:32.025297 30610 replica.cpp:664] Replica learned 2 action at position 3
I0224 23:22:32.025995 30618 registrar.cpp:490] Successfully updated the 'registry' in 6.586112ms
I0224 23:22:32.026228 30604 log.cpp:703] Attempting to truncate the log to 3
I0224 23:22:32.026360 30609 coordinator.cpp:340] Coordinator attempting to write 3 action at position 4
I0224 23:22:32.026669 30609 slave.cpp:2831] Received ping from slave-observer(66)@67.195.81.187:38391
I0224 23:22:32.026772 30609 slave.cpp:792] Registered with master master@67.195.81.187:38391; given slave ID 20150224-232231-3142697795-38391-30589-S0
I0224 23:22:32.026737 30603 master.cpp:2995] Registered slave 20150224-232231-3142697795-38391-30589-S0 at slave(66)@67.195.81.187:38391 (pomona.apache.org) with cpus(*):2; mem(*):1024; disk(*):3.70122e+06; ports(*):[31000-32000]
I0224 23:22:32.026867 30603 status_update_manager.cpp:178] Resuming sending status updates
I0224 23:22:32.026868 30617 hierarchical.hpp:455] Added slave 20150224-232231-3142697795-38391-30589-S0 (pomona.apache.org) with cpus(*):2; mem(*):1024; disk(*):3.70122e+06; ports(*):[31000-32000] (and cpus(*):2; mem(*):1024; disk(*):3.70122e+06; ports(*):[31000-32000] available)
I0224 23:22:32.026921 30615 replica.cpp:511] Replica received write request for position 4
I0224 23:22:32.027276 30617 hierarchical.hpp:759] Performed allocation for slave 20150224-232231-3142697795-38391-30589-S0 in 351257ns
I0224 23:22:32.027580 30615 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 624249ns
I0224 23:22:32.027604 30615 replica.cpp:679] Persisted action at 4
I0224 23:22:32.027642 30618 master.cpp:3755] Sending 1 offers to framework 20150224-232231-3142697795-38391-30589-0000 (default) at scheduler-9a3224cc-aef0-49a7-a240-4b85b913ff44@67.195.81.187:38391
I0224 23:22:32.028223 30617 replica.cpp:658] Replica received learned notice for position 4
I0224 23:22:32.028621 30607 sched.cpp:611] Scheduler::resourceOffers took 648326ns
I0224 23:22:32.028916 30617 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 662416ns
I0224 23:22:32.028991 30617 leveldb.cpp:401] Deleting ~2 keys from leveldb took 47386ns
I0224 23:22:32.029021 30617 replica.cpp:679] Persisted action at 4
I0224 23:22:32.029044 30617 replica.cpp:664] Replica learned 3 action at position 4
I0224 23:22:32.029534 30613 master.cpp:2268] Processing ACCEPT call for offers: [ 20150224-232231-3142697795-38391-30589-O0 ] on slave 20150224-232231-3142697795-38391-30589-S0 at slave(66)@67.195.81.187:38391 (pomona.apache.org) for framework 20150224-232231-3142697795-38391-30589-0000 (default) at scheduler-9a3224cc-aef0-49a7-a240-4b85b913ff44@67.195.81.187:38391
I0224 23:22:32.190521 30613 master.cpp:2112] Authorizing framework principal 'test-principal' to launch task 0 as user 'jenkins'
W0224 23:22:32.191864 30604 validation.cpp:328] Executor default for task 0 uses less CPUs (None) than the minimum required (0.01). Please update your executor, as this will be mandatory in future releases.
W0224 23:22:32.191905 30604 validation.cpp:340] Executor default for task 0 uses less memory (None) than the minimum required (32MB). Please update your executor, as this will be mandatory in future releases.
I0224 23:22:32.192206 30604 master.hpp:822] Adding task 0 with resources cpus(*):1; mem(*):500 on slave 20150224-232231-3142697795-38391-30589-S0 (pomona.apache.org)
I0224 23:22:32.192318 30604 master.cpp:2545] Launching task 0 of framework 20150224-232231-3142697795-38391-30589-0000 (default) at scheduler-9a3224cc-aef0-49a7-a240-4b85b913ff44@67.195.81.187:38391 with resources cpus(*):1; mem(*):500 on slave 20150224-232231-3142697795-38391-30589-S0 at slave(66)@67.195.81.187:38391 (pomona.apache.org)
I0224 23:22:32.192659 30611 slave.cpp:1121] Got assigned task 0 for framework 20150224-232231-3142697795-38391-30589-0000
I0224 23:22:32.192847 30609 hierarchical.hpp:648] Recovered cpus(*):1; mem(*):524; disk(*):3.70122e+06; ports(*):[31000-32000] (total allocatable: cpus(*):1; mem(*):524; disk(*):3.70122e+06; ports(*):[31000-32000]) on slave 20150224-232231-3142697795-38391-30589-S0 from framework 20150224-232231-3142697795-38391-30589-0000
I0224 23:22:32.192916 30609 hierarchical.hpp:684] Framework 20150224-232231-3142697795-38391-30589-0000 filtered slave 20150224-232231-3142697795-38391-30589-S0 for 5secs
I0224 23:22:32.193327 30611 slave.cpp:1231] Launching task 0 for framework 20150224-232231-3142697795-38391-30589-0000
I0224 23:22:32.196038 30611 slave.cpp:4178] Launching executor default of framework 20150224-232231-3142697795-38391-30589-0000 in work directory '/tmp/MasterAllocatorTest_0_FrameworkReregistersFirst_ikVXQM/slaves/20150224-232231-3142697795-38391-30589-S0/frameworks/20150224-232231-3142697795-38391-30589-0000/executors/default/runs/7c203619-b40f-4d0a-9b75-9ddeecb63472'
I0224 23:22:32.197996 30611 exec.cpp:132] Version: 0.23.0
I0224 23:22:32.198206 30605 exec.cpp:182] Executor started at: executor(32)@67.195.81.187:38391 with pid 30589
I0224 23:22:32.198314 30611 slave.cpp:1378] Queuing task '0' for executor default of framework '20150224-232231-3142697795-38391-30589-0000
I0224 23:22:32.198407 30611 slave.cpp:577] Successfully attached file '/tmp/MasterAllocatorTest_0_FrameworkReregistersFirst_ikVXQM/slaves/20150224-232231-3142697795-38391-30589-S0/frameworks/20150224-232231-3142697795-38391-30589-0000/executors/default/runs/7c203619-b40f-4d0a-9b75-9ddeecb63472'
I0224 23:22:32.198508 30611 slave.cpp:3133] Monitoring executor 'default' of framework '20150224-232231-3142697795-38391-30589-0000' in container '7c203619-b40f-4d0a-9b75-9ddeecb63472'
I0224 23:22:32.198637 30611 slave.cpp:2141] Got registration for executor 'default' of framework 20150224-232231-3142697795-38391-30589-0000 from executor(32)@67.195.81.187:38391
I0224 23:22:32.198839 30604 exec.cpp:206] Executor registered on slave 20150224-232231-3142697795-38391-30589-S0
I0224 23:22:32.199090 30611 slave.cpp:1532] Sending queued task '0' to executor 'default' of framework 20150224-232231-3142697795-38391-30589-0000
I0224 23:22:32.200916 30604 exec.cpp:218] Executor::registered took 24976ns
I0224 23:22:32.201087 30604 exec.cpp:293] Executor asked to run task '0'
I0224 23:22:32.201164 30604 exec.cpp:302] Executor::launchTask took 54201ns
I0224 23:22:32.203301 30604 exec.cpp:525] Executor sending status update TASK_RUNNING (UUID: 49340611-fb39-423b-96f6-6c1e724c1a53) for task 0 of framework 20150224-232231-3142697795-38391-30589-0000
I0224 23:22:32.203533 30618 slave.cpp:2508] Handling status update TASK_RUNNING (UUID: 49340611-fb39-423b-96f6-6c1e724c1a53) for task 0 of framework 20150224-232231-3142697795-38391-30589-0000 from executor(32)@67.195.81.187:38391
I0224 23:22:32.203799 30604 status_update_manager.cpp:317] Received status update TASK_RUNNING (UUID: 49340611-fb39-423b-96f6-6c1e724c1a53) for task 0 of framework 20150224-232231-3142697795-38391-30589-0000
I0224 23:22:32.203840 30604 status_update_manager.cpp:494] Creating StatusUpdate stream for task 0 of framework 20150224-232231-3142697795-38391-30589-0000
I0224 23:22:32.204038 30604 status_update_manager.cpp:371] Forwarding update TASK_RUNNING (UUID: 49340611-fb39-423b-96f6-6c1e724c1a53) for task 0 of framework 20150224-232231-3142697795-38391-30589-0000 to the slave
I0224 23:22:32.204262 30607 slave.cpp:2751] Forwarding the update TASK_RUNNING (UUID: 49340611-fb39-423b-96f6-6c1e724c1a53) for task 0 of framework 20150224-232231-3142697795-38391-30589-0000 to master@67.195.81.187:38391
I0224 23:22:32.204473 30607 slave.cpp:2678] Status update manager successfully handled status update TASK_RUNNING (UUID: 49340611-fb39-423b-96f6-6c1e724c1a53) for task 0 of framework 20150224-232231-3142697795-38391-30589-0000
I0224 23:22:32.204511 30607 slave.cpp:2684] Sending acknowledgement for status update TASK_RUNNING (UUID: 49340611-fb39-423b-96f6-6c1e724c1a53) for task 0 of framework 20150224-232231-3142697795-38391-30589-0000 to executor(32)@67.195.81.187:38391
I0224 23:22:32.204558 30616 master.cpp:3295] Status update TASK_RUNNING (UUID: 49340611-fb39-423b-96f6-6c1e724c1a53) for task 0 of framework 20150224-232231-3142697795-38391-30589-0000 from slave 20150224-232231-3142697795-38391-30589-S0 at slave(66)@67.195.81.187:38391 (pomona.apache.org)
I0224 23:22:32.204602 30616 master.cpp:3336] Forwarding status update TASK_RUNNING (UUID: 49340611-fb39-423b-96f6-6c1e724c1a53) for task 0 of framework 20150224-232231-3142697795-38391-30589-0000
I0224 23:22:32.204676 30610 exec.cpp:339] Executor received status update acknowledgement 49340611-fb39-423b-96f6-6c1e724c1a53 for task 0 of framework 20150224-232231-3142697795-38391-30589-0000
I0224 23:22:32.204753 30616 master.cpp:4615] Updating the latest state of task 0 of framework 20150224-232231-3142697795-38391-30589-0000 to TASK_RUNNING
I0224 23:22:32.204874 30603 sched.cpp:717] Scheduler::statusUpdate took 52057ns
I0224 23:22:32.205277 30614 master.cpp:2782] Forwarding status update acknowledgement 49340611-fb39-423b-96f6-6c1e724c1a53 for task 0 of framework 20150224-232231-3142697795-38391-30589-0000 (default) at scheduler-9a3224cc-aef0-49a7-a240-4b85b913ff44@67.195.81.187:38391 to slave 20150224-232231-3142697795-38391-30589-S0 at slave(66)@67.195.81.187:38391 (pomona.apache.org)
I0224 23:22:32.205579 30618 status_update_manager.cpp:389] Received status update acknowledgement (UUID: 49340611-fb39-423b-96f6-6c1e724c1a53) for task 0 of framework 20150224-232231-3142697795-38391-30589-0000
I0224 23:22:32.205798 30604 slave.cpp:2081] Status update manager successfully handled status update acknowledgement (UUID: 49340611-fb39-423b-96f6-6c1e724c1a53) for task 0 of framework 20150224-232231-3142697795-38391-30589-0000
I0224 23:22:32.206073 30607 master.cpp:787] Master terminating
W0224 23:22:32.206197 30607 master.cpp:4668] Removing task 0 with resources cpus(*):1; mem(*):500 of framework 20150224-232231-3142697795-38391-30589-0000 on slave 20150224-232231-3142697795-38391-30589-S0 at slave(66)@67.195.81.187:38391 (pomona.apache.org) in non-terminal state TASK_RUNNING
I0224 23:22:32.206420 30614 hierarchical.hpp:648] Recovered cpus(*):1; mem(*):500 (total allocatable: cpus(*):2; mem(*):1024; disk(*):3.70122e+06; ports(*):[31000-32000]) on slave 20150224-232231-3142697795-38391-30589-S0 from framework 20150224-232231-3142697795-38391-30589-0000
I0224 23:22:32.206552 30607 master.cpp:4711] Removing executor 'default' with resources  of framework 20150224-232231-3142697795-38391-30589-0000 on slave 20150224-232231-3142697795-38391-30589-S0 at slave(66)@67.195.81.187:38391 (pomona.apache.org)
I0224 23:22:32.321058 30619 process.cpp:2117] Dropped / Lost event for PID: hierarchical-allocator(81)@67.195.81.187:38391
I0224 23:22:32.367074 30612 slave.cpp:2916] master@67.195.81.187:38391 exited
W0224 23:22:32.367101 30612 slave.cpp:2919] Master disconnected! Waiting for a new master to be elected
I0224 23:22:32.368388 30589 process.cpp:2117] Dropped / Lost event for PID: master@67.195.81.187:38391
I0224 23:22:32.368482 30589 process.cpp:2117] Dropped / Lost event for PID: master@67.195.81.187:38391
I0224 23:22:32.375794 30589 leveldb.cpp:176] Opened db in 3.725405ms
I0224 23:22:32.379137 30589 leveldb.cpp:183] Compacted db in 3.309337ms
I0224 23:22:32.379196 30589 leveldb.cpp:198] Created db iterator in 21994ns
I0224 23:22:32.379236 30589 leveldb.cpp:204] Seeked to beginning of db in 20370ns
I0224 23:22:32.379356 30589 leveldb.cpp:273] Iterated through 3 keys in the db in 102640ns
I0224 23:22:32.379411 30589 replica.cpp:744] Replica recovered with log positions 3 -> 4 with 0 holes and 0 unlearned
I0224 23:22:32.379884 30610 recover.cpp:449] Starting replica recovery
I0224 23:22:32.380156 30610 recover.cpp:475] Replica is in 1 status
I0224 23:22:32.380460 30610 recover.cpp:464] Recover process terminated
I0224 23:22:32.381878 30616 master.cpp:349] Master 20150224-232232-3142697795-38391-30589 (pomona.apache.org) started on 67.195.81.187:38391
I0224 23:22:32.381927 30616 master.cpp:395] Master only allowing authenticated frameworks to register
I0224 23:22:32.381945 30616 master.cpp:400] Master only allowing authenticated slaves to register
I0224 23:22:32.381974 30616 credentials.hpp:37] Loading credentials for authentication from '/tmp/MasterAllocatorTest_0_FrameworkReregistersFirst_Vy5Nml/credentials'
I0224 23:22:32.382220 30616 master.cpp:442] Authorization enabled
I0224 23:22:32.382783 30604 whitelist_watcher.cpp:79] No whitelist given
I0224 23:22:32.382861 30607 hierarchical.hpp:287] Initialized hierarchical allocator process
I0224 23:22:32.384114 30616 master.cpp:1356] The newly elected leader is master@67.195.81.187:38391 with id 20150224-232232-3142697795-38391-30589
I0224 23:22:32.384150 30616 master.cpp:1369] Elected as the leading master!
I0224 23:22:32.384178 30616 master.cpp:1187] Recovering from registrar
I0224 23:22:32.384330 30617 registrar.cpp:313] Recovering registrar
I0224 23:22:32.384851 30606 log.cpp:660] Attempting to start the writer
I0224 23:22:32.386044 30615 replica.cpp:477] Replica received implicit promise request with proposal 2
I0224 23:22:32.386862 30615 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 790008ns
I0224 23:22:32.386888 30615 replica.cpp:345] Persisted promised to 2
I0224 23:22:32.387544 30612 coordinator.cpp:230] Coordinator attemping to fill missing position
I0224 23:22:32.387799 30615 log.cpp:676] Writer started with ending position 4
I0224 23:22:32.388854 30606 leveldb.cpp:438] Reading position from leveldb took 51391ns
I0224 23:22:32.388949 30606 leveldb.cpp:438] Reading position from leveldb took 40544ns
I0224 23:22:32.390015 30611 registrar.cpp:346] Successfully fetched the registry (277B) in 5.604096ms
I0224 23:22:32.390182 30611 registrar.cpp:445] Applied 1 operations in 46501ns; attempting to update the 'registry'
I0224 23:22:32.392963 30608 log.cpp:684] Attempting to append 316 bytes to the log
I0224 23:22:32.393081 30606 coordinator.cpp:340] Coordinator attempting to write 2 action at position 5
I0224 23:22:32.393875 30607 replica.cpp:511] Replica received write request for position 5
I0224 23:22:32.394582 30607 leveldb.cpp:343] Persisting action (335 bytes) to leveldb took 675133ns
I0224 23:22:32.394609 30607 replica.cpp:679] Persisted action at 5
I0224 23:22:32.395190 30614 replica.cpp:658] Replica received learned notice for position 5
I0224 23:22:32.395917 30614 leveldb.cpp:343] Persisting action (337 bytes) to leveldb took 701360ns
I0224 23:22:32.395944 30614 replica.cpp:679] Persisted action at 5
I0224 23:22:32.395966 30614 replica.cpp:664] Replica learned 2 action at position 5
I0224 23:22:32.396880 30614 registrar.cpp:490] Successfully updated the 'registry' in 6.644224ms
I0224 23:22:32.397056 30614 registrar.cpp:376] Successfully recovered registrar
I0224 23:22:32.397111 30604 log.cpp:703] Attempting to truncate the log to 5
I0224 23:22:32.397274 30616 coordinator.cpp:340] Coordinator attempting to write 3 action at position 6
I0224 23:22:32.397708 30615 master.cpp:1214] Recovered 1 slaves from the Registry (277B) ; allowing 10mins for slaves to re-register
I0224 23:22:32.398107 30612 replica.cpp:511] Replica received write request for position 6
I0224 23:22:32.398763 30612 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 625992ns
I0224 23:22:32.398789 30612 replica.cpp:679] Persisted action at 6
I0224 23:22:32.399373 30615 replica.cpp:658] Replica received learned notice for position 6
I0224 23:22:32.400152 30615 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 754us
I0224 23:22:32.400228 30615 leveldb.cpp:401] Deleting ~2 keys from leveldb took 39811ns
I0224 23:22:32.400250 30615 replica.cpp:679] Persisted action at 6
I0224 23:22:32.400274 30615 replica.cpp:664] Replica learned 3 action at position 6
I0224 23:22:32.410408 30604 sched.cpp:248] Scheduler::disconnected took 15348ns
I0224 23:22:32.410430 30604 sched.cpp:254] New master detected at master@67.195.81.187:38391
I0224 23:22:32.410508 30604 sched.cpp:310] Authenticating with master master@67.195.81.187:38391
I0224 23:22:32.410531 30604 sched.cpp:317] Using default CRAM-MD5 authenticatee
I0224 23:22:32.410781 30605 authenticatee.hpp:139] Creating new client SASL connection
I0224 23:22:32.410964 30608 master.cpp:3813] Authenticating scheduler-9a3224cc-aef0-49a7-a240-4b85b913ff44@67.195.81.187:38391
I0224 23:22:32.410991 30608 master.cpp:3824] Using default CRAM-MD5 authenticator
I0224 23:22:32.411188 30617 authenticator.hpp:170] Creating new server SASL connection
I0224 23:22:32.411362 30612 authenticatee.hpp:230] Received SASL authentication mechanisms: CRAM-MD5
I0224 23:22:32.411391 30612 authenticatee.hpp:256] Attempting to authenticate with mechanism 'CRAM-MD5'
I0224 23:22:32.411499 30618 authenticator.hpp:276] Received SASL authentication start
I0224 23:22:32.411600 30618 authenticator.hpp:398] Authentication requires more steps
I0224 23:22:32.411710 30614 authenticatee.hpp:276] Received SASL authentication step
I0224 23:22:32.411818 30618 authenticator.hpp:304] Received SASL authentication step
I0224 23:22:32.411849 30618 auxprop.cpp:99] Request to lookup properties for user: 'test-principal' realm: 'pomona.apache.org' server FQDN: 'pomona.apache.org' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0224 23:22:32.411861 30618 auxprop.cpp:171] Looking up auxiliary property '*userPassword'
I0224 23:22:32.411897 30618 auxprop.cpp:171] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0224 23:22:32.411922 30618 auxprop.cpp:99] Request to lookup properties for user: 'test-principal' realm: 'pomona.apache.org' server FQDN: 'pomona.apache.org' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0224 23:22:32.411934 30618 auxprop.cpp:121] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0224 23:22:32.411942 30618 auxprop.cpp:121] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0224 23:22:32.411958 30618 authenticator.hpp:390] Authentication success
I0224 23:22:32.412032 30614 authenticatee.hpp:316] Authentication success
I0224 23:22:32.412061 30612 master.cpp:3871] Successfully authenticated principal 'test-principal' at scheduler-9a3224cc-aef0-49a7-a240-4b85b913ff44@67.195.81.187:38391
I0224 23:22:32.412345 30605 sched.cpp:398] Successfully authenticated with master master@67.195.81.187:38391
I0224 23:22:32.412375 30605 sched.cpp:521] Sending registration request to master@67.195.81.187:38391
I0224 23:22:32.528939 30605 sched.cpp:554] Will retry registration in 718.250035ms if necessary
I0224 23:22:32.529072 30605 sched.cpp:521] Sending registration request to master@67.195.81.187:38391
I0224 23:22:32.529136 30605 sched.cpp:554] Will retry registration in 2.186117614secs if necessary
I0224 23:22:32.529161 30618 master.cpp:1711] Received re-registration request from framework 20150224-232231-3142697795-38391-30589-0000 (default) at scheduler-9a3224cc-aef0-49a7-a240-4b85b913ff44@67.195.81.187:38391
I0224 23:22:32.529274 30618 master.cpp:1435] Authorizing framework principal 'test-principal' to receive offers for role '*'
I0224 23:22:32.529597 30618 master.cpp:1711] Received re-registration request from framework 20150224-232231-3142697795-38391-30589-0000 (default) at scheduler-9a3224cc-aef0-49a7-a240-4b85b913ff44@67.195.81.187:38391
I0224 23:22:32.529662 30618 master.cpp:1435] Authorizing framework principal 'test-principal' to receive offers for role '*'
I0224 23:22:32.529856 30618 master.cpp:1764] Re-registering framework 20150224-232231-3142697795-38391-30589-0000 (default)  at scheduler-9a3224cc-aef0-49a7-a240-4b85b913ff44@67.195.81.187:38391
I0224 23:22:32.530449 30614 hierarchical.hpp:321] Added framework 20150224-232231-3142697795-38391-30589-0000
I0224 23:22:32.530490 30614 hierarchical.hpp:834] No resources available to allocate!
I0224 23:22:32.530510 30614 hierarchical.hpp:741] Performed allocation for 0 slaves in 32479ns
I0224 23:22:32.530743 30618 master.cpp:1764] Re-registering framework 20150224-232231-3142697795-38391-30589-0000 (default)  at scheduler-9a3224cc-aef0-49a7-a240-4b85b913ff44@67.195.81.187:38391
I0224 23:22:32.530777 30611 sched.cpp:448] Framework registered with 20150224-232231-3142697795-38391-30589-0000
I0224 23:22:32.530787 30618 master.cpp:1804] Allowing framework 20150224-232231-3142697795-38391-30589-0000 (default) at scheduler-9a3224cc-aef0-49a7-a240-4b85b913ff44@67.195.81.187:38391 to re-register with an already used id
I0224 23:22:32.530824 30611 sched.cpp:462] Scheduler::registered took 20697ns
I0224 23:22:32.530948 30605 sched.cpp:477] Ignoring framework re-registered message because the driver is already connected!
I0224 23:22:32.533220 30606 status_update_manager.cpp:171] Pausing sending status updates
I0224 23:22:32.533226 30605 slave.cpp:624] New master detected at master@67.195.81.187:38391
I0224 23:22:32.533362 30605 slave.cpp:687] Authenticating with master master@67.195.81.187:38391
I0224 23:22:32.533385 30605 slave.cpp:692] Using default CRAM-MD5 authenticatee
I0224 23:22:32.533529 30605 slave.cpp:660] Detecting new master
I0224 23:22:32.533576 30612 authenticatee.hpp:139] Creating new client SASL connection
I0224 23:22:32.533759 30603 master.cpp:3813] Authenticating slave(66)@67.195.81.187:38391
I0224 23:22:32.533790 30603 master.cpp:3824] Using default CRAM-MD5 authenticator
I0224 23:22:32.534018 30605 authenticator.hpp:170] Creating new server SASL connection
I0224 23:22:32.534195 30609 authenticatee.hpp:230] Received SASL authentication mechanisms: CRAM-MD5
I0224 23:22:32.534227 30609 authenticatee.hpp:256] Attempting to authenticate with mechanism 'CRAM-MD5'
I0224 23:22:32.534339 30609 authenticator.hpp:276] Received SASL authentication start
I0224 23:22:32.534394 30609 authenticator.hpp:398] Authentication requires more steps
I0224 23:22:32.534494 30609 authenticatee.hpp:276] Received SASL authentication step
I0224 23:22:32.534600 30609 authenticator.hpp:304] Received SASL authentication step
I0224 23:22:32.534629 30609 auxprop.cpp:99] Request to lookup properties for user: 'test-principal' realm: 'pomona.apache.org' server FQDN: 'pomona.apache.org' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0224 23:22:32.534642 30609 auxprop.cpp:171] Looking up auxiliary property '*userPassword'
I0224 23:22:32.534692 30609 auxprop.cpp:171] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0224 23:22:32.534725 30609 auxprop.cpp:99] Request to lookup properties for user: 'test-principal' realm: 'pomona.apache.org' server FQDN: 'pomona.apache.org' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0224 23:22:32.534740 30609 auxprop.cpp:121] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0224 23:22:32.534749 30609 auxprop.cpp:121] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0224 23:22:32.534765 30609 authenticator.hpp:390] Authentication success
I0224 23:22:32.534891 30604 authenticatee.hpp:316] Authentication success
I0224 23:22:32.534906 30609 master.cpp:3871] Successfully authenticated principal 'test-principal' at slave(66)@67.195.81.187:38391
I0224 23:22:32.535146 30608 slave.cpp:758] Successfully authenticated with master master@67.195.81.187:38391
I0224 23:22:32.535567 30608 slave.cpp:1090] Will retry registration in 10.96399ms if necessary
I0224 23:22:32.535843 30610 master.cpp:3120] Re-registering slave 20150224-232231-3142697795-38391-30589-S0 at slave(66)@67.195.81.187:38391 (pomona.apache.org)
I0224 23:22:32.536542 30606 registrar.cpp:445] Applied 1 operations in 71981ns; attempting to update the 'registry'
I0224 23:22:32.539535 30612 log.cpp:684] Attempting to append 316 bytes to the log
I0224 23:22:32.539682 30617 coordinator.cpp:340] Coordinator attempting to write 2 action at position 7
I0224 23:22:32.540549 30608 replica.cpp:511] Replica received write request for position 7
I0224 23:22:32.540832 30608 leveldb.cpp:343] Persisting action (335 bytes) to leveldb took 249089ns
I0224 23:22:32.540858 30608 replica.cpp:679] Persisted action at 7
I0224 23:22:32.541555 30604 replica.cpp:658] Replica received learned notice for position 7
I0224 23:22:32.542254 30604 leveldb.cpp:343] Persisting action (337 bytes) to leveldb took 671501ns
I0224 23:22:32.542294 30604 replica.cpp:679] Persisted action at 7
I0224 23:22:32.542320 30604 replica.cpp:664] Replica learned 2 action at position 7
I0224 23:22:32.543231 30603 registrar.cpp:490] Successfully updated the 'registry' in 6.617088ms
I0224 23:22:32.543637 30613 log.cpp:703] Attempting to truncate the log to 7
I0224 23:22:32.543784 30611 coordinator.cpp:340] Coordinator attempting to write 3 action at position 8
I0224 23:22:32.544088 30610 master.hpp:822] Adding task 0 with resources cpus(*):1; mem(*):500 on slave 20150224-232231-3142697795-38391-30589-S0 (pomona.apache.org)
I0224 23:22:32.544503 30612 slave.cpp:2831] Received ping from slave-observer(67)@67.195.81.187:38391
I0224 23:22:32.544530 30618 replica.cpp:511] Replica received write request for position 8
I0224 23:22:32.544800 30610 master.cpp:3191] Re-registered slave 20150224-232231-3142697795-38391-30589-S0 at slave(66)@67.195.81.187:38391 (pomona.apache.org) with cpus(*):2; mem(*):1024; disk(*):3.70122e+06; ports(*):[31000-32000]
I0224 23:22:32.544862 30607 slave.cpp:860] Re-registered with master master@67.195.81.187:38391
I0224 23:22:32.544961 30606 status_update_manager.cpp:178] Resuming sending status updates
I0224 23:22:32.544975 30610 master.cpp:3219] Sending updated checkpointed resources  to slave 20150224-232231-3142697795-38391-30589-S0 at slave(66)@67.195.81.187:38391 (pomona.apache.org)
I0224 23:22:32.545047 30612 hierarchical.hpp:455] Added slave 20150224-232231-3142697795-38391-30589-S0 (pomona.apache.org) with cpus(*):2; mem(*):1024; disk(*):3.70122e+06; ports(*):[31000-32000] (and cpus(*):1; mem(*):524; disk(*):3.70122e+06; ports(*):[31000-32000] available)
I0224 23:22:32.545090 30603 slave.cpp:1922] Updating framework 20150224-232231-3142697795-38391-30589-0000 pid to scheduler-9a3224cc-aef0-49a7-a240-4b85b913ff44@67.195.81.187:38391
I0224 23:22:32.545128 30618 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 567893ns
I0224 23:22:32.545155 30618 replica.cpp:679] Persisted action at 8
I0224 23:22:32.545240 30613 status_update_manager.cpp:178] Resuming sending status updates
I0224 23:22:32.545424 30603 slave.cpp:2010] Updated checkpointed resources from  to 
I0224 23:22:32.545569 30612 hierarchical.hpp:759] Performed allocation for slave 20150224-232231-3142697795-38391-30589-S0 in 472208ns
I0224 23:22:32.545915 30610 replica.cpp:658] Replica received learned notice for position 8
I0224 23:22:32.545930 30607 master.cpp:3755] Sending 1 offers to framework 20150224-232231-3142697795-38391-30589-0000 (default) at scheduler-9a3224cc-aef0-49a7-a240-4b85b913ff44@67.195.81.187:38391
I0224 23:22:32.688380 30615 hierarchical.hpp:741] Performed allocation for 1 slaves in 418073ns
I0224 23:22:32.691712 30607 master.cpp:3755] Sending 1 offers to framework 20150224-232231-3142697795-38391-30589-0000 (default) at scheduler-9a3224cc-aef0-49a7-a240-4b85b913ff44@67.195.81.187:38391
I0224 23:22:32.691795 30605 sched.cpp:611] Scheduler::resourceOffers took 95977ns
I0224 23:22:32.691961 30610 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 649479ns
I0224 23:22:32.692033 30610 leveldb.cpp:401] Deleting ~2 keys from leveldb took 43354ns
I0224 23:22:32.692061 30610 replica.cpp:679] Persisted action at 8
I0224 23:22:32.692093 30610 replica.cpp:664] Replica learned 3 action at position 8
I0224 23:22:32.692191 30589 sched.cpp:1589] Asked to stop the driver
../../src/tests/master_allocator_tests.cpp:1308: Failure
Mock function called more times than expected - returning directly.
    Function call: resourceOffers(0x7fff88bc3d10, @0x2b8726c54b30 { 128-byte object <D0-C4 A2-22 87-2B 00-00 00-00 00-00 00-00 00-00 E0-C2 00-3C 87-2B 00-00 60-C3 00-3C 87-2B 00-00 E0-C3 00-3C 87-2B 00-00 D0-C0 00-3C 87-2B 00-00 50-7B 00-3C 87-2B 00-00 04-00 00-00 04-00 00-00 04-00 00-00 20-69 73-20 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 20-6E 65-76 50-C2 00-3C 87-2B 00-00 01-00 00-00 01-00 00-00 04-00 00-00 00-00 00-00 00-00 00-00 0F-00 00-00> })
         Expected: to be called once
           Actual: called twice - over-saturated and active
I0224 23:22:32.692252 30605 sched.cpp:611] Scheduler::resourceOffers took 213312ns
I0224 23:22:32.692296 30610 master.cpp:787] Master terminating
I0224 23:22:32.692387 30605 sched.cpp:831] Stopping framework '20150224-232231-3142697795-38391-30589-0000'
W0224 23:22:32.692448 30610 master.cpp:4668] Removing task 0 with resources cpus(*):1; mem(*):500 of framework 20150224-232231-3142697795-38391-30589-0000 on slave 20150224-232231-3142697795-38391-30589-S0 at slave(66)@67.195.81.187:38391 (pomona.apache.org) in non-terminal state TASK_RUNNING
I0224 23:22:32.692739 30613 hierarchical.hpp:648] Recovered cpus(*):1; mem(*):500 (total allocatable: cpus(*):1; mem(*):500) on slave 20150224-232231-3142697795-38391-30589-S0 from framework 20150224-232231-3142697795-38391-30589-0000
I0224 23:22:32.692939 30610 master.cpp:4711] Removing executor 'default' with resources  of framework 20150224-232231-3142697795-38391-30589-0000 on slave 20150224-232231-3142697795-38391-30589-S0 at slave(66)@67.195.81.187:38391 (pomona.apache.org)
I0224 23:22:32.693953 30610 slave.cpp:2916] master@67.195.81.187:38391 exited
W0224 23:22:32.693982 30610 slave.cpp:2919] Master disconnected! Waiting for a new master to be elected
I0224 23:22:32.695185 30589 process.cpp:2117] Dropped / Lost event for PID: master@67.195.81.187:38391
I0224 23:22:32.695327 30589 process.cpp:2117] Dropped / Lost event for PID: master@67.195.81.187:38391
I0224 23:22:32.697404 30613 slave.cpp:3191] Executor 'default' of framework 20150224-232231-3142697795-38391-30589-0000 exited with status 0
I0224 23:22:32.699574 30613 slave.cpp:2508] Handling status update TASK_LOST (UUID: d33cdd1e-5586-4ae5-94b0-870a1443844b) for task 0 of framework 20150224-232231-3142697795-38391-30589-0000 from @0.0.0.0:0
I0224 23:22:32.699658 30613 slave.cpp:4486] Terminating task 0
I0224 23:22:32.699964 30613 process.cpp:2117] Dropped / Lost event for PID: master@67.195.81.187:38391
I0224 23:22:32.700003 30613 slave.cpp:506] Slave terminating
I0224 23:22:32.700073 30613 slave.cpp:1745] Asked to shut down framework 20150224-232231-3142697795-38391-30589-0000 by @0.0.0.0:0
I0224 23:22:32.700098 30613 slave.cpp:1770] Shutting down framework 20150224-232231-3142697795-38391-30589-0000
I0224 23:22:32.700160 30613 slave.cpp:3300] Cleaning up executor 'default' of framework 20150224-232231-3142697795-38391-30589-0000
I0224 23:22:32.700355 30610 gc.cpp:56] Scheduling '/tmp/MasterAllocatorTest_0_FrameworkReregistersFirst_ikVXQM/slaves/20150224-232231-3142697795-38391-30589-S0/frameworks/20150224-232231-3142697795-38391-30589-0000/executors/default/runs/7c203619-b40f-4d0a-9b75-9ddeecb63472' for gc 6.99999189524148days in the future
I0224 23:22:32.700449 30613 slave.cpp:3379] Cleaning up framework 20150224-232231-3142697795-38391-30589-0000
I0224 23:22:32.700522 30610 gc.cpp:56] Scheduling '/tmp/MasterAllocatorTest_0_FrameworkReregistersFirst_ikVXQM/slaves/20150224-232231-3142697795-38391-30589-S0/frameworks/20150224-232231-3142697795-38391-30589-0000/executors/default' for gc 6.99999189365926days in the future
I0224 23:22:32.700580 30608 status_update_manager.cpp:279] Closing status update streams for framework 20150224-232231-3142697795-38391-30589-0000
I0224 23:22:32.700630 30610 gc.cpp:56] Scheduling '/tmp/MasterAllocatorTest_0_FrameworkReregistersFirst_ikVXQM/slaves/20150224-232231-3142697795-38391-30589-S0/frameworks/20150224-232231-3142697795-38391-30589-0000' for gc 6.99999189199111days in the future
I0224 23:22:32.700666 30608 status_update_manager.cpp:525] Cleaning up status update stream for task 0 of framework 20150224-232231-3142697795-38391-30589-0000
I0224 23:22:32.702638 30589 process.cpp:2117] Dropped / Lost event for PID: scheduler-9a3224cc-aef0-49a7-a240-4b85b913ff44@67.195.81.187:38391
I0224 23:22:32.702955 30589 process.cpp:2117] Dropped / Lost event for PID: slave(66)@67.195.81.187:38391
[  FAILED  ] MasterAllocatorTest/0.FrameworkReregistersFirst, where TypeParam = mesos::internal::master::allocator::MesosAllocator<mesos::internal::master::allocator::HierarchicalAllocatorProcess<mesos::internal::master::allocator::DRFSorter, mesos::internal::master::allocator::DRFSorter> > (1027 ms)

{code}",Bug,Major,vinodkone,2015-03-03T00:08:07.000+0000,5,Resolved,Complete,MasterAllocatorTest/0.FrameworkReregistersFirst is flaky,2015-04-03T22:54:23.000+0000,MESOS-2403,2.0,mesos,Twitter Mesos Q1 Sprint 3
vinodkone,2015-02-25T07:03:11.000+0000,vinodkone,"""Failed to os::execvpe in childMain"". Never seen this one before.

{code}
[ RUN      ] MesosContainerizerDestroyTest.LauncherDestroyFailure
Using temporary directory '/tmp/MesosContainerizerDestroyTest_LauncherDestroyFailure_QpjQEn'
I0224 18:55:49.326912 21391 containerizer.cpp:461] Starting container 'test_container' for executor 'executor' of framework ''
I0224 18:55:49.332252 21391 launcher.cpp:130] Forked child with pid '23496' for container 'test_container'
ABORT: (src/subprocess.cpp:165): Failed to os::execvpe in childMain
*** Aborted at 1424832949 (unix time) try ""date -d @1424832949"" if you are using GNU date ***
PC: @     0x2b178c5db0d5 (unknown)
I0224 18:55:49.340955 21392 process.cpp:2117] Dropped / Lost event for PID: scheduler-509d37ac-296f-4429-b101-af433c1800e9@127.0.1.1:39647
I0224 18:55:49.342300 21386 containerizer.cpp:911] Destroying container 'test_container'
*** SIGABRT (@0x3e800005bc8) received by PID 23496 (TID 0x2b178f9f0700) from PID 23496; stack trace: ***
    @     0x2b178c397cb0 (unknown)
    @     0x2b178c5db0d5 (unknown)
    @     0x2b178c5de83b (unknown)
    @           0x87a945 _Abort()
    @     0x2b1789f610b9 process::childMain()
I0224 18:55:49.391793 21386 containerizer.cpp:1120] Executor for container 'test_container' has exited
I0224 18:55:49.400478 21391 process.cpp:2770] Handling HTTP event for process 'metrics' with path: '/metrics/snapshot'
tests/containerizer_tests.cpp:485: Failure
Value of: metrics.values[""containerizer/mesos/container_destroy_errors""]
  Actual: 16-byte object <02-00 00-00 17-2B 00-00 E0-86 0E-04 00-00 00-00>
Expected: 1u
Which is: 1
[  FAILED  ] MesosContainerizerDestroyTest.LauncherDestroyFailure (89 ms)

{code}",Bug,Major,vinodkone,2015-07-13T18:41:11.000+0000,5,Resolved,Complete,MesosContainerizerDestroyTest.LauncherDestroyFailure is flaky,2015-08-07T03:13:52.000+0000,MESOS-2402,2.0,mesos,Twitter Mesos Q1 Sprint 5
vinodkone,2015-02-25T06:57:44.000+0000,vinodkone,"Looks like the executorShutdownTimeout() was called immediately after executorShutdown() was called!

{code}
[ RUN      ] MasterTest.ShutdownFrameworkWhileTaskRunning
Using temporary directory '/tmp/MasterTest_ShutdownFrameworkWhileTaskRunning_sBd6vK'
I0224 18:51:17.385068 30213 leveldb.cpp:176] Opened db in 1.262442ms
I0224 18:51:17.386360 30213 leveldb.cpp:183] Compacted db in 985102ns
I0224 18:51:17.387025 30213 leveldb.cpp:198] Created db iterator in 78043ns
I0224 18:51:17.387420 30213 leveldb.cpp:204] Seeked to beginning of db in 25814ns
I0224 18:51:17.387804 30213 leveldb.cpp:273] Iterated through 0 keys in the db in 25025ns
I0224 18:51:17.388270 30213 replica.cpp:744] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned
I0224 18:51:17.389760 30227 recover.cpp:449] Starting replica recovery
I0224 18:51:17.395699 30227 recover.cpp:475] Replica is in 4 status
I0224 18:51:17.398294 30227 replica.cpp:641] Replica in 4 status received a broadcasted recover request
I0224 18:51:17.398816 30227 recover.cpp:195] Received a recover response from a replica in 4 status
I0224 18:51:17.402415 30230 recover.cpp:566] Updating replica status to 3
I0224 18:51:17.403473 30229 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 273857ns
I0224 18:51:17.404093 30229 replica.cpp:323] Persisted replica status to 3
I0224 18:51:17.404930 30229 recover.cpp:475] Replica is in 3 status
I0224 18:51:17.407995 30233 replica.cpp:641] Replica in 3 status received a broadcasted recover request
I0224 18:51:17.410697 30231 recover.cpp:195] Received a recover response from a replica in 3 status
I0224 18:51:17.415710 30230 recover.cpp:566] Updating replica status to 1
I0224 18:51:17.416987 30227 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 221966ns
I0224 18:51:17.417579 30227 replica.cpp:323] Persisted replica status to 1
I0224 18:51:17.418803 30234 recover.cpp:580] Successfully joined the Paxos group
I0224 18:51:17.419699 30227 recover.cpp:464] Recover process terminated
I0224 18:51:17.430594 30234 master.cpp:349] Master 20150224-185117-2272962752-44950-30213 (fedora-19) started on 192.168.122.135:44950
I0224 18:51:17.431082 30234 master.cpp:395] Master only allowing authenticated frameworks to register
I0224 18:51:17.431453 30234 master.cpp:400] Master only allowing authenticated slaves to register
I0224 18:51:17.431828 30234 credentials.hpp:37] Loading credentials for authentication from '/tmp/MasterTest_ShutdownFrameworkWhileTaskRunning_sBd6vK/credentials'
I0224 18:51:17.432740 30234 master.cpp:442] Authorization enabled
I0224 18:51:17.434224 30229 hierarchical.hpp:287] Initialized hierarchical allocator process
I0224 18:51:17.434994 30233 whitelist_watcher.cpp:79] No whitelist given
I0224 18:51:17.440687 30234 master.cpp:1356] The newly elected leader is master@192.168.122.135:44950 with id 20150224-185117-2272962752-44950-30213
I0224 18:51:17.441764 30234 master.cpp:1369] Elected as the leading master!
I0224 18:51:17.442430 30234 master.cpp:1187] Recovering from registrar
I0224 18:51:17.443053 30229 registrar.cpp:313] Recovering registrar
I0224 18:51:17.445468 30228 log.cpp:660] Attempting to start the writer
I0224 18:51:17.449970 30233 replica.cpp:477] Replica received implicit promise request with proposal 1
I0224 18:51:17.451359 30233 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 339488ns
I0224 18:51:17.451949 30233 replica.cpp:345] Persisted promised to 1
I0224 18:51:17.456845 30235 process.cpp:2117] Dropped / Lost event for PID: hierarchical-allocator(154)@192.168.122.135:44950
I0224 18:51:17.461741 30231 coordinator.cpp:230] Coordinator attemping to fill missing position
I0224 18:51:17.464686 30228 replica.cpp:378] Replica received explicit promise request for position 0 with proposal 2
I0224 18:51:17.465515 30228 leveldb.cpp:343] Persisting action (8 bytes) to leveldb took 170261ns
I0224 18:51:17.465991 30228 replica.cpp:679] Persisted action at 0
I0224 18:51:17.470512 30229 replica.cpp:511] Replica received write request for position 0
I0224 18:51:17.471437 30229 leveldb.cpp:438] Reading position from leveldb took 139178ns
I0224 18:51:17.472129 30229 leveldb.cpp:343] Persisting action (14 bytes) to leveldb took 141560ns
I0224 18:51:17.472705 30229 replica.cpp:679] Persisted action at 0
I0224 18:51:17.476305 30228 replica.cpp:658] Replica received learned notice for position 0
I0224 18:51:17.477991 30228 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 208112ns
I0224 18:51:17.478574 30228 replica.cpp:679] Persisted action at 0
I0224 18:51:17.479044 30228 replica.cpp:664] Replica learned 1 action at position 0
I0224 18:51:17.484371 30233 log.cpp:676] Writer started with ending position 0
I0224 18:51:17.487396 30233 leveldb.cpp:438] Reading position from leveldb took 96498ns
I0224 18:51:17.498906 30233 registrar.cpp:346] Successfully fetched the registry (0B) in 55.234048ms
I0224 18:51:17.499781 30233 registrar.cpp:445] Applied 1 operations in 97308ns; attempting to update the 'registry'
I0224 18:51:17.503955 30231 log.cpp:684] Attempting to append 131 bytes to the log
I0224 18:51:17.505009 30231 coordinator.cpp:340] Coordinator attempting to write 2 action at position 1
I0224 18:51:17.507428 30228 replica.cpp:511] Replica received write request for position 1
I0224 18:51:17.508517 30228 leveldb.cpp:343] Persisting action (150 bytes) to leveldb took 316570ns
I0224 18:51:17.508985 30228 replica.cpp:679] Persisted action at 1
I0224 18:51:17.512902 30229 replica.cpp:658] Replica received learned notice for position 1
I0224 18:51:17.517261 30229 leveldb.cpp:343] Persisting action (152 bytes) to leveldb took 427860ns
I0224 18:51:17.517470 30229 replica.cpp:679] Persisted action at 1
I0224 18:51:17.517796 30229 replica.cpp:664] Replica learned 2 action at position 1
I0224 18:51:17.532624 30232 registrar.cpp:490] Successfully updated the 'registry' in 32.31104ms
I0224 18:51:17.533957 30228 log.cpp:703] Attempting to truncate the log to 1
I0224 18:51:17.534366 30228 coordinator.cpp:340] Coordinator attempting to write 3 action at position 2
I0224 18:51:17.536684 30227 replica.cpp:511] Replica received write request for position 2
I0224 18:51:17.537406 30227 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 196455ns
I0224 18:51:17.537946 30227 replica.cpp:679] Persisted action at 2
I0224 18:51:17.537695 30232 registrar.cpp:376] Successfully recovered registrar
I0224 18:51:17.544136 30231 master.cpp:1214] Recovered 0 slaves from the Registry (95B) ; allowing 10mins for slaves to re-register
I0224 18:51:17.546041 30227 replica.cpp:658] Replica received learned notice for position 2
I0224 18:51:17.546728 30227 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 192442ns
I0224 18:51:17.547058 30227 leveldb.cpp:401] Deleting ~1 keys from leveldb took 61064ns
I0224 18:51:17.547363 30227 replica.cpp:679] Persisted action at 2
I0224 18:51:17.547669 30227 replica.cpp:664] Replica learned 3 action at position 2
I0224 18:51:17.565460 30234 slave.cpp:174] Slave started on 138)@192.168.122.135:44950
I0224 18:51:17.566038 30234 credentials.hpp:85] Loading credential for authentication from '/tmp/MasterTest_ShutdownFrameworkWhileTaskRunning_lRugms/credential'
I0224 18:51:17.566584 30234 slave.cpp:281] Slave using credential for: test-principal
I0224 18:51:17.567198 30234 slave.cpp:299] Slave resources: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I0224 18:51:17.567930 30234 slave.cpp:328] Slave hostname: fedora-19
I0224 18:51:17.568172 30234 slave.cpp:329] Slave checkpoint: false
W0224 18:51:17.568435 30234 slave.cpp:331] Disabling checkpointing is deprecated and the --checkpoint flag will be removed in a future release. Please avoid using this flag
I0224 18:51:17.570539 30227 state.cpp:35] Recovering state from '/tmp/MasterTest_ShutdownFrameworkWhileTaskRunning_lRugms/meta'
I0224 18:51:17.573499 30232 status_update_manager.cpp:197] Recovering status update manager
I0224 18:51:17.574209 30234 slave.cpp:3775] Finished recovery
I0224 18:51:17.576277 30229 status_update_manager.cpp:171] Pausing sending status updates
I0224 18:51:17.576680 30234 slave.cpp:624] New master detected at master@192.168.122.135:44950
I0224 18:51:17.577131 30234 slave.cpp:687] Authenticating with master master@192.168.122.135:44950
I0224 18:51:17.577385 30234 slave.cpp:692] Using default CRAM-MD5 authenticatee
I0224 18:51:17.577945 30228 authenticatee.hpp:139] Creating new client SASL connection
I0224 18:51:17.578837 30234 slave.cpp:660] Detecting new master
I0224 18:51:17.579270 30228 master.cpp:3813] Authenticating slave(138)@192.168.122.135:44950
I0224 18:51:17.579900 30228 master.cpp:3824] Using default CRAM-MD5 authenticator
I0224 18:51:17.580572 30228 authenticator.hpp:170] Creating new server SASL connection
I0224 18:51:17.581501 30231 authenticatee.hpp:230] Received SASL authentication mechanisms: CRAM-MD5
I0224 18:51:17.581805 30231 authenticatee.hpp:256] Attempting to authenticate with mechanism 'CRAM-MD5'
I0224 18:51:17.582222 30228 authenticator.hpp:276] Received SASL authentication start
I0224 18:51:17.582531 30228 authenticator.hpp:398] Authentication requires more steps
I0224 18:51:17.582945 30230 authenticatee.hpp:276] Received SASL authentication step
I0224 18:51:17.583351 30228 authenticator.hpp:304] Received SASL authentication step
I0224 18:51:17.583643 30228 auxprop.cpp:99] Request to lookup properties for user: 'test-principal' realm: 'fedora-19' server FQDN: 'fedora-19' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0224 18:51:17.583911 30228 auxprop.cpp:171] Looking up auxiliary property '*userPassword'
I0224 18:51:17.584241 30228 auxprop.cpp:171] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0224 18:51:17.584517 30228 auxprop.cpp:99] Request to lookup properties for user: 'test-principal' realm: 'fedora-19' server FQDN: 'fedora-19' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0224 18:51:17.584787 30228 auxprop.cpp:121] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0224 18:51:17.585075 30228 auxprop.cpp:121] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0224 18:51:17.585358 30228 authenticator.hpp:390] Authentication success
I0224 18:51:17.585750 30233 authenticatee.hpp:316] Authentication success
I0224 18:51:17.586354 30232 master.cpp:3871] Successfully authenticated principal 'test-principal' at slave(138)@192.168.122.135:44950
I0224 18:51:17.590953 30234 slave.cpp:758] Successfully authenticated with master master@192.168.122.135:44950
I0224 18:51:17.591686 30233 master.cpp:2938] Registering slave at slave(138)@192.168.122.135:44950 (fedora-19) with id 20150224-185117-2272962752-44950-30213-S0
I0224 18:51:17.592718 30233 registrar.cpp:445] Applied 1 operations in 100358ns; attempting to update the 'registry'
I0224 18:51:17.595989 30227 log.cpp:684] Attempting to append 302 bytes to the log
I0224 18:51:17.596757 30227 coordinator.cpp:340] Coordinator attempting to write 2 action at position 3
I0224 18:51:17.599280 30227 replica.cpp:511] Replica received write request for position 3
I0224 18:51:17.599481 30234 slave.cpp:1090] Will retry registration in 12.331173ms if necessary
I0224 18:51:17.601940 30227 leveldb.cpp:343] Persisting action (321 bytes) to leveldb took 999045ns
I0224 18:51:17.602339 30227 replica.cpp:679] Persisted action at 3
I0224 18:51:17.612349 30229 replica.cpp:658] Replica received learned notice for position 3
I0224 18:51:17.612934 30229 leveldb.cpp:343] Persisting action (323 bytes) to leveldb took 152139ns
I0224 18:51:17.613471 30229 replica.cpp:679] Persisted action at 3
I0224 18:51:17.613796 30229 replica.cpp:664] Replica learned 2 action at position 3
I0224 18:51:17.615980 30229 master.cpp:2926] Ignoring register slave message from slave(138)@192.168.122.135:44950 (fedora-19) as admission is already in progress
I0224 18:51:17.614302 30233 slave.cpp:1090] Will retry registration in 11.014835ms if necessary
I0224 18:51:17.617490 30234 registrar.cpp:490] Successfully updated the 'registry' in 24.179968ms
I0224 18:51:17.618989 30234 master.cpp:2995] Registered slave 20150224-185117-2272962752-44950-30213-S0 at slave(138)@192.168.122.135:44950 (fedora-19) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I0224 18:51:17.619567 30233 hierarchical.hpp:455] Added slave 20150224-185117-2272962752-44950-30213-S0 (fedora-19) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (and cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] available)
I0224 18:51:17.621080 30233 hierarchical.hpp:834] No resources available to allocate!
I0224 18:51:17.621441 30233 hierarchical.hpp:759] Performed allocation for slave 20150224-185117-2272962752-44950-30213-S0 in 544608ns
I0224 18:51:17.619704 30229 slave.cpp:792] Registered with master master@192.168.122.135:44950; given slave ID 20150224-185117-2272962752-44950-30213-S0
I0224 18:51:17.622195 30229 slave.cpp:2830] Received ping from slave-observer(125)@192.168.122.135:44950
I0224 18:51:17.622385 30227 status_update_manager.cpp:178] Resuming sending status updates
I0224 18:51:17.620266 30232 log.cpp:703] Attempting to truncate the log to 3
I0224 18:51:17.623522 30232 coordinator.cpp:340] Coordinator attempting to write 3 action at position 4
I0224 18:51:17.624835 30229 replica.cpp:511] Replica received write request for position 4
I0224 18:51:17.625727 30229 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 259831ns
I0224 18:51:17.626122 30229 replica.cpp:679] Persisted action at 4
I0224 18:51:17.627686 30227 replica.cpp:658] Replica received learned notice for position 4
I0224 18:51:17.628228 30227 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 93777ns
I0224 18:51:17.628785 30227 leveldb.cpp:401] Deleting ~2 keys from leveldb took 57660ns
I0224 18:51:17.629176 30227 replica.cpp:679] Persisted action at 4
I0224 18:51:17.629443 30227 replica.cpp:664] Replica learned 3 action at position 4
I0224 18:51:17.636715 30213 sched.cpp:157] Version: 0.23.0
I0224 18:51:17.638003 30229 sched.cpp:254] New master detected at master@192.168.122.135:44950
I0224 18:51:17.638602 30229 sched.cpp:310] Authenticating with master master@192.168.122.135:44950
I0224 18:51:17.639024 30229 sched.cpp:317] Using default CRAM-MD5 authenticatee
I0224 18:51:17.639580 30228 authenticatee.hpp:139] Creating new client SASL connection
I0224 18:51:17.640455 30235 process.cpp:2117] Dropped / Lost event for PID: scheduler-11bb6bcb-cd51-4927-a28b-dbca9d63772f@192.168.122.135:44950
I0224 18:51:17.641150 30228 master.cpp:3813] Authenticating scheduler-fc72e828-0783-41b6-9892-ffc961e8567e@192.168.122.135:44950
I0224 18:51:17.641597 30228 master.cpp:3824] Using default CRAM-MD5 authenticator
I0224 18:51:17.642643 30228 authenticator.hpp:170] Creating new server SASL connection
I0224 18:51:17.643698 30234 authenticatee.hpp:230] Received SASL authentication mechanisms: CRAM-MD5
I0224 18:51:17.644296 30234 authenticatee.hpp:256] Attempting to authenticate with mechanism 'CRAM-MD5'
I0224 18:51:17.644739 30228 authenticator.hpp:276] Received SASL authentication start
I0224 18:51:17.645143 30228 authenticator.hpp:398] Authentication requires more steps
I0224 18:51:17.645654 30230 authenticatee.hpp:276] Received SASL authentication step
I0224 18:51:17.646122 30228 authenticator.hpp:304] Received SASL authentication step
I0224 18:51:17.646421 30228 auxprop.cpp:99] Request to lookup properties for user: 'test-principal' realm: 'fedora-19' server FQDN: 'fedora-19' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0224 18:51:17.646746 30228 auxprop.cpp:171] Looking up auxiliary property '*userPassword'
I0224 18:51:17.647203 30228 auxprop.cpp:171] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0224 18:51:17.647644 30228 auxprop.cpp:99] Request to lookup properties for user: 'test-principal' realm: 'fedora-19' server FQDN: 'fedora-19' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0224 18:51:17.648454 30228 auxprop.cpp:121] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0224 18:51:17.648788 30228 auxprop.cpp:121] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0224 18:51:17.649210 30228 authenticator.hpp:390] Authentication success
I0224 18:51:17.649705 30231 authenticatee.hpp:316] Authentication success
I0224 18:51:17.653314 30231 sched.cpp:398] Successfully authenticated with master master@192.168.122.135:44950
I0224 18:51:17.653766 30232 master.cpp:3871] Successfully authenticated principal 'test-principal' at scheduler-fc72e828-0783-41b6-9892-ffc961e8567e@192.168.122.135:44950
I0224 18:51:17.654683 30231 sched.cpp:521] Sending registration request to master@192.168.122.135:44950
I0224 18:51:17.655138 30231 sched.cpp:554] Will retry registration in 1.028970132secs if necessary
I0224 18:51:17.657112 30232 master.cpp:1574] Received registration request for framework 'default' at scheduler-fc72e828-0783-41b6-9892-ffc961e8567e@192.168.122.135:44950
I0224 18:51:17.658509 30232 master.cpp:1435] Authorizing framework principal 'test-principal' to receive offers for role '*'
I0224 18:51:17.659765 30232 master.cpp:1638] Registering framework 20150224-185117-2272962752-44950-30213-0000 (default) at scheduler-fc72e828-0783-41b6-9892-ffc961e8567e@192.168.122.135:44950
I0224 18:51:17.660727 30233 hierarchical.hpp:321] Added framework 20150224-185117-2272962752-44950-30213-0000
I0224 18:51:17.661730 30233 hierarchical.hpp:741] Performed allocation for 1 slaves in 529369ns
I0224 18:51:17.662911 30229 sched.cpp:448] Framework registered with 20150224-185117-2272962752-44950-30213-0000
I0224 18:51:17.663374 30229 sched.cpp:462] Scheduler::registered took 35637ns
I0224 18:51:17.664552 30232 master.cpp:3755] Sending 1 offers to framework 20150224-185117-2272962752-44950-30213-0000 (default) at scheduler-fc72e828-0783-41b6-9892-ffc961e8567e@192.168.122.135:44950
I0224 18:51:17.668009 30234 sched.cpp:611] Scheduler::resourceOffers took 2.574292ms
I0224 18:51:17.671038 30232 master.cpp:2268] Processing ACCEPT call for offers: [ 20150224-185117-2272962752-44950-30213-O0 ] on slave 20150224-185117-2272962752-44950-30213-S0 at slave(138)@192.168.122.135:44950 (fedora-19) for framework 20150224-185117-2272962752-44950-30213-0000 (default) at scheduler-fc72e828-0783-41b6-9892-ffc961e8567e@192.168.122.135:44950
I0224 18:51:17.672071 30232 master.cpp:2112] Authorizing framework principal 'test-principal' to launch task 1 as user 'jenkins'
W0224 18:51:17.674675 30232 validation.cpp:326] Executor default for task 1 uses less CPUs (None) than the minimum required (0.01). Please update your executor, as this will be mandatory in future releases.
W0224 18:51:17.675395 30232 validation.cpp:338] Executor default for task 1 uses less memory (None) than the minimum required (32MB). Please update your executor, as this will be mandatory in future releases.
I0224 18:51:17.676460 30232 master.hpp:822] Adding task 1 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave 20150224-185117-2272962752-44950-30213-S0 (fedora-19)
I0224 18:51:17.677078 30232 master.cpp:2545] Launching task 1 of framework 20150224-185117-2272962752-44950-30213-0000 (default) at scheduler-fc72e828-0783-41b6-9892-ffc961e8567e@192.168.122.135:44950 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave 20150224-185117-2272962752-44950-30213-S0 at slave(138)@192.168.122.135:44950 (fedora-19)
I0224 18:51:17.678084 30230 slave.cpp:1121] Got assigned task 1 for framework 20150224-185117-2272962752-44950-30213-0000
I0224 18:51:17.680057 30230 slave.cpp:1231] Launching task 1 for framework 20150224-185117-2272962752-44950-30213-0000
I0224 18:51:17.684798 30230 slave.cpp:4177] Launching executor default of framework 20150224-185117-2272962752-44950-30213-0000 in work directory '/tmp/MasterTest_ShutdownFrameworkWhileTaskRunning_lRugms/slaves/20150224-185117-2272962752-44950-30213-S0/frameworks/20150224-185117-2272962752-44950-30213-0000/executors/default/runs/675638b4-5214-449d-96d8-c50551496152'
I0224 18:51:17.688701 30230 exec.cpp:132] Version: 0.23.0
I0224 18:51:17.689615 30234 exec.cpp:182] Executor started at: executor(41)@192.168.122.135:44950 with pid 30213
I0224 18:51:17.690659 30230 slave.cpp:1379] Queuing task '1' for executor default of framework '20150224-185117-2272962752-44950-30213-0000
I0224 18:51:17.691382 30230 slave.cpp:577] Successfully attached file '/tmp/MasterTest_ShutdownFrameworkWhileTaskRunning_lRugms/slaves/20150224-185117-2272962752-44950-30213-S0/frameworks/20150224-185117-2272962752-44950-30213-0000/executors/default/runs/675638b4-5214-449d-96d8-c50551496152'
I0224 18:51:17.691813 30230 slave.cpp:2140] Got registration for executor 'default' of framework 20150224-185117-2272962752-44950-30213-0000 from executor(41)@192.168.122.135:44950
I0224 18:51:17.692772 30231 exec.cpp:206] Executor registered on slave 20150224-185117-2272962752-44950-30213-S0
I0224 18:51:17.695121 30231 exec.cpp:218] Executor::registered took 80811ns
I0224 18:51:17.697582 30230 slave.cpp:3132] Monitoring executor 'default' of framework '20150224-185117-2272962752-44950-30213-0000' in container '675638b4-5214-449d-96d8-c50551496152'
I0224 18:51:17.699354 30230 slave.cpp:1533] Sending queued task '1' to executor 'default' of framework 20150224-185117-2272962752-44950-30213-0000
I0224 18:51:17.700932 30227 exec.cpp:293] Executor asked to run task '1'
I0224 18:51:17.701679 30227 exec.cpp:302] Executor::launchTask took 140355ns
I0224 18:51:17.705504 30227 exec.cpp:525] Executor sending status update TASK_RUNNING (UUID: 57877913-d602-445c-a0d9-87fc71e8eca5) for task 1 of framework 20150224-185117-2272962752-44950-30213-0000
I0224 18:51:17.707149 30228 slave.cpp:2507] Handling status update TASK_RUNNING (UUID: 57877913-d602-445c-a0d9-87fc71e8eca5) for task 1 of framework 20150224-185117-2272962752-44950-30213-0000 from executor(41)@192.168.122.135:44950
I0224 18:51:17.708539 30228 status_update_manager.cpp:317] Received status update TASK_RUNNING (UUID: 57877913-d602-445c-a0d9-87fc71e8eca5) for task 1 of framework 20150224-185117-2272962752-44950-30213-0000
I0224 18:51:17.709377 30228 status_update_manager.cpp:494] Creating StatusUpdate stream for task 1 of framework 20150224-185117-2272962752-44950-30213-0000
I0224 18:51:17.710360 30228 status_update_manager.cpp:371] Forwarding update TASK_RUNNING (UUID: 57877913-d602-445c-a0d9-87fc71e8eca5) for task 1 of framework 20150224-185117-2272962752-44950-30213-0000 to the slave
I0224 18:51:17.711405 30233 slave.cpp:2750] Forwarding the update TASK_RUNNING (UUID: 57877913-d602-445c-a0d9-87fc71e8eca5) for task 1 of framework 20150224-185117-2272962752-44950-30213-0000 to master@192.168.122.135:44950
I0224 18:51:17.712425 30233 master.cpp:3295] Status update TASK_RUNNING (UUID: 57877913-d602-445c-a0d9-87fc71e8eca5) for task 1 of framework 20150224-185117-2272962752-44950-30213-0000 from slave 20150224-185117-2272962752-44950-30213-S0 at slave(138)@192.168.122.135:44950 (fedora-19)
I0224 18:51:17.713047 30233 master.cpp:3336] Forwarding status update TASK_RUNNING (UUID: 57877913-d602-445c-a0d9-87fc71e8eca5) for task 1 of framework 20150224-185117-2272962752-44950-30213-0000
I0224 18:51:17.713930 30233 master.cpp:4615] Updating the latest state of task 1 of framework 20150224-185117-2272962752-44950-30213-0000 to TASK_RUNNING
I0224 18:51:17.714588 30232 sched.cpp:717] Scheduler::statusUpdate took 118286ns
I0224 18:51:17.715512 30213 sched.cpp:1589] Asked to stop the driver
I0224 18:51:17.718159 30232 master.cpp:2782] Forwarding status update acknowledgement 57877913-d602-445c-a0d9-87fc71e8eca5 for task 1 of framework 20150224-185117-2272962752-44950-30213-0000 (default) at scheduler-fc72e828-0783-41b6-9892-ffc961e8567e@192.168.122.135:44950 to slave 20150224-185117-2272962752-44950-30213-S0 at slave(138)@192.168.122.135:44950 (fedora-19)
I0224 18:51:17.718691 30234 sched.cpp:831] Stopping framework '20150224-185117-2272962752-44950-30213-0000'
I0224 18:51:17.721380 30232 master.cpp:1898] Asked to unregister framework 20150224-185117-2272962752-44950-30213-0000
I0224 18:51:17.722920 30232 master.cpp:4183] Removing framework 20150224-185117-2272962752-44950-30213-0000 (default) at scheduler-fc72e828-0783-41b6-9892-ffc961e8567e@192.168.122.135:44950
I0224 18:51:17.725231 30231 hierarchical.hpp:400] Deactivated framework 20150224-185117-2272962752-44950-30213-0000
I0224 18:51:17.725734 30229 slave.cpp:1746] Asked to shut down framework 20150224-185117-2272962752-44950-30213-0000 by master@192.168.122.135:44950
I0224 18:51:17.726658 30229 slave.cpp:1771] Shutting down framework 20150224-185117-2272962752-44950-30213-0000
I0224 18:51:17.727322 30229 slave.cpp:3440] Shutting down executor 'default' of framework 20150224-185117-2272962752-44950-30213-0000
I0224 18:51:17.728742 30228 status_update_manager.cpp:389] Received status update acknowledgement (UUID: 57877913-d602-445c-a0d9-87fc71e8eca5) for task 1 of framework 20150224-185117-2272962752-44950-30213-0000
I0224 18:51:17.729042 30231 slave.cpp:2677] Status update manager successfully handled status update TASK_RUNNING (UUID: 57877913-d602-445c-a0d9-87fc71e8eca5) for task 1 of framework 20150224-185117-2272962752-44950-30213-0000
I0224 18:51:17.730578 30231 slave.cpp:2683] Sending acknowledgement for status update TASK_RUNNING (UUID: 57877913-d602-445c-a0d9-87fc71e8eca5) for task 1 of framework 20150224-185117-2272962752-44950-30213-0000 to executor(41)@192.168.122.135:44950
I0224 18:51:17.731300 30231 slave.cpp:3510] Killing executor 'default' of framework 20150224-185117-2272962752-44950-30213-0000
I0224 18:51:17.733461 30232 master.cpp:4615] Updating the latest state of task 1 of framework 20150224-185117-2272962752-44950-30213-0000 to TASK_KILLED
I0224 18:51:17.734503 30231 slave.cpp:3190] Executor 'default' of framework 20150224-185117-2272962752-44950-30213-0000 exited with status 0
I0224 18:51:17.736177 30231 slave.cpp:3299] Cleaning up executor 'default' of framework 20150224-185117-2272962752-44950-30213-0000
I0224 18:51:17.737277 30233 gc.cpp:56] Scheduling '/tmp/MasterTest_ShutdownFrameworkWhileTaskRunning_lRugms/slaves/20150224-185117-2272962752-44950-30213-S0/frameworks/20150224-185117-2272962752-44950-30213-0000/executors/default/runs/675638b4-5214-449d-96d8-c50551496152' for gc 6.99999146853037days in the future
I0224 18:51:17.738636 30231 slave.cpp:3378] Cleaning up framework 20150224-185117-2272962752-44950-30213-0000
I0224 18:51:17.739148 30228 gc.cpp:56] Scheduling '/tmp/MasterTest_ShutdownFrameworkWhileTaskRunning_lRugms/slaves/20150224-185117-2272962752-44950-30213-S0/frameworks/20150224-185117-2272962752-44950-30213-0000/executors/default' for gc 6.99999145243259days in the future
I0224 18:51:17.740373 30228 status_update_manager.cpp:279] Closing status update streams for framework 20150224-185117-2272962752-44950-30213-0000
I0224 18:51:17.741170 30228 status_update_manager.cpp:525] Cleaning up status update stream for task 1 of framework 20150224-185117-2272962752-44950-30213-0000
I0224 18:51:17.742255 30229 gc.cpp:56] Scheduling '/tmp/MasterTest_ShutdownFrameworkWhileTaskRunning_lRugms/slaves/20150224-185117-2272962752-44950-30213-S0/frameworks/20150224-185117-2272962752-44950-30213-0000' for gc 6.99999141055704days in the future
I0224 18:51:17.743207 30231 slave.cpp:2080] Status update manager successfully handled status update acknowledgement (UUID: 57877913-d602-445c-a0d9-87fc71e8eca5) for task 1 of framework 20150224-185117-2272962752-44950-30213-0000
E0224 18:51:17.743799 30231 slave.cpp:2091] Status update acknowledgement (UUID: 57877913-d602-445c-a0d9-87fc71e8eca5) for task 1 of unknown framework 20150224-185117-2272962752-44950-30213-0000
I0224 18:51:17.744699 30233 hierarchical.hpp:648] Recovered cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (total allocatable: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]) on slave 20150224-185117-2272962752-44950-30213-S0 from framework 20150224-185117-2272962752-44950-30213-0000
I0224 18:51:17.746369 30232 master.cpp:4682] Removing task 1 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] of framework 20150224-185117-2272962752-44950-30213-0000 on slave 20150224-185117-2272962752-44950-30213-S0 at slave(138)@192.168.122.135:44950 (fedora-19)
I0224 18:51:17.747835 30232 master.cpp:4711] Removing executor 'default' with resources  of framework 20150224-185117-2272962752-44950-30213-0000 on slave 20150224-185117-2272962752-44950-30213-S0 at slave(138)@192.168.122.135:44950 (fedora-19)
I0224 18:51:17.749958 30230 hierarchical.hpp:354] Removed framework 20150224-185117-2272962752-44950-30213-0000
W0224 18:51:17.754004 30232 master.cpp:3382] Ignoring unknown exited executor 'default' of framework 20150224-185117-2272962752-44950-30213-0000 on slave 20150224-185117-2272962752-44950-30213-S0 at slave(138)@192.168.122.135:44950 (fedora-19)
I0224 18:51:17.806908 30235 process.cpp:2117] Dropped / Lost event for PID: hierarchical-allocator(155)@192.168.122.135:44950
I0224 18:51:18.169684 30235 process.cpp:2117] Dropped / Lost event for PID: hierarchical-allocator(156)@192.168.122.135:44950
I0224 18:51:18.277674 30235 process.cpp:2117] Dropped / Lost event for PID: scheduler-279c11f0-4f87-4922-b09c-e75af333d93e@192.168.122.135:44950
I0224 18:51:18.435956 30229 hierarchical.hpp:834] No resources available to allocate!
I0224 18:51:18.438434 30229 hierarchical.hpp:741] Performed allocation for 1 slaves in 2.913091ms
I0224 18:51:18.687819 30235 process.cpp:2117] Dropped / Lost event for PID: scheduler-fc72e828-0783-41b6-9892-ffc961e8567e@192.168.122.135:44950
I0224 18:51:18.840509 30235 process.cpp:2117] Dropped / Lost event for PID: scheduler-11bb6bcb-cd51-4927-a28b-dbca9d63772f@192.168.122.135:44950
I0224 18:51:19.440609 30233 hierarchical.hpp:834] No resources available to allocate!
I0224 18:51:19.444022 30233 hierarchical.hpp:741] Performed allocation for 1 slaves in 3.667907ms
I0224 18:51:20.445341 30229 hierarchical.hpp:834] No resources available to allocate!
I0224 18:51:20.448892 30229 hierarchical.hpp:741] Performed allocation for 1 slaves in 3.787334ms
I0224 18:51:20.840729 30235 process.cpp:2117] Dropped / Lost event for PID: slave(133)@192.168.122.135:44950
I0224 18:51:20.895016 30235 process.cpp:2117] Dropped / Lost event for PID: scheduler-deb787a0-9e87-42c9-813e-fc35f354582f@192.168.122.135:44950
I0224 18:51:21.016639 30235 process.cpp:2117] Dropped / Lost event for PID: slave(133)@192.168.122.135:44950
I0224 18:51:21.258066 30235 process.cpp:2117] Dropped / Lost event for PID: slave(134)@192.168.122.135:44950
I0224 18:51:21.312721 30235 process.cpp:2117] Dropped / Lost event for PID: scheduler-7a62b2f4-6959-49de-9fd8-72ffd048f4e3@192.168.122.135:44950
I0224 18:51:21.450574 30230 hierarchical.hpp:834] No resources available to allocate!
I0224 18:51:21.451004 30230 hierarchical.hpp:741] Performed allocation for 1 slaves in 761280ns
I0224 18:51:21.557883 30235 process.cpp:2117] Dropped / Lost event for PID: slave(135)@192.168.122.135:44950
I0224 18:51:21.611552 30235 process.cpp:2117] Dropped / Lost event for PID: scheduler-279c11f0-4f87-4922-b09c-e75af333d93e@192.168.122.135:44950
I0224 18:51:21.709940 30235 process.cpp:2117] Dropped / Lost event for PID: slave(135)@192.168.122.135:44950
I0224 18:51:21.915220 30235 process.cpp:2117] Dropped / Lost event for PID: slave(136)@192.168.122.135:44950
I0224 18:51:21.997714 30235 process.cpp:2117] Dropped / Lost event for PID: scheduler-11bb6bcb-cd51-4927-a28b-dbca9d63772f@192.168.122.135:44950
I0224 18:51:22.107311 30235 process.cpp:2117] Dropped / Lost event for PID: slave(136)@192.168.122.135:44950
I0224 18:51:22.219341 30235 process.cpp:2117] Dropped / Lost event for PID: scheduler-11bb6bcb-cd51-4927-a28b-dbca9d63772f@192.168.122.135:44950
I0224 18:51:22.269714 30235 process.cpp:2117] Dropped / Lost event for PID: slave(137)@192.168.122.135:44950
I0224 18:51:22.453269 30229 hierarchical.hpp:834] No resources available to allocate!
I0224 18:51:22.457568 30229 hierarchical.hpp:741] Performed allocation for 1 slaves in 4.67818ms
I0224 18:51:22.644316 30235 process.cpp:2117] Dropped / Lost event for PID: scheduler-fc72e828-0783-41b6-9892-ffc961e8567e@192.168.122.135:44950
I0224 18:51:23.459383 30231 hierarchical.hpp:834] No resources available to allocate!
I0224 18:51:23.462417 30231 hierarchical.hpp:741] Performed allocation for 1 slaves in 3.417923ms
I0224 18:51:24.464651 30228 hierarchical.hpp:834] No resources available to allocate!
I0224 18:51:24.468094 30228 hierarchical.hpp:741] Performed allocation for 1 slaves in 3.698248ms
I0224 18:51:25.469254 30232 hierarchical.hpp:834] No resources available to allocate!
I0224 18:51:25.472430 30232 hierarchical.hpp:741] Performed allocation for 1 slaves in 3.477698ms
I0224 18:51:25.971513 30235 process.cpp:2117] Dropped / Lost event for PID: (2965)@192.168.122.135:44950
I0224 18:51:26.474663 30234 hierarchical.hpp:834] No resources available to allocate!
I0224 18:51:26.475232 30234 hierarchical.hpp:741] Performed allocation for 1 slaves in 942399ns
I0224 18:51:26.672420 30235 process.cpp:2117] Dropped / Lost event for PID: (2996)@192.168.122.135:44950
I0224 18:51:27.069792 30235 process.cpp:2117] Dropped / Lost event for PID: (3010)@192.168.122.135:44950
I0224 18:51:27.476572 30228 hierarchical.hpp:834] No resources available to allocate!
I0224 18:51:27.479708 30228 hierarchical.hpp:741] Performed allocation for 1 slaves in 3.419391ms
I0224 18:51:28.481403 30228 hierarchical.hpp:834] No resources available to allocate!
I0224 18:51:28.484798 30228 hierarchical.hpp:741] Performed allocation for 1 slaves in 3.709639ms
I0224 18:51:29.487264 30228 hierarchical.hpp:834] No resources available to allocate!
I0224 18:51:29.491909 30228 hierarchical.hpp:741] Performed allocation for 1 slaves in 5.065187ms
I0224 18:51:29.623121 30235 process.cpp:2117] Dropped / Lost event for PID: __waiter__(739)@192.168.122.135:44950
I0224 18:51:29.641655 30235 process.cpp:2117] Dropped / Lost event for PID: slave-observer(120)@192.168.122.135:44950
I0224 18:51:29.647222 30235 process.cpp:2117] Dropped / Lost event for PID: __waiter__(740)@192.168.122.135:44950
I0224 18:51:30.493526 30232 hierarchical.hpp:834] No resources available to allocate!
I0224 18:51:30.496922 30232 hierarchical.hpp:741] Performed allocation for 1 slaves in 3.714894ms
I0224 18:51:30.670241 30235 process.cpp:2117] Dropped / Lost event for PID: __waiter__(741)@192.168.122.135:44950
I0224 18:51:30.670737 30235 process.cpp:2117] Dropped / Lost event for PID: __waiter__(742)@192.168.122.135:44950
I0224 18:51:30.882052 30235 process.cpp:2117] Dropped / Lost event for PID: slave-observer(121)@192.168.122.135:44950
I0224 18:51:30.922494 30235 process.cpp:2117] Dropped / Lost event for PID: __waiter__(744)@192.168.122.135:44950
I0224 18:51:30.985663 30235 process.cpp:2117] Dropped / Lost event for PID: __waiter__(745)@192.168.122.135:44950
I0224 18:51:31.293728 30235 process.cpp:2117] Dropped / Lost event for PID: slave-observer(122)@192.168.122.135:44950
I0224 18:51:31.328766 30235 process.cpp:2117] Dropped / Lost event for PID: __waiter__(747)@192.168.122.135:44950
I0224 18:51:31.497968 30234 hierarchical.hpp:834] No resources available to allocate!
I0224 18:51:31.501803 30234 hierarchical.hpp:741] Performed allocation for 1 slaves in 4.13526ms
I0224 18:51:31.604324 30235 process.cpp:2117] Dropped / Lost event for PID: slave-observer(123)@192.168.122.135:44950
I0224 18:51:31.645259 30235 process.cpp:2117] Dropped / Lost event for PID: __waiter__(749)@192.168.122.135:44950
I0224 18:51:31.676254 30235 process.cpp:2117] Dropped / Lost event for PID: __waiter__(750)@192.168.122.135:44950
I0224 18:51:31.913120 30235 process.cpp:2117] Dropped / Lost event for PID: __waiter__(752)@192.168.122.135:44950
I0224 18:51:31.957001 30235 process.cpp:2117] Dropped / Lost event for PID: slave-observer(124)@192.168.122.135:44950
I0224 18:51:31.996151 30235 process.cpp:2117] Dropped / Lost event for PID: __waiter__(753)@192.168.122.135:44950
I0224 18:51:32.033216 30235 process.cpp:2117] Dropped / Lost event for PID: __waiter__(754)@192.168.122.135:44950
I0224 18:51:32.231158 30235 process.cpp:2117] Dropped / Lost event for PID: __waiter__(756)@192.168.122.135:44950
I0224 18:51:32.267324 30235 process.cpp:2117] Dropped / Lost event for PID: __waiter__(757)@192.168.122.135:44950
I0224 18:51:32.503401 30227 hierarchical.hpp:834] No resources available to allocate!
I0224 18:51:32.503978 30227 hierarchical.hpp:741] Performed allocation for 1 slaves in 1.062132ms
I0224 18:51:32.621012 30232 slave.cpp:2830] Received ping from slave-observer(125)@192.168.122.135:44950
I0224 18:51:32.658608 30235 process.cpp:2117] Dropped / Lost event for PID: __waiter__(759)@192.168.122.135:44950
I0224 18:51:32.671058 30235 process.cpp:2117] Dropped / Lost event for PID: __waiter__(760)@192.168.122.135:44950
I0224 18:51:32.719501 30235 process.cpp:2117] Dropped / Lost event for PID: __waiter__(761)@192.168.122.135:44950
tests/master_tests.cpp:259: Failure
Failed to wait 15secs for shutdown
I0224 18:51:32.737337 30213 process.cpp:2117] Dropped / Lost event for PID: scheduler-fc72e828-0783-41b6-9892-ffc961e8567e@192.168.122.135:44950
tests/master_tests.cpp:249: Failure
Actual function call count doesn't match EXPECT_CALL(exec, shutdown(_))...
         Expected: to be called once
           Actual: never called - unsatisfied and active
I0224 18:51:32.741822 30229 master.cpp:787] Master terminating
I0224 18:51:32.750730 30234 slave.cpp:2915] master@192.168.122.135:44950 exited
W0224 18:51:32.751667 30234 slave.cpp:2918] Master disconnected! Waiting for a new master to be elected
I0224 18:51:32.769243 30213 process.cpp:2117] Dropped / Lost event for PID: master@192.168.122.135:44950
I0224 18:51:32.770519 30213 process.cpp:2117] Dropped / Lost event for PID: master@192.168.122.135:44950
*** Aborted at 1424832692 (unix time) try ""date -d @1424832692"" if you are using GNU date ***
PC: @          0x4612540 (unknown)
*** SIGSEGV (@0x4612540) received by PID 30213 (TID 0x7fc7f2fa6880) from PID 73475392; stack trace: ***
    @       0x3aa2a0efa0 (unknown)
    @          0x4612540 (unknown)
make[3]: *** [check-local] Segmentation fault (core dumped)

{code}",Bug,Major,vinodkone,2015-03-06T21:50:59.000+0000,5,Resolved,Complete,MasterTest.ShutdownFrameworkWhileTaskRunning is flaky,2015-04-03T22:54:43.000+0000,MESOS-2401,1.0,mesos,Twitter Mesos Q1 Sprint 4
chzhcn,2015-02-24T20:12:44.000+0000,chzhcn,"- Use symbol NAME directly to launch the subprocess instead of the hard-coded string.
 - Replaced the static string with char[].
",Improvement,Minor,chzhcn,2015-02-25T18:56:53.000+0000,5,Resolved,Complete,Improve NsTest.ROOT_setns,2015-02-25T19:37:17.000+0000,MESOS-2400,1.0,mesos,Twitter Mesos Q1 Sprint 3
js84,2015-02-24T14:17:20.000+0000,js84,"As of right now different pages in our documentation use quite different styles. Consider for example the different emphasis for NOTE:
* {noformat}> NOTE: http://mesos.apache.org/documentation/latest/slave-recovery/{noformat}
*  {noformat}*NOTE*: http://mesos.apache.org/documentation/latest/upgrades/ {noformat} 

Would be great to establish a common style for the documentation!",Documentation,Minor,js84,2015-06-23T13:47:04.000+0000,5,Resolved,Complete,Create styleguide for documentation,2015-06-23T13:47:04.000+0000,MESOS-2394,2.0,mesos,Mesosphere Sprint 13
bmahler,2015-02-24T02:22:48.000+0000,bmahler,"Much like we rate limit slave removals in the common path (MESOS-1148), we need to rate limit slave removals that occur during master recovery. When a master recovers and is using a strict registry, slaves that do not re-register within a timeout will be removed.

Currently there is a safeguard in place to abort when too many slaves have not re-registered. However, in the case of a transient partition, we don't want to remove large sections of slaves without rate limiting.",Improvement,Major,bmahler,2015-02-28T02:10:17.000+0000,5,Resolved,Complete,Rate limit slaves removals during master recovery.,2015-03-27T00:28:49.000+0000,MESOS-2392,3.0,mesos,Twitter Mesos Q1 Sprint 3
jieyu,2015-02-24T00:48:53.000+0000,jieyu,We introduced a posix disk isolator for Mesos containerizer in 0.22.0. This isolator allows us to get container disk usage as well as enforcing container disk quota. It's based on 'du'. We need to document this feature.,Documentation,Major,jieyu,2015-02-24T19:54:07.000+0000,5,Resolved,Complete,Provide user doc for the new posix disk isolator in Mesos containerizer,2015-02-26T00:29:45.000+0000,MESOS-2391,2.0,mesos,Twitter Mesos Q1 Sprint 3
,2015-02-23T21:52:42.000+0000,vinodkone,"Observed this on internal CI. Not sure if it is due to ""GroupTest.LabelledGroup"" or an earlier test.

{code}
I0219 01:04:17.980598 27766 zookeeper_test_server.cpp:117] Shutting down ZooKeeperTestServer on port 39597
[       OK ] GroupTest.RetryableErrors (30150 ms)
[ RUN      ] GroupTest.LabelledGroup
Makefile:6656: recipe for target 'check-local' failed
make[3]: *** [check-local] Segmentation fault (core dumped)

{code}",Bug,Major,vinodkone,,1,Open,New,GroupTest.LabelledGroup segfaults,2015-03-17T19:40:13.000+0000,MESOS-2388,2.0,mesos,Twitter Mesos Q1 Sprint 3
jieyu,2015-02-23T20:22:21.000+0000,vinodkone,"Observed on internal CI

{code}
[ RUN      ] SlaveTest.TaskLaunchContainerizerUpdateFails
Using temporary directory '/tmp/SlaveTest_TaskLaunchContainerizerUpdateFails_tUjtcI'
I0222 04:59:56.568491 21813 process.cpp:2117] Dropped / Lost event for PID: slave(52)@192.168.122.68:39461
I0222 04:59:56.595433 21791 leveldb.cpp:175] Opened db in 27.59732ms
I0222 04:59:56.603965 21791 leveldb.cpp:182] Compacted db in 8.49192ms
I0222 04:59:56.604019 21791 leveldb.cpp:197] Created db iterator in 19206ns
I0222 04:59:56.604037 21791 leveldb.cpp:203] Seeked to beginning of db in 1802ns
I0222 04:59:56.604046 21791 leveldb.cpp:272] Iterated through 0 keys in the db in 467ns
I0222 04:59:56.604081 21791 replica.cpp:743] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned
I0222 04:59:56.607413 21809 recover.cpp:448] Starting replica recovery
I0222 04:59:56.607687 21809 recover.cpp:474] Replica is in 4 status
I0222 04:59:56.609011 21809 replica.cpp:640] Replica in 4 status received a broadcasted recover request
I0222 04:59:56.609262 21809 recover.cpp:194] Received a recover response from a replica in 4 status
I0222 04:59:56.609709 21809 recover.cpp:565] Updating replica status to 3
I0222 04:59:56.610749 21811 master.cpp:347] Master 20150222-045956-1148889280-39461-21791 (centos-7) started on 192.168.122.68:39461
I0222 04:59:56.610791 21811 master.cpp:393] Master only allowing authenticated frameworks to register
I0222 04:59:56.610802 21811 master.cpp:398] Master only allowing authenticated slaves to register
I0222 04:59:56.610821 21811 credentials.hpp:36] Loading credentials for authentication from '/tmp/SlaveTest_TaskLaunchContainerizerUpdateFails_tUjtcI/credentials'
I0222 04:59:56.611042 21811 master.cpp:440] Authorization enabled
I0222 04:59:56.612329 21811 hierarchical.hpp:286] Initialized hierarchical allocator process
I0222 04:59:56.612416 21811 whitelist_watcher.cpp:78] No whitelist given
I0222 04:59:56.613005 21811 master.cpp:1354] The newly elected leader is master@192.168.122.68:39461 with id 20150222-045956-1148889280-39461-21791
I0222 04:59:56.613034 21811 master.cpp:1367] Elected as the leading master!
I0222 04:59:56.613050 21811 master.cpp:1185] Recovering from registrar
I0222 04:59:56.613229 21811 registrar.cpp:312] Recovering registrar
I0222 04:59:56.622866 21809 leveldb.cpp:305] Persisting metadata (8 bytes) to leveldb took 12.988429ms
I0222 04:59:56.622913 21809 replica.cpp:322] Persisted replica status to 3
I0222 04:59:56.623118 21809 recover.cpp:474] Replica is in 3 status
I0222 04:59:56.624419 21809 replica.cpp:640] Replica in 3 status received a broadcasted recover request
I0222 04:59:56.624685 21809 recover.cpp:194] Received a recover response from a replica in 3 status
I0222 04:59:56.625200 21809 recover.cpp:565] Updating replica status to 1
I0222 04:59:56.635154 21809 leveldb.cpp:305] Persisting metadata (8 bytes) to leveldb took 9.799671ms
I0222 04:59:56.635197 21809 replica.cpp:322] Persisted replica status to 1
I0222 04:59:56.635296 21809 recover.cpp:579] Successfully joined the Paxos group
I0222 04:59:56.635426 21809 recover.cpp:463] Recover process terminated
I0222 04:59:56.635812 21809 log.cpp:659] Attempting to start the writer
I0222 04:59:56.637075 21809 replica.cpp:476] Replica received implicit promise request with proposal 1
I0222 04:59:56.648674 21809 leveldb.cpp:305] Persisting metadata (8 bytes) to leveldb took 11.566146ms
I0222 04:59:56.648717 21809 replica.cpp:344] Persisted promised to 1
I0222 04:59:56.649456 21809 coordinator.cpp:229] Coordinator attemping to fill missing position
I0222 04:59:56.650800 21809 replica.cpp:377] Replica received explicit promise request for position 0 with proposal 2
I0222 04:59:56.659916 21809 leveldb.cpp:342] Persisting action (8 bytes) to leveldb took 9.078258ms
I0222 04:59:56.659981 21809 replica.cpp:678] Persisted action at 0
I0222 04:59:56.661075 21809 replica.cpp:510] Replica received write request for position 0
I0222 04:59:56.661129 21809 leveldb.cpp:437] Reading position from leveldb took 26387ns
I0222 04:59:56.671227 21809 leveldb.cpp:342] Persisting action (14 bytes) to leveldb took 10.064302ms
I0222 04:59:56.671262 21809 replica.cpp:678] Persisted action at 0
I0222 04:59:56.671821 21809 replica.cpp:657] Replica received learned notice for position 0
I0222 04:59:56.684200 21809 leveldb.cpp:342] Persisting action (16 bytes) to leveldb took 12.346897ms
I0222 04:59:56.684242 21809 replica.cpp:678] Persisted action at 0
I0222 04:59:56.684262 21809 replica.cpp:663] Replica learned 1 action at position 0
I0222 04:59:56.684875 21809 log.cpp:675] Writer started with ending position 0
I0222 04:59:56.685932 21809 leveldb.cpp:437] Reading position from leveldb took 27308ns
I0222 04:59:56.688256 21809 registrar.cpp:345] Successfully fetched the registry (0B) in 74.992128ms
I0222 04:59:56.688344 21809 registrar.cpp:444] Applied 1 operations in 19566ns; attempting to update the 'registry'
I0222 04:59:56.690690 21809 log.cpp:683] Attempting to append 129 bytes to the log
I0222 04:59:56.690848 21809 coordinator.cpp:339] Coordinator attempting to write 2 action at position 1
I0222 04:59:56.691661 21809 replica.cpp:510] Replica received write request for position 1
I0222 04:59:56.701247 21809 leveldb.cpp:342] Persisting action (148 bytes) to leveldb took 9.550768ms
I0222 04:59:56.701292 21809 replica.cpp:678] Persisted action at 1
I0222 04:59:56.702066 21809 replica.cpp:657] Replica received learned notice for position 1
I0222 04:59:56.712136 21809 leveldb.cpp:342] Persisting action (150 bytes) to leveldb took 10.041696ms
I0222 04:59:56.712175 21809 replica.cpp:678] Persisted action at 1
I0222 04:59:56.712198 21809 replica.cpp:663] Replica learned 2 action at position 1
I0222 04:59:56.713289 21809 registrar.cpp:489] Successfully updated the 'registry' in 24.890112ms
I0222 04:59:56.713397 21809 registrar.cpp:375] Successfully recovered registrar
I0222 04:59:56.713537 21809 log.cpp:702] Attempting to truncate the log to 1
I0222 04:59:56.713795 21809 master.cpp:1212] Recovered 0 slaves from the Registry (93B) ; allowing 10mins for slaves to re-register
I0222 04:59:56.713871 21809 coordinator.cpp:339] Coordinator attempting to write 3 action at position 2
I0222 04:59:56.714879 21809 replica.cpp:510] Replica received write request for position 2
I0222 04:59:56.725225 21809 leveldb.cpp:342] Persisting action (16 bytes) to leveldb took 10.311704ms
I0222 04:59:56.725270 21809 replica.cpp:678] Persisted action at 2
I0222 04:59:56.726066 21809 replica.cpp:657] Replica received learned notice for position 2
I0222 04:59:56.734110 21809 leveldb.cpp:342] Persisting action (18 bytes) to leveldb took 8.012327ms
I0222 04:59:56.734180 21809 leveldb.cpp:400] Deleting ~1 keys from leveldb took 36578ns
I0222 04:59:56.734201 21809 replica.cpp:678] Persisted action at 2
I0222 04:59:56.734221 21809 replica.cpp:663] Replica learned 3 action at position 2
I0222 04:59:56.747556 21809 slave.cpp:173] Slave started on 53)@192.168.122.68:39461
I0222 04:59:56.747601 21809 credentials.hpp:84] Loading credential for authentication from '/tmp/SlaveTest_TaskLaunchContainerizerUpdateFails_qkhaJP/credential'
I0222 04:59:56.747774 21809 slave.cpp:280] Slave using credential for: test-principal
I0222 04:59:56.748021 21809 slave.cpp:298] Slave resources: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I0222 04:59:56.748682 21809 slave.cpp:327] Slave hostname: centos-7
I0222 04:59:56.748705 21809 slave.cpp:328] Slave checkpoint: false
W0222 04:59:56.748714 21809 slave.cpp:330] Disabling checkpointing is deprecated and the --checkpoint flag will be removed in a future release. Please avoid using this flag
I0222 04:59:56.749826 21809 state.cpp:34] Recovering state from '/tmp/SlaveTest_TaskLaunchContainerizerUpdateFails_qkhaJP/meta'
I0222 04:59:56.750191 21809 status_update_manager.cpp:196] Recovering status update manager
I0222 04:59:56.750465 21809 slave.cpp:3775] Finished recovery
I0222 04:59:56.751260 21809 slave.cpp:623] New master detected at master@192.168.122.68:39461
I0222 04:59:56.751349 21809 slave.cpp:686] Authenticating with master master@192.168.122.68:39461
I0222 04:59:56.751369 21809 slave.cpp:691] Using default CRAM-MD5 authenticatee
I0222 04:59:56.751502 21809 slave.cpp:659] Detecting new master
I0222 04:59:56.751596 21809 status_update_manager.cpp:170] Pausing sending status updates
I0222 04:59:56.751668 21809 authenticatee.hpp:138] Creating new client SASL connection
I0222 04:59:56.752781 21809 master.cpp:3811] Authenticating slave(53)@192.168.122.68:39461
I0222 04:59:56.752820 21809 master.cpp:3822] Using default CRAM-MD5 authenticator
I0222 04:59:56.753124 21809 authenticator.hpp:169] Creating new server SASL connection
I0222 04:59:56.755609 21809 authenticatee.hpp:229] Received SASL authentication mechanisms: CRAM-MD5
I0222 04:59:56.755641 21809 authenticatee.hpp:255] Attempting to authenticate with mechanism 'CRAM-MD5'
I0222 04:59:56.755708 21809 authenticator.hpp:275] Received SASL authentication start
I0222 04:59:56.755751 21809 authenticator.hpp:397] Authentication requires more steps
I0222 04:59:56.755813 21809 authenticatee.hpp:275] Received SASL authentication step
I0222 04:59:56.755887 21809 authenticator.hpp:303] Received SASL authentication step
I0222 04:59:56.755920 21809 auxprop.cpp:98] Request to lookup properties for user: 'test-principal' realm: 'centos-7' server FQDN: 'centos-7' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0222 04:59:56.755934 21809 auxprop.cpp:170] Looking up auxiliary property '*userPassword'
I0222 04:59:56.756005 21809 auxprop.cpp:170] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0222 04:59:56.756036 21809 auxprop.cpp:98] Request to lookup properties for user: 'test-principal' realm: 'centos-7' server FQDN: 'centos-7' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0222 04:59:56.756047 21809 auxprop.cpp:120] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0222 04:59:56.756054 21809 auxprop.cpp:120] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0222 04:59:56.756068 21809 authenticator.hpp:389] Authentication success
I0222 04:59:56.756155 21809 authenticatee.hpp:315] Authentication success
I0222 04:59:56.756219 21809 master.cpp:3869] Successfully authenticated principal 'test-principal' at slave(53)@192.168.122.68:39461
I0222 04:59:56.756503 21809 slave.cpp:757] Successfully authenticated with master master@192.168.122.68:39461
I0222 04:59:56.756611 21809 slave.cpp:1089] Will retry registration in 11.221976ms if necessary
I0222 04:59:56.756876 21809 master.cpp:2936] Registering slave at slave(53)@192.168.122.68:39461 (centos-7) with id 20150222-045956-1148889280-39461-21791-S0
I0222 04:59:56.757323 21809 registrar.cpp:444] Applied 1 operations in 70787ns; attempting to update the 'registry'
I0222 04:59:56.759790 21809 log.cpp:683] Attempting to append 299 bytes to the log
I0222 04:59:56.760000 21809 coordinator.cpp:339] Coordinator attempting to write 2 action at position 3
I0222 04:59:56.760920 21809 replica.cpp:510] Replica received write request for position 3
I0222 04:59:56.762037 21791 sched.cpp:154] Version: 0.22.0
I0222 04:59:56.762763 21806 sched.cpp:251] New master detected at master@192.168.122.68:39461
I0222 04:59:56.762835 21806 sched.cpp:307] Authenticating with master master@192.168.122.68:39461
I0222 04:59:56.762856 21806 sched.cpp:314] Using default CRAM-MD5 authenticatee
I0222 04:59:56.763082 21806 authenticatee.hpp:138] Creating new client SASL connection
I0222 04:59:56.763753 21806 master.cpp:3811] Authenticating scheduler-d9c22c4e-8dec-42a6-a350-a98472642891@192.168.122.68:39461
I0222 04:59:56.763784 21806 master.cpp:3822] Using default CRAM-MD5 authenticator
I0222 04:59:56.764040 21806 authenticator.hpp:169] Creating new server SASL connection
I0222 04:59:56.764624 21806 authenticatee.hpp:229] Received SASL authentication mechanisms: CRAM-MD5
I0222 04:59:56.764653 21806 authenticatee.hpp:255] Attempting to authenticate with mechanism 'CRAM-MD5'
I0222 04:59:56.764719 21806 authenticator.hpp:275] Received SASL authentication start
I0222 04:59:56.764758 21806 authenticator.hpp:397] Authentication requires more steps
I0222 04:59:56.764819 21806 authenticatee.hpp:275] Received SASL authentication step
I0222 04:59:56.764889 21806 authenticator.hpp:303] Received SASL authentication step
I0222 04:59:56.764911 21806 auxprop.cpp:98] Request to lookup properties for user: 'test-principal' realm: 'centos-7' server FQDN: 'centos-7' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0222 04:59:56.764922 21806 auxprop.cpp:170] Looking up auxiliary property '*userPassword'
I0222 04:59:56.764974 21806 auxprop.cpp:170] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0222 04:59:56.765005 21806 auxprop.cpp:98] Request to lookup properties for user: 'test-principal' realm: 'centos-7' server FQDN: 'centos-7' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0222 04:59:56.765017 21806 auxprop.cpp:120] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0222 04:59:56.765023 21806 auxprop.cpp:120] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0222 04:59:56.765036 21806 authenticator.hpp:389] Authentication success
I0222 04:59:56.765120 21806 authenticatee.hpp:315] Authentication success
I0222 04:59:56.765182 21806 master.cpp:3869] Successfully authenticated principal 'test-principal' at scheduler-d9c22c4e-8dec-42a6-a350-a98472642891@192.168.122.68:39461
I0222 04:59:56.765442 21806 sched.cpp:395] Successfully authenticated with master master@192.168.122.68:39461
I0222 04:59:56.765465 21806 sched.cpp:518] Sending registration request to master@192.168.122.68:39461
I0222 04:59:56.765522 21806 sched.cpp:551] Will retry registration in 1.283564292secs if necessary
I0222 04:59:56.765637 21806 master.cpp:1572] Received registration request for framework 'default' at scheduler-d9c22c4e-8dec-42a6-a350-a98472642891@192.168.122.68:39461
I0222 04:59:56.765699 21806 master.cpp:1433] Authorizing framework principal 'test-principal' to receive offers for role '*'
I0222 04:59:56.766120 21806 master.cpp:1636] Registering framework 20150222-045956-1148889280-39461-21791-0000 (default) at scheduler-d9c22c4e-8dec-42a6-a350-a98472642891@192.168.122.68:39461
I0222 04:59:56.766572 21806 hierarchical.hpp:320] Added framework 20150222-045956-1148889280-39461-21791-0000
I0222 04:59:56.766598 21806 hierarchical.hpp:831] No resources available to allocate!
I0222 04:59:56.766609 21806 hierarchical.hpp:738] Performed allocation for 0 slaves in 15902ns
I0222 04:59:56.766753 21806 sched.cpp:445] Framework registered with 20150222-045956-1148889280-39461-21791-0000
I0222 04:59:56.766790 21806 sched.cpp:459] Scheduler::registered took 15076ns
I0222 04:59:56.773710 21806 slave.cpp:1089] Will retry registration in 3.454005ms if necessary
I0222 04:59:56.773900 21806 master.cpp:2924] Ignoring register slave message from slave(53)@192.168.122.68:39461 (centos-7) as admission is already in progress
I0222 04:59:56.775297 21809 leveldb.cpp:342] Persisting action (318 bytes) to leveldb took 14.319807ms
I0222 04:59:56.775344 21809 replica.cpp:678] Persisted action at 3
I0222 04:59:56.776139 21809 replica.cpp:657] Replica received learned notice for position 3
I0222 04:59:56.778630 21806 slave.cpp:1089] Will retry registration in 32.764468ms if necessary
I0222 04:59:56.778779 21806 master.cpp:2924] Ignoring register slave message from slave(53)@192.168.122.68:39461 (centos-7) as admission is already in progress
I0222 04:59:56.783778 21809 leveldb.cpp:342] Persisting action (320 bytes) to leveldb took 7.609533ms
I0222 04:59:56.783828 21809 replica.cpp:678] Persisted action at 3
I0222 04:59:56.783849 21809 replica.cpp:663] Replica learned 2 action at position 3
I0222 04:59:56.785058 21809 registrar.cpp:489] Successfully updated the 'registry' in 27.669248ms
I0222 04:59:56.785274 21809 log.cpp:702] Attempting to truncate the log to 3
I0222 04:59:56.785815 21809 master.cpp:2993] Registered slave 20150222-045956-1148889280-39461-21791-S0 at slave(53)@192.168.122.68:39461 (centos-7) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I0222 04:59:56.785913 21809 coordinator.cpp:339] Coordinator attempting to write 3 action at position 4
I0222 04:59:56.786267 21809 hierarchical.hpp:452] Added slave 20150222-045956-1148889280-39461-21791-S0 (centos-7) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (and cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] available)
I0222 04:59:56.786600 21809 hierarchical.hpp:756] Performed allocation for slave 20150222-045956-1148889280-39461-21791-S0 in 292298ns
I0222 04:59:56.786684 21809 slave.cpp:791] Registered with master master@192.168.122.68:39461; given slave ID 20150222-045956-1148889280-39461-21791-S0
I0222 04:59:56.786792 21809 slave.cpp:2830] Received ping from slave-observer(52)@192.168.122.68:39461
I0222 04:59:56.787230 21809 master.cpp:3753] Sending 1 offers to framework 20150222-045956-1148889280-39461-21791-0000 (default) at scheduler-d9c22c4e-8dec-42a6-a350-a98472642891@192.168.122.68:39461
I0222 04:59:56.787334 21809 status_update_manager.cpp:177] Resuming sending status updates
I0222 04:59:56.788156 21809 sched.cpp:608] Scheduler::resourceOffers took 557128ns
I0222 04:59:56.788936 21809 master.cpp:2266] Processing ACCEPT call for offers: [ 20150222-045956-1148889280-39461-21791-O0 ] on slave 20150222-045956-1148889280-39461-21791-S0 at slave(53)@192.168.122.68:39461 (centos-7) for framework 20150222-045956-1148889280-39461-21791-0000 (default) at scheduler-d9c22c4e-8dec-42a6-a350-a98472642891@192.168.122.68:39461
I0222 04:59:56.789000 21809 master.cpp:2110] Authorizing framework principal 'test-principal' to launch task 0 as user 'jenkins'
W0222 04:59:56.790506 21809 validation.cpp:327] Executor default for task 0 uses less CPUs (None) than the minimum required (0.01). Please update your executor, as this will be mandatory in future releases.
W0222 04:59:56.790546 21809 validation.cpp:339] Executor default for task 0 uses less memory (None) than the minimum required (32MB). Please update your executor, as this will be mandatory in future releases.
I0222 04:59:56.790808 21809 master.hpp:821] Adding task 0 with resources cpus(*):1; mem(*):128 on slave 20150222-045956-1148889280-39461-21791-S0 (centos-7)
I0222 04:59:56.790885 21809 master.cpp:2543] Launching task 0 of framework 20150222-045956-1148889280-39461-21791-0000 (default) at scheduler-d9c22c4e-8dec-42a6-a350-a98472642891@192.168.122.68:39461 with resources cpus(*):1; mem(*):128 on slave 20150222-045956-1148889280-39461-21791-S0 at slave(53)@192.168.122.68:39461 (centos-7)
I0222 04:59:56.791201 21809 replica.cpp:510] Replica received write request for position 4
I0222 04:59:56.791610 21806 slave.cpp:1120] Got assigned task 0 for framework 20150222-045956-1148889280-39461-21791-0000
I0222 04:59:56.792140 21806 slave.cpp:1230] Launching task 0 for framework 20150222-045956-1148889280-39461-21791-0000
I0222 04:59:56.794872 21806 slave.cpp:4177] Launching executor default of framework 20150222-045956-1148889280-39461-21791-0000 in work directory '/tmp/SlaveTest_TaskLaunchContainerizerUpdateFails_qkhaJP/slaves/20150222-045956-1148889280-39461-21791-S0/frameworks/20150222-045956-1148889280-39461-21791-0000/executors/default/runs/753232b5-43ff-4fbf-b29a-0f76161132ab'
I0222 04:59:56.796846 21806 exec.cpp:130] Version: 0.22.0
I0222 04:59:56.797173 21806 slave.cpp:1377] Queuing task '0' for executor default of framework '20150222-045956-1148889280-39461-21791-0000
I0222 04:59:56.797355 21806 slave.cpp:3132] Monitoring executor 'default' of framework '20150222-045956-1148889280-39461-21791-0000' in container '753232b5-43ff-4fbf-b29a-0f76161132ab'
I0222 04:59:56.797570 21806 hierarchical.hpp:645] Recovered cpus(*):1; mem(*):896; disk(*):1024; ports(*):[31000-32000] (total allocatable: cpus(*):1; mem(*):896; disk(*):1024; ports(*):[31000-32000]) on slave 20150222-045956-1148889280-39461-21791-S0 from framework 20150222-045956-1148889280-39461-21791-0000
I0222 04:59:56.797613 21806 hierarchical.hpp:681] Framework 20150222-045956-1148889280-39461-21791-0000 filtered slave 20150222-045956-1148889280-39461-21791-S0 for 5secs
I0222 04:59:56.797796 21806 exec.cpp:180] Executor started at: executor(24)@192.168.122.68:39461 with pid 21791
I0222 04:59:56.798068 21806 slave.cpp:576] Successfully attached file '/tmp/SlaveTest_TaskLaunchContainerizerUpdateFails_qkhaJP/slaves/20150222-045956-1148889280-39461-21791-S0/frameworks/20150222-045956-1148889280-39461-21791-0000/executors/default/runs/753232b5-43ff-4fbf-b29a-0f76161132ab'
I0222 04:59:56.798136 21806 slave.cpp:2140] Got registration for executor 'default' of framework 20150222-045956-1148889280-39461-21791-0000 from executor(24)@192.168.122.68:39461
E0222 04:59:56.798573 21806 slave.cpp:1445] Failed to update resources for container 753232b5-43ff-4fbf-b29a-0f76161132ab of executor 'default' of framework 20150222-045956-1148889280-39461-21791-0000, destroying container: update() failed
I0222 04:59:56.798811 21806 slave.cpp:3190] Executor 'default' of framework 20150222-045956-1148889280-39461-21791-0000 exited with status 0
I0222 04:59:56.800436 21806 slave.cpp:2507] Handling status update TASK_LOST (UUID: 45243922-bcad-4e11-9a9f-db9213111a2a) for task 0 of framework 20150222-045956-1148889280-39461-21791-0000 from @0.0.0.0:0
I0222 04:59:56.800520 21806 slave.cpp:4485] Terminating task 0
I0222 04:59:56.801142 21806 master.cpp:3386] Executor default of framework 20150222-045956-1148889280-39461-21791-0000 on slave 20150222-045956-1148889280-39461-21791-S0 at slave(53)@192.168.122.68:39461 (centos-7) exited with status 0
I0222 04:59:56.801211 21806 master.cpp:4712] Removing executor 'default' with resources  of framework 20150222-045956-1148889280-39461-21791-0000 on slave 20150222-045956-1148889280-39461-21791-S0 at slave(53)@192.168.122.68:39461 (centos-7)
I0222 04:59:56.801378 21806 status_update_manager.cpp:316] Received status update TASK_LOST (UUID: 45243922-bcad-4e11-9a9f-db9213111a2a) for task 0 of framework 20150222-045956-1148889280-39461-21791-0000
I0222 04:59:56.801412 21806 status_update_manager.cpp:493] Creating StatusUpdate stream for task 0 of framework 20150222-045956-1148889280-39461-21791-0000
I0222 04:59:56.801574 21806 status_update_manager.cpp:370] Forwarding update TASK_LOST (UUID: 45243922-bcad-4e11-9a9f-db9213111a2a) for task 0 of framework 20150222-045956-1148889280-39461-21791-0000 to the slave
I0222 04:59:56.801831 21806 slave.cpp:2750] Forwarding the update TASK_LOST (UUID: 45243922-bcad-4e11-9a9f-db9213111a2a) for task 0 of framework 20150222-045956-1148889280-39461-21791-0000 to master@192.168.122.68:39461
I0222 04:59:56.802109 21805 master.cpp:3293] Status update TASK_LOST (UUID: 45243922-bcad-4e11-9a9f-db9213111a2a) for task 0 of framework 20150222-045956-1148889280-39461-21791-0000 from slave 20150222-045956-1148889280-39461-21791-S0 at slave(53)@192.168.122.68:39461 (centos-7)
I0222 04:59:56.802145 21805 master.cpp:3334] Forwarding status update TASK_LOST (UUID: 45243922-bcad-4e11-9a9f-db9213111a2a) for task 0 of framework 20150222-045956-1148889280-39461-21791-0000
I0222 04:59:56.802266 21805 master.cpp:4616] Updating the latest state of task 0 of framework 20150222-045956-1148889280-39461-21791-0000 to TASK_LOST
I0222 04:59:56.802685 21805 sched.cpp:714] Scheduler::statusUpdate took 40465ns
I0222 04:59:56.802821 21805 hierarchical.hpp:645] Recovered cpus(*):1; mem(*):128 (total allocatable: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]) on slave 20150222-045956-1148889280-39461-21791-S0 from framework 20150222-045956-1148889280-39461-21791-0000
I0222 04:59:56.803130 21805 master.cpp:4683] Removing task 0 with resources cpus(*):1; mem(*):128 of framework 20150222-045956-1148889280-39461-21791-0000 on slave 20150222-045956-1148889280-39461-21791-S0 at slave(53)@192.168.122.68:39461 (centos-7)
I0222 04:59:56.803268 21805 master.cpp:2780] Forwarding status update acknowledgement 45243922-bcad-4e11-9a9f-db9213111a2a for task 0 of framework 20150222-045956-1148889280-39461-21791-0000 (default) at scheduler-d9c22c4e-8dec-42a6-a350-a98472642891@192.168.122.68:39461 to slave 20150222-045956-1148889280-39461-21791-S0 at slave(53)@192.168.122.68:39461 (centos-7)
I0222 04:59:56.803473 21791 sched.cpp:1585] Asked to stop the driver
I0222 04:59:56.803547 21791 master.cpp:785] Master terminating
I0222 04:59:56.804844 21791 process.cpp:2117] Dropped / Lost event for PID: master@192.168.122.68:39461
I0222 04:59:56.804921 21791 process.cpp:2117] Dropped / Lost event for PID: master@192.168.122.68:39461
I0222 04:59:56.805624 21812 sched.cpp:828] Stopping framework '20150222-045956-1148889280-39461-21791-0000'
I0222 04:59:56.805675 21812 process.cpp:2117] Dropped / Lost event for PID: master@192.168.122.68:39461
I0222 04:59:56.807793 21812 process.cpp:2117] Dropped / Lost event for PID: log-coordinator(89)@192.168.122.68:39461
I0222 04:59:56.809552 21806 slave.cpp:2677] Status update manager successfully handled status update TASK_LOST (UUID: 45243922-bcad-4e11-9a9f-db9213111a2a) for task 0 of framework 20150222-045956-1148889280-39461-21791-0000
I0222 04:59:56.809736 21806 slave.cpp:2915] master@192.168.122.68:39461 exited
W0222 04:59:56.809759 21806 slave.cpp:2918] Master disconnected! Waiting for a new master to be elected
I0222 04:59:56.809788 21806 status_update_manager.cpp:388] Received status update acknowledgement (UUID: 45243922-bcad-4e11-9a9f-db9213111a2a) for task 0 of framework 20150222-045956-1148889280-39461-21791-0000
I0222 04:59:56.809855 21806 status_update_manager.cpp:524] Cleaning up status update stream for task 0 of framework 20150222-045956-1148889280-39461-21791-0000
I0222 04:59:56.810042 21806 slave.cpp:2080] Status update manager successfully handled status update acknowledgement (UUID: 45243922-bcad-4e11-9a9f-db9213111a2a) for task 0 of framework 20150222-045956-1148889280-39461-21791-0000
I0222 04:59:56.810088 21806 slave.cpp:4526] Completing task 0
I0222 04:59:56.810117 21806 slave.cpp:3299] Cleaning up executor 'default' of framework 20150222-045956-1148889280-39461-21791-0000
I0222 04:59:56.810361 21806 slave.cpp:3378] Cleaning up framework 20150222-045956-1148889280-39461-21791-0000
I0222 04:59:56.810509 21806 gc.cpp:55] Scheduling '/tmp/SlaveTest_TaskLaunchContainerizerUpdateFails_qkhaJP/slaves/20150222-045956-1148889280-39461-21791-S0/frameworks/20150222-045956-1148889280-39461-21791-0000/executors/default/runs/753232b5-43ff-4fbf-b29a-0f76161132ab' for gc 6.99999062248889days in the future
I0222 04:59:56.810673 21806 gc.cpp:55] Scheduling '/tmp/SlaveTest_TaskLaunchContainerizerUpdateFails_qkhaJP/slaves/20150222-045956-1148889280-39461-21791-S0/frameworks/20150222-045956-1148889280-39461-21791-0000/executors/default' for gc 6.99999062130963days in the future
I0222 04:59:56.810768 21806 gc.cpp:55] Scheduling '/tmp/SlaveTest_TaskLaunchContainerizerUpdateFails_qkhaJP/slaves/20150222-045956-1148889280-39461-21791-S0/frameworks/20150222-045956-1148889280-39461-21791-0000' for gc 6.9999906199763days in the future
I0222 04:59:56.810861 21806 status_update_manager.cpp:278] Closing status update streams for framework 20150222-045956-1148889280-39461-21791-0000
I0222 04:59:56.817010 21809 leveldb.cpp:342] Persisting action (16 bytes) to leveldb took 25.747365ms
I0222 04:59:56.817047 21809 replica.cpp:678] Persisted action at 4
I0222 04:59:56.817087 21809 process.cpp:2117] Dropped / Lost event for PID: (1371)@192.168.122.68:39461
I0222 04:59:56.817679 21791 slave.cpp:505] Slave terminating
I0222 04:59:56.818411 21791 process.cpp:2117] Dropped / Lost event for PID: slave(53)@192.168.122.68:39461
I0222 04:59:56.818869 21791 process.cpp:2117] Dropped / Lost event for PID: scheduler-d9c22c4e-8dec-42a6-a350-a98472642891@192.168.122.68:39461
tests/slave_tests.cpp:1183: Failure
Actual function call count doesn't match EXPECT_CALL(exec, registered(_, _, _, _))...
         Expected: to be called once
           Actual: never called - unsatisfied and active
[  FAILED  ] SlaveTest.TaskLaunchContainerizerUpdateFails (253 ms)

{code}",Bug,Major,vinodkone,2015-02-23T21:42:45.000+0000,5,Resolved,Complete,SlaveTest.TaskLaunchContainerizerUpdateFails is flaky,2015-04-03T22:53:20.000+0000,MESOS-2387,1.0,mesos,Twitter Mesos Q1 Sprint 3
darroyo,2015-02-23T10:51:10.000+0000,lloesche,"The problem exists in
 1194:src/Makefile.am
 47:src/tests/balloon_framework_test.sh

The current ""find | xargs rm -rf"" in the Makefile could potentially destroy data if mesos source was in a folder with a space in the name. E.g. if you for some reason checkout mesos to ""/ mesos"" the command in src/Makefile.am would turn into a rm -rf /

""find | xargs"" should be NUL delimited with ""find -print0 | xargs -0"" for safer execution or can just be replaced with the find build-in option ""find -exec '{}' \+"" which behaves similar to xargs.

There was a second occurrence of this in a test script, though in that case it would only rmdir empty folders, so is less critical.

I submitted a PR here: https://github.com/apache/mesos/pull/36
",Bug,Major,lloesche,,10020,Accepted,In Progress,"replace unsafe ""find | xargs"" with ""find -exec""",2015-08-31T14:34:24.000+0000,MESOS-2382,1.0,mesos,
mcypark,2015-02-18T23:14:00.000+0000,mcypark,"Currently the {{DRFSorter}} aggregates total and allocated resources across multiple slaves, which only works for scalar resources. We need to distinguish resources from different slaves.

Suppose we have 2 slaves and 1 framework. The framework is allocated all resources from both slaves.

{code}
Resources slaveResources =
  Resources::parse(""cpus:2;mem:512;ports:[31000-32000]"").get();

DRFSorter sorter;

sorter.add(slaveResources);  // Add slave1 resources
sorter.add(slaveResources);  // Add slave2 resources

// Total resources in sorter at this point is
// cpus(*):4; mem(*):1024; ports(*):[31000-32000].
// The scalar resources get aggregated correctly but ports do not.

sorter.add(""F"");

// The 2 calls to allocated only works because we simply do:
//   allocation[name] += resources;
// without checking that the 'resources' is available in the total.

sorter.allocated(""F"", slaveResources);
sorter.allocated(""F"", slaveResources);

// At this point, sorter.allocation(""F"") is:
// cpus(*):4; mem(*):1024; ports(*):[31000-32000].
{code}

To provide some context, this issue came up while trying to reserve all unreserved resources from every offer.

{code}
for (const Offer& offer : offers) { 
  Resources unreserved = offer.resources().unreserved();
  Resources reserved = unreserved.flatten(role, Resource::FRAMEWORK); 

  Offer::Operation reserve;
  reserve.set_type(Offer::Operation::RESERVE); 
  reserve.mutable_reserve()->mutable_resources()->CopyFrom(reserved); 
 
  driver->acceptOffers({offer.id()}, {reserve}); 
} 
{code}

Suppose the slave resources are the same as above:

{quote}
Slave1: {{cpus(\*):2; mem(\*):512; ports(\*):\[31000-32000\]}}
Slave2: {{cpus(\*):2; mem(\*):512; ports(\*):\[31000-32000\]}}
{quote}

Initial (incorrect) total resources in the DRFSorter is:

{quote}
{{cpus(\*):4; mem(\*):1024; ports(\*):\[31000-32000\]}}
{quote}

We receive 2 offers, 1 from each slave:

{quote}
Offer1: {{cpus(\*):2; mem(\*):512; ports(\*):\[31000-32000\]}}
Offer2: {{cpus(\*):2; mem(\*):512; ports(\*):\[31000-32000\]}}
{quote}

At this point, the resources allocated for the framework is:

{quote}
{{cpus(\*):4; mem(\*):1024; ports(\*):\[31000-32000\]}}
{quote}

After first {{RESERVE}} operation with Offer1:

The allocated resources for the framework becomes:

{quote}
{{cpus(\*):2; mem(\*):512; cpus(role):2; mem(role):512; ports(role):\[31000-32000\]}}
{quote}

During second {{RESERVE}} operation with Offer2:

{code:title=HierarchicalAllocatorProcess::updateAllocation}
  // ...

  FrameworkSorter* frameworkSorter =
    frameworkSorters[frameworks\[frameworkId\].role];

  Resources allocation = frameworkSorter->allocation(frameworkId.value());

  // Update the allocated resources.
  Try<Resources> updatedAllocation = allocation.apply(operations);
  CHECK_SOME(updatedAllocation);

  // ...
{code}

{{allocation}} in the above code is:

{quote}
{{cpus(\*):2; mem(\*):512; cpus(role):2; mem(role):512; ports(role):\[31000-32000\]}}
{quote}

We try to {{apply}} a {{RESERVE}} operation and we fail to find {{ports(\*):\[31000-32000\]}} which leads to the {{CHECK}} fail at {{CHECK_SOME(updatedAllocation);}}",Bug,Major,mcypark,2015-05-13T23:03:56.000+0000,5,Resolved,Complete,DRFSorter needs to distinguish resources from different slaves.,2015-06-18T21:43:37.000+0000,MESOS-2373,2.0,mesos,Mesosphere Q1 Sprint 3 - 2/20
greggomann,2015-02-18T20:34:31.000+0000,vinodkone,"While our current unit/integration test suite catches functional bugs, it doesn't catch compatibility bugs (e.g, MESOS-2371). This is really crucial to provide operators the ability to do seamless upgrades on live clusters.

We should have a test suite / framework (ideally running on CI vetting each review on RB) that tests upgrade paths between master, slave, scheduler and executor.",Improvement,Major,vinodkone,2016-03-21T23:01:42.000+0000,5,Resolved,Complete,Test script for verifying compatibility between Mesos components,2016-03-21T23:01:42.000+0000,MESOS-2372,2.0,mesos,Mesosphere Q1 Sprint 5 - 3/20
jieyu,2015-02-18T02:35:38.000+0000,yasumoto,"Right now there's a case where a misbehaving executor can cause a slave process to flap:

{panel:title=Quote From [~jieyu]}
{quote}
1) User tries to kill an instance
2) Slave sends {{KillTaskMessage}} to executor
3) Executor sends kill signals to task processes
4) Executor sends {{TASK_KILLED}} to slave
5) Slave updates container cpu limit to be 0.01 cpus
6) A user-process is still processing the kill signal
7) the task process cannot exit since it has too little cpu share and is throttled
8) Executor itself terminates
9) Slave tries to destroy the container, but cannot because the user-process is stuck in the exit path.
10) Slave restarts, and is constantly flapping because it cannot kill orphan containers
{quote}
{panel}

The slave's orphan container handling should be improved to deal with this case despite ill-behaved users (framework writers).",Bug,Critical,yasumoto,2015-04-24T23:38:08.000+0000,5,Resolved,Complete,Improve slave resiliency in the face of orphan containers ,2015-06-20T00:05:22.000+0000,MESOS-2367,5.0,mesos,Twitter Mesos Q1 Sprint 5
dhamon,2015-02-18T02:12:25.000+0000,nnielsen,"https://builds.apache.org/job/Mesos-Trunk-Ubuntu-Build-Out-Of-Src-Disable-Java-Disable-Python-Disable-Webui/2746/changes

{code}
[ RUN      ] MasterSlaveReconciliationTest.ReconcileLostTask
Using temporary directory '/tmp/MasterSlaveReconciliationTest_ReconcileLostTask_Rgb8FF'
I0218 01:53:26.881561 13918 leveldb.cpp:175] Opened db in 2.891605ms
I0218 01:53:26.882547 13918 leveldb.cpp:182] Compacted db in 953447ns
I0218 01:53:26.882596 13918 leveldb.cpp:197] Created db iterator in 20629ns
I0218 01:53:26.882616 13918 leveldb.cpp:203] Seeked to beginning of db in 2370ns
I0218 01:53:26.882627 13918 leveldb.cpp:272] Iterated through 0 keys in the db in 348ns
I0218 01:53:26.882664 13918 replica.cpp:743] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned
I0218 01:53:26.883124 13947 recover.cpp:448] Starting replica recovery
I0218 01:53:26.883625 13941 recover.cpp:474] Replica is in 4 status
I0218 01:53:26.884744 13945 replica.cpp:640] Replica in 4 status received a broadcasted recover request
I0218 01:53:26.885118 13939 recover.cpp:194] Received a recover response from a replica in 4 status
I0218 01:53:26.885565 13933 recover.cpp:565] Updating replica status to 3
I0218 01:53:26.886548 13932 leveldb.cpp:305] Persisting metadata (8 bytes) to leveldb took 733223ns
I0218 01:53:26.886574 13932 replica.cpp:322] Persisted replica status to 3
I0218 01:53:26.886714 13943 master.cpp:347] Master 20150218-015326-3142697795-57268-13918 (pomona.apache.org) started on 67.195.81.187:57268
I0218 01:53:26.886760 13943 master.cpp:393] Master only allowing authenticated frameworks to register
I0218 01:53:26.886772 13943 master.cpp:398] Master only allowing authenticated slaves to register
I0218 01:53:26.886798 13943 credentials.hpp:36] Loading credentials for authentication from '/tmp/MasterSlaveReconciliationTest_ReconcileLostTask_Rgb8FF/credentials'
I0218 01:53:26.886826 13934 recover.cpp:474] Replica is in 3 status
I0218 01:53:26.887151 13943 master.cpp:440] Authorization enabled
I0218 01:53:26.887866 13944 replica.cpp:640] Replica in 3 status received a broadcasted recover request
I0218 01:53:26.887969 13942 whitelist_watcher.cpp:78] No whitelist given
I0218 01:53:26.888021 13940 hierarchical.hpp:286] Initialized hierarchical allocator process
I0218 01:53:26.888178 13934 recover.cpp:194] Received a recover response from a replica in 3 status
I0218 01:53:26.889114 13943 master.cpp:1354] The newly elected leader is master@67.195.81.187:57268 with id 20150218-015326-3142697795-57268-13918
I0218 01:53:27.064930 13948 process.cpp:2117] Dropped / Lost event for PID: hierarchical-allocator(183)@67.195.81.187:57268
I0218 01:53:27.911870 13943 master.cpp:1367] Elected as the leading master!
I0218 01:53:27.911911 13943 master.cpp:1185] Recovering from registrar
I0218 01:53:27.912106 13948 process.cpp:2117] Dropped / Lost event for PID: scheduler-93f78006-5b69-498b-b4e3-87cdf8062263@67.195.81.187:57268
I0218 01:53:27.912255 13932 registrar.cpp:312] Recovering registrar
I0218 01:53:27.912307 13948 process.cpp:2117] Dropped / Lost event for PID: hierarchical-allocator(179)@67.195.81.187:57268
I0218 01:53:27.912626 13940 hierarchical.hpp:831] No resources available to allocate!
I0218 01:53:27.912658 13940 hierarchical.hpp:738] Performed allocation for 0 slaves in 60316ns
I0218 01:53:27.912838 13947 recover.cpp:565] Updating replica status to 1
I0218 01:53:27.913966 13947 leveldb.cpp:305] Persisting metadata (8 bytes) to leveldb took 921045ns
I0218 01:53:27.913998 13947 replica.cpp:322] Persisted replica status to 1
I0218 01:53:27.914106 13932 recover.cpp:579] Successfully joined the Paxos group
I0218 01:53:27.914378 13932 recover.cpp:463] Recover process terminated
I0218 01:53:27.914916 13939 log.cpp:659] Attempting to start the writer
I0218 01:53:27.916374 13937 replica.cpp:476] Replica received implicit promise request with proposal 1
I0218 01:53:27.916941 13937 leveldb.cpp:305] Persisting metadata (8 bytes) to leveldb took 534122ns
I0218 01:53:27.916967 13937 replica.cpp:344] Persisted promised to 1
I0218 01:53:27.917795 13936 coordinator.cpp:229] Coordinator attemping to fill missing position
I0218 01:53:27.919147 13941 replica.cpp:377] Replica received explicit promise request for position 0 with proposal 2
I0218 01:53:27.919492 13941 leveldb.cpp:342] Persisting action (8 bytes) to leveldb took 306270ns
I0218 01:53:27.919517 13941 replica.cpp:678] Persisted action at 0
I0218 01:53:27.920755 13934 replica.cpp:510] Replica received write request for position 0
I0218 01:53:27.920819 13934 leveldb.cpp:437] Reading position from leveldb took 33747ns
I0218 01:53:27.921195 13934 leveldb.cpp:342] Persisting action (14 bytes) to leveldb took 340479ns
I0218 01:53:27.921221 13934 replica.cpp:678] Persisted action at 0
I0218 01:53:27.921916 13932 replica.cpp:657] Replica received learned notice for position 0
I0218 01:53:27.922339 13932 leveldb.cpp:342] Persisting action (16 bytes) to leveldb took 392653ns
I0218 01:53:27.922365 13932 replica.cpp:678] Persisted action at 0
I0218 01:53:27.922386 13932 replica.cpp:663] Replica learned 1 action at position 0
I0218 01:53:27.923009 13945 log.cpp:675] Writer started with ending position 0
I0218 01:53:27.924167 13937 leveldb.cpp:437] Reading position from leveldb took 29219ns
I0218 01:53:27.927683 13932 registrar.cpp:345] Successfully fetched the registry (0B) in 15.376128ms
I0218 01:53:27.927789 13932 registrar.cpp:444] Applied 1 operations in 23004ns; attempting to update the 'registry'
I0218 01:53:27.929957 13947 log.cpp:683] Attempting to append 139 bytes to the log
I0218 01:53:27.930058 13936 coordinator.cpp:339] Coordinator attempting to write 2 action at position 1
I0218 01:53:27.930637 13934 replica.cpp:510] Replica received write request for position 1
I0218 01:53:27.930954 13934 leveldb.cpp:342] Persisting action (158 bytes) to leveldb took 286664ns
I0218 01:53:27.930975 13934 replica.cpp:678] Persisted action at 1
I0218 01:53:27.931521 13942 replica.cpp:657] Replica received learned notice for position 1
I0218 01:53:27.931813 13942 leveldb.cpp:342] Persisting action (160 bytes) to leveldb took 267316ns
I0218 01:53:27.931833 13942 replica.cpp:678] Persisted action at 1
I0218 01:53:27.931849 13942 replica.cpp:663] Replica learned 2 action at position 1
I0218 01:53:27.932617 13935 registrar.cpp:489] Successfully updated the 'registry' in 4.722944ms
I0218 01:53:27.932726 13935 registrar.cpp:375] Successfully recovered registrar
I0218 01:53:27.932751 13940 log.cpp:702] Attempting to truncate the log to 1
I0218 01:53:27.932865 13944 coordinator.cpp:339] Coordinator attempting to write 3 action at position 2
I0218 01:53:27.932998 13939 master.cpp:1212] Recovered 0 slaves from the Registry (101B) ; allowing 10mins for slaves to re-register
I0218 01:53:27.933732 13936 replica.cpp:510] Replica received write request for position 2
I0218 01:53:27.934146 13936 leveldb.cpp:342] Persisting action (16 bytes) to leveldb took 386584ns
I0218 01:53:27.934167 13936 replica.cpp:678] Persisted action at 2
I0218 01:53:27.934708 13935 replica.cpp:657] Replica received learned notice for position 2
I0218 01:53:27.935081 13935 leveldb.cpp:342] Persisting action (18 bytes) to leveldb took 350891ns
I0218 01:53:27.935127 13935 leveldb.cpp:400] Deleting ~1 keys from leveldb took 24983ns
I0218 01:53:27.935140 13935 replica.cpp:678] Persisted action at 2
I0218 01:53:27.935158 13935 replica.cpp:663] Replica learned 3 action at position 2
I0218 01:53:27.947561 13918 containerizer.cpp:104] Using isolation: posix/cpu,posix/mem
I0218 01:53:27.948971 13941 slave.cpp:173] Slave started on 150)@67.195.81.187:57268
I0218 01:53:27.949003 13941 credentials.hpp:84] Loading credential for authentication from '/tmp/MasterSlaveReconciliationTest_ReconcileLostTask_5No5Rj/credential'
I0218 01:53:27.949167 13941 slave.cpp:280] Slave using credential for: test-principal
I0218 01:53:27.949465 13941 slave.cpp:298] Slave resources: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I0218 01:53:27.949556 13941 slave.cpp:327] Slave hostname: pomona.apache.org
I0218 01:53:27.949575 13941 slave.cpp:328] Slave checkpoint: false
W0218 01:53:27.949587 13941 slave.cpp:330] Disabling checkpointing is deprecated and the --checkpoint flag will be removed in a future release. Please avoid using this flag
I0218 01:53:27.950536 13932 state.cpp:34] Recovering state from '/tmp/MasterSlaveReconciliationTest_ReconcileLostTask_5No5Rj/meta'
I0218 01:53:27.950783 13940 status_update_manager.cpp:196] Recovering status update manager
I0218 01:53:27.953531 13944 containerizer.cpp:301] Recovering containerizer
I0218 01:53:27.953944 13918 sched.cpp:151] Version: 0.22.0
I0218 01:53:27.954617 13932 slave.cpp:3611] Finished recovery
I0218 01:53:27.954732 13935 sched.cpp:248] New master detected at master@67.195.81.187:57268
I0218 01:53:27.954833 13935 sched.cpp:304] Authenticating with master master@67.195.81.187:57268
I0218 01:53:27.954856 13935 sched.cpp:311] Using default CRAM-MD5 authenticatee
I0218 01:53:27.955037 13947 authenticatee.hpp:138] Creating new client SASL connection
I0218 01:53:27.955198 13944 status_update_manager.cpp:170] Pausing sending status updates
I0218 01:53:27.955195 13941 slave.cpp:623] New master detected at master@67.195.81.187:57268
I0218 01:53:27.955238 13934 master.cpp:3811] Authenticating scheduler-17aa8fa2-195f-43d6-85d7-87b949d4419b@67.195.81.187:57268
I0218 01:53:27.955270 13934 master.cpp:3822] Using default CRAM-MD5 authenticator
I0218 01:53:27.955317 13941 slave.cpp:686] Authenticating with master master@67.195.81.187:57268
I0218 01:53:27.955348 13941 slave.cpp:691] Using default CRAM-MD5 authenticatee
I0218 01:53:27.955518 13933 authenticator.hpp:169] Creating new server SASL connection
I0218 01:53:27.955534 13939 authenticatee.hpp:138] Creating new client SASL connection
I0218 01:53:27.955693 13935 authenticatee.hpp:229] Received SASL authentication mechanisms: CRAM-MD5
I0218 01:53:27.955732 13935 authenticatee.hpp:255] Attempting to authenticate with mechanism 'CRAM-MD5'
I0218 01:53:27.955844 13932 authenticator.hpp:275] Received SASL authentication start
I0218 01:53:27.955905 13932 authenticator.hpp:397] Authentication requires more steps
I0218 01:53:27.955999 13935 authenticatee.hpp:275] Received SASL authentication step
I0218 01:53:27.956120 13932 authenticator.hpp:303] Received SASL authentication step
I0218 01:53:27.957321 13941 slave.cpp:659] Detecting new master
I0218 01:53:27.957473 13934 master.cpp:3811] Authenticating slave(150)@67.195.81.187:57268
I0218 01:53:28.009866 13948 process.cpp:2117] Dropped / Lost event for PID: slave(146)@67.195.81.187:57268
I0218 01:53:28.592335 13932 auxprop.cpp:98] Request to lookup properties for user: 'test-principal' realm: 'pomona.apache.org' server FQDN: 'pomona.apache.org' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0218 01:53:28.592350 13934 master.cpp:3822] Using default CRAM-MD5 authenticator
I0218 01:53:28.592367 13932 auxprop.cpp:170] Looking up auxiliary property '*userPassword'
I0218 01:53:28.592434 13932 auxprop.cpp:170] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0218 01:53:28.592483 13932 auxprop.cpp:98] Request to lookup properties for user: 'test-principal' realm: 'pomona.apache.org' server FQDN: 'pomona.apache.org' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0218 01:53:28.592501 13932 auxprop.cpp:120] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0218 01:53:28.592510 13932 auxprop.cpp:120] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0218 01:53:28.592530 13932 authenticator.hpp:389] Authentication success
I0218 01:53:28.592646 13935 authenticatee.hpp:315] Authentication success
I0218 01:53:28.592686 13948 process.cpp:2117] Dropped / Lost event for PID: scheduler-4eee5e93-d6bb-4af4-9795-0aec0916dfa5@67.195.81.187:57268
I0218 01:53:28.592800 13939 authenticator.hpp:169] Creating new server SASL connection
I0218 01:53:28.592836 13948 process.cpp:2117] Dropped / Lost event for PID: hierarchical-allocator(180)@67.195.81.187:57268
I0218 01:53:28.592864 13934 master.cpp:3869] Successfully authenticated principal 'test-principal' at scheduler-17aa8fa2-195f-43d6-85d7-87b949d4419b@67.195.81.187:57268
I0218 01:53:28.592990 13933 authenticatee.hpp:229] Received SASL authentication mechanisms: CRAM-MD5
I0218 01:53:28.593029 13933 authenticatee.hpp:255] Attempting to authenticate with mechanism 'CRAM-MD5'
I0218 01:53:28.593245 13933 authenticator.hpp:275] Received SASL authentication start
I0218 01:53:28.593364 13933 authenticator.hpp:397] Authentication requires more steps
I0218 01:53:28.593490 13941 sched.cpp:392] Successfully authenticated with master master@67.195.81.187:57268
I0218 01:53:28.593519 13941 sched.cpp:515] Sending registration request to master@67.195.81.187:57268
I0218 01:53:28.593531 13945 authenticatee.hpp:275] Received SASL authentication step
I0218 01:53:28.593606 13941 sched.cpp:548] Will retry registration in 1.707160316secs if necessary
I0218 01:53:28.593720 13933 authenticator.hpp:303] Received SASL authentication step
I0218 01:53:28.593731 13939 master.cpp:1572] Received registration request for framework 'default' at scheduler-17aa8fa2-195f-43d6-85d7-87b949d4419b@67.195.81.187:57268
I0218 01:53:28.593757 13933 auxprop.cpp:98] Request to lookup properties for user: 'test-principal' realm: 'pomona.apache.org' server FQDN: 'pomona.apache.org' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0218 01:53:28.593780 13933 auxprop.cpp:170] Looking up auxiliary property '*userPassword'
I0218 01:53:28.593818 13939 master.cpp:1433] Authorizing framework principal 'test-principal' to receive offers for role '*'
I0218 01:53:28.593823 13933 auxprop.cpp:170] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0218 01:53:28.593891 13933 auxprop.cpp:98] Request to lookup properties for user: 'test-principal' realm: 'pomona.apache.org' server FQDN: 'pomona.apache.org' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0218 01:53:28.593909 13933 auxprop.cpp:120] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0218 01:53:28.593919 13933 auxprop.cpp:120] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0218 01:53:28.593947 13933 authenticator.hpp:389] Authentication success
I0218 01:53:28.594048 13945 authenticatee.hpp:315] Authentication success
I0218 01:53:28.594140 13946 master.cpp:3869] Successfully authenticated principal 'test-principal' at slave(150)@67.195.81.187:57268
I0218 01:53:28.594383 13947 slave.cpp:757] Successfully authenticated with master master@67.195.81.187:57268
I0218 01:53:28.594571 13947 slave.cpp:1089] Will retry registration in 17.484321ms if necessary
I0218 01:53:28.594606 13946 master.cpp:1636] Registering framework 20150218-015326-3142697795-57268-13918-0000 (default) at scheduler-17aa8fa2-195f-43d6-85d7-87b949d4419b@67.195.81.187:57268
I0218 01:53:28.594995 13944 hierarchical.hpp:320] Added framework 20150218-015326-3142697795-57268-13918-0000
I0218 01:53:28.595034 13944 hierarchical.hpp:831] No resources available to allocate!
I0218 01:53:28.595057 13944 hierarchical.hpp:738] Performed allocation for 0 slaves in 35451ns
I0218 01:53:28.595185 13937 sched.cpp:442] Framework registered with 20150218-015326-3142697795-57268-13918-0000
I0218 01:53:28.595232 13937 sched.cpp:456] Scheduler::registered took 22922ns
I0218 01:53:28.595273 13946 master.cpp:2936] Registering slave at slave(150)@67.195.81.187:57268 (pomona.apache.org) with id 20150218-015326-3142697795-57268-13918-S0
I0218 01:53:28.595803 13934 registrar.cpp:444] Applied 1 operations in 74798ns; attempting to update the 'registry'
I0218 01:53:28.598387 13939 log.cpp:683] Attempting to append 316 bytes to the log
I0218 01:53:28.598578 13938 coordinator.cpp:339] Coordinator attempting to write 2 action at position 3
I0218 01:53:28.599488 13932 replica.cpp:510] Replica received write request for position 3
I0218 01:53:28.599758 13932 leveldb.cpp:342] Persisting action (335 bytes) to leveldb took 234907ns
I0218 01:53:28.599786 13932 replica.cpp:678] Persisted action at 3
I0218 01:53:28.600777 13939 replica.cpp:657] Replica received learned notice for position 3
I0218 01:53:28.601304 13939 leveldb.cpp:342] Persisting action (337 bytes) to leveldb took 503852ns
I0218 01:53:28.601326 13939 replica.cpp:678] Persisted action at 3
I0218 01:53:28.601346 13939 replica.cpp:663] Replica learned 2 action at position 3
I0218 01:53:28.602901 13934 log.cpp:702] Attempting to truncate the log to 3
I0218 01:53:28.603011 13938 coordinator.cpp:339] Coordinator attempting to write 3 action at position 4
I0218 01:53:28.603135 13932 registrar.cpp:489] Successfully updated the 'registry' in 7.035904ms
I0218 01:53:28.603687 13932 replica.cpp:510] Replica received write request for position 4
I0218 01:53:28.603844 13934 slave.cpp:2666] Received ping from slave-observer(147)@67.195.81.187:57268
I0218 01:53:28.603945 13941 master.cpp:2993] Registered slave 20150218-015326-3142697795-57268-13918-S0 at slave(150)@67.195.81.187:57268 (pomona.apache.org) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I0218 01:53:28.604046 13933 hierarchical.hpp:452] Added slave 20150218-015326-3142697795-57268-13918-S0 (pomona.apache.org) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (and cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] available)
I0218 01:53:28.604112 13932 leveldb.cpp:342] Persisting action (16 bytes) to leveldb took 399822ns
I0218 01:53:28.604131 13932 replica.cpp:678] Persisted action at 4
I0218 01:53:28.605741 13933 hierarchical.hpp:756] Performed allocation for slave 20150218-015326-3142697795-57268-13918-S0 in 1.649293ms
I0218 01:53:28.605836 13934 slave.cpp:791] Registered with master master@67.195.81.187:57268; given slave ID 20150218-015326-3142697795-57268-13918-S0
I0218 01:53:28.606003 13933 replica.cpp:657] Replica received learned notice for position 4
I0218 01:53:28.606037 13947 status_update_manager.cpp:177] Resuming sending status updates
I0218 01:53:28.606075 13937 master.cpp:3753] Sending 1 offers to framework 20150218-015326-3142697795-57268-13918-0000 (default) at scheduler-17aa8fa2-195f-43d6-85d7-87b949d4419b@67.195.81.187:57268
I0218 01:53:28.606547 13933 leveldb.cpp:342] Persisting action (18 bytes) to leveldb took 517378ns
I0218 01:53:29.008322 13933 leveldb.cpp:400] Deleting ~2 keys from leveldb took 86406ns
I0218 01:53:29.008350 13933 replica.cpp:678] Persisted action at 4
I0218 01:53:29.008380 13933 replica.cpp:663] Replica learned 3 action at position 4
I0218 01:53:28.912961 13946 hierarchical.hpp:831] No resources available to allocate!
I0218 01:53:29.008543 13946 hierarchical.hpp:738] Performed allocation for 1 slaves in 95.683965ms
I0218 01:53:29.008621 13944 sched.cpp:605] Scheduler::resourceOffers took 74896ns
I0218 01:53:29.009996 13932 master.cpp:2266] Processing ACCEPT call for offers: [ 20150218-015326-3142697795-57268-13918-O0 ] on slave 20150218-015326-3142697795-57268-13918-S0 at slave(150)@67.195.81.187:57268 (pomona.apache.org) for framework 20150218-015326-3142697795-57268-13918-0000 (default) at scheduler-17aa8fa2-195f-43d6-85d7-87b949d4419b@67.195.81.187:57268
I0218 01:53:29.010035 13932 master.cpp:2110] Authorizing framework principal 'test-principal' to launch task 1 as user 'jenkins'
W0218 01:53:29.011081 13932 validation.cpp:326] Executor default for task 1 uses less CPUs (None) than the minimum required (0.01). Please update your executor, as this will be mandatory in future releases.
W0218 01:53:29.011111 13932 validation.cpp:338] Executor default for task 1 uses less memory (None) than the minimum required (32MB). Please update your executor, as this will be mandatory in future releases.
I0218 01:53:29.011418 13932 master.hpp:821] Adding task 1 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave 20150218-015326-3142697795-57268-13918-S0 (pomona.apache.org)
I0218 01:53:29.011518 13932 master.cpp:2543] Launching task 1 of framework 20150218-015326-3142697795-57268-13918-0000 (default) at scheduler-17aa8fa2-195f-43d6-85d7-87b949d4419b@67.195.81.187:57268 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave 20150218-015326-3142697795-57268-13918-S0 at slave(150)@67.195.81.187:57268 (pomona.apache.org)
I0218 01:53:29.014303 13944 slave.cpp:623] New master detected at master@67.195.81.187:57268
I0218 01:53:29.014310 13935 status_update_manager.cpp:170] Pausing sending status updates
I0218 01:53:29.014451 13944 slave.cpp:686] Authenticating with master master@67.195.81.187:57268
I0218 01:53:29.014467 13944 slave.cpp:691] Using default CRAM-MD5 authenticatee
I0218 01:53:29.014600 13938 authenticatee.hpp:138] Creating new client SASL connection
I0218 01:53:29.014634 13944 slave.cpp:659] Detecting new master
I0218 01:53:29.014930 13944 master.cpp:3811] Authenticating slave(150)@67.195.81.187:57268
I0218 01:53:29.014953 13944 master.cpp:3822] Using default CRAM-MD5 authenticator
I0218 01:53:29.015183 13932 authenticator.hpp:169] Creating new server SASL connection
I0218 01:53:29.015403 13932 authenticatee.hpp:229] Received SASL authentication mechanisms: CRAM-MD5
I0218 01:53:29.015426 13932 authenticatee.hpp:255] Attempting to authenticate with mechanism 'CRAM-MD5'
I0218 01:53:29.015540 13941 authenticator.hpp:275] Received SASL authentication start
I0218 01:53:29.015609 13941 authenticator.hpp:397] Authentication requires more steps
I0218 01:53:29.015678 13941 authenticatee.hpp:275] Received SASL authentication step
I0218 01:53:29.015746 13941 authenticator.hpp:303] Received SASL authentication step
I0218 01:53:29.015771 13941 auxprop.cpp:98] Request to lookup properties for user: 'test-principal' realm: 'pomona.apache.org' server FQDN: 'pomona.apache.org' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0218 01:53:29.015785 13941 auxprop.cpp:170] Looking up auxiliary property '*userPassword'
I0218 01:53:29.015820 13941 auxprop.cpp:170] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0218 01:53:29.015843 13941 auxprop.cpp:98] Request to lookup properties for user: 'test-principal' realm: 'pomona.apache.org' server FQDN: 'pomona.apache.org' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0218 01:53:29.015908 13941 auxprop.cpp:120] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0218 01:53:29.015915 13941 auxprop.cpp:120] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0218 01:53:29.015928 13941 authenticator.hpp:389] Authentication success
I0218 01:53:29.016016 13941 authenticatee.hpp:315] Authentication success
I0218 01:53:29.016041 13932 master.cpp:3869] Successfully authenticated principal 'test-principal' at slave(150)@67.195.81.187:57268
I0218 01:53:29.016445 13938 slave.cpp:757] Successfully authenticated with master master@67.195.81.187:57268
I0218 01:53:29.016556 13938 slave.cpp:1089] Will retry registration in 3.449224ms if necessary
I0218 01:53:29.016757 13932 master.cpp:3067] Re-registering slave 20150218-015326-3142697795-57268-13918-S0 at slave(150)@67.195.81.187:57268 (pomona.apache.org)
W0218 01:53:29.016921 13932 master.cpp:3932] Task 1 of framework 20150218-015326-3142697795-57268-13918-0000 unknown to the slave 20150218-015326-3142697795-57268-13918-S0 at slave(150)@67.195.81.187:57268 (pomona.apache.org) during re-registration: reconciling with the slave
W0218 01:53:29.017177 13932 master.cpp:4013] Executor default of framework 20150218-015326-3142697795-57268-13918-0000 possibly unknown to the slave 20150218-015326-3142697795-57268-13918-S0 at slave(150)@67.195.81.187:57268 (pomona.apache.org)
I0218 01:53:29.017258 13932 master.cpp:4712] Removing executor 'default' with resources  of framework 20150218-015326-3142697795-57268-13918-0000 on slave 20150218-015326-3142697795-57268-13918-S0 at slave(150)@67.195.81.187:57268 (pomona.apache.org)
I0218 01:53:29.017482 13943 slave.cpp:859] Re-registered with master master@67.195.81.187:57268
I0218 01:53:29.017513 13932 master.cpp:3217] Sending updated checkpointed resources  to slave 20150218-015326-3142697795-57268-13918-S0 at slave(150)@67.195.81.187:57268 (pomona.apache.org)
I0218 01:53:29.017704 13944 status_update_manager.cpp:177] Resuming sending status updates
W0218 01:53:29.017705 13943 slave.cpp:917] Slave reconciling task 1 of framework 20150218-015326-3142697795-57268-13918-0000 in state TASK_LOST: task unknown to the slave
I0218 01:53:29.020056 13934 status_update_manager.cpp:316] Received status update TASK_LOST (UUID: 46c6fdee-5cda-4bf9-8dfd-4ea3b2e4bb05) for task 1 of framework 20150218-015326-3142697795-57268-13918-0000
I0218 01:53:29.020102 13934 status_update_manager.cpp:493] Creating StatusUpdate stream for task 1 of framework 20150218-015326-3142697795-57268-13918-0000
I0218 01:53:29.020321 13934 status_update_manager.cpp:370] Forwarding update TASK_LOST (UUID: 46c6fdee-5cda-4bf9-8dfd-4ea3b2e4bb05) for task 1 of framework 20150218-015326-3142697795-57268-13918-0000 to the slave
I0218 01:53:29.020339 13943 slave.cpp:1859] Updated checkpointed resources from  to 
I0218 01:53:29.020434 13943 slave.cpp:2586] Forwarding the update TASK_LOST (UUID: 46c6fdee-5cda-4bf9-8dfd-4ea3b2e4bb05) for task 1 of framework 20150218-015326-3142697795-57268-13918-0000 to master@67.195.81.187:57268
I0218 01:53:29.020776 13932 slave.cpp:2513] Status update manager successfully handled status update TASK_LOST (UUID: 46c6fdee-5cda-4bf9-8dfd-4ea3b2e4bb05) for task 1 of framework 20150218-015326-3142697795-57268-13918-0000
I0218 01:53:29.020819 13943 master.cpp:3293] Status update TASK_LOST (UUID: 46c6fdee-5cda-4bf9-8dfd-4ea3b2e4bb05) for task 1 of framework 20150218-015326-3142697795-57268-13918-0000 from slave 20150218-015326-3142697795-57268-13918-S0 at slave(150)@67.195.81.187:57268 (pomona.apache.org)
I0218 01:53:29.020858 13943 master.cpp:3334] Forwarding status update TASK_LOST (UUID: 46c6fdee-5cda-4bf9-8dfd-4ea3b2e4bb05) for task 1 of framework 20150218-015326-3142697795-57268-13918-0000
I0218 01:53:29.020985 13943 master.cpp:4616] Updating the latest state of task 1 of framework 20150218-015326-3142697795-57268-13918-0000 to TASK_LOST
I0218 01:53:29.021073 13937 sched.cpp:696] Scheduler::statusUpdate took 29989ns
I0218 01:53:29.023674 13940 process.cpp:2770] Handling HTTP event for process 'metrics' with path: '/metrics/snapshot'
I0218 01:53:29.068902 13948 process.cpp:2117] Dropped / Lost event for PID: slave(147)@67.195.81.187:57268
I0218 01:53:29.160341 13939 hierarchical.hpp:645] Recovered cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (total allocatable: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]) on slave 20150218-015326-3142697795-57268-13918-S0 from framework 20150218-015326-3142697795-57268-13918-0000
I0218 01:53:29.160444 13948 process.cpp:2117] Dropped / Lost event for PID: scheduler-7c9ec50c-5c07-4e08-b863-850694a30a74@67.195.81.187:57268
I0218 01:53:29.160960 13943 master.cpp:4683] Removing task 1 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] of framework 20150218-015326-3142697795-57268-13918-0000 on slave 20150218-015326-3142697795-57268-13918-S0 at slave(150)@67.195.81.187:57268 (pomona.apache.org)
I0218 01:53:29.161125 13943 master.cpp:2780] Forwarding status update acknowledgement 46c6fdee-5cda-4bf9-8dfd-4ea3b2e4bb05 for task 1 of framework 20150218-015326-3142697795-57268-13918-0000 (default) at scheduler-17aa8fa2-195f-43d6-85d7-87b949d4419b@67.195.81.187:57268 to slave 20150218-015326-3142697795-57268-13918-S0 at slave(150)@67.195.81.187:57268 (pomona.apache.org)
I0218 01:53:29.161495 13943 status_update_manager.cpp:388] Received status update acknowledgement (UUID: 46c6fdee-5cda-4bf9-8dfd-4ea3b2e4bb05) for task 1 of framework 20150218-015326-3142697795-57268-13918-0000
I0218 01:53:29.161571 13943 status_update_manager.cpp:524] Cleaning up status update stream for task 1 of framework 20150218-015326-3142697795-57268-13918-0000
I0218 01:53:29.161779 13943 slave.cpp:1930] Status update manager successfully handled status update acknowledgement (UUID: 46c6fdee-5cda-4bf9-8dfd-4ea3b2e4bb05) for task 1 of framework 20150218-015326-3142697795-57268-13918-0000
E0218 01:53:29.161803 13943 slave.cpp:1941] Status update acknowledgement (UUID: 46c6fdee-5cda-4bf9-8dfd-4ea3b2e4bb05) for task 1 of unknown framework 20150218-015326-3142697795-57268-13918-0000
../../src/tests/master_slave_reconciliation_tests.cpp:240: Failure
Value of: stats.values.count( ""master/task_lost/source_slave/reason_reconciliation"")
  Actual: 0
Expected: 1u
Which is: 1
../../src/tests/master_slave_reconciliation_tests.cpp:243: Failure
Value of: stats.values[""master/task_lost/source_slave/reason_reconciliation""]
  Actual: 16-byte object <00-00 00-00 74-65 73-74 B0-B1 25-05 00-00 00-00>
Expected: 1u
Which is: 1
I0218 01:53:29.212201 13918 sched.cpp:1470] Asked to stop the driver
I0218 01:53:29.212276 13918 master.cpp:785] Master terminating
I0218 01:53:29.212319 13937 sched.cpp:808] Stopping framework '20150218-015326-3142697795-57268-13918-0000'
I0218 01:53:29.212798 13942 slave.cpp:2751] master@67.195.81.187:57268 exited
W0218 01:53:29.212821 13942 slave.cpp:2754] Master disconnected! Waiting for a new master to be elected
I0218 01:53:29.213912 13918 process.cpp:2117] Dropped / Lost event for PID: master@67.195.81.187:57268
I0218 01:53:29.214001 13918 process.cpp:2117] Dropped / Lost event for PID: master@67.195.81.187:57268
I0218 01:53:29.216212 13934 slave.cpp:505] Slave terminating
I0218 01:53:29.217798 13918 process.cpp:2117] Dropped / Lost event for PID: scheduler-17aa8fa2-195f-43d6-85d7-87b949d4419b@67.195.81.187:57268
I0218 01:53:29.217994 13918 process.cpp:2117] Dropped / Lost event for PID: slave(150)@67.195.81.187:57268
[  FAILED  ] MasterSlaveReconciliationTest.ReconcileLostTask (2341 ms)
{code}",Task,Major,nnielsen,2015-02-24T03:13:03.000+0000,5,Resolved,Complete,MasterSlaveReconciliationTest.ReconcileLostTask is flaky,2015-02-24T03:13:03.000+0000,MESOS-2366,1.0,mesos,Twitter Mesos Q1 Sprint 3
mcypark,2015-02-14T01:20:40.000+0000,bmahler,"The master's state.json endpoint consistently takes a long time to compute the JSON result, for large clusters:

{noformat}
$ time curl -s -o /dev/null localhost:5050/master/state.json
Mon Jan 26 22:38:50 UTC 2015

real	0m13.174s
user	0m0.003s
sys	0m0.022s
{noformat}

This can cause the master to get backlogged if there are many state.json requests in flight.

Looking at {{perf}} data, it seems most of the time is spent doing memory allocation / de-allocation. This ticket will try to capture any low hanging fruit to speed this up. Possibly we can leverage moves if they are not already being used by the compiler.",Improvement,Major,bmahler,2016-01-15T18:16:22.000+0000,5,Resolved,Complete,Improve performance of the state.json endpoint for large clusters.,2016-02-27T00:23:23.000+0000,MESOS-2353,5.0,mesos,Twitter Mesos Q1 Sprint 5
idownes,2015-02-12T05:44:17.000+0000,idownes,"In preparation for the MesosContainerizer to support a filesystem isolator the MesosContainerizerLauncher must support chrooting. Optionally, it should also configure the chroot environment by (re-)mounting special filesystems such as /proc and /sys and making device nodes such as /dev/zero, etc., such that the chroot environment is functional.",Improvement,Major,idownes,2015-07-08T21:21:57.000+0000,5,Resolved,Complete,Add support for MesosContainerizerLaunch to chroot to a specified path,2015-07-08T21:21:57.000+0000,MESOS-2350,5.0,mesos,Twitter Mesos Q1 Sprint 3
idownes,2015-02-12T05:40:17.000+0000,idownes,"Include a separate binary that when provided with a container_id, path to an executable, and optional arguments will find the container context, enter it, and exec the executable.

e.g.,
{noformat}
mesos-container-exec --container_id=abc123 [--] /path/to/executable [arg1 ...]
{noformat}

This need only support (initially) containers created with the MesosContainerizer and will support all isolators shipped with Mesos, i.e., it should find and enter the cgroups and namespaces for the running executor of the specified container.",Improvement,Major,idownes,,10020,Accepted,In Progress,Provide a way to execute an arbitrary process in a MesosContainerizer container context,2015-08-12T16:30:37.000+0000,MESOS-2349,5.0,mesos,Twitter Mesos Q1 Sprint 3
bmahler,2015-02-12T00:34:25.000+0000,bmahler,"In order for schedulers to be able to handle status updates in a scalable manner, they need the ability to send acknowledgements through the driver. This enables optimizations in schedulers (e.g. process status updates asynchronously w/o backing up the driver, processing/acking updates in batch).

Without this, an implicit reconciliation can overload a scheduler (hence the motivation for MESOS-2308).",Improvement,Major,bmahler,2015-02-21T22:29:34.000+0000,5,Resolved,Complete,Add ability for schedulers to explicitly acknowledge status updates on the driver.,2015-04-06T18:47:56.000+0000,MESOS-2347,8.0,mesos,Twitter Mesos Q1 Sprint 3
marco-mesos,2015-02-10T21:57:42.000+0000,zmanji,"Currently to discover the master a client needs the ZK node location and access to the MasterInfo protobuf so it can deserialize the binary blob in the node.

I think it would be nice to publish JSON (like Twitter's ServerSets) so clients are not tied to protobuf to do service discovery.

This ticket is an intermediate (compatibility) step: we add in {{0.23}} the ability for the {{Detector}} to ""understand"" JSON **alongside** Protobuf serialized format; this makes it compatible with both earlier versions, as well a future one (most likely, {{0.24}}) that will write the {{MasterInfo}} information in JSON format.",Improvement,Major,zmanji,2015-06-19T18:39:44.000+0000,5,Resolved,Complete,Add ability to decode JSON serialized MasterInfo from ZK,2015-08-04T09:15:01.000+0000,MESOS-2340,5.0,mesos,Mesosphere Sprint 11
marco-mesos,2015-02-10T18:25:37.000+0000,karya,"When doing a {{make install}}, the src/python/native/src/mesos/__init__.py file is not getting installed in {{$PREFIX/lib/pythonX.Y/site-packages/mesos/}}.  

This makes it impossible to do the following import when {{PYTHONPATH}} is set to the {{site-packages}} directory.

{code}
import mesos.interface.mesos_pb2
{code}

The directories {{$PREFIX/lib/pythonX.Y/site-packages/mesos/interface, native}} do have their corresponding {{__init__.py}} files.

Reproducing the bug:
{code}
../configure --prefix=$HOME/test-install && make install
{code}",Bug,Critical,karya,2015-08-05T15:59:28.000+0000,5,Resolved,Complete,__init__.py not getting installed in $PREFIX/lib/pythonX.Y/site-packages/mesos,2015-08-15T00:26:02.000+0000,MESOS-2337,2.0,mesos,Mesosphere Q1 Sprint 4 - 3/6
tillt,2015-02-10T16:57:45.000+0000,bernd-mesos,"A new kind of module that receives callbacks at significant life cycle events of its host libprocess process. Typically the latter is a Mesos slave or master and the life time of the libprocess process coincides with the underlying OS process. 

h4. Motivation and Use Cases

We want to add customized and experimental capabilities that concern the life time of Mesos components without protruding into Mesos source code and without creating new build process dependencies for everybody. 

Example use cases:
1. A slave or master life cycle module that gathers fail-over incidents and reports summaries thereof to a remote data sink.
2. A slave module that observes host computer metrics and correlates these with task activity. This can be used to find resources leaks and to prevent, respectively guide, oversubscription.
3. Upgrades and provisioning that require shutdown and restart.

h4. Specifics

The specific life cycle events that we want to get notified about and want to be able to act upon are:

- Process is spawning/initializing
- Process is terminating/finalizing

In all these cases, a reference to the process is passed as a parameter, giving the module access for inspection and reaction. 

h4. Module Classification

Unlike other named modules, a life cycle module does not directly replace or provide essential Mesos functionality (such as an Isolator module does). Unlike a decorator module it does not directly add or inject data into Mesos core either.",Improvement,Major,bernd-mesos,2015-03-06T16:43:16.000+0000,5,Resolved,Complete,Mesos Lifecycle Modules,2015-03-06T16:43:16.000+0000,MESOS-2335,0.5,mesos,Mesosphere Q1 Sprint 3 - 2/20
pbrett,2015-02-10T01:53:38.000+0000,pbrett,"Export metrics from the network isolation to identify scope and duration of container throttling.  

Packet loss can be identified from the overlimits and requeues fields of the htb qdisc report for the virtual interface, e.g.

{noformat}
$ tc -s -d qdisc show dev mesos19223
qdisc pfifo_fast 0: root refcnt 2 bands 3 priomap  1 2 2 2 1 2 0 0 1 1 1 1 1 1 1 1
 Sent 158213287452 bytes 1030876393 pkt (dropped 0, overlimits 0 requeues 0)
 backlog 0b 0p requeues 0
qdisc ingress ffff: parent ffff:fff1 ----------------
 Sent 119381747824 bytes 1144549901 pkt (dropped 2044879, overlimits 0 requeues 0)
 backlog 0b 0p requeues 0
{noformat}

Note that since a packet can be examined multiple times before transmission, overlimits can exceed total packets sent.  

Add to the port_mapping isolator usage() and the container statistics protobuf. Carefully consider the naming (esp tx/rx) + commenting of the protobuf fields so it's clear what these represent and how they are different to the existing dropped packet counts from the network stack.",Improvement,Major,pbrett,2015-06-18T00:18:19.000+0000,5,Resolved,Complete,Report per-container metrics for network bandwidth throttling,2015-07-07T19:38:10.000+0000,MESOS-2332,5.0,mesos,Twitter Mesos Q1 Sprint 2
xujyan,2015-02-06T18:59:21.000+0000,xujyan,"
{noformat:title=}
[ RUN      ] MasterAllocatorTest/0.OutOfOrderDispatch
Using temporary directory '/tmp/MasterAllocatorTest_0_OutOfOrderDispatch_kjLb9b'
I0206 07:55:44.084333 15065 leveldb.cpp:175] Opened db in 25.006293ms
I0206 07:55:44.089635 15065 leveldb.cpp:182] Compacted db in 5.256332ms
I0206 07:55:44.089695 15065 leveldb.cpp:197] Created db iterator in 23534ns
I0206 07:55:44.089710 15065 leveldb.cpp:203] Seeked to beginning of db in 2175ns
I0206 07:55:44.089720 15065 leveldb.cpp:272] Iterated through 0 keys in the db in 417ns
I0206 07:55:44.089781 15065 replica.cpp:743] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned
I0206 07:55:44.093750 15086 recover.cpp:448] Starting replica recovery
I0206 07:55:44.094044 15086 recover.cpp:474] Replica is in EMPTY status
I0206 07:55:44.095473 15086 replica.cpp:640] Replica in EMPTY status received a broadcasted recover request
I0206 07:55:44.095724 15086 recover.cpp:194] Received a recover response from a replica in EMPTY status
I0206 07:55:44.096097 15086 recover.cpp:565] Updating replica status to STARTING
I0206 07:55:44.106575 15086 leveldb.cpp:305] Persisting metadata (8 bytes) to leveldb took 10.289939ms
I0206 07:55:44.106613 15086 replica.cpp:322] Persisted replica status to STARTING
I0206 07:55:44.108144 15086 recover.cpp:474] Replica is in STARTING status
I0206 07:55:44.109122 15086 replica.cpp:640] Replica in STARTING status received a broadcasted recover request
I0206 07:55:44.110879 15091 recover.cpp:194] Received a recover response from a replica in STARTING status
I0206 07:55:44.117267 15087 recover.cpp:565] Updating replica status to VOTING
I0206 07:55:44.124771 15087 leveldb.cpp:305] Persisting metadata (8 bytes) to leveldb took 2.66794ms
I0206 07:55:44.124814 15087 replica.cpp:322] Persisted replica status to VOTING
I0206 07:55:44.124948 15087 recover.cpp:579] Successfully joined the Paxos group
I0206 07:55:44.125095 15087 recover.cpp:463] Recover process terminated
I0206 07:55:44.126204 15087 master.cpp:344] Master 20150206-075544-16842879-38895-15065 (utopic) started on 127.0.1.1:38895
I0206 07:55:44.126268 15087 master.cpp:390] Master only allowing authenticated frameworks to register
I0206 07:55:44.126281 15087 master.cpp:395] Master only allowing authenticated slaves to register
I0206 07:55:44.126307 15087 credentials.hpp:35] Loading credentials for authentication from '/tmp/MasterAllocatorTest_0_OutOfOrderDispatch_kjLb9b/credentials'
I0206 07:55:44.126683 15087 master.cpp:439] Authorization enabled
I0206 07:55:44.129329 15086 master.cpp:1350] The newly elected leader is master@127.0.1.1:38895 with id 20150206-075544-16842879-38895-15065
I0206 07:55:44.129361 15086 master.cpp:1363] Elected as the leading master!
I0206 07:55:44.129389 15086 master.cpp:1181] Recovering from registrar
I0206 07:55:44.129653 15088 registrar.cpp:312] Recovering registrar
I0206 07:55:44.130859 15088 log.cpp:659] Attempting to start the writer
I0206 07:55:44.132334 15088 replica.cpp:476] Replica received implicit promise request with proposal 1
I0206 07:55:44.135187 15088 leveldb.cpp:305] Persisting metadata (8 bytes) to leveldb took 2.825465ms
I0206 07:55:44.135390 15088 replica.cpp:344] Persisted promised to 1
I0206 07:55:44.138062 15091 coordinator.cpp:229] Coordinator attemping to fill missing position
I0206 07:55:44.139576 15091 replica.cpp:377] Replica received explicit promise request for position 0 with proposal 2
I0206 07:55:44.142156 15091 leveldb.cpp:342] Persisting action (8 bytes) to leveldb took 2.545543ms
I0206 07:55:44.142189 15091 replica.cpp:678] Persisted action at 0
I0206 07:55:44.143414 15091 replica.cpp:510] Replica received write request for position 0
I0206 07:55:44.143468 15091 leveldb.cpp:437] Reading position from leveldb took 28872ns
I0206 07:55:44.145982 15091 leveldb.cpp:342] Persisting action (14 bytes) to leveldb took 2.480277ms
I0206 07:55:44.146015 15091 replica.cpp:678] Persisted action at 0
I0206 07:55:44.147050 15089 replica.cpp:657] Replica received learned notice for position 0
I0206 07:55:44.154364 15089 leveldb.cpp:342] Persisting action (16 bytes) to leveldb took 7.281644ms
I0206 07:55:44.154400 15089 replica.cpp:678] Persisted action at 0
I0206 07:55:44.154422 15089 replica.cpp:663] Replica learned NOP action at position 0
I0206 07:55:44.155506 15091 log.cpp:675] Writer started with ending position 0
I0206 07:55:44.156746 15091 leveldb.cpp:437] Reading position from leveldb took 30248ns
I0206 07:55:44.173681 15091 registrar.cpp:345] Successfully fetched the registry (0B) in 43.977984ms
I0206 07:55:44.173821 15091 registrar.cpp:444] Applied 1 operations in 30768ns; attempting to update the 'registry'
I0206 07:55:44.176213 15086 log.cpp:683] Attempting to append 119 bytes to the log
I0206 07:55:44.176426 15086 coordinator.cpp:339] Coordinator attempting to write APPEND action at position 1
I0206 07:55:44.177608 15088 replica.cpp:510] Replica received write request for position 1
I0206 07:55:44.180059 15088 leveldb.cpp:342] Persisting action (136 bytes) to leveldb took 2.415145ms
I0206 07:55:44.180094 15088 replica.cpp:678] Persisted action at 1
I0206 07:55:44.181324 15084 replica.cpp:657] Replica received learned notice for position 1
I0206 07:55:44.183831 15084 leveldb.cpp:342] Persisting action (138 bytes) to leveldb took 2.473724ms
I0206 07:55:44.183866 15084 replica.cpp:678] Persisted action at 1
I0206 07:55:44.183887 15084 replica.cpp:663] Replica learned APPEND action at position 1
I0206 07:55:44.185510 15084 registrar.cpp:489] Successfully updated the 'registry' in 11.619072ms
I0206 07:55:44.185678 15086 log.cpp:702] Attempting to truncate the log to 1
I0206 07:55:44.186111 15086 coordinator.cpp:339] Coordinator attempting to write TRUNCATE action at position 2
I0206 07:55:44.186944 15086 replica.cpp:510] Replica received write request for position 2
I0206 07:55:44.187492 15084 registrar.cpp:375] Successfully recovered registrar
I0206 07:55:44.188016 15087 master.cpp:1208] Recovered 0 slaves from the Registry (83B) ; allowing 10mins for slaves to re-register
I0206 07:55:44.189678 15086 leveldb.cpp:342] Persisting action (16 bytes) to leveldb took 2.702559ms
I0206 07:55:44.189713 15086 replica.cpp:678] Persisted action at 2
I0206 07:55:44.190620 15086 replica.cpp:657] Replica received learned notice for position 2
I0206 07:55:44.193383 15086 leveldb.cpp:342] Persisting action (18 bytes) to leveldb took 2.737088ms
I0206 07:55:44.193455 15086 leveldb.cpp:400] Deleting ~1 keys from leveldb took 37762ns
I0206 07:55:44.193475 15086 replica.cpp:678] Persisted action at 2
I0206 07:55:44.193496 15086 replica.cpp:663] Replica learned TRUNCATE action at position 2
I0206 07:55:44.200028 15065 containerizer.cpp:102] Using isolation: posix/cpu,posix/mem
I0206 07:55:44.212924 15088 slave.cpp:172] Slave started on 46)@127.0.1.1:38895
I0206 07:55:44.213762 15088 credentials.hpp:83] Loading credential for authentication from '/tmp/MasterAllocatorTest_0_OutOfOrderDispatch_RuNyVQ/credential'
I0206 07:55:44.214251 15088 slave.cpp:281] Slave using credential for: test-principal
I0206 07:55:44.214653 15088 slave.cpp:299] Slave resources: cpus(*):2; mem(*):1024; disk(*):24988; ports(*):[31000-32000]
I0206 07:55:44.214918 15088 slave.cpp:328] Slave hostname: utopic
I0206 07:55:44.215116 15088 slave.cpp:329] Slave checkpoint: false
W0206 07:55:44.215332 15088 slave.cpp:331] Disabling checkpointing is deprecated and the --checkpoint flag will be removed in a future release. Please avoid using this flag
I0206 07:55:44.217061 15090 state.cpp:32] Recovering state from '/tmp/MasterAllocatorTest_0_OutOfOrderDispatch_RuNyVQ/meta'
I0206 07:55:44.235409 15088 status_update_manager.cpp:196] Recovering status update manager
I0206 07:55:44.235601 15088 containerizer.cpp:299] Recovering containerizer
I0206 07:55:44.236486 15088 slave.cpp:3526] Finished recovery
I0206 07:55:44.237709 15087 status_update_manager.cpp:170] Pausing sending status updates
I0206 07:55:44.237890 15088 slave.cpp:620] New master detected at master@127.0.1.1:38895
I0206 07:55:44.241575 15088 slave.cpp:683] Authenticating with master master@127.0.1.1:38895
I0206 07:55:44.247459 15088 slave.cpp:688] Using default CRAM-MD5 authenticatee
I0206 07:55:44.248617 15089 authenticatee.hpp:137] Creating new client SASL connection
I0206 07:55:44.249099 15089 master.cpp:3788] Authenticating slave(46)@127.0.1.1:38895
I0206 07:55:44.249137 15089 master.cpp:3799] Using default CRAM-MD5 authenticator
I0206 07:55:44.249728 15089 authenticator.hpp:169] Creating new server SASL connection
I0206 07:55:44.250285 15089 authenticatee.hpp:228] Received SASL authentication mechanisms: CRAM-MD5
I0206 07:55:44.250496 15089 authenticatee.hpp:254] Attempting to authenticate with mechanism 'CRAM-MD5'
I0206 07:55:44.250452 15088 slave.cpp:656] Detecting new master
I0206 07:55:44.251063 15091 authenticator.hpp:275] Received SASL authentication start
I0206 07:55:44.251124 15091 authenticator.hpp:397] Authentication requires more steps
I0206 07:55:44.251256 15089 authenticatee.hpp:274] Received SASL authentication step
I0206 07:55:44.251451 15090 authenticator.hpp:303] Received SASL authentication step
I0206 07:55:44.251575 15090 authenticator.hpp:389] Authentication success
I0206 07:55:44.251687 15090 master.cpp:3846] Successfully authenticated principal 'test-principal' at slave(46)@127.0.1.1:38895
I0206 07:55:44.253306 15089 authenticatee.hpp:314] Authentication success
I0206 07:55:44.258015 15089 slave.cpp:754] Successfully authenticated with master master@127.0.1.1:38895
I0206 07:55:44.258468 15089 master.cpp:2913] Registering slave at slave(46)@127.0.1.1:38895 (utopic) with id 20150206-075544-16842879-38895-15065-S0
I0206 07:55:44.259028 15089 registrar.cpp:444] Applied 1 operations in 88902ns; attempting to update the 'registry'
I0206 07:55:44.269492 15065 sched.cpp:149] Version: 0.22.0
I0206 07:55:44.270539 15090 sched.cpp:246] New master detected at master@127.0.1.1:38895
I0206 07:55:44.270614 15090 sched.cpp:302] Authenticating with master master@127.0.1.1:38895
I0206 07:55:44.270634 15090 sched.cpp:309] Using default CRAM-MD5 authenticatee
I0206 07:55:44.270900 15090 authenticatee.hpp:137] Creating new client SASL connection
I0206 07:55:44.272300 15089 log.cpp:683] Attempting to append 285 bytes to the log
I0206 07:55:44.272552 15089 coordinator.cpp:339] Coordinator attempting to write APPEND action at position 3
I0206 07:55:44.273609 15086 master.cpp:3788] Authenticating scheduler-d6cac0a1-d461-4a05-b19d-5cbdae239eb0@127.0.1.1:38895
I0206 07:55:44.273643 15086 master.cpp:3799] Using default CRAM-MD5 authenticator
I0206 07:55:44.273955 15086 authenticator.hpp:169] Creating new server SASL connection
I0206 07:55:44.274617 15090 authenticatee.hpp:228] Received SASL authentication mechanisms: CRAM-MD5
I0206 07:55:44.274813 15090 authenticatee.hpp:254] Attempting to authenticate with mechanism 'CRAM-MD5'
I0206 07:55:44.275171 15088 authenticator.hpp:275] Received SASL authentication start
I0206 07:55:44.275215 15088 authenticator.hpp:397] Authentication requires more steps
I0206 07:55:44.275408 15090 authenticatee.hpp:274] Received SASL authentication step
I0206 07:55:44.275696 15084 authenticator.hpp:303] Received SASL authentication step
I0206 07:55:44.275774 15084 authenticator.hpp:389] Authentication success
I0206 07:55:44.275876 15084 master.cpp:3846] Successfully authenticated principal 'test-principal' at scheduler-d6cac0a1-d461-4a05-b19d-5cbdae239eb0@127.0.1.1:38895
I0206 07:55:44.277593 15090 authenticatee.hpp:314] Authentication success
I0206 07:55:44.278201 15086 sched.cpp:390] Successfully authenticated with master master@127.0.1.1:38895
I0206 07:55:44.278548 15086 master.cpp:1568] Received registration request for framework 'framework1' at scheduler-d6cac0a1-d461-4a05-b19d-5cbdae239eb0@127.0.1.1:38895
I0206 07:55:44.278642 15086 master.cpp:1429] Authorizing framework principal 'test-principal' to receive offers for role '*'
I0206 07:55:44.279157 15086 master.cpp:1632] Registering framework 20150206-075544-16842879-38895-15065-0000 (framework1) at scheduler-d6cac0a1-d461-4a05-b19d-5cbdae239eb0@127.0.1.1:38895
I0206 07:55:44.280081 15086 sched.cpp:440] Framework registered with 20150206-075544-16842879-38895-15065-0000
I0206 07:55:44.280320 15086 hierarchical_allocator_process.hpp:318] Added framework 20150206-075544-16842879-38895-15065-0000
I0206 07:55:44.281411 15089 replica.cpp:510] Replica received write request for position 3
I0206 07:55:44.282289 15085 master.cpp:2901] Ignoring register slave message from slave(46)@127.0.1.1:38895 (utopic) as admission is already in progress
I0206 07:55:44.284984 15089 leveldb.cpp:342] Persisting action (304 bytes) to leveldb took 3.368213ms
I0206 07:55:44.285020 15089 replica.cpp:678] Persisted action at 3
I0206 07:55:44.285893 15089 replica.cpp:657] Replica received learned notice for position 3
I0206 07:55:44.288350 15089 leveldb.cpp:342] Persisting action (306 bytes) to leveldb took 2.430449ms
I0206 07:55:44.288384 15089 replica.cpp:678] Persisted action at 3
I0206 07:55:44.288405 15089 replica.cpp:663] Replica learned APPEND action at position 3
I0206 07:55:44.290154 15089 registrar.cpp:489] Successfully updated the 'registry' in 31.046912ms
I0206 07:55:44.290307 15085 log.cpp:702] Attempting to truncate the log to 3
I0206 07:55:44.290671 15085 coordinator.cpp:339] Coordinator attempting to write TRUNCATE action at position 4
I0206 07:55:44.291482 15085 replica.cpp:510] Replica received write request for position 4
I0206 07:55:44.292559 15087 master.cpp:2970] Registered slave 20150206-075544-16842879-38895-15065-S0 at slave(46)@127.0.1.1:38895 (utopic) with cpus(*):2; mem(*):1024; disk(*):24988; ports(*):[31000-32000]
I0206 07:55:44.292940 15087 slave.cpp:788] Registered with master master@127.0.1.1:38895; given slave ID 20150206-075544-16842879-38895-15065-S0
I0206 07:55:44.293298 15087 hierarchical_allocator_process.hpp:450] Added slave 20150206-075544-16842879-38895-15065-S0 (utopic) with cpus(*):2; mem(*):1024; disk(*):24988; ports(*):[31000-32000] (and cpus(*):2; mem(*):1024; disk(*):24988; ports(*):[31000-32000] available)
I0206 07:55:44.293684 15087 status_update_manager.cpp:177] Resuming sending status updates
I0206 07:55:44.294085 15087 master.cpp:3730] Sending 1 offers to framework 20150206-075544-16842879-38895-15065-0000 (framework1) at scheduler-d6cac0a1-d461-4a05-b19d-5cbdae239eb0@127.0.1.1:38895
I0206 07:55:44.299957 15085 leveldb.cpp:342] Persisting action (16 bytes) to leveldb took 8.442691ms
I0206 07:55:44.300165 15085 replica.cpp:678] Persisted action at 4
I0206 07:55:44.300698 15065 sched.cpp:1468] Asked to stop the driver
I0206 07:55:44.301127 15090 sched.cpp:806] Stopping framework '20150206-075544-16842879-38895-15065-0000'
I0206 07:55:44.301503 15090 master.cpp:1892] Asked to unregister framework 20150206-075544-16842879-38895-15065-0000
I0206 07:55:44.301535 15090 master.cpp:4158] Removing framework 20150206-075544-16842879-38895-15065-0000 (framework1) at scheduler-d6cac0a1-d461-4a05-b19d-5cbdae239eb0@127.0.1.1:38895
I0206 07:55:44.302376 15090 slave.cpp:1592] Asked to shut down framework 20150206-075544-16842879-38895-15065-0000 by master@127.0.1.1:38895
W0206 07:55:44.302407 15090 slave.cpp:1607] Cannot shut down unknown framework 20150206-075544-16842879-38895-15065-0000
I0206 07:55:44.302814 15090 hierarchical_allocator_process.hpp:397] Deactivated framework 20150206-075544-16842879-38895-15065-0000
I0206 07:55:44.302947 15090 hierarchical_allocator_process.hpp:351] Removed framework 20150206-075544-16842879-38895-15065-0000
I0206 07:55:44.309281 15086 hierarchical_allocator_process.hpp:642] Recovered cpus(*):2; mem(*):1024; disk(*):24988; ports(*):[31000-32000] (total allocatable: cpus(*):2; mem(*):1024; disk(*):24988; ports(*):[31000-32000]) on slave 20150206-075544-16842879-38895-15065-S0 from framework 20150206-075544-16842879-38895-15065-0000
I0206 07:55:44.310158 15084 replica.cpp:657] Replica received learned notice for position 4
I0206 07:55:44.313246 15084 leveldb.cpp:342] Persisting action (18 bytes) to leveldb took 3.055049ms
I0206 07:55:44.313328 15084 leveldb.cpp:400] Deleting ~2 keys from leveldb took 45270ns
I0206 07:55:44.313349 15084 replica.cpp:678] Persisted action at 4
I0206 07:55:44.313374 15084 replica.cpp:663] Replica learned TRUNCATE action at position 4
I0206 07:55:44.329591 15065 sched.cpp:149] Version: 0.22.0
I0206 07:55:44.330258 15088 sched.cpp:246] New master detected at master@127.0.1.1:38895
I0206 07:55:44.330346 15088 sched.cpp:302] Authenticating with master master@127.0.1.1:38895
I0206 07:55:44.330368 15088 sched.cpp:309] Using default CRAM-MD5 authenticatee
I0206 07:55:44.330652 15088 authenticatee.hpp:137] Creating new client SASL connection
I0206 07:55:44.331403 15088 master.cpp:3788] Authenticating scheduler-7bdaa90b-eb9f-4009-bd5a-d07fd3f24cec@127.0.1.1:38895
I0206 07:55:44.331717 15088 master.cpp:3799] Using default CRAM-MD5 authenticator
I0206 07:55:44.332293 15088 authenticator.hpp:169] Creating new server SASL connection
I0206 07:55:44.332655 15088 authenticatee.hpp:228] Received SASL authentication mechanisms: CRAM-MD5
I0206 07:55:44.332684 15088 authenticatee.hpp:254] Attempting to authenticate with mechanism 'CRAM-MD5'
I0206 07:55:44.332792 15088 authenticator.hpp:275] Received SASL authentication start
I0206 07:55:44.332835 15088 authenticator.hpp:397] Authentication requires more steps
I0206 07:55:44.332903 15088 authenticatee.hpp:274] Received SASL authentication step
I0206 07:55:44.332983 15088 authenticator.hpp:303] Received SASL authentication step
I0206 07:55:44.333056 15088 authenticator.hpp:389] Authentication success
I0206 07:55:44.333153 15088 authenticatee.hpp:314] Authentication success
I0206 07:55:44.333297 15091 master.cpp:3846] Successfully authenticated principal 'test-principal' at scheduler-7bdaa90b-eb9f-4009-bd5a-d07fd3f24cec@127.0.1.1:38895
I0206 07:55:44.334326 15087 sched.cpp:390] Successfully authenticated with master master@127.0.1.1:38895
I0206 07:55:44.334645 15087 master.cpp:1568] Received registration request for framework 'framework2' at scheduler-7bdaa90b-eb9f-4009-bd5a-d07fd3f24cec@127.0.1.1:38895
I0206 07:55:44.334722 15087 master.cpp:1429] Authorizing framework principal 'test-principal' to receive offers for role '*'
I0206 07:55:44.335153 15087 master.cpp:1632] Registering framework 20150206-075544-16842879-38895-15065-0001 (framework2) at scheduler-7bdaa90b-eb9f-4009-bd5a-d07fd3f24cec@127.0.1.1:38895
I0206 07:55:44.336019 15087 sched.cpp:440] Framework registered with 20150206-075544-16842879-38895-15065-0001
I0206 07:55:44.336156 15087 hierarchical_allocator_process.hpp:318] Added framework 20150206-075544-16842879-38895-15065-0001
I0206 07:55:44.336796 15087 master.cpp:3730] Sending 1 offers to framework 20150206-075544-16842879-38895-15065-0001 (framework2) at scheduler-7bdaa90b-eb9f-4009-bd5a-d07fd3f24cec@127.0.1.1:38895
I0206 07:55:44.337725 15065 sched.cpp:1468] Asked to stop the driver
I0206 07:55:44.338002 15086 sched.cpp:806] Stopping framework '20150206-075544-16842879-38895-15065-0001'
I0206 07:55:44.338297 15090 master.cpp:1892] Asked to unregister framework 20150206-075544-16842879-38895-15065-0001
I0206 07:55:44.338353 15090 master.cpp:4158] Removing framework 20150206-075544-16842879-38895-15065-0001 (framework2) at scheduler-7bdaa90b-eb9f-4009-bd5a-d07fd3f24cec@127.0.1.1:38895
../../src/tests/master_allocator_tests.cpp:300: Failure
Mock function called more times than expected - taking default action specified at:
../../src/tests/mesos.hpp:713:
    Function call: deactivateFramework(@0x7fdb74008d70 20150206-075544-16842879-38895-15065-0001)
         Expected: to be called once
           Actual: called twice - over-saturated and active
../../src/tests/master_allocator_tests.cpp:312: Failure
Mock function called more times than expected - taking default action specified at:
../../src/tests/mesos.hpp:753:
    Function call: recoverResources(@0x7fdb74013040 20150206-075544-16842879-38895-15065-0001, @0x7fdb74013060 20150206-075544-16842879-38895-15065-S0, @0x7fdb74013080 { cpus(*):2, mem(*):1024, disk(*):24988, ports(*):[31000-32000] }, @0x7fdb74013098 16-byte object <01-00 00-00 DB-7F 00-00 00-00 00-00 00-00 00-00>)
         Expected: to be called once
           Actual: called twice - over-saturated and active
I0206 07:55:44.339527 15090 slave.cpp:1592] Asked to shut down framework 20150206-075544-16842879-38895-15065-0001 by master@127.0.1.1:38895
W0206 07:55:44.339558 15090 slave.cpp:1607] Cannot shut down unknown framework 20150206-075544-16842879-38895-15065-0001
I0206 07:55:44.339954 15090 hierarchical_allocator_process.hpp:397] Deactivated framework 20150206-075544-16842879-38895-15065-0001
I0206 07:55:44.340095 15090 hierarchical_allocator_process.hpp:642] Recovered cpus(*):2; mem(*):1024; disk(*):24988; ports(*):[31000-32000] (total allocatable: cpus(*):2; mem(*):1024; disk(*):24988; ports(*):[31000-32000]) on slave 20150206-075544-16842879-38895-15065-S0 from framework 20150206-075544-16842879-38895-15065-0001
I0206 07:55:44.340181 15090 hierarchical_allocator_process.hpp:351] Removed framework 20150206-075544-16842879-38895-15065-0001
I0206 07:55:44.340852 15085 master.cpp:781] Master terminating
I0206 07:55:44.345564 15086 slave.cpp:2680] master@127.0.1.1:38895 exited
W0206 07:55:44.345593 15086 slave.cpp:2683] Master disconnected! Waiting for a new master to be elected
I0206 07:55:44.393707 15065 slave.cpp:502] Slave terminating
[  FAILED  ] MasterAllocatorTest/0.OutOfOrderDispatch, where TypeParam = mesos::master::allocator::HierarchicalAllocatorProcess<mesos::master::allocator::DRFSorter, mesos::master::allocator::DRFSorter> (360 ms)
{noformat}",Bug,Major,xujyan,2015-02-09T21:06:43.000+0000,5,Resolved,Complete,MasterAllocatorTest/0.OutOfOrderDispatch is flaky,2015-02-09T21:26:05.000+0000,MESOS-2324,1.0,mesos,Twitter Mesos Q1 Sprint 2
jieyu,2015-02-04T18:35:01.000+0000,jlingmann,"When starting mesos-slave with --work_dir set to a directory which is not the same device as /tmp results in mesos-slave throwing a core dump:
{code}
mesos # GLOG_v=1 sbin/mesos-slave --master=zk://10.171.59.83:2181/mesos --work_dir=/var/lib/mesos/
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0204 18:24:49.274619 22922 process.cpp:958] libprocess is initialized on 10.169.146.67:5051 for 8 cpus
I0204 18:24:49.274978 22922 logging.cpp:177] Logging to STDERR
I0204 18:24:49.275111 22922 main.cpp:152] Build: 2015-02-03 22:59:30 by 
I0204 18:24:49.275233 22922 main.cpp:154] Version: 0.22.0
I0204 18:24:49.275485 22922 containerizer.cpp:103] Using isolation: posix/cpu,posix/mem
2015-02-04 18:24:49,275:22922(0x7ffdd4d5c700):ZOO_INFO@log_env@712: Client environment:zookeeper.version=zookeeper C client 3.4.5
2015-02-04 18:24:49,275:22922(0x7ffdd4d5c700):ZOO_INFO@log_env@716: Client environment:host.name=ip-10-169-146-67.ec2.internal
2015-02-04 18:24:49,276:22922(0x7ffdd4d5c700):ZOO_INFO@log_env@723: Client environment:os.name=Linux
2015-02-04 18:24:49,276:22922(0x7ffdd4d5c700):ZOO_INFO@log_env@724: Client environment:os.arch=3.18.2
2015-02-04 18:24:49,276:22922(0x7ffdd4d5c700):ZOO_INFO@log_env@725: Client environment:os.version=#2 SMP Tue Jan 27 23:34:36 UTC 2015
2015-02-04 18:24:49,276:22922(0x7ffdd4d5c700):ZOO_INFO@log_env@733: Client environment:user.name=core
2015-02-04 18:24:49,276:22922(0x7ffdd4d5c700):ZOO_INFO@log_env@741: Client environment:user.home=/root
2015-02-04 18:24:49,276:22922(0x7ffdd4d5c700):ZOO_INFO@log_env@753: Client environment:user.dir=/opt/mesosphere/dcos/0.0.1-0.1.20150203225612/mesos
2015-02-04 18:24:49,276:22922(0x7ffdd4d5c700):ZOO_INFO@zookeeper_init@786: Initiating client connection, host=10.171.59.83:2181 sessionTimeout=10000 watcher=0x7ffdd97bccf0 sessionId=0 sessionPasswd=<null> context=0x7ffdc8000ba0 flags=0
I0204 18:24:49.276793 22922 main.cpp:180] Starting Mesos slave
2015-02-04 18:24:49,307:22922(0x7ffdd151f700):ZOO_INFO@check_events@1703: initiated connection to server [10.171.59.83:2181]
I0204 18:24:49.307548 22922 slave.cpp:173] Slave started on 1)@10.169.146.67:5051
I0204 18:24:49.307955 22922 slave.cpp:300] Slave resources: cpus(*):1; mem(*):2728; disk(*):24736; ports(*):[31000-32000]
I0204 18:24:49.308404 22922 slave.cpp:329] Slave hostname: ip-10-169-146-67.ec2.internal
I0204 18:24:49.308459 22922 slave.cpp:330] Slave checkpoint: true
I0204 18:24:49.310431 22924 state.cpp:33] Recovering state from '/var/lib/mesos/meta'
I0204 18:24:49.310583 22924 state.cpp:668] Failed to find resources file '/var/lib/mesos/meta/resources/resources.info'
I0204 18:24:49.310670 22924 state.cpp:74] Failed to find the latest slave from '/var/lib/mesos/meta'
I0204 18:24:49.310803 22924 status_update_manager.cpp:197] Recovering status update manager
I0204 18:24:49.310916 22924 containerizer.cpp:300] Recovering containerizer
I0204 18:24:49.311110 22924 slave.cpp:3527] Finished recovery
F0204 18:24:49.311312 22924 slave.cpp:3537] CHECK_SOME(state::checkpoint(path, bootId.get())): Failed to rename '/tmp/PSHLqV' to '/var/lib/mesos/meta/boot_id': Invalid cross-device link 
2015-02-04 18:24:49,310:22922(0x7ffdd151f700):ZOO_INFO@check_events@1750: session establishment complete on server [10.171.59.83:2181], sessionId=0x14b51bc8506039a, negotiated timeout=10000
*** Check failure stack trace: ***
    @     0x7ffdd9a6596d  google::LogMessage::Fail()
I0204 18:24:49.313356 22930 group.cpp:313] Group process (group(1)@10.169.146.67:5051) connected to ZooKeeper
    @     0x7ffdd9a677ad  google::LogMessage::SendToLog()
I0204 18:24:49.313786 22930 group.cpp:790] Syncing group operations: queue size (joins, cancels, datas) = (0, 0, 0)
I0204 18:24:49.314487 22930 group.cpp:385] Trying to create path '/mesos' in ZooKeeper
I0204 18:24:49.323668 22930 group.cpp:717] Found non-sequence node 'log_replicas' at '/mesos' in ZooKeeper
I0204 18:24:49.323806 22930 detector.cpp:138] Detected a new leader: (id='1')
I0204 18:24:49.323958 22930 group.cpp:659] Trying to get '/mesos/info_0000000001' in ZooKeeper
I0204 18:24:49.324595 22930 detector.cpp:433] A new leading master (UPID=master@10.171.59.83:5050) is detected
    @     0x7ffdd9a6555c  google::LogMessage::Flush()
    @     0x7ffdd9a680a9  google::LogMessageFatal::~LogMessageFatal()
    @     0x7ffdd94b7179  _CheckFatal::~_CheckFatal()
    @     0x7ffdd96718e2  mesos::internal::slave::Slave::__recover()
    @     0x7ffdd9a1524a  process::ProcessManager::resume()
    @     0x7ffdd9a1550c  process::schedule()
    @     0x7ffdd83832ad  (unknown)
    @     0x7ffdd80b834d  (unknown)
Aborted (core dumped)
{code}

Removing the --work_dir option results in the slave starting successfully.",Bug,Major,jlingmann,2015-02-04T20:51:21.000+0000,5,Resolved,Complete,Unable to set --work_dir to a non /tmp device,2015-02-06T16:33:13.000+0000,MESOS-2319,2.0,mesos,Twitter Mesos Q1 Sprint 2
js84,2015-02-04T00:05:44.000+0000,adam-mesos,"Cody's plan from MESOS-444 was:
1) -Make it so the flag can't be changed at the command line-
2) -Remove the checkpoint variable entirely from slave/flags.hpp. This is a fairly involved change since a number of unit tests depend on manually setting the flag, as well as the default being non-checkpointing.-
3) -Remove logic around checkpointing in the slave, remove logic inside the master.-
4) Drop the flag from the SlaveInfo struct (Will require a deprecation cycle).
",Epic,Major,adam-mesos,,3,In Progress,In Progress,Remove deprecated checkpoint=false code,2016-03-30T05:33:57.000+0000,MESOS-2317,3.0,mesos,Mesosphere Q1 Sprint 6 - 4/3
vaibhavkhanduja,2015-02-02T21:40:49.000+0000,idownes,IIUC this has been deprecated and all current code (except examples/docker_no_executor_framework.cpp) uses the top-level ContainerInfo?,Task,Minor,idownes,2015-12-15T05:38:54.000+0000,5,Resolved,Complete,Deprecate / Remove CommandInfo::ContainerInfo,2015-12-15T05:38:54.000+0000,MESOS-2315,2.0,mesos,Mesosphere Sprint 24
dhamon,2015-02-02T18:38:59.000+0000,dhamon,"In {{src/slave/paths.cpp}} a number of string constants are defined to describe the formats of various paths. However, given there is a 1:1 mapping between the string constant and the functions that build the paths, the code would be more readable if the format strings were inline in the functions.

In the cases where one constant depends on another (see the {{EXECUTOR_INFO_PATH, EXECUTOR_PATH, FRAMEWORK_PATH, SLAVE_PATH, ROOT_PATH}} chain, for example) the function calls can just be chained together.

This will have the added benefit of removing some statically constructed string constants, which are dangerous.",Improvement,Minor,dhamon,2015-02-04T18:01:54.000+0000,5,Resolved,Complete,remove unnecessary constants,2015-02-04T18:01:54.000+0000,MESOS-2314,2.0,mesos,Twitter Mesos Q1 Sprint 2
vinodkone,2015-01-30T23:46:20.000+0000,zmanji,"In AURORA-1076 it was discovered that if an ExecutorInfo was changed such that a previously unset optional field with a default value was changed to have the field set with the default value, it would be rejected as not compatible.

For example if we have an ExecutorInfo with a CommandInfo with the {{shell}} attribute unset and then we change the CommandInfo to set the {{shell}} attribute to true Mesos will reject the task with:

{noformat}
I0130 21:50:05.373389 50869 master.cpp:3441] Sending status update TASK_LOST (UUID: 82ef615c-0d59-4427-95d5-80cf0e52b3fc) for task system-gc-c89c0c05-200c-462e-958a-ecd7b9a76831 of framework 201103282247-0000000019-0000 'Task has invalid ExecutorInfo (existing ExecutorInfo with same ExecutorID is not compatible).
{noformat}

This is not intuitive because the default value of the {{shell}} attribute is true. There should be no difference between not setting an optional field with a default value and setting that field to the default value.",Bug,Minor,zmanji,2015-03-13T01:30:43.000+0000,5,Resolved,Complete,Mesos rejects ExecutorInfo as incompatible when there is no functional difference,2015-04-03T22:55:01.000+0000,MESOS-2309,3.0,mesos,Twitter Mesos Q1 Sprint 4
bmahler,2015-01-29T22:58:04.000+0000,bmahler,"Good run:

{noformat}
[ RUN      ] MasterAuthorizationTest.FrameworkRemovedBeforeReregistration
Using temporary directory '/tmp/MasterAuthorizationTest_FrameworkRemovedBeforeReregistration_ZU7oaD'
I0122 19:23:06.481690 17483 leveldb.cpp:176] Opened db in 21.058723ms
I0122 19:23:06.488590 17483 leveldb.cpp:183] Compacted db in 6.6715ms
I0122 19:23:06.488816 17483 leveldb.cpp:198] Created db iterator in 30034ns
I0122 19:23:06.489053 17483 leveldb.cpp:204] Seeked to beginning of db in 2908ns
I0122 19:23:06.489073 17483 leveldb.cpp:273] Iterated through 0 keys in the db in 492ns
I0122 19:23:06.489148 17483 replica.cpp:744] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned
I0122 19:23:06.490272 17504 recover.cpp:449] Starting replica recovery
I0122 19:23:06.490900 17504 recover.cpp:475] Replica is in EMPTY status
I0122 19:23:06.492422 17504 replica.cpp:641] Replica in EMPTY status received a broadcasted recover request
I0122 19:23:06.492694 17504 recover.cpp:195] Received a recover response from a replica in EMPTY status
I0122 19:23:06.493185 17504 recover.cpp:566] Updating replica status to STARTING
I0122 19:23:06.514881 17504 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 21.459963ms
I0122 19:23:06.514920 17504 replica.cpp:323] Persisted replica status to STARTING
I0122 19:23:06.515861 17501 master.cpp:262] Master 20150122-192306-16842879-46283-17483 (lucid) started on 127.0.1.1:46283
I0122 19:23:06.515910 17501 master.cpp:308] Master only allowing authenticated frameworks to register
I0122 19:23:06.515923 17501 master.cpp:313] Master only allowing authenticated slaves to register
I0122 19:23:06.515946 17501 credentials.hpp:36] Loading credentials for authentication from '/tmp/MasterAuthorizationTest_FrameworkRemovedBeforeReregistration_ZU7oaD/credentials'
I0122 19:23:06.516150 17501 master.cpp:357] Authorization enabled
I0122 19:23:06.517511 17501 hierarchical_allocator_process.hpp:285] Initialized hierarchical allocator process
I0122 19:23:06.517607 17501 whitelist_watcher.cpp:65] No whitelist given
I0122 19:23:06.518066 17498 master.cpp:1219] The newly elected leader is master@127.0.1.1:46283 with id 20150122-192306-16842879-46283-17483
I0122 19:23:06.518095 17498 master.cpp:1232] Elected as the leading master!
I0122 19:23:06.518121 17498 master.cpp:1050] Recovering from registrar
I0122 19:23:06.518333 17498 registrar.cpp:313] Recovering registrar
I0122 19:23:06.523987 17504 recover.cpp:475] Replica is in STARTING status
I0122 19:23:06.525090 17504 replica.cpp:641] Replica in STARTING status received a broadcasted recover request
I0122 19:23:06.525337 17504 recover.cpp:195] Received a recover response from a replica in STARTING status
I0122 19:23:06.525693 17504 recover.cpp:566] Updating replica status to VOTING
I0122 19:23:06.532680 17504 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 6.810884ms
I0122 19:23:06.532714 17504 replica.cpp:323] Persisted replica status to VOTING
I0122 19:23:06.532835 17504 recover.cpp:580] Successfully joined the Paxos group
I0122 19:23:06.533004 17504 recover.cpp:464] Recover process terminated
I0122 19:23:06.533833 17500 log.cpp:660] Attempting to start the writer
I0122 19:23:06.535225 17500 replica.cpp:477] Replica received implicit promise request with proposal 1
I0122 19:23:06.540340 17500 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 5.086139ms
I0122 19:23:06.540371 17500 replica.cpp:345] Persisted promised to 1
I0122 19:23:06.541502 17504 coordinator.cpp:230] Coordinator attemping to fill missing position
I0122 19:23:06.543021 17504 replica.cpp:378] Replica received explicit promise request for position 0 with proposal 2
I0122 19:23:06.548140 17504 leveldb.cpp:343] Persisting action (8 bytes) to leveldb took 5.083443ms
I0122 19:23:06.548171 17504 replica.cpp:679] Persisted action at 0
I0122 19:23:06.549746 17500 replica.cpp:511] Replica received write request for position 0
I0122 19:23:06.549926 17500 leveldb.cpp:438] Reading position from leveldb took 31962ns
I0122 19:23:06.555033 17500 leveldb.cpp:343] Persisting action (14 bytes) to leveldb took 5.065823ms
I0122 19:23:06.555064 17500 replica.cpp:679] Persisted action at 0
I0122 19:23:06.556094 17504 replica.cpp:658] Replica received learned notice for position 0
I0122 19:23:06.558815 17504 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 2.688382ms
I0122 19:23:06.558847 17504 replica.cpp:679] Persisted action at 0
I0122 19:23:06.558868 17504 replica.cpp:664] Replica learned NOP action at position 0
I0122 19:23:06.559917 17500 log.cpp:676] Writer started with ending position 0
I0122 19:23:06.560995 17500 leveldb.cpp:438] Reading position from leveldb took 27742ns
I0122 19:23:06.563467 17500 registrar.cpp:346] Successfully fetched the registry (0B) in 45.095936ms
I0122 19:23:06.563551 17500 registrar.cpp:445] Applied 1 operations in 19686ns; attempting to update the 'registry'
I0122 19:23:06.566107 17500 log.cpp:684] Attempting to append 118 bytes to the log
I0122 19:23:06.566267 17500 coordinator.cpp:340] Coordinator attempting to write APPEND action at position 1
I0122 19:23:06.567126 17500 replica.cpp:511] Replica received write request for position 1
I0122 19:23:06.582588 17500 leveldb.cpp:343] Persisting action (135 bytes) to leveldb took 15.425511ms
I0122 19:23:06.582631 17500 replica.cpp:679] Persisted action at 1
I0122 19:23:06.583425 17500 replica.cpp:658] Replica received learned notice for position 1
I0122 19:23:06.589001 17500 leveldb.cpp:343] Persisting action (137 bytes) to leveldb took 5.549486ms
I0122 19:23:06.589200 17500 replica.cpp:679] Persisted action at 1
I0122 19:23:06.589416 17500 replica.cpp:664] Replica learned APPEND action at position 1
I0122 19:23:06.596420 17500 registrar.cpp:490] Successfully updated the 'registry' in 32.815104ms
I0122 19:23:06.596551 17500 registrar.cpp:376] Successfully recovered registrar
I0122 19:23:06.596923 17500 master.cpp:1077] Recovered 0 slaves from the Registry (82B) ; allowing 10mins for slaves to re-register
I0122 19:23:06.597007 17500 log.cpp:703] Attempting to truncate the log to 1
I0122 19:23:06.597239 17500 coordinator.cpp:340] Coordinator attempting to write TRUNCATE action at position 2
I0122 19:23:06.598464 17501 replica.cpp:511] Replica received write request for position 2
I0122 19:23:06.604038 17501 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 5.536264ms
I0122 19:23:06.604084 17501 replica.cpp:679] Persisted action at 2
I0122 19:23:06.608747 17503 replica.cpp:658] Replica received learned notice for position 2
I0122 19:23:06.614094 17503 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 5.315347ms
I0122 19:23:06.614171 17503 leveldb.cpp:401] Deleting ~1 keys from leveldb took 33021ns
I0122 19:23:06.614188 17503 replica.cpp:679] Persisted action at 2
I0122 19:23:06.614208 17503 replica.cpp:664] Replica learned TRUNCATE action at position 2
I0122 19:23:06.628820 17483 sched.cpp:151] Version: 0.22.0
I0122 19:23:06.629879 17505 sched.cpp:248] New master detected at master@127.0.1.1:46283
I0122 19:23:06.629973 17505 sched.cpp:304] Authenticating with master master@127.0.1.1:46283
I0122 19:23:06.629995 17505 sched.cpp:311] Using default CRAM-MD5 authenticatee
I0122 19:23:06.630314 17505 authenticatee.hpp:138] Creating new client SASL connection
I0122 19:23:06.630722 17505 master.cpp:4129] Authenticating scheduler-4156eae6-8d7f-423a-920a-02b11b7bd1ba@127.0.1.1:46283
I0122 19:23:06.630750 17505 master.cpp:4140] Using default CRAM-MD5 authenticator
I0122 19:23:06.631115 17505 authenticator.hpp:170] Creating new server SASL connection
I0122 19:23:06.631423 17505 authenticatee.hpp:229] Received SASL authentication mechanisms: CRAM-MD5
I0122 19:23:06.631459 17505 authenticatee.hpp:255] Attempting to authenticate with mechanism 'CRAM-MD5'
I0122 19:23:06.631563 17505 authenticator.hpp:276] Received SASL authentication start
I0122 19:23:06.631605 17505 authenticator.hpp:398] Authentication requires more steps
I0122 19:23:06.631671 17505 authenticatee.hpp:275] Received SASL authentication step
I0122 19:23:06.631748 17505 authenticator.hpp:304] Received SASL authentication step
I0122 19:23:06.631774 17505 auxprop.cpp:99] Request to lookup properties for user: 'test-principal' realm: 'lucid' server FQDN: 'lucid' SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0122 19:23:06.631784 17505 auxprop.cpp:171] Looking up auxiliary property '*userPassword'
I0122 19:23:06.631822 17505 auxprop.cpp:171] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0122 19:23:06.631856 17505 auxprop.cpp:99] Request to lookup properties for user: 'test-principal' realm: 'lucid' server FQDN: 'lucid' SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0122 19:23:06.631870 17505 auxprop.cpp:121] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0122 19:23:06.631877 17505 auxprop.cpp:121] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0122 19:23:06.631892 17505 authenticator.hpp:390] Authentication success
I0122 19:23:06.631988 17505 authenticatee.hpp:315] Authentication success
I0122 19:23:06.632066 17505 master.cpp:4187] Successfully authenticated principal 'test-principal' at scheduler-4156eae6-8d7f-423a-920a-02b11b7bd1ba@127.0.1.1:46283
I0122 19:23:06.632359 17505 sched.cpp:392] Successfully authenticated with master master@127.0.1.1:46283
I0122 19:23:06.632382 17505 sched.cpp:515] Sending registration request to master@127.0.1.1:46283
I0122 19:23:06.632432 17505 sched.cpp:548] Will retry registration in 598.155756ms if necessary
I0122 19:23:06.632575 17505 master.cpp:1420] Received registration request for framework 'default' at scheduler-4156eae6-8d7f-423a-920a-02b11b7bd1ba@127.0.1.1:46283
I0122 19:23:06.632639 17505 master.cpp:1298] Authorizing framework principal 'test-principal' to receive offers for role '*'
I0122 19:23:06.632912 17505 master.cpp:1484] Registering framework 20150122-192306-16842879-46283-17483-0000 (default) at scheduler-4156eae6-8d7f-423a-920a-02b11b7bd1ba@127.0.1.1:46283
I0122 19:23:06.633421 17505 hierarchical_allocator_process.hpp:319] Added framework 20150122-192306-16842879-46283-17483-0000
I0122 19:23:06.633448 17505 hierarchical_allocator_process.hpp:839] No resources available to allocate!
I0122 19:23:06.633458 17505 hierarchical_allocator_process.hpp:746] Performed allocation for 0 slaves in 17704ns
I0122 19:23:06.633919 17505 sched.cpp:442] Framework registered with 20150122-192306-16842879-46283-17483-0000
I0122 19:23:06.633980 17505 sched.cpp:456] Scheduler::registered took 37063ns
I0122 19:23:06.636554 17500 sched.cpp:242] Scheduler::disconnected took 14843ns
I0122 19:23:06.636579 17500 sched.cpp:248] New master detected at master@127.0.1.1:46283
I0122 19:23:06.636625 17500 sched.cpp:304] Authenticating with master master@127.0.1.1:46283
I0122 19:23:06.636641 17500 sched.cpp:311] Using default CRAM-MD5 authenticatee
I0122 19:23:06.636914 17500 authenticatee.hpp:138] Creating new client SASL connection
I0122 19:23:06.637313 17500 master.cpp:4129] Authenticating scheduler-4156eae6-8d7f-423a-920a-02b11b7bd1ba@127.0.1.1:46283
I0122 19:23:06.637341 17500 master.cpp:4140] Using default CRAM-MD5 authenticator
I0122 19:23:06.637675 17500 authenticator.hpp:170] Creating new server SASL connection
I0122 19:23:06.638056 17501 authenticatee.hpp:229] Received SASL authentication mechanisms: CRAM-MD5
I0122 19:23:06.638083 17501 authenticatee.hpp:255] Attempting to authenticate with mechanism 'CRAM-MD5'
I0122 19:23:06.638182 17501 authenticator.hpp:276] Received SASL authentication start
I0122 19:23:06.638221 17501 authenticator.hpp:398] Authentication requires more steps
I0122 19:23:06.638286 17501 authenticatee.hpp:275] Received SASL authentication step
I0122 19:23:06.638360 17501 authenticator.hpp:304] Received SASL authentication step
I0122 19:23:06.638383 17501 auxprop.cpp:99] Request to lookup properties for user: 'test-principal' realm: 'lucid' server FQDN: 'lucid' SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0122 19:23:06.638393 17501 auxprop.cpp:171] Looking up auxiliary property '*userPassword'
I0122 19:23:06.638422 17501 auxprop.cpp:171] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0122 19:23:06.638447 17501 auxprop.cpp:99] Request to lookup properties for user: 'test-principal' realm: 'lucid' server FQDN: 'lucid' SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0122 19:23:06.638458 17501 auxprop.cpp:121] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0122 19:23:06.638464 17501 auxprop.cpp:121] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0122 19:23:06.638478 17501 authenticator.hpp:390] Authentication success
I0122 19:23:06.638566 17501 authenticatee.hpp:315] Authentication success
I0122 19:23:06.638643 17501 master.cpp:4187] Successfully authenticated principal 'test-principal' at scheduler-4156eae6-8d7f-423a-920a-02b11b7bd1ba@127.0.1.1:46283
I0122 19:23:06.638919 17501 sched.cpp:392] Successfully authenticated with master master@127.0.1.1:46283
I0122 19:23:06.638942 17501 sched.cpp:515] Sending registration request to master@127.0.1.1:46283
I0122 19:23:06.638994 17501 sched.cpp:548] Will retry registration in 489.304713ms if necessary
I0122 19:23:06.639169 17501 master.cpp:1557] Received re-registration request from framework 20150122-192306-16842879-46283-17483-0000 (default) at scheduler-4156eae6-8d7f-423a-920a-02b11b7bd1ba@127.0.1.1:46283
I0122 19:23:06.639242 17501 master.cpp:1298] Authorizing framework principal 'test-principal' to receive offers for role '*'
I0122 19:23:06.639839 17483 sched.cpp:1471] Asked to stop the driver
I0122 19:23:06.640379 17499 sched.cpp:808] Stopping framework '20150122-192306-16842879-46283-17483-0000'
I0122 19:23:06.640697 17499 master.cpp:745] Framework 20150122-192306-16842879-46283-17483-0000 (default) at scheduler-4156eae6-8d7f-423a-920a-02b11b7bd1ba@127.0.1.1:46283 disconnected
I0122 19:23:06.640723 17499 master.cpp:1789] Disconnecting framework 20150122-192306-16842879-46283-17483-0000 (default) at scheduler-4156eae6-8d7f-423a-920a-02b11b7bd1ba@127.0.1.1:46283
I0122 19:23:06.640744 17499 master.cpp:1805] Deactivating framework 20150122-192306-16842879-46283-17483-0000 (default) at scheduler-4156eae6-8d7f-423a-920a-02b11b7bd1ba@127.0.1.1:46283
I0122 19:23:06.640806 17499 master.cpp:767] Giving framework 20150122-192306-16842879-46283-17483-0000 (default) at scheduler-4156eae6-8d7f-423a-920a-02b11b7bd1ba@127.0.1.1:46283 0ns to failover
I0122 19:23:06.640951 17499 hierarchical_allocator_process.hpp:398] Deactivated framework 20150122-192306-16842879-46283-17483-0000
I0122 19:23:06.646342 17498 master.cpp:1604] Dropping re-registration request of framework 20150122-192306-16842879-46283-17483-0000 (default)  at scheduler-4156eae6-8d7f-423a-920a-02b11b7bd1ba@127.0.1.1:46283 because it is not authenticated
I0122 19:23:06.648844 17498 master.cpp:3941] Framework failover timeout, removing framework 20150122-192306-16842879-46283-17483-0000 (default) at scheduler-4156eae6-8d7f-423a-920a-02b11b7bd1ba@127.0.1.1:46283
I0122 19:23:06.648871 17498 master.cpp:4499] Removing framework 20150122-192306-16842879-46283-17483-0000 (default) at scheduler-4156eae6-8d7f-423a-920a-02b11b7bd1ba@127.0.1.1:46283
I0122 19:23:06.649624 17498 hierarchical_allocator_process.hpp:352] Removed framework 20150122-192306-16842879-46283-17483-0000
I0122 19:23:06.656532 17483 master.cpp:654] Master terminating
[       OK ] MasterAuthorizationTest.FrameworkRemovedBeforeReregistration (216 ms)
{noformat}

Bad run:

{noformat}
[ RUN      ] MasterAuthorizationTest.FrameworkRemovedBeforeReregistration
Using temporary directory '/tmp/MasterAuthorizationTest_FrameworkRemovedBeforeReregistration_JDM2sm'
I0126 19:19:55.517570  2381 leveldb.cpp:176] Opened db in 34.341401ms
I0126 19:19:55.529630  2381 leveldb.cpp:183] Compacted db in 11.824435ms
I0126 19:19:55.529878  2381 leveldb.cpp:198] Created db iterator in 26176ns
I0126 19:19:55.530200  2381 leveldb.cpp:204] Seeked to beginning of db in 3457ns
I0126 19:19:55.530455  2381 leveldb.cpp:273] Iterated through 0 keys in the db in 902ns
I0126 19:19:55.530658  2381 replica.cpp:744] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned
I0126 19:19:55.531492  2397 recover.cpp:449] Starting replica recovery
I0126 19:19:55.531793  2397 recover.cpp:475] Replica is in EMPTY status
I0126 19:19:55.533327  2397 replica.cpp:641] Replica in EMPTY status received a broadcasted recover request
I0126 19:19:55.533608  2397 recover.cpp:195] Received a recover response from a replica in EMPTY status
I0126 19:19:55.534101  2397 recover.cpp:566] Updating replica status to STARTING
I0126 19:19:55.550417  2397 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 16.106821ms
I0126 19:19:55.550472  2397 replica.cpp:323] Persisted replica status to STARTING
I0126 19:19:55.551434  2397 recover.cpp:475] Replica is in STARTING status
I0126 19:19:55.552846  2397 replica.cpp:641] Replica in STARTING status received a broadcasted recover request
I0126 19:19:55.553099  2397 recover.cpp:195] Received a recover response from a replica in STARTING status
I0126 19:19:55.553565  2397 recover.cpp:566] Updating replica status to VOTING
I0126 19:19:55.564590  2397 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 10.719218ms
I0126 19:19:55.564919  2397 replica.cpp:323] Persisted replica status to VOTING
I0126 19:19:55.565982  2397 recover.cpp:580] Successfully joined the Paxos group
I0126 19:19:55.566231  2397 recover.cpp:464] Recover process terminated
I0126 19:19:55.567878  2401 master.cpp:262] Master 20150126-191955-16842879-51862-2381 (lucid) started on 127.0.1.1:51862
I0126 19:19:55.567927  2401 master.cpp:308] Master only allowing authenticated frameworks to register
I0126 19:19:55.567950  2401 master.cpp:313] Master only allowing authenticated slaves to register
I0126 19:19:55.567978  2401 credentials.hpp:36] Loading credentials for authentication from '/tmp/MasterAuthorizationTest_FrameworkRemovedBeforeReregistration_JDM2sm/credentials'
I0126 19:19:55.568220  2401 master.cpp:357] Authorization enabled
I0126 19:19:55.569890  2401 hierarchical_allocator_process.hpp:285] Initialized hierarchical allocator process
I0126 19:19:55.569999  2401 whitelist_watcher.cpp:65] No whitelist given
I0126 19:19:55.570694  2401 master.cpp:1219] The newly elected leader is master@127.0.1.1:51862 with id 20150126-191955-16842879-51862-2381
I0126 19:19:55.570721  2401 master.cpp:1232] Elected as the leading master!
I0126 19:19:55.570742  2401 master.cpp:1050] Recovering from registrar
I0126 19:19:55.570977  2401 registrar.cpp:313] Recovering registrar
I0126 19:19:55.571959  2401 log.cpp:660] Attempting to start the writer
I0126 19:19:55.573441  2401 replica.cpp:477] Replica received implicit promise request with proposal 1
I0126 19:19:55.590724  2401 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 17.243964ms
I0126 19:19:55.590785  2401 replica.cpp:345] Persisted promised to 1
I0126 19:19:55.592140  2396 coordinator.cpp:230] Coordinator attemping to fill missing position
I0126 19:19:55.593834  2396 replica.cpp:378] Replica received explicit promise request for position 0 with proposal 2
I0126 19:19:55.603837  2396 leveldb.cpp:343] Persisting action (8 bytes) to leveldb took 9.955824ms
I0126 19:19:55.603902  2396 replica.cpp:679] Persisted action at 0
I0126 19:19:55.606082  2401 replica.cpp:511] Replica received write request for position 0
I0126 19:19:55.606331  2401 leveldb.cpp:438] Reading position from leveldb took 44524ns
I0126 19:19:55.612546  2401 leveldb.cpp:343] Persisting action (14 bytes) to leveldb took 5.870411ms
I0126 19:19:55.612597  2401 replica.cpp:679] Persisted action at 0
I0126 19:19:55.613416  2401 replica.cpp:658] Replica received learned notice for position 0
I0126 19:19:55.616269  2401 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 2.82145ms
I0126 19:19:55.616305  2401 replica.cpp:679] Persisted action at 0
I0126 19:19:55.616328  2401 replica.cpp:664] Replica learned NOP action at position 0
I0126 19:19:55.628062  2399 log.cpp:676] Writer started with ending position 0
I0126 19:19:55.629328  2399 leveldb.cpp:438] Reading position from leveldb took 57003ns
I0126 19:19:55.631995  2399 registrar.cpp:346] Successfully fetched the registry (0B) in 60.973824ms
I0126 19:19:55.632109  2399 registrar.cpp:445] Applied 1 operations in 35531ns; attempting to update the 'registry'
I0126 19:19:55.634799  2399 log.cpp:684] Attempting to append 117 bytes to the log
I0126 19:19:55.634996  2399 coordinator.cpp:340] Coordinator attempting to write APPEND action at position 1
I0126 19:19:55.636651  2397 replica.cpp:511] Replica received write request for position 1
I0126 19:19:55.642165  2397 leveldb.cpp:343] Persisting action (134 bytes) to leveldb took 5.474306ms
I0126 19:19:55.642215  2397 replica.cpp:679] Persisted action at 1
I0126 19:19:55.643226  2397 replica.cpp:658] Replica received learned notice for position 1
I0126 19:19:55.648574  2397 leveldb.cpp:343] Persisting action (136 bytes) to leveldb took 5.317891ms
I0126 19:19:55.648808  2397 replica.cpp:679] Persisted action at 1
I0126 19:19:55.649158  2397 replica.cpp:664] Replica learned APPEND action at position 1
I0126 19:19:55.663101  2397 registrar.cpp:490] Successfully updated the 'registry' in 30.918144ms
I0126 19:19:55.663267  2397 registrar.cpp:376] Successfully recovered registrar
I0126 19:19:55.663699  2397 master.cpp:1077] Recovered 0 slaves from the Registry (81B) ; allowing 10mins for slaves to re-register
I0126 19:19:55.663795  2397 log.cpp:703] Attempting to truncate the log to 1
I0126 19:19:55.664083  2397 coordinator.cpp:340] Coordinator attempting to write TRUNCATE action at position 2
I0126 19:19:55.665573  2403 replica.cpp:511] Replica received write request for position 2
I0126 19:19:55.671500  2403 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 5.883759ms
I0126 19:19:55.671547  2403 replica.cpp:679] Persisted action at 2
I0126 19:19:55.672780  2403 replica.cpp:658] Replica received learned notice for position 2
I0126 19:19:55.685999  2403 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 12.808643ms
I0126 19:19:55.686099  2403 leveldb.cpp:401] Deleting ~1 keys from leveldb took 49867ns
I0126 19:19:55.686121  2403 replica.cpp:679] Persisted action at 2
I0126 19:19:55.686149  2403 replica.cpp:664] Replica learned TRUNCATE action at position 2
I0126 19:19:55.722545  2381 sched.cpp:151] Version: 0.22.0
I0126 19:19:55.723795  2401 sched.cpp:248] New master detected at master@127.0.1.1:51862
I0126 19:19:55.723891  2401 sched.cpp:304] Authenticating with master master@127.0.1.1:51862
I0126 19:19:55.723914  2401 sched.cpp:311] Using default CRAM-MD5 authenticatee
I0126 19:19:55.724244  2401 authenticatee.hpp:138] Creating new client SASL connection
I0126 19:19:55.724694  2401 master.cpp:4129] Authenticating scheduler-80465e3f-73a3-4bd0-ba66-4dca62e9cdee@127.0.1.1:51862
I0126 19:19:55.724725  2401 master.cpp:4140] Using default CRAM-MD5 authenticator
I0126 19:19:55.725108  2401 authenticator.hpp:170] Creating new server SASL connection
I0126 19:19:55.725390  2401 authenticatee.hpp:229] Received SASL authentication mechanisms: CRAM-MD5
I0126 19:19:55.725415  2401 authenticatee.hpp:255] Attempting to authenticate with mechanism 'CRAM-MD5'
I0126 19:19:55.725515  2401 authenticator.hpp:276] Received SASL authentication start
I0126 19:19:55.725566  2401 authenticator.hpp:398] Authentication requires more steps
I0126 19:19:55.725632  2401 authenticatee.hpp:275] Received SASL authentication step
I0126 19:19:55.725710  2401 authenticator.hpp:304] Received SASL authentication step
I0126 19:19:55.725744  2401 auxprop.cpp:99] Request to lookup properties for user: 'test-principal' realm: 'lucid' server FQDN: 'lucid' SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0126 19:19:55.725757  2401 auxprop.cpp:171] Looking up auxiliary property '*userPassword'
I0126 19:19:55.725808  2401 auxprop.cpp:171] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0126 19:19:55.725834  2401 auxprop.cpp:99] Request to lookup properties for user: 'test-principal' realm: 'lucid' server FQDN: 'lucid' SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0126 19:19:55.725847  2401 auxprop.cpp:121] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0126 19:19:55.725853  2401 auxprop.cpp:121] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0126 19:19:55.725867  2401 authenticator.hpp:390] Authentication success
I0126 19:19:55.728629  2399 authenticatee.hpp:315] Authentication success
I0126 19:19:55.729228  2399 sched.cpp:392] Successfully authenticated with master master@127.0.1.1:51862
I0126 19:19:55.729277  2399 sched.cpp:515] Sending registration request to master@127.0.1.1:51862
I0126 19:19:55.729365  2399 sched.cpp:548] Will retry registration in 3.855403ms if necessary
I0126 19:19:55.729671  2399 master.cpp:1411] Queuing up registration request for framework 'default' at scheduler-80465e3f-73a3-4bd0-ba66-4dca62e9cdee@127.0.1.1:51862 because authentication is still in progress
I0126 19:19:55.733487  2400 master.cpp:4187] Successfully authenticated principal 'test-principal' at scheduler-80465e3f-73a3-4bd0-ba66-4dca62e9cdee@127.0.1.1:51862
I0126 19:19:55.734094  2400 master.cpp:1420] Received registration request for framework 'default' at scheduler-80465e3f-73a3-4bd0-ba66-4dca62e9cdee@127.0.1.1:51862
I0126 19:19:55.734177  2400 master.cpp:1298] Authorizing framework principal 'test-principal' to receive offers for role '*'
I0126 19:19:55.734724  2400 master.cpp:1484] Registering framework 20150126-191955-16842879-51862-2381-0000 (default) at scheduler-80465e3f-73a3-4bd0-ba66-4dca62e9cdee@127.0.1.1:51862
I0126 19:19:55.735335  2402 hierarchical_allocator_process.hpp:319] Added framework 20150126-191955-16842879-51862-2381-0000
I0126 19:19:55.735376  2402 hierarchical_allocator_process.hpp:831] No resources available to allocate!
I0126 19:19:55.735389  2402 hierarchical_allocator_process.hpp:738] Performed allocation for 0 slaves in 22978ns
I0126 19:19:55.741891  2398 sched.cpp:515] Sending registration request to master@127.0.1.1:51862
I0126 19:19:55.744575  2398 sched.cpp:548] Will retry registration in 3.86742709secs if necessary
I0126 19:19:55.744742  2398 sched.cpp:442] Framework registered with 20150126-191955-16842879-51862-2381-0000
I0126 19:19:55.744827  2398 sched.cpp:456] Scheduler::registered took 60111ns
I0126 19:19:55.744956  2398 master.cpp:1420] Received registration request for framework 'default' at scheduler-80465e3f-73a3-4bd0-ba66-4dca62e9cdee@127.0.1.1:51862
I0126 19:19:55.745020  2398 master.cpp:1298] Authorizing framework principal 'test-principal' to receive offers for role '*'
I0126 19:19:55.749315  2401 sched.cpp:242] Scheduler::disconnected took 19450ns
I0126 19:19:55.749343  2401 sched.cpp:248] New master detected at master@127.0.1.1:51862
I0126 19:19:55.749394  2401 sched.cpp:304] Authenticating with master master@127.0.1.1:51862
I0126 19:19:55.749411  2401 sched.cpp:311] Using default CRAM-MD5 authenticatee
I0126 19:19:55.749743  2401 authenticatee.hpp:138] Creating new client SASL connection
I0126 19:19:55.750208  2401 master.cpp:4129] Authenticating scheduler-80465e3f-73a3-4bd0-ba66-4dca62e9cdee@127.0.1.1:51862
I0126 19:19:55.750238  2401 master.cpp:4140] Using default CRAM-MD5 authenticator
I0126 19:19:55.750629  2401 authenticator.hpp:170] Creating new server SASL connection
I0126 19:19:55.750938  2401 authenticatee.hpp:229] Received SASL authentication mechanisms: CRAM-MD5
I0126 19:19:55.750963  2401 authenticatee.hpp:255] Attempting to authenticate with mechanism 'CRAM-MD5'
I0126 19:19:55.751063  2401 authenticator.hpp:276] Received SASL authentication start
I0126 19:19:55.751109  2401 authenticator.hpp:398] Authentication requires more steps
I0126 19:19:55.751175  2401 authenticatee.hpp:275] Received SASL authentication step
I0126 19:19:55.751269  2401 authenticator.hpp:304] Received SASL authentication step
I0126 19:19:55.751296  2401 auxprop.cpp:99] Request to lookup properties for user: 'test-principal' realm: 'lucid' server FQDN: 'lucid' SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0126 19:19:55.751307  2401 auxprop.cpp:171] Looking up auxiliary property '*userPassword'
I0126 19:19:55.751358  2401 auxprop.cpp:171] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0126 19:19:55.751392  2401 auxprop.cpp:99] Request to lookup properties for user: 'test-principal' realm: 'lucid' server FQDN: 'lucid' SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0126 19:19:55.751405  2401 auxprop.cpp:121] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0126 19:19:55.751413  2401 auxprop.cpp:121] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0126 19:19:55.751427  2401 authenticator.hpp:390] Authentication success
I0126 19:19:55.751524  2401 authenticatee.hpp:315] Authentication success
I0126 19:19:55.751605  2401 master.cpp:4187] Successfully authenticated principal 'test-principal' at scheduler-80465e3f-73a3-4bd0-ba66-4dca62e9cdee@127.0.1.1:51862
I0126 19:19:55.751898  2401 sched.cpp:392] Successfully authenticated with master master@127.0.1.1:51862
I0126 19:19:55.751922  2401 sched.cpp:515] Sending registration request to master@127.0.1.1:51862
I0126 19:19:55.751996  2401 sched.cpp:548] Will retry registration in 1.511226315secs if necessary
I0126 19:19:55.752174  2401 master.cpp:1557] Received re-registration request from framework 20150126-191955-16842879-51862-2381-0000 (default) at scheduler-80465e3f-73a3-4bd0-ba66-4dca62e9cdee@127.0.1.1:51862
I0126 19:19:55.752256  2401 master.cpp:1298] Authorizing framework principal 'test-principal' to receive offers for role '*'
I0126 19:19:55.752485  2401 master.cpp:1610] Re-registering framework 20150126-191955-16842879-51862-2381-0000 (default)  at scheduler-80465e3f-73a3-4bd0-ba66-4dca62e9cdee@127.0.1.1:51862
I0126 19:19:55.752527  2401 master.cpp:1650] Allowing framework 20150126-191955-16842879-51862-2381-0000 (default) at scheduler-80465e3f-73a3-4bd0-ba66-4dca62e9cdee@127.0.1.1:51862 to re-register with an already used id
I0126 19:19:55.752689  2401 sched.cpp:484] Framework re-registered with 20150126-191955-16842879-51862-2381-0000
tests/master_authorization_tests.cpp:980: Failure
Mock function called more times than expected - returning directly.
    Function call: reregistered(0x7fff5cef57e0, @0x56077d0 id: ""20150126-191955-16842879-51862-2381""
ip: 16842879
port: 51862
pid: ""master@127.0.1.1:51862""
hostname: ""lucid""
)
         Expected: to be never called
           Actual: called once - over-saturated and active
I0126 19:19:55.753191  2401 sched.cpp:498] Scheduler::reregistered took 478798ns
I0126 19:19:55.753600  2381 sched.cpp:1471] Asked to stop the driver
I0126 19:19:55.754518  2402 sched.cpp:808] Stopping framework '20150126-191955-16842879-51862-2381-0000'
I0126 19:19:55.755089  2402 master.cpp:1744] Asked to unregister framework 20150126-191955-16842879-51862-2381-0000
I0126 19:19:55.755302  2402 master.cpp:4499] Removing framework 20150126-191955-16842879-51862-2381-0000 (default) at scheduler-80465e3f-73a3-4bd0-ba66-4dca62e9cdee@127.0.1.1:51862
I0126 19:19:55.759419  2402 hierarchical_allocator_process.hpp:398] Deactivated framework 20150126-191955-16842879-51862-2381-0000
I0126 19:19:55.759850  2402 hierarchical_allocator_process.hpp:352] Removed framework 20150126-191955-16842879-51862-2381-0000
I0126 19:19:55.761160  2400 master.cpp:1462] Dropping registration request for framework 'default' at scheduler-80465e3f-73a3-4bd0-ba66-4dca62e9cdee@127.0.1.1:51862 because it is not authenticated
I0126 19:19:55.771309  2381 master.cpp:654] Master terminating
[  FAILED  ] MasterAuthorizationTest.FrameworkRemovedBeforeReregistration	 (312 ms)
{noformat}",Bug,Major,bmahler,2015-01-30T01:29:40.000+0000,5,Resolved,Complete,MasterAuthorizationTest.FrameworkRemovedBeforeReregistration is flaky.,2015-01-30T01:29:40.000+0000,MESOS-2306,1.0,mesos,Twitter Mesos Q1 Sprint 1
jieyu,2015-01-29T22:27:45.000+0000,jieyu,"There are several motivation for this. We are in the process of adding dynamic reservations and persistent volumes support in master. To do that, master needs to validate relevant operations from the framework (See Offer::Operation in mesos.proto). The existing validator style in master is hard to extend, compose and re-use.

Another motivation for this is for unit testing (MESOS-1064). Right now, we write integration tests for those validators which is unfortunate.",Bug,Major,jieyu,2015-02-03T18:59:30.000+0000,5,Resolved,Complete,Refactor validators in Master.,2015-07-02T18:43:31.000+0000,MESOS-2305,3.0,mesos,Twitter Mesos Q1 Sprint 1
bmahler,2015-01-29T19:40:42.000+0000,bmahler,"Bad Run:
{noformat}
[ RUN      ] FaultToleranceTest.SchedulerFailoverFrameworkMessage
Using temporary directory '/tmp/FaultToleranceTest_SchedulerFailoverFrameworkMessage_f3jYkr'
I0123 18:50:11.669674 15688 leveldb.cpp:176] Opened db in 31.920683ms
I0123 18:50:11.678328 15688 leveldb.cpp:183] Compacted db in 8.580569ms
I0123 18:50:11.678455 15688 leveldb.cpp:198] Created db iterator in 38478ns
I0123 18:50:11.678478 15688 leveldb.cpp:204] Seeked to beginning of db in 3057ns
I0123 18:50:11.678489 15688 leveldb.cpp:273] Iterated through 0 keys in the db in 427ns
I0123 18:50:11.678539 15688 replica.cpp:744] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned
I0123 18:50:11.682271 15705 recover.cpp:449] Starting replica recovery
I0123 18:50:11.682634 15705 recover.cpp:475] Replica is in EMPTY status
I0123 18:50:11.684389 15708 replica.cpp:641] Replica in EMPTY status received a broadcasted recover request
I0123 18:50:11.685132 15708 recover.cpp:195] Received a recover response from a replica in EMPTY status
I0123 18:50:11.689842 15708 recover.cpp:566] Updating replica status to STARTING
I0123 18:50:11.702548 15708 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 12.484558ms
I0123 18:50:11.702615 15708 replica.cpp:323] Persisted replica status to STARTING
I0123 18:50:11.703531 15708 recover.cpp:475] Replica is in STARTING status
I0123 18:50:11.705080 15704 replica.cpp:641] Replica in STARTING status received a broadcasted recover request
I0123 18:50:11.712587 15708 recover.cpp:195] Received a recover response from a replica in STARTING status
I0123 18:50:11.722898 15708 recover.cpp:566] Updating replica status to VOTING
I0123 18:50:11.725427 15703 master.cpp:262] Master 20150123-185011-16777343-37526-15688 (localhost.localdomain) started on 127.0.0.1:37526
W0123 18:50:11.725464 15703 master.cpp:266] 
**************************************************
Master bound to loopback interface! Cannot communicate with remote schedulers or slaves. You might want to set '--ip' flag to a routable IP address.
**************************************************
I0123 18:50:11.725502 15703 master.cpp:308] Master only allowing authenticated frameworks to register
I0123 18:50:11.725513 15703 master.cpp:313] Master only allowing authenticated slaves to register
I0123 18:50:11.725543 15703 credentials.hpp:36] Loading credentials for authentication from '/tmp/FaultToleranceTest_SchedulerFailoverFrameworkMessage_f3jYkr/credentials'
I0123 18:50:11.725774 15703 master.cpp:357] Authorization enabled
I0123 18:50:11.728428 15707 whitelist_watcher.cpp:65] No whitelist given
I0123 18:50:11.729169 15707 master.cpp:1219] The newly elected leader is master@127.0.0.1:37526 with id 20150123-185011-16777343-37526-15688
I0123 18:50:11.729200 15707 master.cpp:1232] Elected as the leading master!
I0123 18:50:11.729223 15707 master.cpp:1050] Recovering from registrar
I0123 18:50:11.729595 15706 registrar.cpp:313] Recovering registrar
I0123 18:50:11.730715 15703 hierarchical_allocator_process.hpp:285] Initialized hierarchical allocator process
I0123 18:50:11.737431 15708 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 14.259597ms
I0123 18:50:11.737511 15708 replica.cpp:323] Persisted replica status to VOTING
I0123 18:50:11.737768 15708 recover.cpp:580] Successfully joined the Paxos group
I0123 18:50:11.737977 15708 recover.cpp:464] Recover process terminated
I0123 18:50:11.739083 15706 log.cpp:660] Attempting to start the writer
I0123 18:50:11.741236 15706 replica.cpp:477] Replica received implicit promise request with proposal 1
I0123 18:50:11.750435 15706 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 8.813783ms
I0123 18:50:11.750514 15706 replica.cpp:345] Persisted promised to 1
I0123 18:50:11.752239 15708 coordinator.cpp:230] Coordinator attemping to fill missing position
I0123 18:50:11.754176 15706 replica.cpp:378] Replica received explicit promise request for position 0 with proposal 2
I0123 18:50:11.763464 15706 leveldb.cpp:343] Persisting action (8 bytes) to leveldb took 8.799822ms
I0123 18:50:11.763535 15706 replica.cpp:679] Persisted action at 0
I0123 18:50:11.765697 15709 replica.cpp:511] Replica received write request for position 0
I0123 18:50:11.766293 15709 leveldb.cpp:438] Reading position from leveldb took 54028ns
I0123 18:50:11.776468 15709 leveldb.cpp:343] Persisting action (14 bytes) to leveldb took 9.789169ms
I0123 18:50:11.776561 15709 replica.cpp:679] Persisted action at 0
I0123 18:50:11.777515 15709 replica.cpp:658] Replica received learned notice for position 0
I0123 18:50:11.785459 15709 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 7.897242ms
I0123 18:50:11.785531 15709 replica.cpp:679] Persisted action at 0
I0123 18:50:11.785565 15709 replica.cpp:664] Replica learned NOP action at position 0
I0123 18:50:11.786633 15709 log.cpp:676] Writer started with ending position 0
I0123 18:50:11.788460 15709 leveldb.cpp:438] Reading position from leveldb took 266087ns
I0123 18:50:11.801141 15709 registrar.cpp:346] Successfully fetched the registry (0B) in 71.491072ms
I0123 18:50:11.801300 15709 registrar.cpp:445] Applied 1 operations in 41795ns; attempting to update the 'registry'
I0123 18:50:11.805186 15707 log.cpp:684] Attempting to append 136 bytes to the log
I0123 18:50:11.805454 15707 coordinator.cpp:340] Coordinator attempting to write APPEND action at position 1
I0123 18:50:11.806677 15703 replica.cpp:511] Replica received write request for position 1
I0123 18:50:11.815621 15703 leveldb.cpp:343] Persisting action (155 bytes) to leveldb took 8.89177ms
I0123 18:50:11.815692 15703 replica.cpp:679] Persisted action at 1
I0123 18:50:11.817358 15704 replica.cpp:658] Replica received learned notice for position 1
I0123 18:50:11.825014 15704 leveldb.cpp:343] Persisting action (157 bytes) to leveldb took 7.578558ms
I0123 18:50:11.825088 15704 replica.cpp:679] Persisted action at 1
I0123 18:50:11.825124 15704 replica.cpp:664] Replica learned APPEND action at position 1
I0123 18:50:11.827008 15705 registrar.cpp:490] Successfully updated the 'registry' in 25.629952ms
I0123 18:50:11.827143 15705 registrar.cpp:376] Successfully recovered registrar
I0123 18:50:11.827517 15705 master.cpp:1077] Recovered 0 slaves from the Registry (98B) ; allowing 10mins for slaves to re-register
I0123 18:50:11.828515 15704 log.cpp:703] Attempting to truncate the log to 1
I0123 18:50:11.829074 15704 coordinator.cpp:340] Coordinator attempting to write TRUNCATE action at position 2
I0123 18:50:11.830546 15709 replica.cpp:511] Replica received write request for position 2
I0123 18:50:11.837752 15709 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 7.142431ms
I0123 18:50:11.837826 15709 replica.cpp:679] Persisted action at 2
I0123 18:50:11.839334 15709 replica.cpp:658] Replica received learned notice for position 2
I0123 18:50:11.847069 15709 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 7.116607ms
I0123 18:50:11.847214 15709 leveldb.cpp:401] Deleting ~1 keys from leveldb took 74008ns
I0123 18:50:11.847241 15709 replica.cpp:679] Persisted action at 2
I0123 18:50:11.847295 15709 replica.cpp:664] Replica learned TRUNCATE action at position 2
I0123 18:50:11.870337 15710 slave.cpp:173] Slave started on 94)@127.0.0.1:37526
W0123 18:50:11.870980 15710 slave.cpp:176] 
**************************************************
Slave bound to loopback interface! Cannot communicate with remote master(s). You might want to set '--ip' flag to a routable IP address.
**************************************************
I0123 18:50:11.871412 15710 credentials.hpp:84] Loading credential for authentication from '/tmp/FaultToleranceTest_SchedulerFailoverFrameworkMessage_TB8Rh3/credential'
I0123 18:50:11.871819 15710 slave.cpp:282] Slave using credential for: test-principal
I0123 18:50:11.873178 15710 slave.cpp:300] Slave resources: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I0123 18:50:11.873620 15710 slave.cpp:329] Slave hostname: localhost.localdomain
I0123 18:50:11.873837 15710 slave.cpp:330] Slave checkpoint: false
W0123 18:50:11.874068 15710 slave.cpp:332] Disabling checkpointing is deprecated and the --checkpoint flag will be removed in a future release. Please avoid using this flag
I0123 18:50:11.879103 15705 state.cpp:33] Recovering state from '/tmp/FaultToleranceTest_SchedulerFailoverFrameworkMessage_TB8Rh3/meta'
W0123 18:50:11.882972 15688 sched.cpp:1246] 
**************************************************
Scheduler driver bound to loopback interface! Cannot communicate with remote master(s). You might want to set 'LIBPROCESS_IP' environment variable to use a routable IP address.
**************************************************
I0123 18:50:11.884106 15709 status_update_manager.cpp:197] Recovering status update manager
I0123 18:50:11.884703 15710 slave.cpp:3519] Finished recovery
I0123 18:50:11.892076 15704 status_update_manager.cpp:171] Pausing sending status updates
I0123 18:50:11.892590 15710 slave.cpp:613] New master detected at master@127.0.0.1:37526
I0123 18:50:11.892937 15710 slave.cpp:676] Authenticating with master master@127.0.0.1:37526
I0123 18:50:11.893165 15710 slave.cpp:681] Using default CRAM-MD5 authenticatee
I0123 18:50:11.893754 15708 authenticatee.hpp:138] Creating new client SASL connection
I0123 18:50:11.894120 15708 master.cpp:4129] Authenticating slave(94)@127.0.0.1:37526
I0123 18:50:11.894153 15708 master.cpp:4140] Using default CRAM-MD5 authenticator
I0123 18:50:11.894628 15708 authenticator.hpp:170] Creating new server SASL connection
I0123 18:50:11.894913 15708 authenticatee.hpp:229] Received SASL authentication mechanisms: CRAM-MD5
I0123 18:50:11.894942 15708 authenticatee.hpp:255] Attempting to authenticate with mechanism 'CRAM-MD5'
I0123 18:50:11.895043 15708 authenticator.hpp:276] Received SASL authentication start
I0123 18:50:11.895095 15708 authenticator.hpp:398] Authentication requires more steps
I0123 18:50:11.895165 15708 authenticatee.hpp:275] Received SASL authentication step
I0123 18:50:11.895261 15708 authenticator.hpp:304] Received SASL authentication step
I0123 18:50:11.895292 15708 auxprop.cpp:99] Request to lookup properties for user: 'test-principal' realm: 'localhost.localdomain' server FQDN: 'localhost.localdomain' SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0123 18:50:11.895305 15708 auxprop.cpp:171] Looking up auxiliary property '*userPassword'
I0123 18:50:11.895354 15708 auxprop.cpp:171] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0123 18:50:11.895881 15710 slave.cpp:649] Detecting new master
I0123 18:50:11.898449 15708 auxprop.cpp:99] Request to lookup properties for user: 'test-principal' realm: 'localhost.localdomain' server FQDN: 'localhost.localdomain' SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0123 18:50:11.899024 15708 auxprop.cpp:121] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0123 18:50:11.899106 15708 auxprop.cpp:121] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0123 18:50:11.899190 15708 authenticator.hpp:390] Authentication success
I0123 18:50:11.899569 15706 authenticatee.hpp:315] Authentication success
I0123 18:50:11.902299 15706 slave.cpp:747] Successfully authenticated with master master@127.0.0.1:37526
I0123 18:50:11.902847 15706 slave.cpp:1075] Will retry registration in 19.809649ms if necessary
I0123 18:50:11.903264 15705 master.cpp:3214] Queuing up registration request from slave(94)@127.0.0.1:37526 because authentication is still in progress
I0123 18:50:11.903497 15705 master.cpp:4187] Successfully authenticated principal 'test-principal' at slave(94)@127.0.0.1:37526
I0123 18:50:11.903940 15705 master.cpp:3275] Registering slave at slave(94)@127.0.0.1:37526 (localhost.localdomain) with id 20150123-185011-16777343-37526-15688-S0
I0123 18:50:11.904398 15705 registrar.cpp:445] Applied 1 operations in 63679ns; attempting to update the 'registry'
I0123 18:50:11.917883 15688 sched.cpp:151] Version: 0.22.0
I0123 18:50:11.919347 15703 log.cpp:684] Attempting to append 315 bytes to the log
I0123 18:50:11.921039 15703 coordinator.cpp:340] Coordinator attempting to write APPEND action at position 3
I0123 18:50:11.919992 15706 sched.cpp:248] New master detected at master@127.0.0.1:37526
I0123 18:50:11.921352 15706 sched.cpp:304] Authenticating with master master@127.0.0.1:37526
I0123 18:50:11.921408 15706 sched.cpp:311] Using default CRAM-MD5 authenticatee
I0123 18:50:11.921773 15706 authenticatee.hpp:138] Creating new client SASL connection
I0123 18:50:11.922266 15706 master.cpp:4129] Authenticating scheduler-2cecb105-ca23-4048-9707-12b1e4422e11@127.0.0.1:37526
I0123 18:50:11.922301 15706 master.cpp:4140] Using default CRAM-MD5 authenticator
I0123 18:50:11.923928 15703 replica.cpp:511] Replica received write request for position 3
I0123 18:50:11.924285 15707 authenticator.hpp:170] Creating new server SASL connection
I0123 18:50:11.925091 15707 authenticatee.hpp:229] Received SASL authentication mechanisms: CRAM-MD5
I0123 18:50:11.925122 15707 authenticatee.hpp:255] Attempting to authenticate with mechanism 'CRAM-MD5'
I0123 18:50:11.925194 15707 authenticator.hpp:276] Received SASL authentication start
I0123 18:50:11.925257 15707 authenticator.hpp:398] Authentication requires more steps
I0123 18:50:11.925325 15707 authenticatee.hpp:275] Received SASL authentication step
I0123 18:50:11.925442 15707 authenticator.hpp:304] Received SASL authentication step
I0123 18:50:11.925473 15707 auxprop.cpp:99] Request to lookup properties for user: 'test-principal' realm: 'localhost.localdomain' server FQDN: 'localhost.localdomain' SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0123 18:50:11.925487 15707 auxprop.cpp:171] Looking up auxiliary property '*userPassword'
I0123 18:50:11.925532 15707 auxprop.cpp:171] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0123 18:50:11.925559 15707 auxprop.cpp:99] Request to lookup properties for user: 'test-principal' realm: 'localhost.localdomain' server FQDN: 'localhost.localdomain' SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0123 18:50:11.925571 15707 auxprop.cpp:121] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0123 18:50:11.925580 15707 auxprop.cpp:121] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0123 18:50:11.925595 15707 authenticator.hpp:390] Authentication success
I0123 18:50:11.925695 15707 authenticatee.hpp:315] Authentication success
I0123 18:50:11.925792 15707 master.cpp:4187] Successfully authenticated principal 'test-principal' at scheduler-2cecb105-ca23-4048-9707-12b1e4422e11@127.0.0.1:37526
I0123 18:50:11.926127 15707 sched.cpp:392] Successfully authenticated with master master@127.0.0.1:37526
I0123 18:50:11.926154 15707 sched.cpp:515] Sending registration request to master@127.0.0.1:37526
I0123 18:50:11.926215 15707 sched.cpp:548] Will retry registration in 866.81063ms if necessary
I0123 18:50:11.926640 15707 master.cpp:1420] Received registration request for framework 'default' at scheduler-2cecb105-ca23-4048-9707-12b1e4422e11@127.0.0.1:37526
I0123 18:50:11.926960 15707 master.cpp:1298] Authorizing framework principal 'test-principal' to receive offers for role '*'
I0123 18:50:11.927691 15707 master.cpp:1484] Registering framework 20150123-185011-16777343-37526-15688-0000 (default) at scheduler-2cecb105-ca23-4048-9707-12b1e4422e11@127.0.0.1:37526
I0123 18:50:11.928292 15708 hierarchical_allocator_process.hpp:319] Added framework 20150123-185011-16777343-37526-15688-0000
I0123 18:50:11.928326 15708 hierarchical_allocator_process.hpp:839] No resources available to allocate!
I0123 18:50:11.928340 15708 hierarchical_allocator_process.hpp:746] Performed allocation for 0 slaves in 21080ns
I0123 18:50:11.934458 15707 sched.cpp:442] Framework registered with 20150123-185011-16777343-37526-15688-0000
I0123 18:50:11.934927 15707 sched.cpp:456] Scheduler::registered took 112885ns
I0123 18:50:11.935747 15709 slave.cpp:1075] Will retry registration in 19.609252ms if necessary
I0123 18:50:11.935981 15709 master.cpp:3263] Ignoring register slave message from slave(94)@127.0.0.1:37526 (localhost.localdomain) as admission is already in progress
I0123 18:50:11.938997 15703 leveldb.cpp:343] Persisting action (334 bytes) to leveldb took 10.171709ms
I0123 18:50:11.939049 15703 replica.cpp:679] Persisted action at 3
I0123 18:50:11.940630 15709 replica.cpp:658] Replica received learned notice for position 3
I0123 18:50:11.945473 15709 leveldb.cpp:343] Persisting action (336 bytes) to leveldb took 4.804742ms
I0123 18:50:11.945521 15709 replica.cpp:679] Persisted action at 3
I0123 18:50:11.945550 15709 replica.cpp:664] Replica learned APPEND action at position 3
I0123 18:50:11.947105 15709 registrar.cpp:490] Successfully updated the 'registry' in 42.637056ms
I0123 18:50:11.948020 15703 master.cpp:3329] Registered slave 20150123-185011-16777343-37526-15688-S0 at slave(94)@127.0.0.1:37526 (localhost.localdomain) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I0123 18:50:11.948318 15703 hierarchical_allocator_process.hpp:453] Added slave 20150123-185011-16777343-37526-15688-S0 (localhost.localdomain) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (and cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] available)
I0123 18:50:11.948719 15703 hierarchical_allocator_process.hpp:764] Performed allocation for slave 20150123-185011-16777343-37526-15688-S0 in 355831ns
I0123 18:50:11.948813 15703 slave.cpp:781] Registered with master master@127.0.0.1:37526; given slave ID 20150123-185011-16777343-37526-15688-S0
I0123 18:50:11.948969 15703 slave.cpp:2588] Received ping from slave-observer(92)@127.0.0.1:37526
I0123 18:50:11.949324 15703 master.cpp:4071] Sending 1 offers to framework 20150123-185011-16777343-37526-15688-0000 (default) at scheduler-2cecb105-ca23-4048-9707-12b1e4422e11@127.0.0.1:37526
I0123 18:50:11.949571 15706 status_update_manager.cpp:178] Resuming sending status updates
I0123 18:50:11.950023 15709 log.cpp:703] Attempting to truncate the log to 3
I0123 18:50:11.950810 15705 sched.cpp:605] Scheduler::resourceOffers took 135580ns
I0123 18:50:11.952793 15708 master.cpp:2677] Processing ACCEPT call for offers: [ 20150123-185011-16777343-37526-15688-O0 ] on slave 20150123-185011-16777343-37526-15688-S0 at slave(94)@127.0.0.1:37526 (localhost.localdomain) for framework 20150123-185011-16777343-37526-15688-0000 (default) at scheduler-2cecb105-ca23-4048-9707-12b1e4422e11@127.0.0.1:37526
I0123 18:50:11.952852 15708 master.cpp:2513] Authorizing framework principal 'test-principal' to launch task 1 as user 'jenkins'
W0123 18:50:11.954649 15708 master.cpp:2130] Executor default for task 1 uses less CPUs (None) than the minimum required (0.01). Please update your executor, as this will be mandatory in future releases.
W0123 18:50:11.954988 15708 master.cpp:2142] Executor default for task 1 uses less memory (None) than the minimum required (32MB). Please update your executor, as this will be mandatory in future releases.
I0123 18:50:11.955579 15708 master.hpp:782] Adding task 1 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave 20150123-185011-16777343-37526-15688-S0 (localhost.localdomain)
I0123 18:50:11.956035 15703 coordinator.cpp:340] Coordinator attempting to write TRUNCATE action at position 4
I0123 18:50:11.957592 15704 replica.cpp:511] Replica received write request for position 4
I0123 18:50:11.958485 15708 master.cpp:2885] Launching task 1 of framework 20150123-185011-16777343-37526-15688-0000 (default) at scheduler-2cecb105-ca23-4048-9707-12b1e4422e11@127.0.0.1:37526 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave 20150123-185011-16777343-37526-15688-S0 at slave(94)@127.0.0.1:37526 (localhost.localdomain)
I0123 18:50:11.960578 15706 slave.cpp:1130] Got assigned task 1 for framework 20150123-185011-16777343-37526-15688-0000
I0123 18:50:11.961293 15706 slave.cpp:1245] Launching task 1 for framework 20150123-185011-16777343-37526-15688-0000
I0123 18:50:11.964450 15704 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 6.81421ms
I0123 18:50:11.964496 15704 replica.cpp:679] Persisted action at 4
I0123 18:50:11.966328 15705 replica.cpp:658] Replica received learned notice for position 4
I0123 18:50:11.969648 15706 slave.cpp:3921] Launching executor default of framework 20150123-185011-16777343-37526-15688-0000 in work directory '/tmp/FaultToleranceTest_SchedulerFailoverFrameworkMessage_TB8Rh3/slaves/20150123-185011-16777343-37526-15688-S0/frameworks/20150123-185011-16777343-37526-15688-0000/executors/default/runs/02536e4f-fb59-4b75-99aa-611fd7fffcb1'
I0123 18:50:11.976954 15705 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 10.584003ms
I0123 18:50:11.977078 15705 leveldb.cpp:401] Deleting ~2 keys from leveldb took 72466ns
I0123 18:50:11.977104 15705 replica.cpp:679] Persisted action at 4
I0123 18:50:11.977138 15705 replica.cpp:664] Replica learned TRUNCATE action at position 4
I0123 18:50:11.978016 15706 exec.cpp:147] Version: 0.22.0
I0123 18:50:11.978646 15710 exec.cpp:197] Executor started at: executor(50)@127.0.0.1:37526 with pid 15688
I0123 18:50:11.982480 15706 slave.cpp:1368] Queuing task '1' for executor default of framework '20150123-185011-16777343-37526-15688-0000
I0123 18:50:11.982676 15706 slave.cpp:566] Successfully attached file '/tmp/FaultToleranceTest_SchedulerFailoverFrameworkMessage_TB8Rh3/slaves/20150123-185011-16777343-37526-15688-S0/frameworks/20150123-185011-16777343-37526-15688-0000/executors/default/runs/02536e4f-fb59-4b75-99aa-611fd7fffcb1'
I0123 18:50:11.982770 15706 slave.cpp:1912] Got registration for executor 'default' of framework 20150123-185011-16777343-37526-15688-0000 from executor(50)@127.0.0.1:37526
I0123 18:50:11.983203 15706 slave.cpp:2031] Flushing queued task 1 for executor 'default' of framework 20150123-185011-16777343-37526-15688-0000
I0123 18:50:11.983505 15706 slave.cpp:2890] Monitoring executor 'default' of framework '20150123-185011-16777343-37526-15688-0000' in container '02536e4f-fb59-4b75-99aa-611fd7fffcb1'
I0123 18:50:11.983749 15706 exec.cpp:221] Executor registered on slave 20150123-185011-16777343-37526-15688-S0
I0123 18:50:11.986131 15706 exec.cpp:233] Executor::registered took 30292ns
I0123 18:50:11.989857 15706 exec.cpp:308] Executor asked to run task '1'
I0123 18:50:11.990216 15706 exec.cpp:317] Executor::launchTask took 83992ns
I0123 18:50:11.992413 15706 exec.cpp:540] Executor sending status update TASK_RUNNING (UUID: 3887f5e3-3349-4d1d-8e18-299be6c3c294) for task 1 of framework 20150123-185011-16777343-37526-15688-0000
I0123 18:50:11.996598 15703 slave.cpp:2265] Handling status update TASK_RUNNING (UUID: 3887f5e3-3349-4d1d-8e18-299be6c3c294) for task 1 of framework 20150123-185011-16777343-37526-15688-0000 from executor(50)@127.0.0.1:37526
I0123 18:50:11.996922 15703 status_update_manager.cpp:317] Received status update TASK_RUNNING (UUID: 3887f5e3-3349-4d1d-8e18-299be6c3c294) for task 1 of framework 20150123-185011-16777343-37526-15688-0000
I0123 18:50:11.996960 15703 status_update_manager.cpp:494] Creating StatusUpdate stream for task 1 of framework 20150123-185011-16777343-37526-15688-0000
I0123 18:50:11.997187 15703 status_update_manager.cpp:371] Forwarding update TASK_RUNNING (UUID: 3887f5e3-3349-4d1d-8e18-299be6c3c294) for task 1 of framework 20150123-185011-16777343-37526-15688-0000 to the slave
I0123 18:50:11.997541 15703 slave.cpp:2508] Forwarding the update TASK_RUNNING (UUID: 3887f5e3-3349-4d1d-8e18-299be6c3c294) for task 1 of framework 20150123-185011-16777343-37526-15688-0000 to master@127.0.0.1:37526
I0123 18:50:11.997678 15703 slave.cpp:2435] Status update manager successfully handled status update TASK_RUNNING (UUID: 3887f5e3-3349-4d1d-8e18-299be6c3c294) for task 1 of framework 20150123-185011-16777343-37526-15688-0000
I0123 18:50:11.997707 15703 slave.cpp:2441] Sending acknowledgement for status update TASK_RUNNING (UUID: 3887f5e3-3349-4d1d-8e18-299be6c3c294) for task 1 of framework 20150123-185011-16777343-37526-15688-0000 to executor(50)@127.0.0.1:37526
I0123 18:50:11.997936 15703 master.cpp:3652] Forwarding status update TASK_RUNNING (UUID: 3887f5e3-3349-4d1d-8e18-299be6c3c294) for task 1 of framework 20150123-185011-16777343-37526-15688-0000
I0123 18:50:11.998054 15703 master.cpp:3624] Status update TASK_RUNNING (UUID: 3887f5e3-3349-4d1d-8e18-299be6c3c294) for task 1 of framework 20150123-185011-16777343-37526-15688-0000 from slave 20150123-185011-16777343-37526-15688-S0 at slave(94)@127.0.0.1:37526 (localhost.localdomain)
I0123 18:50:11.998106 15703 master.cpp:4934] Updating the latest state of task 1 of framework 20150123-185011-16777343-37526-15688-0000 to TASK_RUNNING
I0123 18:50:11.998301 15703 sched.cpp:696] Scheduler::statusUpdate took 54363ns
I0123 18:50:11.998615 15707 master.cpp:3125] Forwarding status update acknowledgement 3887f5e3-3349-4d1d-8e18-299be6c3c294 for task 1 of framework 20150123-185011-16777343-37526-15688-0000 (default) at scheduler-2cecb105-ca23-4048-9707-12b1e4422e11@127.0.0.1:37526 to slave 20150123-185011-16777343-37526-15688-S0 at slave(94)@127.0.0.1:37526 (localhost.localdomain)
I0123 18:50:11.998867 15707 status_update_manager.cpp:389] Received status update acknowledgement (UUID: 3887f5e3-3349-4d1d-8e18-299be6c3c294) for task 1 of framework 20150123-185011-16777343-37526-15688-0000
I0123 18:50:11.999047 15707 slave.cpp:1852] Status update manager successfully handled status update acknowledgement (UUID: 3887f5e3-3349-4d1d-8e18-299be6c3c294) for task 1 of framework 20150123-185011-16777343-37526-15688-0000
W0123 18:50:12.001930 15688 sched.cpp:1246] 
**************************************************
Scheduler driver bound to loopback interface! Cannot communicate with remote master(s). You might want to set 'LIBPROCESS_IP' environment variable to use a routable IP address.
**************************************************
I0123 18:50:12.006674 15706 exec.cpp:354] Executor received status update acknowledgement 3887f5e3-3349-4d1d-8e18-299be6c3c294 for task 1 of framework 20150123-185011-16777343-37526-15688-0000
I0123 18:50:12.015889 15688 sched.cpp:151] Version: 0.22.0
I0123 18:50:12.017143 15706 sched.cpp:248] New master detected at master@127.0.0.1:37526
I0123 18:50:12.017241 15706 sched.cpp:304] Authenticating with master master@127.0.0.1:37526
I0123 18:50:12.017264 15706 sched.cpp:311] Using default CRAM-MD5 authenticatee
I0123 18:50:12.017680 15710 authenticatee.hpp:138] Creating new client SASL connection
I0123 18:50:12.018093 15710 master.cpp:4129] Authenticating scheduler-9b22c538-3b80-4309-80bd-e4c06956dd3e@127.0.0.1:37526
I0123 18:50:12.018129 15710 master.cpp:4140] Using default CRAM-MD5 authenticator
I0123 18:50:12.018590 15710 authenticator.hpp:170] Creating new server SASL connection
I0123 18:50:12.018904 15710 authenticatee.hpp:229] Received SASL authentication mechanisms: CRAM-MD5
I0123 18:50:12.018934 15710 authenticatee.hpp:255] Attempting to authenticate with mechanism 'CRAM-MD5'
I0123 18:50:12.019039 15710 authenticator.hpp:276] Received SASL authentication start
I0123 18:50:12.019101 15710 authenticator.hpp:398] Authentication requires more steps
I0123 18:50:12.019172 15710 authenticatee.hpp:275] Received SASL authentication step
I0123 18:50:12.019273 15710 authenticator.hpp:304] Received SASL authentication step
I0123 18:50:12.019304 15710 auxprop.cpp:99] Request to lookup properties for user: 'test-principal' realm: 'localhost.localdomain' server FQDN: 'localhost.localdomain' SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0123 18:50:12.019316 15710 auxprop.cpp:171] Looking up auxiliary property '*userPassword'
I0123 18:50:12.019364 15710 auxprop.cpp:171] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0123 18:50:12.020604 15710 auxprop.cpp:99] Request to lookup properties for user: 'test-principal' realm: 'localhost.localdomain' server FQDN: 'localhost.localdomain' SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0123 18:50:12.020859 15710 auxprop.cpp:121] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0123 18:50:12.021114 15710 auxprop.cpp:121] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0123 18:50:12.021402 15710 authenticator.hpp:390] Authentication success
I0123 18:50:12.021790 15705 authenticatee.hpp:315] Authentication success
I0123 18:50:12.029628 15705 sched.cpp:392] Successfully authenticated with master master@127.0.0.1:37526
I0123 18:50:12.029682 15705 sched.cpp:515] Sending registration request to master@127.0.0.1:37526
I0123 18:50:12.029784 15705 sched.cpp:548] Will retry registration in 371.903559ms if necessary
I0123 18:50:12.030015 15705 master.cpp:1525] Queuing up re-registration request for framework 20150123-185011-16777343-37526-15688-0000 (default) at scheduler-9b22c538-3b80-4309-80bd-e4c06956dd3e@127.0.0.1:37526 because authentication is still in progress
I0123 18:50:12.030215 15710 master.cpp:4187] Successfully authenticated principal 'test-principal' at scheduler-9b22c538-3b80-4309-80bd-e4c06956dd3e@127.0.0.1:37526
I0123 18:50:12.030539 15710 master.cpp:1557] Received re-registration request from framework 20150123-185011-16777343-37526-15688-0000 (default) at scheduler-9b22c538-3b80-4309-80bd-e4c06956dd3e@127.0.0.1:37526
I0123 18:50:12.030618 15710 master.cpp:1298] Authorizing framework principal 'test-principal' to receive offers for role '*'
I0123 18:50:12.031014 15710 master.cpp:1610] Re-registering framework 20150123-185011-16777343-37526-15688-0000 (default)  at scheduler-9b22c538-3b80-4309-80bd-e4c06956dd3e@127.0.0.1:37526
I0123 18:50:12.031060 15710 master.cpp:1639] Framework 20150123-185011-16777343-37526-15688-0000 (default) at scheduler-2cecb105-ca23-4048-9707-12b1e4422e11@127.0.0.1:37526 failed over
I0123 18:50:12.031723 15703 sched.cpp:442] Framework registered with 20150123-185011-16777343-37526-15688-0000
I0123 18:50:12.031841 15703 sched.cpp:456] Scheduler::registered took 54566ns
I0123 18:50:12.032662 15709 slave.cpp:1762] Updating framework 20150123-185011-16777343-37526-15688-0000 pid to scheduler-9b22c538-3b80-4309-80bd-e4c06956dd3e@127.0.0.1:37526
I0123 18:50:12.032924 15709 status_update_manager.cpp:178] Resuming sending status updates
I0123 18:50:12.034113 15703 slave.cpp:2571] Sending message for framework 20150123-185011-16777343-37526-15688-0000 to scheduler-9b22c538-3b80-4309-80bd-e4c06956dd3e@127.0.0.1:37526
I0123 18:50:12.034302 15703 sched.cpp:782] Scheduler::frameworkMessage took 53684ns
I0123 18:50:12.034771 15688 sched.cpp:1471] Asked to stop the driver
I0123 18:50:12.034864 15688 sched.cpp:1471] Asked to stop the driver
I0123 18:50:12.034942 15688 master.cpp:654] Master terminating
W0123 18:50:12.035094 15688 master.cpp:4979] Removing task 1 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] of framework 20150123-185011-16777343-37526-15688-0000 on slave 20150123-185011-16777343-37526-15688-S0 at slave(94)@127.0.0.1:37526 (localhost.localdomain) in non-terminal state TASK_RUNNING
I0123 18:50:12.035724 15688 master.cpp:5022] Removing executor 'default' with resources  of framework 20150123-185011-16777343-37526-15688-0000 on slave 20150123-185011-16777343-37526-15688-S0 at slave(94)@127.0.0.1:37526 (localhost.localdomain)
I0123 18:50:12.036705 15709 sched.cpp:808] Stopping framework '20150123-185011-16777343-37526-15688-0000'
I0123 18:50:12.036960 15709 hierarchical_allocator_process.hpp:653] Recovered cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (total allocatable: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]) on slave 20150123-185011-16777343-37526-15688-S0 from framework 20150123-185011-16777343-37526-15688-0000
I0123 18:50:12.037048 15709 slave.cpp:2673] master@127.0.0.1:37526 exited
W0123 18:50:12.037071 15709 slave.cpp:2676] Master disconnected! Waiting for a new master to be elected
I0123 18:50:12.037359 15710 sched.cpp:788] Ignoring error message because the driver is not running!
I0123 18:50:12.037513 15710 sched.cpp:808] Stopping framework '20150123-185011-16777343-37526-15688-0000'
I0123 18:50:12.076481 15688 slave.cpp:495] Slave terminating
I0123 18:50:12.080759 15688 slave.cpp:1585] Asked to shut down framework 20150123-185011-16777343-37526-15688-0000 by @0.0.0.0:0
I0123 18:50:12.081023 15688 slave.cpp:1610] Shutting down framework 20150123-185011-16777343-37526-15688-0000
I0123 18:50:12.081351 15688 slave.cpp:3198] Shutting down executor 'default' of framework 20150123-185011-16777343-37526-15688-0000
tests/fault_tolerance_tests.cpp:1383: Failure
Actual function call count doesn't match EXPECT_CALL(sched1, error(&driver1, ""Framework failed over""))...
         Expected: to be called once
           Actual: never called - unsatisfied and active
[  FAILED  ] FaultToleranceTest.SchedulerFailoverFrameworkMessage (481 ms)
{noformat}

Good Run:
{noformat}
[ RUN      ] FaultToleranceTest.SchedulerFailoverFrameworkMessage
Using temporary directory '/tmp/FaultToleranceTest_SchedulerFailoverFrameworkMessage_hEc3n7'
I0122 19:15:01.356081  3518 leveldb.cpp:176] Opened db in 19.797885ms
I0122 19:15:01.362119  3518 leveldb.cpp:183] Compacted db in 5.953605ms
I0122 19:15:01.362191  3518 leveldb.cpp:198] Created db iterator in 30691ns
I0122 19:15:01.362210  3518 leveldb.cpp:204] Seeked to beginning of db in 2240ns
I0122 19:15:01.362221  3518 leveldb.cpp:273] Iterated through 0 keys in the db in 517ns
I0122 19:15:01.362295  3518 replica.cpp:744] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned
I0122 19:15:01.364575  3534 recover.cpp:449] Starting replica recovery
I0122 19:15:01.365314  3534 recover.cpp:475] Replica is in EMPTY status
I0122 19:15:01.389731  3534 replica.cpp:641] Replica in EMPTY status received a broadcasted recover request
I0122 19:15:01.390005  3534 recover.cpp:195] Received a recover response from a replica in EMPTY status
I0122 19:15:01.391346  3538 recover.cpp:566] Updating replica status to STARTING
I0122 19:15:01.403445  3538 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 11.806565ms
I0122 19:15:01.403795  3538 replica.cpp:323] Persisted replica status to STARTING
I0122 19:15:01.406898  3538 recover.cpp:475] Replica is in STARTING status
I0122 19:15:01.408671  3537 replica.cpp:641] Replica in STARTING status received a broadcasted recover request
I0122 19:15:01.413719  3537 recover.cpp:195] Received a recover response from a replica in STARTING status
I0122 19:15:01.419553  3538 recover.cpp:566] Updating replica status to VOTING
I0122 19:15:01.426426  3536 master.cpp:262] Master 20150122-191501-16777343-50172-3518 (localhost.localdomain) started on 127.0.0.1:50172
W0122 19:15:01.426473  3536 master.cpp:266] 
**************************************************
Master bound to loopback interface! Cannot communicate with remote schedulers or slaves. You might want to set '--ip' flag to a routable IP address.
**************************************************
I0122 19:15:01.426519  3536 master.cpp:308] Master only allowing authenticated frameworks to register
I0122 19:15:01.426532  3536 master.cpp:313] Master only allowing authenticated slaves to register
I0122 19:15:01.426564  3536 credentials.hpp:36] Loading credentials for authentication from '/tmp/FaultToleranceTest_SchedulerFailoverFrameworkMessage_hEc3n7/credentials'
I0122 19:15:01.426841  3536 master.cpp:357] Authorization enabled
I0122 19:15:01.428205  3533 hierarchical_allocator_process.hpp:285] Initialized hierarchical allocator process
I0122 19:15:01.428627  3534 whitelist_watcher.cpp:65] No whitelist given
I0122 19:15:01.429839  3538 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 6.340373ms
I0122 19:15:01.429879  3538 replica.cpp:323] Persisted replica status to VOTING
I0122 19:15:01.430024  3538 recover.cpp:580] Successfully joined the Paxos group
I0122 19:15:01.430233  3538 recover.cpp:464] Recover process terminated
I0122 19:15:01.432348  3536 master.cpp:1219] The newly elected leader is master@127.0.0.1:50172 with id 20150122-191501-16777343-50172-3518
I0122 19:15:01.436343  3536 master.cpp:1232] Elected as the leading master!
I0122 19:15:01.436738  3536 master.cpp:1050] Recovering from registrar
I0122 19:15:01.437191  3535 registrar.cpp:313] Recovering registrar
I0122 19:15:01.438340  3535 log.cpp:660] Attempting to start the writer
I0122 19:15:01.440163  3533 replica.cpp:477] Replica received implicit promise request with proposal 1
I0122 19:15:01.445287  3533 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 4.550707ms
I0122 19:15:01.445327  3533 replica.cpp:345] Persisted promised to 1
I0122 19:15:01.446691  3537 coordinator.cpp:230] Coordinator attemping to fill missing position
I0122 19:15:01.448724  3537 replica.cpp:378] Replica received explicit promise request for position 0 with proposal 2
I0122 19:15:01.453824  3537 leveldb.cpp:343] Persisting action (8 bytes) to leveldb took 4.787009ms
I0122 19:15:01.453860  3537 replica.cpp:679] Persisted action at 0
I0122 19:15:01.455684  3533 replica.cpp:511] Replica received write request for position 0
I0122 19:15:01.456087  3533 leveldb.cpp:438] Reading position from leveldb took 37133ns
I0122 19:15:01.460862  3533 leveldb.cpp:343] Persisting action (14 bytes) to leveldb took 4.458448ms
I0122 19:15:01.460897  3533 replica.cpp:679] Persisted action at 0
I0122 19:15:01.461601  3533 replica.cpp:658] Replica received learned notice for position 0
I0122 19:15:01.466660  3533 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 5.028194ms
I0122 19:15:01.466696  3533 replica.cpp:679] Persisted action at 0
I0122 19:15:01.466718  3533 replica.cpp:664] Replica learned NOP action at position 0
I0122 19:15:01.467931  3537 log.cpp:676] Writer started with ending position 0
I0122 19:15:01.469182  3537 leveldb.cpp:438] Reading position from leveldb took 32199ns
I0122 19:15:01.479857  3537 registrar.cpp:346] Successfully fetched the registry (0B) in 42.6048ms
I0122 19:15:01.480340  3537 registrar.cpp:445] Applied 1 operations in 42179ns; attempting to update the 'registry'
I0122 19:15:01.484465  3535 log.cpp:684] Attempting to append 134 bytes to the log
I0122 19:15:01.484661  3535 coordinator.cpp:340] Coordinator attempting to write APPEND action at position 1
I0122 19:15:01.486250  3535 replica.cpp:511] Replica received write request for position 1
I0122 19:15:01.491243  3535 leveldb.cpp:343] Persisting action (153 bytes) to leveldb took 4.582496ms
I0122 19:15:01.491296  3535 replica.cpp:679] Persisted action at 1
I0122 19:15:01.492647  3539 replica.cpp:658] Replica received learned notice for position 1
I0122 19:15:01.497707  3539 leveldb.cpp:343] Persisting action (155 bytes) to leveldb took 5.027112ms
I0122 19:15:01.497743  3539 replica.cpp:679] Persisted action at 1
I0122 19:15:01.497767  3539 replica.cpp:664] Replica learned APPEND action at position 1
I0122 19:15:01.499428  3539 registrar.cpp:490] Successfully updated the 'registry' in 18.743808ms
I0122 19:15:01.499609  3535 log.cpp:703] Attempting to truncate the log to 1
I0122 19:15:01.500036  3535 coordinator.cpp:340] Coordinator attempting to write TRUNCATE action at position 2
I0122 19:15:01.501029  3535 replica.cpp:511] Replica received write request for position 2
I0122 19:15:01.501694  3539 registrar.cpp:376] Successfully recovered registrar
I0122 19:15:01.502358  3536 master.cpp:1077] Recovered 0 slaves from the Registry (97B) ; allowing 10mins for slaves to re-register
I0122 19:15:01.514142  3535 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 13.074759ms
I0122 19:15:01.514189  3535 replica.cpp:679] Persisted action at 2
I0122 19:15:01.515473  3535 replica.cpp:658] Replica received learned notice for position 2
I0122 19:15:01.527171  3535 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 11.064363ms
I0122 19:15:01.527456  3535 leveldb.cpp:401] Deleting ~1 keys from leveldb took 118227ns
I0122 19:15:01.527495  3535 replica.cpp:679] Persisted action at 2
I0122 19:15:01.527537  3535 replica.cpp:664] Replica learned TRUNCATE action at position 2
I0122 19:15:01.563684  3538 slave.cpp:173] Slave started on 76)@127.0.0.1:50172
W0122 19:15:01.563736  3538 slave.cpp:176] 
**************************************************
Slave bound to loopback interface! Cannot communicate with remote master(s). You might want to set '--ip' flag to a routable IP address.
**************************************************
I0122 19:15:01.563751  3538 credentials.hpp:84] Loading credential for authentication from '/tmp/FaultToleranceTest_SchedulerFailoverFrameworkMessage_XZCq6R/credential'
I0122 19:15:01.563921  3538 slave.cpp:282] Slave using credential for: test-principal
I0122 19:15:01.564209  3538 slave.cpp:300] Slave resources: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I0122 19:15:01.564327  3538 slave.cpp:329] Slave hostname: localhost.localdomain
I0122 19:15:01.564348  3538 slave.cpp:330] Slave checkpoint: false
W0122 19:15:01.564357  3538 slave.cpp:332] Disabling checkpointing is deprecated and the --checkpoint flag will be removed in a future release. Please avoid using this flag
I0122 19:15:01.566193  3537 state.cpp:33] Recovering state from '/tmp/FaultToleranceTest_SchedulerFailoverFrameworkMessage_XZCq6R/meta'
I0122 19:15:01.566629  3537 status_update_manager.cpp:197] Recovering status update manager
I0122 19:15:01.566962  3537 slave.cpp:3519] Finished recovery
I0122 19:15:01.571022  3533 status_update_manager.cpp:171] Pausing sending status updates
I0122 19:15:01.571466  3537 slave.cpp:613] New master detected at master@127.0.0.1:50172
I0122 19:15:01.573503  3537 slave.cpp:676] Authenticating with master master@127.0.0.1:50172
I0122 19:15:01.573771  3537 slave.cpp:681] Using default CRAM-MD5 authenticatee
I0122 19:15:01.574427  3540 authenticatee.hpp:138] Creating new client SASL connection
I0122 19:15:01.574857  3535 master.cpp:4129] Authenticating slave(76)@127.0.0.1:50172
I0122 19:15:01.574893  3535 master.cpp:4140] Using default CRAM-MD5 authenticator
W0122 19:15:01.575266  3518 sched.cpp:1246] 
**************************************************
Scheduler driver bound to loopback interface! Cannot communicate with remote master(s). You might want to set 'LIBPROCESS_IP' environment variable to use a routable IP address.
**************************************************
I0122 19:15:01.576125  3535 authenticator.hpp:170] Creating new server SASL connection
I0122 19:15:01.576526  3535 authenticatee.hpp:229] Received SASL authentication mechanisms: CRAM-MD5
I0122 19:15:01.576563  3535 authenticatee.hpp:255] Attempting to authenticate with mechanism 'CRAM-MD5'
I0122 19:15:01.576671  3535 authenticator.hpp:276] Received SASL authentication start
I0122 19:15:01.576740  3535 authenticator.hpp:398] Authentication requires more steps
I0122 19:15:01.576812  3535 authenticatee.hpp:275] Received SASL authentication step
I0122 19:15:01.576915  3535 authenticator.hpp:304] Received SASL authentication step
I0122 19:15:01.576943  3535 auxprop.cpp:99] Request to lookup properties for user: 'test-principal' realm: 'localhost.localdomain' server FQDN: 'localhost.localdomain' SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0122 19:15:01.576967  3535 auxprop.cpp:171] Looking up auxiliary property '*userPassword'
I0122 19:15:01.577026  3535 auxprop.cpp:171] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0122 19:15:01.577061  3535 auxprop.cpp:99] Request to lookup properties for user: 'test-principal' realm: 'localhost.localdomain' server FQDN: 'localhost.localdomain' SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0122 19:15:01.577076  3535 auxprop.cpp:121] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0122 19:15:01.577085  3535 auxprop.cpp:121] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0122 19:15:01.577101  3535 authenticator.hpp:390] Authentication success
I0122 19:15:01.577209  3535 authenticatee.hpp:315] Authentication success
I0122 19:15:01.577304  3535 master.cpp:4187] Successfully authenticated principal 'test-principal' at slave(76)@127.0.0.1:50172
I0122 19:15:01.577615  3537 slave.cpp:649] Detecting new master
I0122 19:15:01.580585  3537 slave.cpp:747] Successfully authenticated with master master@127.0.0.1:50172
I0122 19:15:01.585831  3537 slave.cpp:1075] Will retry registration in 18.23486ms if necessary
I0122 19:15:01.588697  3535 master.cpp:3275] Registering slave at slave(76)@127.0.0.1:50172 (localhost.localdomain) with id 20150122-191501-16777343-50172-3518-S0
I0122 19:15:01.589609  3539 registrar.cpp:445] Applied 1 operations in 117629ns; attempting to update the 'registry'
I0122 19:15:01.592538  3536 log.cpp:684] Attempting to append 313 bytes to the log
I0122 19:15:01.592766  3536 coordinator.cpp:340] Coordinator attempting to write APPEND action at position 3
I0122 19:15:01.594276  3536 replica.cpp:511] Replica received write request for position 3
I0122 19:15:01.599052  3518 sched.cpp:151] Version: 0.22.0
I0122 19:15:01.600783  3533 sched.cpp:248] New master detected at master@127.0.0.1:50172
I0122 19:15:01.600873  3533 sched.cpp:304] Authenticating with master master@127.0.0.1:50172
I0122 19:15:01.600896  3533 sched.cpp:311] Using default CRAM-MD5 authenticatee
I0122 19:15:01.601238  3533 authenticatee.hpp:138] Creating new client SASL connection
I0122 19:15:01.601773  3534 master.cpp:4129] Authenticating scheduler-1491b8db-aae5-47fb-b234-d44c2f509ec0@127.0.0.1:50172
I0122 19:15:01.601809  3534 master.cpp:4140] Using default CRAM-MD5 authenticator
I0122 19:15:01.602197  3534 authenticator.hpp:170] Creating new server SASL connection
I0122 19:15:01.602519  3534 authenticatee.hpp:229] Received SASL authentication mechanisms: CRAM-MD5
I0122 19:15:01.602548  3534 authenticatee.hpp:255] Attempting to authenticate with mechanism 'CRAM-MD5'
I0122 19:15:01.602651  3534 authenticator.hpp:276] Received SASL authentication start
I0122 19:15:01.602705  3534 authenticator.hpp:398] Authentication requires more steps
I0122 19:15:01.602774  3534 authenticatee.hpp:275] Received SASL authentication step
I0122 19:15:01.602854  3534 authenticator.hpp:304] Received SASL authentication step
I0122 19:15:01.602881  3534 auxprop.cpp:99] Request to lookup properties for user: 'test-principal' realm: 'localhost.localdomain' server FQDN: 'localhost.localdomain' SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0122 19:15:01.602895  3534 auxprop.cpp:171] Looking up auxiliary property '*userPassword'
I0122 19:15:01.602936  3534 auxprop.cpp:171] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0122 19:15:01.602960  3534 auxprop.cpp:99] Request to lookup properties for user: 'test-principal' realm: 'localhost.localdomain' server FQDN: 'localhost.localdomain' SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0122 19:15:01.602973  3534 auxprop.cpp:121] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0122 19:15:01.602982  3534 auxprop.cpp:121] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0122 19:15:01.602996  3534 authenticator.hpp:390] Authentication success
I0122 19:15:01.603091  3534 authenticatee.hpp:315] Authentication success
I0122 19:15:01.603174  3534 master.cpp:4187] Successfully authenticated principal 'test-principal' at scheduler-1491b8db-aae5-47fb-b234-d44c2f509ec0@127.0.0.1:50172
I0122 19:15:01.603533  3535 sched.cpp:392] Successfully authenticated with master master@127.0.0.1:50172
I0122 19:15:01.603559  3535 sched.cpp:515] Sending registration request to master@127.0.0.1:50172
I0122 19:15:01.603626  3535 sched.cpp:548] Will retry registration in 64.334563ms if necessary
I0122 19:15:01.604531  3534 master.cpp:1420] Received registration request for framework 'default' at scheduler-1491b8db-aae5-47fb-b234-d44c2f509ec0@127.0.0.1:50172
I0122 19:15:01.604869  3536 leveldb.cpp:343] Persisting action (332 bytes) to leveldb took 10.363251ms
I0122 19:15:01.605108  3536 replica.cpp:679] Persisted action at 3
I0122 19:15:01.606084  3536 replica.cpp:658] Replica received learned notice for position 3
I0122 19:15:01.606972  3534 master.cpp:1298] Authorizing framework principal 'test-principal' to receive offers for role '*'
I0122 19:15:01.607828  3534 master.cpp:1484] Registering framework 20150122-191501-16777343-50172-3518-0000 (default) at scheduler-1491b8db-aae5-47fb-b234-d44c2f509ec0@127.0.0.1:50172
I0122 19:15:01.608331  3540 slave.cpp:1075] Will retry registration in 28.084371ms if necessary
I0122 19:15:01.610283  3539 hierarchical_allocator_process.hpp:319] Added framework 20150122-191501-16777343-50172-3518-0000
I0122 19:15:01.610349  3539 hierarchical_allocator_process.hpp:839] No resources available to allocate!
I0122 19:15:01.610391  3539 hierarchical_allocator_process.hpp:746] Performed allocation for 0 slaves in 54938ns
I0122 19:15:01.614012  3536 leveldb.cpp:343] Persisting action (334 bytes) to leveldb took 7.895962ms
I0122 19:15:01.614048  3536 replica.cpp:679] Persisted action at 3
I0122 19:15:01.614073  3536 replica.cpp:664] Replica learned APPEND action at position 3
I0122 19:15:01.615972  3536 registrar.cpp:490] Successfully updated the 'registry' in 26.294016ms
I0122 19:15:01.616164  3533 log.cpp:703] Attempting to truncate the log to 3
I0122 19:15:01.616703  3533 coordinator.cpp:340] Coordinator attempting to write TRUNCATE action at position 4
I0122 19:15:01.617676  3533 replica.cpp:511] Replica received write request for position 4
I0122 19:15:01.625704  3537 sched.cpp:442] Framework registered with 20150122-191501-16777343-50172-3518-0000
I0122 19:15:01.625782  3537 sched.cpp:456] Scheduler::registered took 45146ns
I0122 19:15:01.626240  3534 master.cpp:3263] Ignoring register slave message from slave(76)@127.0.0.1:50172 (localhost.localdomain) as admission is already in progress
I0122 19:15:01.627259  3534 master.cpp:3329] Registered slave 20150122-191501-16777343-50172-3518-S0 at slave(76)@127.0.0.1:50172 (localhost.localdomain) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I0122 19:15:01.627607  3534 hierarchical_allocator_process.hpp:453] Added slave 20150122-191501-16777343-50172-3518-S0 (localhost.localdomain) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (and cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] available)
I0122 19:15:01.628016  3534 hierarchical_allocator_process.hpp:764] Performed allocation for slave 20150122-191501-16777343-50172-3518-S0 in 361785ns
I0122 19:15:01.628105  3534 slave.cpp:781] Registered with master master@127.0.0.1:50172; given slave ID 20150122-191501-16777343-50172-3518-S0
I0122 19:15:01.628268  3534 slave.cpp:2588] Received ping from slave-observer(62)@127.0.0.1:50172
I0122 19:15:01.628720  3535 master.cpp:4071] Sending 1 offers to framework 20150122-191501-16777343-50172-3518-0000 (default) at scheduler-1491b8db-aae5-47fb-b234-d44c2f509ec0@127.0.0.1:50172
I0122 19:15:01.628861  3535 status_update_manager.cpp:178] Resuming sending status updates
I0122 19:15:01.629256  3535 sched.cpp:605] Scheduler::resourceOffers took 76294ns
I0122 19:15:01.629585  3533 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 11.873298ms
I0122 19:15:01.629623  3533 replica.cpp:679] Persisted action at 4
I0122 19:15:01.631567  3533 replica.cpp:658] Replica received learned notice for position 4
I0122 19:15:01.633208  3540 master.cpp:2677] Processing ACCEPT call for offers: [ 20150122-191501-16777343-50172-3518-O0 ] on slave 20150122-191501-16777343-50172-3518-S0 at slave(76)@127.0.0.1:50172 (localhost.localdomain) for framework 20150122-191501-16777343-50172-3518-0000 (default) at scheduler-1491b8db-aae5-47fb-b234-d44c2f509ec0@127.0.0.1:50172
I0122 19:15:01.633386  3540 master.cpp:2513] Authorizing framework principal 'test-principal' to launch task 1 as user 'jenkins'
W0122 19:15:01.635479  3540 master.cpp:2130] Executor default for task 1 uses less CPUs (None) than the minimum required (0.01). Please update your executor, as this will be mandatory in future releases.
W0122 19:15:01.636101  3540 master.cpp:2142] Executor default for task 1 uses less memory (None) than the minimum required (32MB). Please update your executor, as this will be mandatory in future releases.
I0122 19:15:01.636804  3540 master.hpp:782] Adding task 1 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave 20150122-191501-16777343-50172-3518-S0 (localhost.localdomain)
I0122 19:15:01.638121  3540 master.cpp:2885] Launching task 1 of framework 20150122-191501-16777343-50172-3518-0000 (default) at scheduler-1491b8db-aae5-47fb-b234-d44c2f509ec0@127.0.0.1:50172 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave 20150122-191501-16777343-50172-3518-S0 at slave(76)@127.0.0.1:50172 (localhost.localdomain)
I0122 19:15:01.642609  3536 slave.cpp:1130] Got assigned task 1 for framework 20150122-191501-16777343-50172-3518-0000
I0122 19:15:01.643496  3536 slave.cpp:1245] Launching task 1 for framework 20150122-191501-16777343-50172-3518-0000
I0122 19:15:01.644604  3533 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 13.001968ms
I0122 19:15:01.644726  3533 leveldb.cpp:401] Deleting ~2 keys from leveldb took 61434ns
I0122 19:15:01.644752  3533 replica.cpp:679] Persisted action at 4
I0122 19:15:01.644778  3533 replica.cpp:664] Replica learned TRUNCATE action at position 4
I0122 19:15:01.648011  3536 slave.cpp:3921] Launching executor default of framework 20150122-191501-16777343-50172-3518-0000 in work directory '/tmp/FaultToleranceTest_SchedulerFailoverFrameworkMessage_XZCq6R/slaves/20150122-191501-16777343-50172-3518-S0/frameworks/20150122-191501-16777343-50172-3518-0000/executors/default/runs/c386f98d-3df4-4aee-b3ad-9e9c1ec7cc15'
I0122 19:15:01.657420  3536 exec.cpp:147] Version: 0.22.0
I0122 19:15:01.661609  3534 exec.cpp:197] Executor started at: executor(14)@127.0.0.1:50172 with pid 3518
I0122 19:15:01.662360  3536 slave.cpp:1368] Queuing task '1' for executor default of framework '20150122-191501-16777343-50172-3518-0000
I0122 19:15:01.663007  3536 slave.cpp:566] Successfully attached file '/tmp/FaultToleranceTest_SchedulerFailoverFrameworkMessage_XZCq6R/slaves/20150122-191501-16777343-50172-3518-S0/frameworks/20150122-191501-16777343-50172-3518-0000/executors/default/runs/c386f98d-3df4-4aee-b3ad-9e9c1ec7cc15'
I0122 19:15:01.665674  3536 slave.cpp:1912] Got registration for executor 'default' of framework 20150122-191501-16777343-50172-3518-0000 from executor(14)@127.0.0.1:50172
I0122 19:15:01.666738  3539 exec.cpp:221] Executor registered on slave 20150122-191501-16777343-50172-3518-S0
I0122 19:15:01.668758  3539 exec.cpp:233] Executor::registered took 76393ns
I0122 19:15:01.669208  3536 slave.cpp:2031] Flushing queued task 1 for executor 'default' of framework 20150122-191501-16777343-50172-3518-0000
I0122 19:15:01.670045  3533 exec.cpp:308] Executor asked to run task '1'
I0122 19:15:01.670194  3533 exec.cpp:317] Executor::launchTask took 118431ns
I0122 19:15:01.670605  3536 slave.cpp:2890] Monitoring executor 'default' of framework '20150122-191501-16777343-50172-3518-0000' in container 'c386f98d-3df4-4aee-b3ad-9e9c1ec7cc15'
I0122 19:15:01.673183  3533 exec.cpp:540] Executor sending status update TASK_RUNNING (UUID: 2150029d-e89c-40a6-998f-c0295d72d964) for task 1 of framework 20150122-191501-16777343-50172-3518-0000
I0122 19:15:01.675230  3534 slave.cpp:2265] Handling status update TASK_RUNNING (UUID: 2150029d-e89c-40a6-998f-c0295d72d964) for task 1 of framework 20150122-191501-16777343-50172-3518-0000 from executor(14)@127.0.0.1:50172
I0122 19:15:01.675647  3534 status_update_manager.cpp:317] Received status update TASK_RUNNING (UUID: 2150029d-e89c-40a6-998f-c0295d72d964) for task 1 of framework 20150122-191501-16777343-50172-3518-0000
I0122 19:15:01.675689  3534 status_update_manager.cpp:494] Creating StatusUpdate stream for task 1 of framework 20150122-191501-16777343-50172-3518-0000
I0122 19:15:01.675981  3534 status_update_manager.cpp:371] Forwarding update TASK_RUNNING (UUID: 2150029d-e89c-40a6-998f-c0295d72d964) for task 1 of framework 20150122-191501-16777343-50172-3518-0000 to the slave
I0122 19:15:01.676338  3534 slave.cpp:2508] Forwarding the update TASK_RUNNING (UUID: 2150029d-e89c-40a6-998f-c0295d72d964) for task 1 of framework 20150122-191501-16777343-50172-3518-0000 to master@127.0.0.1:50172
I0122 19:15:01.676910  3538 master.cpp:3652] Forwarding status update TASK_RUNNING (UUID: 2150029d-e89c-40a6-998f-c0295d72d964) for task 1 of framework 20150122-191501-16777343-50172-3518-0000
I0122 19:15:01.677034  3538 master.cpp:3624] Status update TASK_RUNNING (UUID: 2150029d-e89c-40a6-998f-c0295d72d964) for task 1 of framework 20150122-191501-16777343-50172-3518-0000 from slave 20150122-191501-16777343-50172-3518-S0 at slave(76)@127.0.0.1:50172 (localhost.localdomain)
I0122 19:15:01.677098  3538 master.cpp:4934] Updating the latest state of task 1 of framework 20150122-191501-16777343-50172-3518-0000 to TASK_RUNNING
I0122 19:15:01.677338  3538 sched.cpp:696] Scheduler::statusUpdate took 71579ns
I0122 19:15:01.677701  3538 master.cpp:3125] Forwarding status update acknowledgement 2150029d-e89c-40a6-998f-c0295d72d964 for task 1 of framework 20150122-191501-16777343-50172-3518-0000 (default) at scheduler-1491b8db-aae5-47fb-b234-d44c2f509ec0@127.0.0.1:50172 to slave 20150122-191501-16777343-50172-3518-S0 at slave(76)@127.0.0.1:50172 (localhost.localdomain)
W0122 19:15:01.680450  3518 sched.cpp:1246] 
**************************************************
Scheduler driver bound to loopback interface! Cannot communicate with remote master(s). You might want to set 'LIBPROCESS_IP' environment variable to use a routable IP address.
**************************************************
I0122 19:15:01.684923  3534 slave.cpp:2435] Status update manager successfully handled status update TASK_RUNNING (UUID: 2150029d-e89c-40a6-998f-c0295d72d964) for task 1 of framework 20150122-191501-16777343-50172-3518-0000
I0122 19:15:01.685042  3534 slave.cpp:2441] Sending acknowledgement for status update TASK_RUNNING (UUID: 2150029d-e89c-40a6-998f-c0295d72d964) for task 1 of framework 20150122-191501-16777343-50172-3518-0000 to executor(14)@127.0.0.1:50172
I0122 19:15:01.687777  3534 exec.cpp:354] Executor received status update acknowledgement 2150029d-e89c-40a6-998f-c0295d72d964 for task 1 of framework 20150122-191501-16777343-50172-3518-0000
I0122 19:15:01.687896  3537 status_update_manager.cpp:389] Received status update acknowledgement (UUID: 2150029d-e89c-40a6-998f-c0295d72d964) for task 1 of framework 20150122-191501-16777343-50172-3518-0000
I0122 19:15:01.688174  3537 slave.cpp:1852] Status update manager successfully handled status update acknowledgement (UUID: 2150029d-e89c-40a6-998f-c0295d72d964) for task 1 of framework 20150122-191501-16777343-50172-3518-0000
I0122 19:15:01.707738  3518 sched.cpp:151] Version: 0.22.0
I0122 19:15:01.708892  3540 sched.cpp:248] New master detected at master@127.0.0.1:50172
I0122 19:15:01.708973  3540 sched.cpp:304] Authenticating with master master@127.0.0.1:50172
I0122 19:15:01.708997  3540 sched.cpp:311] Using default CRAM-MD5 authenticatee
I0122 19:15:01.709345  3540 authenticatee.hpp:138] Creating new client SASL connection
I0122 19:15:01.710389  3534 master.cpp:4129] Authenticating scheduler-ece17f18-6f5c-4807-8204-35771496dd9f@127.0.0.1:50172
I0122 19:15:01.710440  3534 master.cpp:4140] Using default CRAM-MD5 authenticator
I0122 19:15:01.710844  3534 authenticator.hpp:170] Creating new server SASL connection
I0122 19:15:01.711359  3540 authenticatee.hpp:229] Received SASL authentication mechanisms: CRAM-MD5
I0122 19:15:01.711762  3540 authenticatee.hpp:255] Attempting to authenticate with mechanism 'CRAM-MD5'
I0122 19:15:01.712133  3540 authenticator.hpp:276] Received SASL authentication start
I0122 19:15:01.712434  3540 authenticator.hpp:398] Authentication requires more steps
I0122 19:15:01.712754  3540 authenticatee.hpp:275] Received SASL authentication step
I0122 19:15:01.713156  3536 authenticator.hpp:304] Received SASL authentication step
I0122 19:15:01.713191  3536 auxprop.cpp:99] Request to lookup properties for user: 'test-principal' realm: 'localhost.localdomain' server FQDN: 'localhost.localdomain' SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0122 19:15:01.713204  3536 auxprop.cpp:171] Looking up auxiliary property '*userPassword'
I0122 19:15:01.713263  3536 auxprop.cpp:171] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0122 19:15:01.713290  3536 auxprop.cpp:99] Request to lookup properties for user: 'test-principal' realm: 'localhost.localdomain' server FQDN: 'localhost.localdomain' SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0122 19:15:01.713304  3536 auxprop.cpp:121] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0122 19:15:01.713311  3536 auxprop.cpp:121] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0122 19:15:01.713326  3536 authenticator.hpp:390] Authentication success
I0122 19:15:01.713470  3536 master.cpp:4187] Successfully authenticated principal 'test-principal' at scheduler-ece17f18-6f5c-4807-8204-35771496dd9f@127.0.0.1:50172
I0122 19:15:01.716526  3540 authenticatee.hpp:315] Authentication success
I0122 19:15:01.720747  3540 sched.cpp:392] Successfully authenticated with master master@127.0.0.1:50172
I0122 19:15:01.720780  3540 sched.cpp:515] Sending registration request to master@127.0.0.1:50172
I0122 19:15:01.720852  3540 sched.cpp:548] Will retry registration in 1.20284193secs if necessary
I0122 19:15:01.721050  3540 master.cpp:1557] Received re-registration request from framework 20150122-191501-16777343-50172-3518-0000 (default) at scheduler-ece17f18-6f5c-4807-8204-35771496dd9f@127.0.0.1:50172
I0122 19:15:01.721143  3540 master.cpp:1298] Authorizing framework principal 'test-principal' to receive offers for role '*'
I0122 19:15:01.721583  3540 master.cpp:1610] Re-registering framework 20150122-191501-16777343-50172-3518-0000 (default)  at scheduler-ece17f18-6f5c-4807-8204-35771496dd9f@127.0.0.1:50172
I0122 19:15:01.721629  3540 master.cpp:1639] Framework 20150122-191501-16777343-50172-3518-0000 (default) at scheduler-1491b8db-aae5-47fb-b234-d44c2f509ec0@127.0.0.1:50172 failed over
I0122 19:15:01.721943  3540 sched.cpp:792] Got error 'Framework failed over'
I0122 19:15:01.721972  3540 sched.cpp:1505] Asked to abort the driver
I0122 19:15:01.722039  3540 sched.cpp:803] Scheduler::error took 26469ns
I0122 19:15:01.722084  3540 sched.cpp:833] Aborting framework '20150122-191501-16777343-50172-3518-0000'
I0122 19:15:01.722196  3540 sched.cpp:442] Framework registered with 20150122-191501-16777343-50172-3518-0000
I0122 19:15:01.722262  3540 sched.cpp:456] Scheduler::registered took 40517ns
W0122 19:15:01.722734  3538 master.cpp:1775] Ignoring deactivate framework message for framework 20150122-191501-16777343-50172-3518-0000 (default) at scheduler-ece17f18-6f5c-4807-8204-35771496dd9f@127.0.0.1:50172 because it is not expected from scheduler-1491b8db-aae5-47fb-b234-d44c2f509ec0@127.0.0.1:50172
I0122 19:15:01.724819  3540 slave.cpp:1762] Updating framework 20150122-191501-16777343-50172-3518-0000 pid to scheduler-ece17f18-6f5c-4807-8204-35771496dd9f@127.0.0.1:50172
I0122 19:15:01.725316  3533 status_update_manager.cpp:178] Resuming sending status updates
I0122 19:15:01.726033  3540 slave.cpp:2571] Sending message for framework 20150122-191501-16777343-50172-3518-0000 to scheduler-ece17f18-6f5c-4807-8204-35771496dd9f@127.0.0.1:50172
I0122 19:15:01.727016  3534 sched.cpp:782] Scheduler::frameworkMessage took 57233ns
I0122 19:15:01.727601  3518 sched.cpp:1471] Asked to stop the driver
I0122 19:15:01.727665  3518 sched.cpp:1471] Asked to stop the driver
I0122 19:15:01.727743  3518 master.cpp:654] Master terminating
W0122 19:15:01.727893  3518 master.cpp:4979] Removing task 1 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] of framework 20150122-191501-16777343-50172-3518-0000 on slave 20150122-191501-16777343-50172-3518-S0 at slave(76)@127.0.0.1:50172 (localhost.localdomain) in non-terminal state TASK_RUNNING
I0122 19:15:01.728521  3518 master.cpp:5022] Removing executor 'default' with resources  of framework 20150122-191501-16777343-50172-3518-0000 on slave 20150122-191501-16777343-50172-3518-S0 at slave(76)@127.0.0.1:50172 (localhost.localdomain)
I0122 19:15:01.729651  3534 sched.cpp:808] Stopping framework '20150122-191501-16777343-50172-3518-0000'
I0122 19:15:01.729786  3534 sched.cpp:808] Stopping framework '20150122-191501-16777343-50172-3518-0000'
I0122 19:15:01.730036  3534 hierarchical_allocator_process.hpp:653] Recovered cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (total allocatable: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]) on slave 20150122-191501-16777343-50172-3518-S0 from framework 20150122-191501-16777343-50172-3518-0000
I0122 19:15:01.730134  3534 slave.cpp:2673] master@127.0.0.1:50172 exited
W0122 19:15:01.730156  3534 slave.cpp:2676] Master disconnected! Waiting for a new master to be elected
I0122 19:15:01.782312  3518 slave.cpp:495] Slave terminating
I0122 19:15:01.786846  3518 slave.cpp:1585] Asked to shut down framework 20150122-191501-16777343-50172-3518-0000 by @0.0.0.0:0
I0122 19:15:01.787127  3518 slave.cpp:1610] Shutting down framework 20150122-191501-16777343-50172-3518-0000
I0122 19:15:01.787394  3518 slave.cpp:3198] Shutting down executor 'default' of framework 20150122-191501-16777343-50172-3518-0000
[       OK ] FaultToleranceTest.SchedulerFailoverFrameworkMessage (495 ms)
{noformat}",Bug,Major,bmahler,2015-01-30T01:29:57.000+0000,5,Resolved,Complete,FaultToleranceTest.SchedulerFailoverFrameworkMessage is flaky.,2015-01-30T01:29:57.000+0000,MESOS-2302,1.0,mesos,Twitter Mesos Q1 Sprint 1
marco-mesos,2015-01-28T23:05:59.000+0000,vinodkone,"When schedulers start interacting with Mesos master via HTTP endpoints, they need a way to detect masters. 

Mesos should provide a master detection Java library to make this easy for frameworks.",Task,Major,vinodkone,2015-08-14T23:36:46.000+0000,5,Resolved,Complete,Provide a Java library for master detection,2015-08-14T23:36:46.000+0000,MESOS-2298,5.0,mesos,
arojas,2015-01-28T23:03:29.000+0000,vinodkone,"Since most of the communication between mesos components will happen through HTTP with the arrival of the [HTTP API|https://issues.apache.org/jira/browse/MESOS-2288], it makes sense to use HTTP standard mechanisms to authenticate this communication.",Epic,Major,vinodkone,2016-04-29T09:12:19.000+0000,5,Resolved,Complete,Add authentication support for HTTP API,2016-04-29T09:12:20.000+0000,MESOS-2297,1.0,mesos,
ijimenez,2015-01-28T22:54:42.000+0000,vinodkone,"With HTTP API, the scheduler driver will no longer exist and hence all the validations should move to the master.",Task,Major,vinodkone,2015-06-10T20:55:27.000+0000,5,Resolved,Complete,Move all scheduler driver validations to master,2015-09-09T17:44:57.000+0000,MESOS-2290,3.0,mesos,Mesosphere Sprint 12
vinodkone,2015-01-28T22:53:23.000+0000,vinodkone,This tracks the design of the HTTP API.,Task,Major,vinodkone,2015-03-16T17:07:55.000+0000,5,Resolved,Complete,Design doc for the HTTP API,2015-08-03T17:42:45.000+0000,MESOS-2289,13.0,mesos,Twitter Mesos Q1 Sprint 2
bmahler,2015-01-28T02:31:40.000+0000,bmahler,"Saw this on an internal CI:

{noformat}
[ RUN      ] SlaveRecoveryTest/0.ReconcileKillTask
Using temporary directory '/tmp/SlaveRecoveryTest_0_ReconcileKillTask_D5wSwg'
I0126 19:10:52.005317 13291 leveldb.cpp:176] Opened db in 978670ns
I0126 19:10:52.006155 13291 leveldb.cpp:183] Compacted db in 541346ns
I0126 19:10:52.006494 13291 leveldb.cpp:198] Created db iterator in 24562ns
I0126 19:10:52.006798 13291 leveldb.cpp:204] Seeked to beginning of db in 3254ns
I0126 19:10:52.007036 13291 leveldb.cpp:273] Iterated through 0 keys in the db in 949ns
I0126 19:10:52.007369 13291 replica.cpp:744] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned
I0126 19:10:52.008362 13308 recover.cpp:449] Starting replica recovery
I0126 19:10:52.009141 13308 recover.cpp:475] Replica is in EMPTY status
I0126 19:10:52.016494 13308 replica.cpp:641] Replica in EMPTY status received a broadcasted recover request
I0126 19:10:52.017333 13309 recover.cpp:195] Received a recover response from a replica in EMPTY status
I0126 19:10:52.018244 13309 recover.cpp:566] Updating replica status to STARTING
I0126 19:10:52.019064 13305 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 113577ns
I0126 19:10:52.019487 13305 replica.cpp:323] Persisted replica status to STARTING
I0126 19:10:52.019937 13309 recover.cpp:475] Replica is in STARTING status
I0126 19:10:52.021492 13307 replica.cpp:641] Replica in STARTING status received a broadcasted recover request
I0126 19:10:52.022665 13309 recover.cpp:195] Received a recover response from a replica in STARTING status
I0126 19:10:52.027971 13312 recover.cpp:566] Updating replica status to VOTING
I0126 19:10:52.028590 13312 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 78452ns
I0126 19:10:52.028869 13312 replica.cpp:323] Persisted replica status to VOTING
I0126 19:10:52.029252 13312 recover.cpp:580] Successfully joined the Paxos group
I0126 19:10:52.030828 13307 recover.cpp:464] Recover process terminated
I0126 19:10:52.049947 13306 master.cpp:262] Master 20150126-191052-2272962752-35545-13291 (fedora-19) started on 192.168.122.135:35545
I0126 19:10:52.050499 13306 master.cpp:308] Master only allowing authenticated frameworks to register
I0126 19:10:52.050765 13306 master.cpp:313] Master only allowing authenticated slaves to register
I0126 19:10:52.051048 13306 credentials.hpp:36] Loading credentials for authentication from '/tmp/SlaveRecoveryTest_0_ReconcileKillTask_D5wSwg/credentials'
I0126 19:10:52.051589 13306 master.cpp:357] Authorization enabled
I0126 19:10:52.052531 13305 hierarchical_allocator_process.hpp:285] Initialized hierarchical allocator process
I0126 19:10:52.052881 13311 whitelist_watcher.cpp:65] No whitelist given
I0126 19:10:52.055524 13306 master.cpp:1219] The newly elected leader is master@192.168.122.135:35545 with id 20150126-191052-2272962752-35545-13291
I0126 19:10:52.056226 13306 master.cpp:1232] Elected as the leading master!
I0126 19:10:52.056639 13306 master.cpp:1050] Recovering from registrar
I0126 19:10:52.057045 13307 registrar.cpp:313] Recovering registrar
I0126 19:10:52.058554 13312 log.cpp:660] Attempting to start the writer
I0126 19:10:52.060868 13309 replica.cpp:477] Replica received implicit promise request with proposal 1
I0126 19:10:52.061691 13309 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 91680ns
I0126 19:10:52.062261 13309 replica.cpp:345] Persisted promised to 1
I0126 19:10:52.064559 13310 coordinator.cpp:230] Coordinator attemping to fill missing position
I0126 19:10:52.069105 13311 replica.cpp:378] Replica received explicit promise request for position 0 with proposal 2
I0126 19:10:52.069860 13311 leveldb.cpp:343] Persisting action (8 bytes) to leveldb took 94858ns
I0126 19:10:52.070350 13311 replica.cpp:679] Persisted action at 0
I0126 19:10:52.080348 13305 replica.cpp:511] Replica received write request for position 0
I0126 19:10:52.081153 13305 leveldb.cpp:438] Reading position from leveldb took 62247ns
I0126 19:10:52.081676 13305 leveldb.cpp:343] Persisting action (14 bytes) to leveldb took 81487ns
I0126 19:10:52.082053 13305 replica.cpp:679] Persisted action at 0
I0126 19:10:52.083566 13309 replica.cpp:658] Replica received learned notice for position 0
I0126 19:10:52.085734 13309 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 283144ns
I0126 19:10:52.086067 13309 replica.cpp:679] Persisted action at 0
I0126 19:10:52.086448 13309 replica.cpp:664] Replica learned NOP action at position 0
I0126 19:10:52.089784 13306 log.cpp:676] Writer started with ending position 0
I0126 19:10:52.093415 13309 leveldb.cpp:438] Reading position from leveldb took 66744ns
I0126 19:10:52.104814 13306 registrar.cpp:346] Successfully fetched the registry (0B) in 47.451136ms
I0126 19:10:52.105731 13306 registrar.cpp:445] Applied 1 operations in 42124ns; attempting to update the 'registry'
I0126 19:10:52.111935 13305 log.cpp:684] Attempting to append 131 bytes to the log
I0126 19:10:52.112754 13305 coordinator.cpp:340] Coordinator attempting to write APPEND action at position 1
I0126 19:10:52.114297 13308 replica.cpp:511] Replica received write request for position 1
I0126 19:10:52.114908 13308 leveldb.cpp:343] Persisting action (150 bytes) to leveldb took 98332ns
I0126 19:10:52.115387 13308 replica.cpp:679] Persisted action at 1
I0126 19:10:52.117277 13305 replica.cpp:658] Replica received learned notice for position 1
I0126 19:10:52.118142 13305 leveldb.cpp:343] Persisting action (152 bytes) to leveldb took 227799ns
I0126 19:10:52.118621 13305 replica.cpp:679] Persisted action at 1
I0126 19:10:52.118979 13305 replica.cpp:664] Replica learned APPEND action at position 1
I0126 19:10:52.121311 13305 registrar.cpp:490] Successfully updated the 'registry' in 15.161088ms
I0126 19:10:52.121548 13311 log.cpp:703] Attempting to truncate the log to 1
I0126 19:10:52.122697 13311 coordinator.cpp:340] Coordinator attempting to write TRUNCATE action at position 2
I0126 19:10:52.124316 13307 replica.cpp:511] Replica received write request for position 2
I0126 19:10:52.124913 13307 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 87281ns
I0126 19:10:52.125334 13307 replica.cpp:679] Persisted action at 2
I0126 19:10:52.127018 13311 replica.cpp:658] Replica received learned notice for position 2
I0126 19:10:52.127835 13311 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 201050ns
I0126 19:10:52.128232 13311 leveldb.cpp:401] Deleting ~1 keys from leveldb took 78012ns
I0126 19:10:52.128835 13305 registrar.cpp:376] Successfully recovered registrar
I0126 19:10:52.128551 13311 replica.cpp:679] Persisted action at 2
I0126 19:10:52.130105 13311 replica.cpp:664] Replica learned TRUNCATE action at position 2
I0126 19:10:52.131479 13312 master.cpp:1077] Recovered 0 slaves from the Registry (95B) ; allowing 10mins for slaves to re-register
I0126 19:10:52.143465 13291 containerizer.cpp:103] Using isolation: posix/cpu,posix/mem
I0126 19:10:52.170471 13309 slave.cpp:173] Slave started on 101)@192.168.122.135:35545
I0126 19:10:52.171723 13309 credentials.hpp:84] Loading credential for authentication from '/tmp/SlaveRecoveryTest_0_ReconcileKillTask_qbguuM/credential'
I0126 19:10:52.172286 13309 slave.cpp:282] Slave using credential for: test-principal
I0126 19:10:52.172821 13309 slave.cpp:300] Slave resources: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I0126 19:10:52.173982 13309 slave.cpp:329] Slave hostname: fedora-19
I0126 19:10:52.174505 13309 slave.cpp:330] Slave checkpoint: true
I0126 19:10:52.179308 13309 state.cpp:33] Recovering state from '/tmp/SlaveRecoveryTest_0_ReconcileKillTask_qbguuM/meta'
I0126 19:10:52.180075 13308 status_update_manager.cpp:197] Recovering status update manager
I0126 19:10:52.180611 13308 containerizer.cpp:300] Recovering containerizer
I0126 19:10:52.182473 13309 slave.cpp:3519] Finished recovery
I0126 19:10:52.184403 13312 slave.cpp:613] New master detected at master@192.168.122.135:35545
I0126 19:10:52.184916 13312 slave.cpp:676] Authenticating with master master@192.168.122.135:35545
I0126 19:10:52.185230 13312 slave.cpp:681] Using default CRAM-MD5 authenticatee
I0126 19:10:52.185715 13312 slave.cpp:649] Detecting new master
I0126 19:10:52.186420 13312 authenticatee.hpp:138] Creating new client SASL connection
I0126 19:10:52.186002 13311 status_update_manager.cpp:171] Pausing sending status updates
I0126 19:10:52.188293 13312 master.cpp:4129] Authenticating slave(101)@192.168.122.135:35545
I0126 19:10:52.188748 13312 master.cpp:4140] Using default CRAM-MD5 authenticator
I0126 19:10:52.189525 13312 authenticator.hpp:170] Creating new server SASL connection
I0126 19:10:52.191082 13305 authenticatee.hpp:229] Received SASL authentication mechanisms: CRAM-MD5
I0126 19:10:52.191550 13305 authenticatee.hpp:255] Attempting to authenticate with mechanism 'CRAM-MD5'
I0126 19:10:52.191990 13312 authenticator.hpp:276] Received SASL authentication start
I0126 19:10:52.192365 13312 authenticator.hpp:398] Authentication requires more steps
I0126 19:10:52.192800 13311 authenticatee.hpp:275] Received SASL authentication step
I0126 19:10:52.193244 13312 authenticator.hpp:304] Received SASL authentication step
I0126 19:10:52.193565 13312 auxprop.cpp:99] Request to lookup properties for user: 'test-principal' realm: 'fedora-19' server FQDN: 'fedora-19' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0126 19:10:52.193902 13312 auxprop.cpp:171] Looking up auxiliary property '*userPassword'
I0126 19:10:52.194301 13312 auxprop.cpp:171] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0126 19:10:52.195669 13312 auxprop.cpp:99] Request to lookup properties for user: 'test-principal' realm: 'fedora-19' server FQDN: 'fedora-19' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0126 19:10:52.196048 13312 auxprop.cpp:121] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0126 19:10:52.196395 13312 auxprop.cpp:121] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0126 19:10:52.196723 13312 authenticator.hpp:390] Authentication success
I0126 19:10:52.197206 13305 authenticatee.hpp:315] Authentication success
I0126 19:10:52.204121 13305 slave.cpp:747] Successfully authenticated with master master@192.168.122.135:35545
I0126 19:10:52.204676 13310 master.cpp:4187] Successfully authenticated principal 'test-principal' at slave(101)@192.168.122.135:35545
I0126 19:10:52.205729 13305 slave.cpp:1075] Will retry registration in 5.608661ms if necessary
I0126 19:10:52.206451 13310 master.cpp:3275] Registering slave at slave(101)@192.168.122.135:35545 (fedora-19) with id 20150126-191052-2272962752-35545-13291-S0
I0126 19:10:52.210019 13310 registrar.cpp:445] Applied 1 operations in 235087ns; attempting to update the 'registry'
I0126 19:10:52.220736 13308 slave.cpp:1075] Will retry registration in 9.28397ms if necessary
I0126 19:10:52.221309 13311 master.cpp:3263] Ignoring register slave message from slave(101)@192.168.122.135:35545 (fedora-19) as admission is already in progress
I0126 19:10:52.224818 13307 log.cpp:684] Attempting to append 302 bytes to the log
I0126 19:10:52.225554 13307 coordinator.cpp:340] Coordinator attempting to write APPEND action at position 3
I0126 19:10:52.227422 13305 replica.cpp:511] Replica received write request for position 3
I0126 19:10:52.227969 13305 leveldb.cpp:343] Persisting action (321 bytes) to leveldb took 100350ns
I0126 19:10:52.228276 13305 replica.cpp:679] Persisted action at 3
I0126 19:10:52.232475 13312 replica.cpp:658] Replica received learned notice for position 3
I0126 19:10:52.233280 13312 leveldb.cpp:343] Persisting action (323 bytes) to leveldb took 546567ns
I0126 19:10:52.233726 13312 replica.cpp:679] Persisted action at 3
I0126 19:10:52.234035 13312 replica.cpp:664] Replica learned APPEND action at position 3
I0126 19:10:52.236556 13310 registrar.cpp:490] Successfully updated the 'registry' in 26.040064ms
I0126 19:10:52.237330 13305 log.cpp:703] Attempting to truncate the log to 3
I0126 19:10:52.238056 13311 coordinator.cpp:340] Coordinator attempting to write TRUNCATE action at position 4
I0126 19:10:52.239594 13311 replica.cpp:511] Replica received write request for position 4
I0126 19:10:52.240129 13311 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 92868ns
I0126 19:10:52.240458 13311 replica.cpp:679] Persisted action at 4
I0126 19:10:52.241976 13308 replica.cpp:658] Replica received learned notice for position 4
I0126 19:10:52.242645 13308 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 95635ns
I0126 19:10:52.242990 13308 leveldb.cpp:401] Deleting ~2 keys from leveldb took 58066ns
I0126 19:10:52.243337 13308 replica.cpp:679] Persisted action at 4
I0126 19:10:52.243695 13308 replica.cpp:664] Replica learned TRUNCATE action at position 4
I0126 19:10:52.245657 13291 sched.cpp:151] Version: 0.22.0
I0126 19:10:52.247625 13305 master.cpp:3329] Registered slave 20150126-191052-2272962752-35545-13291-S0 at slave(101)@192.168.122.135:35545 (fedora-19) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I0126 19:10:52.248942 13307 slave.cpp:781] Registered with master master@192.168.122.135:35545; given slave ID 20150126-191052-2272962752-35545-13291-S0
I0126 19:10:52.250396 13307 slave.cpp:797] Checkpointing SlaveInfo to '/tmp/SlaveRecoveryTest_0_ReconcileKillTask_qbguuM/meta/slaves/20150126-191052-2272962752-35545-13291-S0/slave.info'
I0126 19:10:52.250731 13309 status_update_manager.cpp:178] Resuming sending status updates
I0126 19:10:52.251765 13307 slave.cpp:2588] Received ping from slave-observer(99)@192.168.122.135:35545
I0126 19:10:52.247951 13310 hierarchical_allocator_process.hpp:453] Added slave 20150126-191052-2272962752-35545-13291-S0 (fedora-19) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (and cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] available)
I0126 19:10:52.252810 13310 hierarchical_allocator_process.hpp:831] No resources available to allocate!
I0126 19:10:52.254365 13310 hierarchical_allocator_process.hpp:756] Performed allocation for slave 20150126-191052-2272962752-35545-13291-S0 in 1.732701ms
I0126 19:10:52.254137 13307 sched.cpp:248] New master detected at master@192.168.122.135:35545
I0126 19:10:52.257863 13307 sched.cpp:304] Authenticating with master master@192.168.122.135:35545
I0126 19:10:52.258249 13307 sched.cpp:311] Using default CRAM-MD5 authenticatee
I0126 19:10:52.258908 13306 authenticatee.hpp:138] Creating new client SASL connection
I0126 19:10:52.261397 13309 master.cpp:4129] Authenticating scheduler-6da85b48-b57f-4202-b630-c45f8f652321@192.168.122.135:35545
I0126 19:10:52.261776 13309 master.cpp:4140] Using default CRAM-MD5 authenticator
I0126 19:10:52.264528 13309 authenticator.hpp:170] Creating new server SASL connection
I0126 19:10:52.266248 13312 authenticatee.hpp:229] Received SASL authentication mechanisms: CRAM-MD5
I0126 19:10:52.266749 13312 authenticatee.hpp:255] Attempting to authenticate with mechanism 'CRAM-MD5'
I0126 19:10:52.267143 13312 authenticator.hpp:276] Received SASL authentication start
I0126 19:10:52.267525 13312 authenticator.hpp:398] Authentication requires more steps
I0126 19:10:52.267917 13312 authenticatee.hpp:275] Received SASL authentication step
I0126 19:10:52.268404 13312 authenticator.hpp:304] Received SASL authentication step
I0126 19:10:52.268725 13312 auxprop.cpp:99] Request to lookup properties for user: 'test-principal' realm: 'fedora-19' server FQDN: 'fedora-19' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0126 19:10:52.269078 13312 auxprop.cpp:171] Looking up auxiliary property '*userPassword'
I0126 19:10:52.269498 13312 auxprop.cpp:171] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0126 19:10:52.269881 13312 auxprop.cpp:99] Request to lookup properties for user: 'test-principal' realm: 'fedora-19' server FQDN: 'fedora-19' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0126 19:10:52.270385 13312 auxprop.cpp:121] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0126 19:10:52.271015 13312 auxprop.cpp:121] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0126 19:10:52.271599 13312 authenticator.hpp:390] Authentication success
I0126 19:10:52.272126 13312 authenticatee.hpp:315] Authentication success
I0126 19:10:52.272415 13305 master.cpp:4187] Successfully authenticated principal 'test-principal' at scheduler-6da85b48-b57f-4202-b630-c45f8f652321@192.168.122.135:35545
I0126 19:10:52.273998 13307 sched.cpp:392] Successfully authenticated with master master@192.168.122.135:35545
I0126 19:10:52.274415 13307 sched.cpp:515] Sending registration request to master@192.168.122.135:35545
I0126 19:10:52.274842 13307 sched.cpp:548] Will retry registration in 674.656506ms if necessary
I0126 19:10:52.275235 13305 master.cpp:1420] Received registration request for framework 'default' at scheduler-6da85b48-b57f-4202-b630-c45f8f652321@192.168.122.135:35545
I0126 19:10:52.276017 13305 master.cpp:1298] Authorizing framework principal 'test-principal' to receive offers for role '*'
I0126 19:10:52.277027 13305 master.cpp:1484] Registering framework 20150126-191052-2272962752-35545-13291-0000 (default) at scheduler-6da85b48-b57f-4202-b630-c45f8f652321@192.168.122.135:35545
I0126 19:10:52.278285 13308 hierarchical_allocator_process.hpp:319] Added framework 20150126-191052-2272962752-35545-13291-0000
I0126 19:10:52.279575 13308 hierarchical_allocator_process.hpp:738] Performed allocation for 1 slaves in 697902ns
I0126 19:10:52.287966 13305 master.cpp:4071] Sending 1 offers to framework 20150126-191052-2272962752-35545-13291-0000 (default) at scheduler-6da85b48-b57f-4202-b630-c45f8f652321@192.168.122.135:35545
I0126 19:10:52.288776 13307 sched.cpp:442] Framework registered with 20150126-191052-2272962752-35545-13291-0000
I0126 19:10:52.289373 13307 sched.cpp:456] Scheduler::registered took 21674ns
I0126 19:10:52.289932 13307 sched.cpp:605] Scheduler::resourceOffers took 76147ns
I0126 19:10:52.293220 13311 master.cpp:2677] Processing ACCEPT call for offers: [ 20150126-191052-2272962752-35545-13291-O0 ] on slave 20150126-191052-2272962752-35545-13291-S0 at slave(101)@192.168.122.135:35545 (fedora-19) for framework 20150126-191052-2272962752-35545-13291-0000 (default) at scheduler-6da85b48-b57f-4202-b630-c45f8f652321@192.168.122.135:35545
I0126 19:10:52.293586 13311 master.cpp:2513] Authorizing framework principal 'test-principal' to launch task 61eaeec3-e8ca-4e15-82d6-284c05c3bb6e as user 'jenkins'
I0126 19:10:52.295825 13311 master.hpp:782] Adding task 61eaeec3-e8ca-4e15-82d6-284c05c3bb6e with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave 20150126-191052-2272962752-35545-13291-S0 (fedora-19)
I0126 19:10:52.296272 13311 master.cpp:2885] Launching task 61eaeec3-e8ca-4e15-82d6-284c05c3bb6e of framework 20150126-191052-2272962752-35545-13291-0000 (default) at scheduler-6da85b48-b57f-4202-b630-c45f8f652321@192.168.122.135:35545 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave 20150126-191052-2272962752-35545-13291-S0 at slave(101)@192.168.122.135:35545 (fedora-19)
I0126 19:10:52.296886 13309 slave.cpp:1130] Got assigned task 61eaeec3-e8ca-4e15-82d6-284c05c3bb6e for framework 20150126-191052-2272962752-35545-13291-0000
I0126 19:10:52.297324 13309 slave.cpp:3846] Checkpointing FrameworkInfo to '/tmp/SlaveRecoveryTest_0_ReconcileKillTask_qbguuM/meta/slaves/20150126-191052-2272962752-35545-13291-S0/frameworks/20150126-191052-2272962752-35545-13291-0000/framework.info'
I0126 19:10:52.297919 13309 slave.cpp:3853] Checkpointing framework pid 'scheduler-6da85b48-b57f-4202-b630-c45f8f652321@192.168.122.135:35545' to '/tmp/SlaveRecoveryTest_0_ReconcileKillTask_qbguuM/meta/slaves/20150126-191052-2272962752-35545-13291-S0/frameworks/20150126-191052-2272962752-35545-13291-0000/framework.pid'
I0126 19:10:52.299072 13309 slave.cpp:1245] Launching task 61eaeec3-e8ca-4e15-82d6-284c05c3bb6e for framework 20150126-191052-2272962752-35545-13291-0000
I0126 19:10:52.308050 13309 slave.cpp:4289] Checkpointing ExecutorInfo to '/tmp/SlaveRecoveryTest_0_ReconcileKillTask_qbguuM/meta/slaves/20150126-191052-2272962752-35545-13291-S0/frameworks/20150126-191052-2272962752-35545-13291-0000/executors/61eaeec3-e8ca-4e15-82d6-284c05c3bb6e/executor.info'
I0126 19:10:52.310894 13309 slave.cpp:3921] Launching executor 61eaeec3-e8ca-4e15-82d6-284c05c3bb6e of framework 20150126-191052-2272962752-35545-13291-0000 in work directory '/tmp/SlaveRecoveryTest_0_ReconcileKillTask_qbguuM/slaves/20150126-191052-2272962752-35545-13291-S0/frameworks/20150126-191052-2272962752-35545-13291-0000/executors/61eaeec3-e8ca-4e15-82d6-284c05c3bb6e/runs/960eca2c-9e2c-415a-b6a5-159efca1f1b0'
I0126 19:10:52.311957 13308 containerizer.cpp:445] Starting container '960eca2c-9e2c-415a-b6a5-159efca1f1b0' for executor '61eaeec3-e8ca-4e15-82d6-284c05c3bb6e' of framework '20150126-191052-2272962752-35545-13291-0000'
W0126 19:10:52.313951 13307 containerizer.cpp:296] CommandInfo.grace_period flag is not set, using default value: 3secs
I0126 19:10:52.330166 13309 slave.cpp:4312] Checkpointing TaskInfo to '/tmp/SlaveRecoveryTest_0_ReconcileKillTask_qbguuM/meta/slaves/20150126-191052-2272962752-35545-13291-S0/frameworks/20150126-191052-2272962752-35545-13291-0000/executors/61eaeec3-e8ca-4e15-82d6-284c05c3bb6e/runs/960eca2c-9e2c-415a-b6a5-159efca1f1b0/tasks/61eaeec3-e8ca-4e15-82d6-284c05c3bb6e/task.info'
I0126 19:10:52.333307 13309 slave.cpp:1368] Queuing task '61eaeec3-e8ca-4e15-82d6-284c05c3bb6e' for executor 61eaeec3-e8ca-4e15-82d6-284c05c3bb6e of framework '20150126-191052-2272962752-35545-13291-0000
I0126 19:10:52.332506 13307 launcher.cpp:137] Forked child with pid '15795' for container '960eca2c-9e2c-415a-b6a5-159efca1f1b0'
I0126 19:10:52.334852 13307 containerizer.cpp:655] Checkpointing executor's forked pid 15795 to '/tmp/SlaveRecoveryTest_0_ReconcileKillTask_qbguuM/meta/slaves/20150126-191052-2272962752-35545-13291-S0/frameworks/20150126-191052-2272962752-35545-13291-0000/executors/61eaeec3-e8ca-4e15-82d6-284c05c3bb6e/runs/960eca2c-9e2c-415a-b6a5-159efca1f1b0/pids/forked.pid'
I0126 19:10:52.339607 13309 slave.cpp:566] Successfully attached file '/tmp/SlaveRecoveryTest_0_ReconcileKillTask_qbguuM/slaves/20150126-191052-2272962752-35545-13291-S0/frameworks/20150126-191052-2272962752-35545-13291-0000/executors/61eaeec3-e8ca-4e15-82d6-284c05c3bb6e/runs/960eca2c-9e2c-415a-b6a5-159efca1f1b0'
I0126 19:10:52.341423 13309 slave.cpp:2890] Monitoring executor '61eaeec3-e8ca-4e15-82d6-284c05c3bb6e' of framework '20150126-191052-2272962752-35545-13291-0000' in container '960eca2c-9e2c-415a-b6a5-159efca1f1b0'
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0126 19:10:52.584766 15795 process.cpp:958] libprocess is initialized on 192.168.122.135:41245 for 8 cpus
I0126 19:10:52.597306 15795 logging.cpp:177] Logging to STDERR
I0126 19:10:52.606741 15795 exec.cpp:147] Version: 0.22.0
I0126 19:10:52.617653 15825 exec.cpp:197] Executor started at: executor(1)@192.168.122.135:41245 with pid 15795
I0126 19:10:52.643771 13309 slave.cpp:1912] Got registration for executor '61eaeec3-e8ca-4e15-82d6-284c05c3bb6e' of framework 20150126-191052-2272962752-35545-13291-0000 from executor(1)@192.168.122.135:41245
I0126 19:10:52.644484 13309 slave.cpp:1998] Checkpointing executor pid 'executor(1)@192.168.122.135:41245' to '/tmp/SlaveRecoveryTest_0_ReconcileKillTask_qbguuM/meta/slaves/20150126-191052-2272962752-35545-13291-S0/frameworks/20150126-191052-2272962752-35545-13291-0000/executors/61eaeec3-e8ca-4e15-82d6-284c05c3bb6e/runs/960eca2c-9e2c-415a-b6a5-159efca1f1b0/pids/libprocess.pid'
I0126 19:10:52.648509 13309 slave.cpp:2031] Flushing queued task 61eaeec3-e8ca-4e15-82d6-284c05c3bb6e for executor '61eaeec3-e8ca-4e15-82d6-284c05c3bb6e' of framework 20150126-191052-2272962752-35545-13291-0000
I0126 19:10:52.701879 15830 exec.cpp:221] Executor registered on slave 20150126-191052-2272962752-35545-13291-S0
Shutdown timeout is set to 3secsRegistered executor on fedora-19
I0126 19:10:52.706497 15830 exec.cpp:233] Executor::registered took 2.369798ms
I0126 19:10:52.710708 15830 exec.cpp:308] Executor asked to run task '61eaeec3-e8ca-4e15-82d6-284c05c3bb6e'
Starting task 61eaeec3-e8ca-4e15-82d6-284c05c3bb6e
I0126 19:10:52.713075 15830 exec.cpp:317] Executor::launchTask took 1.248631ms
sh -c 'sleep 1000'
Forked command at 15832
I0126 19:10:52.720675 15824 exec.cpp:540] Executor sending status update TASK_RUNNING (UUID: 9577ef79-7e59-4be6-a892-5a20baa13647) for task 61eaeec3-e8ca-4e15-82d6-284c05c3bb6e of framework 20150126-191052-2272962752-35545-13291-0000
I0126 19:10:52.722925 13308 slave.cpp:2265] Handling status update TASK_RUNNING (UUID: 9577ef79-7e59-4be6-a892-5a20baa13647) for task 61eaeec3-e8ca-4e15-82d6-284c05c3bb6e of framework 20150126-191052-2272962752-35545-13291-0000 from executor(1)@192.168.122.135:41245
I0126 19:10:52.723328 13308 status_update_manager.cpp:317] Received status update TASK_RUNNING (UUID: 9577ef79-7e59-4be6-a892-5a20baa13647) for task 61eaeec3-e8ca-4e15-82d6-284c05c3bb6e of framework 20150126-191052-2272962752-35545-13291-0000
I0126 19:10:52.723371 13308 status_update_manager.cpp:494] Creating StatusUpdate stream for task 61eaeec3-e8ca-4e15-82d6-284c05c3bb6e of framework 20150126-191052-2272962752-35545-13291-0000
I0126 19:10:52.723803 13308 status_update_manager.hpp:346] Checkpointing UPDATE for status update TASK_RUNNING (UUID: 9577ef79-7e59-4be6-a892-5a20baa13647) for task 61eaeec3-e8ca-4e15-82d6-284c05c3bb6e of framework 20150126-191052-2272962752-35545-13291-0000
I0126 19:10:52.723963 13308 status_update_manager.cpp:371] Forwarding update TASK_RUNNING (UUID: 9577ef79-7e59-4be6-a892-5a20baa13647) for task 61eaeec3-e8ca-4e15-82d6-284c05c3bb6e of framework 20150126-191052-2272962752-35545-13291-0000 to the slave
I0126 19:10:52.724717 13312 slave.cpp:2508] Forwarding the update TASK_RUNNING (UUID: 9577ef79-7e59-4be6-a892-5a20baa13647) for task 61eaeec3-e8ca-4e15-82d6-284c05c3bb6e of framework 20150126-191052-2272962752-35545-13291-0000 to master@192.168.122.135:35545
I0126 19:10:52.725385 13305 master.cpp:3652] Forwarding status update TASK_RUNNING (UUID: 9577ef79-7e59-4be6-a892-5a20baa13647) for task 61eaeec3-e8ca-4e15-82d6-284c05c3bb6e of framework 20150126-191052-2272962752-35545-13291-0000
I0126 19:10:52.725857 13305 master.cpp:3624] Status update TASK_RUNNING (UUID: 9577ef79-7e59-4be6-a892-5a20baa13647) for task 61eaeec3-e8ca-4e15-82d6-284c05c3bb6e of framework 20150126-191052-2272962752-35545-13291-0000 from slave 20150126-191052-2272962752-35545-13291-S0 at slave(101)@192.168.122.135:35545 (fedora-19)
I0126 19:10:52.726471 13305 master.cpp:4934] Updating the latest state of task 61eaeec3-e8ca-4e15-82d6-284c05c3bb6e of framework 20150126-191052-2272962752-35545-13291-0000 to TASK_RUNNING
I0126 19:10:52.726269 13311 sched.cpp:696] Scheduler::statusUpdate took 22534ns
I0126 19:10:52.727679 13311 master.cpp:3125] Forwarding status update acknowledgement 9577ef79-7e59-4be6-a892-5a20baa13647 for task 61eaeec3-e8ca-4e15-82d6-284c05c3bb6e of framework 20150126-191052-2272962752-35545-13291-0000 (default) at scheduler-6da85b48-b57f-4202-b630-c45f8f652321@192.168.122.135:35545 to slave 20150126-191052-2272962752-35545-13291-S0 at slave(101)@192.168.122.135:35545 (fedora-19)
I0126 19:10:52.728380 13308 status_update_manager.cpp:389] Received status update acknowledgement (UUID: 9577ef79-7e59-4be6-a892-5a20baa13647) for task 61eaeec3-e8ca-4e15-82d6-284c05c3bb6e of framework 20150126-191052-2272962752-35545-13291-0000
I0126 19:10:52.728579 13311 slave.cpp:2435] Status update manager successfully handled status update TASK_RUNNING (UUID: 9577ef79-7e59-4be6-a892-5a20baa13647) for task 61eaeec3-e8ca-4e15-82d6-284c05c3bb6e of framework 20150126-191052-2272962752-35545-13291-0000
I0126 19:10:52.729403 13311 slave.cpp:2441] Sending acknowledgement for status update TASK_RUNNING (UUID: 9577ef79-7e59-4be6-a892-5a20baa13647) for task 61eaeec3-e8ca-4e15-82d6-284c05c3bb6e of framework 20150126-191052-2272962752-35545-13291-0000 to executor(1)@192.168.122.135:41245
I0126 19:10:52.728869 13308 status_update_manager.hpp:346] Checkpointing ACK for status update TASK_RUNNING (UUID: 9577ef79-7e59-4be6-a892-5a20baa13647) for task 61eaeec3-e8ca-4e15-82d6-284c05c3bb6e of framework 20150126-191052-2272962752-35545-13291-0000
I0126 19:10:52.731828 13307 slave.cpp:1852] Status update manager successfully handled status update acknowledgement (UUID: 9577ef79-7e59-4be6-a892-5a20baa13647) for task 61eaeec3-e8ca-4e15-82d6-284c05c3bb6e of framework 20150126-191052-2272962752-35545-13291-0000
I0126 19:10:52.732923 13307 slave.cpp:495] Slave terminating
I0126 19:10:52.739572 15827 exec.cpp:354] Executor received status update acknowledgement 9577ef79-7e59-4be6-a892-5a20baa13647 for task 61eaeec3-e8ca-4e15-82d6-284c05c3bb6e of framework 20150126-191052-2272962752-35545-13291-0000
I0126 19:10:52.743466 13306 master.cpp:795] Slave 20150126-191052-2272962752-35545-13291-S0 at slave(101)@192.168.122.135:35545 (fedora-19) disconnected
I0126 19:10:52.743948 13306 master.cpp:1826] Disconnecting slave 20150126-191052-2272962752-35545-13291-S0 at slave(101)@192.168.122.135:35545 (fedora-19)
I0126 19:10:52.744940 13306 master.cpp:1845] Deactivating slave 20150126-191052-2272962752-35545-13291-S0 at slave(101)@192.168.122.135:35545 (fedora-19)
I0126 19:10:52.752821 13306 hierarchical_allocator_process.hpp:512] Slave 20150126-191052-2272962752-35545-13291-S0 deactivated
I0126 19:10:52.765900 13291 containerizer.cpp:103] Using isolation: posix/cpu,posix/mem
I0126 19:10:52.766723 13309 master.cpp:2961] Asked to kill task 61eaeec3-e8ca-4e15-82d6-284c05c3bb6e of framework 20150126-191052-2272962752-35545-13291-0000
W0126 19:10:52.767549 13309 master.cpp:3030] Cannot kill task 61eaeec3-e8ca-4e15-82d6-284c05c3bb6e of framework 20150126-191052-2272962752-35545-13291-0000 (default) at scheduler-6da85b48-b57f-4202-b630-c45f8f652321@192.168.122.135:35545 because the slave 20150126-191052-2272962752-35545-13291-S0 at slave(101)@192.168.122.135:35545 (fedora-19) is disconnected. Kill will be retried if the slave re-registers
I0126 19:10:52.789048 13307 slave.cpp:173] Slave started on 102)@192.168.122.135:35545
I0126 19:10:52.790671 13307 credentials.hpp:84] Loading credential for authentication from '/tmp/SlaveRecoveryTest_0_ReconcileKillTask_qbguuM/credential'
I0126 19:10:52.791497 13307 slave.cpp:282] Slave using credential for: test-principal
I0126 19:10:52.792064 13307 slave.cpp:300] Slave resources: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I0126 19:10:52.793090 13307 slave.cpp:329] Slave hostname: fedora-19
I0126 19:10:52.793556 13307 slave.cpp:330] Slave checkpoint: true
I0126 19:10:52.795727 13311 state.cpp:33] Recovering state from '/tmp/SlaveRecoveryTest_0_ReconcileKillTask_qbguuM/meta'
I0126 19:10:52.796282 13311 state.cpp:668] Failed to find resources file '/tmp/SlaveRecoveryTest_0_ReconcileKillTask_qbguuM/meta/resources/resources.info'
I0126 19:10:52.804524 13309 slave.cpp:3601] Recovering framework 20150126-191052-2272962752-35545-13291-0000
I0126 19:10:52.805106 13309 slave.cpp:4040] Recovering executor '61eaeec3-e8ca-4e15-82d6-284c05c3bb6e' of framework 20150126-191052-2272962752-35545-13291-0000
I0126 19:10:52.807494 13309 slave.cpp:566] Successfully attached file '/tmp/SlaveRecoveryTest_0_ReconcileKillTask_qbguuM/slaves/20150126-191052-2272962752-35545-13291-S0/frameworks/20150126-191052-2272962752-35545-13291-0000/executors/61eaeec3-e8ca-4e15-82d6-284c05c3bb6e/runs/960eca2c-9e2c-415a-b6a5-159efca1f1b0'
I0126 19:10:52.807888 13310 status_update_manager.cpp:197] Recovering status update manager
I0126 19:10:52.808390 13310 status_update_manager.cpp:205] Recovering executor '61eaeec3-e8ca-4e15-82d6-284c05c3bb6e' of framework 20150126-191052-2272962752-35545-13291-0000
I0126 19:10:52.808830 13310 status_update_manager.cpp:494] Creating StatusUpdate stream for task 61eaeec3-e8ca-4e15-82d6-284c05c3bb6e of framework 20150126-191052-2272962752-35545-13291-0000
I0126 19:10:52.809484 13310 status_update_manager.hpp:310] Replaying status update stream for task 61eaeec3-e8ca-4e15-82d6-284c05c3bb6e
I0126 19:10:52.810966 13308 containerizer.cpp:300] Recovering containerizer
I0126 19:10:52.811550 13308 containerizer.cpp:342] Recovering container '960eca2c-9e2c-415a-b6a5-159efca1f1b0' for executor '61eaeec3-e8ca-4e15-82d6-284c05c3bb6e' of framework 20150126-191052-2272962752-35545-13291-0000
I0126 19:10:52.816074 13305 slave.cpp:3460] Sending reconnect request to executor 61eaeec3-e8ca-4e15-82d6-284c05c3bb6e of framework 20150126-191052-2272962752-35545-13291-0000 at executor(1)@192.168.122.135:41245
I0126 19:10:52.929554 15827 exec.cpp:267] Received reconnect request from slave 20150126-191052-2272962752-35545-13291-S0
I0126 19:10:52.946156 13305 slave.cpp:2089] Re-registering executor 61eaeec3-e8ca-4e15-82d6-284c05c3bb6e of framework 20150126-191052-2272962752-35545-13291-0000
I0126 19:10:53.010731 15829 exec.cpp:244] Executor re-registered on slave 20150126-191052-2272962752-35545-13291-S0
Re-registered executor on fedora-19
I0126 19:10:53.012980 15829 exec.cpp:256] Executor::reregistered took 313096ns
I0126 19:10:53.054590 13309 hierarchical_allocator_process.hpp:831] No resources available to allocate!
I0126 19:10:53.054930 13309 hierarchical_allocator_process.hpp:738] Performed allocation for 1 slaves in 388184ns
I0126 19:10:54.055598 13312 hierarchical_allocator_process.hpp:831] No resources available to allocate!
I0126 19:10:54.058614 13312 hierarchical_allocator_process.hpp:738] Performed allocation for 1 slaves in 3.086403ms
I0126 19:10:54.818248 13310 slave.cpp:2214] Cleaning up un-reregistered executors
I0126 19:10:54.821072 13310 slave.cpp:3519] Finished recovery
I0126 19:10:54.823081 13312 status_update_manager.cpp:171] Pausing sending status updates
I0126 19:10:54.823719 13310 slave.cpp:613] New master detected at master@192.168.122.135:35545
I0126 19:10:54.824260 13310 slave.cpp:676] Authenticating with master master@192.168.122.135:35545
I0126 19:10:54.824583 13310 slave.cpp:681] Using default CRAM-MD5 authenticatee
I0126 19:10:54.825479 13307 authenticatee.hpp:138] Creating new client SASL connection
I0126 19:10:54.826686 13310 slave.cpp:649] Detecting new master
I0126 19:10:54.827214 13307 master.cpp:4129] Authenticating slave(102)@192.168.122.135:35545
I0126 19:10:54.827747 13307 master.cpp:4140] Using default CRAM-MD5 authenticator
I0126 19:10:54.828635 13307 authenticator.hpp:170] Creating new server SASL connection
I0126 19:10:54.830049 13306 authenticatee.hpp:229] Received SASL authentication mechanisms: CRAM-MD5
I0126 19:10:54.830447 13306 authenticatee.hpp:255] Attempting to authenticate with mechanism 'CRAM-MD5'
I0126 19:10:54.830934 13307 authenticator.hpp:276] Received SASL authentication start
I0126 19:10:54.831362 13307 authenticator.hpp:398] Authentication requires more steps
I0126 19:10:54.831837 13309 authenticatee.hpp:275] Received SASL authentication step
I0126 19:10:54.832283 13307 authenticator.hpp:304] Received SASL authentication step
I0126 19:10:54.832615 13307 auxprop.cpp:99] Request to lookup properties for user: 'test-principal' realm: 'fedora-19' server FQDN: 'fedora-19' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0126 19:10:54.833143 13307 auxprop.cpp:171] Looking up auxiliary property '*userPassword'
I0126 19:10:54.833549 13307 auxprop.cpp:171] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0126 19:10:54.833904 13307 auxprop.cpp:99] Request to lookup properties for user: 'test-principal' realm: 'fedora-19' server FQDN: 'fedora-19' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0126 19:10:54.834241 13307 auxprop.cpp:121] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0126 19:10:54.834539 13307 auxprop.cpp:121] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0126 19:10:54.834869 13307 authenticator.hpp:390] Authentication success
I0126 19:10:54.836004 13311 authenticatee.hpp:315] Authentication success
I0126 19:10:54.842200 13311 slave.cpp:747] Successfully authenticated with master master@192.168.122.135:35545
I0126 19:10:54.842851 13308 master.cpp:4187] Successfully authenticated principal 'test-principal' at slave(102)@192.168.122.135:35545
I0126 19:10:54.844679 13309 master.cpp:3401] Re-registering slave 20150126-191052-2272962752-35545-13291-S0 at slave(101)@192.168.122.135:35545 (fedora-19)
W0126 19:10:54.845654 13309 master.cpp:4347]  Slave 20150126-191052-2272962752-35545-13291-S0 at slave(102)@192.168.122.135:35545 (fedora-19) has non-terminal task 61eaeec3-e8ca-4e15-82d6-284c05c3bb6e that is supposed to be killed. Killing it now!
I0126 19:10:54.846365 13308 hierarchical_allocator_process.hpp:498] Slave 20150126-191052-2272962752-35545-13291-S0 reactivated
I0126 19:10:54.846976 13311 slave.cpp:1075] Will retry registration in 10.72364ms if necessary
I0126 19:10:54.847618 13311 slave.cpp:849] Re-registered with master master@192.168.122.135:35545
I0126 19:10:54.848054 13309 status_update_manager.cpp:178] Resuming sending status updates
I0126 19:10:54.848565 13311 slave.cpp:1424] Asked to kill task 61eaeec3-e8ca-4e15-82d6-284c05c3bb6e of framework 20150126-191052-2272962752-35545-13291-0000
I0126 19:10:54.850329 13311 slave.cpp:1762] Updating framework 20150126-191052-2272962752-35545-13291-0000 pid to scheduler-6da85b48-b57f-4202-b630-c45f8f652321@192.168.122.135:35545
I0126 19:10:54.853868 13311 slave.cpp:1770] Checkpointing framework pid 'scheduler-6da85b48-b57f-4202-b630-c45f8f652321@192.168.122.135:35545' to '/tmp/SlaveRecoveryTest_0_ReconcileKillTask_qbguuM/meta/slaves/20150126-191052-2272962752-35545-13291-S0/frameworks/20150126-191052-2272962752-35545-13291-0000/framework.pid'
I0126 19:10:54.854627 13312 status_update_manager.cpp:178] Resuming sending status updates
I0126 19:10:54.920938 15824 exec.cpp:328] Executor asked to kill task '61eaeec3-e8ca-4e15-82d6-284c05c3bb6e'
I0126 19:10:54.921044 15824 exec.cpp:337] Executor::killTask took 56676ns
Shutting down
Sending SIGTERM to process tree at pid 15832
Killing the following process trees:
[ 
--- 15832 sleep 1000 
]
Command terminated with signal Terminated (pid: 15832)
I0126 19:10:55.045547 15825 exec.cpp:540] Executor sending status update TASK_KILLED (UUID: 2c2ef52e-8c0d-4a83-be36-e6433316989e) for task 61eaeec3-e8ca-4e15-82d6-284c05c3bb6e of framework 20150126-191052-2272962752-35545-13291-0000
I0126 19:10:55.059789 13312 hierarchical_allocator_process.hpp:831] No resources available to allocate!
I0126 19:10:55.060405 13312 hierarchical_allocator_process.hpp:738] Performed allocation for 1 slaves in 918365ns
I0126 19:10:55.115810 13309 slave.cpp:2265] Handling status update TASK_KILLED (UUID: 2c2ef52e-8c0d-4a83-be36-e6433316989e) for task 61eaeec3-e8ca-4e15-82d6-284c05c3bb6e of framework 20150126-191052-2272962752-35545-13291-0000 from executor(1)@192.168.122.135:41245
I0126 19:10:55.116387 13309 slave.cpp:4229] Terminating task 61eaeec3-e8ca-4e15-82d6-284c05c3bb6e
I0126 19:10:55.119729 13305 status_update_manager.cpp:317] Received status update TASK_KILLED (UUID: 2c2ef52e-8c0d-4a83-be36-e6433316989e) for task 61eaeec3-e8ca-4e15-82d6-284c05c3bb6e of framework 20150126-191052-2272962752-35545-13291-0000
I0126 19:10:55.120384 13305 status_update_manager.hpp:346] Checkpointing UPDATE for status update TASK_KILLED (UUID: 2c2ef52e-8c0d-4a83-be36-e6433316989e) for task 61eaeec3-e8ca-4e15-82d6-284c05c3bb6e of framework 20150126-191052-2272962752-35545-13291-0000
I0126 19:10:55.120900 13305 status_update_manager.cpp:371] Forwarding update TASK_KILLED (UUID: 2c2ef52e-8c0d-4a83-be36-e6433316989e) for task 61eaeec3-e8ca-4e15-82d6-284c05c3bb6e of framework 20150126-191052-2272962752-35545-13291-0000 to the slave
I0126 19:10:55.121579 13305 slave.cpp:2508] Forwarding the update TASK_KILLED (UUID: 2c2ef52e-8c0d-4a83-be36-e6433316989e) for task 61eaeec3-e8ca-4e15-82d6-284c05c3bb6e of framework 20150126-191052-2272962752-35545-13291-0000 to master@192.168.122.135:35545
I0126 19:10:55.122256 13310 master.cpp:3652] Forwarding status update TASK_KILLED (UUID: 2c2ef52e-8c0d-4a83-be36-e6433316989e) for task 61eaeec3-e8ca-4e15-82d6-284c05c3bb6e of framework 20150126-191052-2272962752-35545-13291-0000
I0126 19:10:55.122685 13310 master.cpp:3624] Status update TASK_KILLED (UUID: 2c2ef52e-8c0d-4a83-be36-e6433316989e) for task 61eaeec3-e8ca-4e15-82d6-284c05c3bb6e of framework 20150126-191052-2272962752-35545-13291-0000 from slave 20150126-191052-2272962752-35545-13291-S0 at slave(102)@192.168.122.135:35545 (fedora-19)
I0126 19:10:55.123086 13308 sched.cpp:696] Scheduler::statusUpdate took 79719ns
I0126 19:10:55.124562 13310 master.cpp:4934] Updating the latest state of task 61eaeec3-e8ca-4e15-82d6-284c05c3bb6e of framework 20150126-191052-2272962752-35545-13291-0000 to TASK_KILLED
I0126 19:10:55.125345 13310 master.cpp:4993] Removing task 61eaeec3-e8ca-4e15-82d6-284c05c3bb6e with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] of framework 20150126-191052-2272962752-35545-13291-0000 on slave 20150126-191052-2272962752-35545-13291-S0 at slave(102)@192.168.122.135:35545 (fedora-19)
I0126 19:10:55.125810 13306 hierarchical_allocator_process.hpp:645] Recovered cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (total allocatable: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]) on slave 20150126-191052-2272962752-35545-13291-S0 from framework 20150126-191052-2272962752-35545-13291-0000
I0126 19:10:55.126552 13310 master.cpp:3125] Forwarding status update acknowledgement 2c2ef52e-8c0d-4a83-be36-e6433316989e for task 61eaeec3-e8ca-4e15-82d6-284c05c3bb6e of framework 20150126-191052-2272962752-35545-13291-0000 (default) at scheduler-6da85b48-b57f-4202-b630-c45f8f652321@192.168.122.135:35545 to slave 20150126-191052-2272962752-35545-13291-S0 at slave(102)@192.168.122.135:35545 (fedora-19)
I0126 19:10:55.126843 13305 slave.cpp:2435] Status update manager successfully handled status update TASK_KILLED (UUID: 2c2ef52e-8c0d-4a83-be36-e6433316989e) for task 61eaeec3-e8ca-4e15-82d6-284c05c3bb6e of framework 20150126-191052-2272962752-35545-13291-0000
I0126 19:10:55.127396 13305 slave.cpp:2441] Sending acknowledgement for status update TASK_KILLED (UUID: 2c2ef52e-8c0d-4a83-be36-e6433316989e) for task 61eaeec3-e8ca-4e15-82d6-284c05c3bb6e of framework 20150126-191052-2272962752-35545-13291-0000 to executor(1)@192.168.122.135:41245
I0126 19:10:55.129451 13306 status_update_manager.cpp:389] Received status update acknowledgement (UUID: 2c2ef52e-8c0d-4a83-be36-e6433316989e) for task 61eaeec3-e8ca-4e15-82d6-284c05c3bb6e of framework 20150126-191052-2272962752-35545-13291-0000
I0126 19:10:55.129976 13306 status_update_manager.hpp:346] Checkpointing ACK for status update TASK_KILLED (UUID: 2c2ef52e-8c0d-4a83-be36-e6433316989e) for task 61eaeec3-e8ca-4e15-82d6-284c05c3bb6e of framework 20150126-191052-2272962752-35545-13291-0000
I0126 19:10:55.130367 13306 status_update_manager.cpp:525] Cleaning up status update stream for task 61eaeec3-e8ca-4e15-82d6-284c05c3bb6e of framework 20150126-191052-2272962752-35545-13291-0000
I0126 19:10:55.130980 13305 slave.cpp:1852] Status update manager successfully handled status update acknowledgement (UUID: 2c2ef52e-8c0d-4a83-be36-e6433316989e) for task 61eaeec3-e8ca-4e15-82d6-284c05c3bb6e of framework 20150126-191052-2272962752-35545-13291-0000
I0126 19:10:55.131376 13305 slave.cpp:4268] Completing task 61eaeec3-e8ca-4e15-82d6-284c05c3bb6e
I0126 19:10:55.148888 15823 exec.cpp:354] Executor received status update acknowledgement 2c2ef52e-8c0d-4a83-be36-e6433316989e for task 61eaeec3-e8ca-4e15-82d6-284c05c3bb6e of framework 20150126-191052-2272962752-35545-13291-0000
I0126 19:10:56.061642 13305 hierarchical_allocator_process.hpp:738] Performed allocation for 1 slaves in 612945ns
I0126 19:10:56.065135 13310 master.cpp:4071] Sending 1 offers to framework 20150126-191052-2272962752-35545-13291-0000 (default) at scheduler-6da85b48-b57f-4202-b630-c45f8f652321@192.168.122.135:35545
I0126 19:10:56.068106 13310 sched.cpp:605] Scheduler::resourceOffers took 98788ns
I0126 19:10:56.068989 13291 sched.cpp:1471] Asked to stop the driver
I0126 19:10:56.069831 13291 master.cpp:654] Master terminating
I0126 19:10:56.070969 13310 sched.cpp:808] Stopping framework '20150126-191052-2272962752-35545-13291-0000'
I0126 19:10:56.072089 13312 slave.cpp:2673] master@192.168.122.135:35545 exited
W0126 19:10:56.072654 13312 slave.cpp:2676] Master disconnected! Waiting for a new master to be elected
I0126 19:10:56.110337 13310 containerizer.cpp:1084] Executor for container '960eca2c-9e2c-415a-b6a5-159efca1f1b0' has exited
I0126 19:10:56.110785 13310 containerizer.cpp:879] Destroying container '960eca2c-9e2c-415a-b6a5-159efca1f1b0'
I./tests/cluster.hpp:451: Failure
(wait).failure(): Unknown container: 960eca2c-9e2c-415a-b6a5-159efca1f1b0
0126 19:10:56.146338 13307 slave.cpp:2948] Executor '61eaeec3-e8ca-4e15-82d6-284c05c3bb6e' of framework 20150126-191052-2272962752-35545-13291-0000 exited with status 0
W0126 19:10:56.147302 13309 containerizer.cpp:868] Ignoring destroy of unknown container: 960eca2c-9e2c-415a-b6a5-159efca1f1b0
*** Aborted at 1422328256 (unix time) try ""date -d @1422328256"" if you are using GNU date ***
I0126 19:10:56.151959 13307 slave.cpp:3057] Cleaning up executor '61eaeec3-e8ca-4e15-82d6-284c05c3bb6e' of framework 20150126-191052-2272962752-35545-13291-0000
I0126 19:10:56.153216 13309 gc.cpp:56] Scheduling '/tmp/SlaveRecoveryTest_0_ReconcileKillTask_qbguuM/slaves/20150126-191052-2272962752-35545-13291-S0/frameworks/20150126-191052-2272962752-35545-13291-0000/executors/61eaeec3-e8ca-4e15-82d6-284c05c3bb6e/runs/960eca2c-9e2c-415a-b6a5-159efca1f1b0' for gc 6.99999822829926days in the future
I0126 19:10:56.154017 13305 gc.cpp:56] Scheduling '/tmp/SlaveRecoveryTest_0_ReconcileKillTask_qbguuM/slaves/20150126-191052-2272962752-35545-13291-S0/frameworks/20150126-191052-2272962752-35545-13291-0000/executors/61eaeec3-e8ca-4e15-82d6-284c05c3bb6e' for gc 6.99999821866963days in the future
I0126 19:10:56.154710 13312 gc.cpp:56] Scheduling '/tmp/SlaveRecoveryTest_0_ReconcileKillTask_qbguuM/meta/slaves/20150126-191052-2272962752-35545-13291-S0/frameworks/20150126-191052-2272962752-35545-13291-0000/executors/61eaeec3-e8ca-4e15-82d6-284c05c3bb6e/runs/960eca2c-9e2c-415a-b6a5-159efca1f1b0' for gc 6.99999821037037days in the future
IPC: @           0x8f9d48 mesos::internal::tests::Cluster::Slaves::shutdown()
0126 19:10:56.155350 13307 slave.cpp:3136] Cleaning up framework 20150126-191052-2272962752-35545-13291-0000
I0126 19:10:56.155609 13310 gc.cpp:56] Scheduling '/tmp/SlaveRecoveryTest_0_ReconcileKillTask_qbguuM/meta/slaves/20150126-191052-2272962752-35545-13291-S0/frameworks/20150126-191052-2272962752-35545-13291-0000/executors/61eaeec3-e8ca-4e15-82d6-284c05c3bb6e' for gc 6.99999820308148days in the future
I0126 19:10:56.158103 13310 status_update_manager.cpp:279] Closing status update streams for framework 20150126-191052-2272962752-35545-13291-0000
I0126 19:10:56.163135 13310 gc.cpp:56] Scheduling '/tmp/SlaveRecoveryTest_0_ReconcileKillTask_qbguuM/slaves/20150126-191052-2272962752-35545-13291-S0/frameworks/20150126-191052-2272962752-35545-13291-0000' for gc 6.99999817088days in the future
I0126 19:10:56.168100 13307 gc.cpp:56] Scheduling '/tmp/SlaveRecoveryTest_0_ReconcileKillTask_qbguuM/meta/slaves/20150126-191052-2272962752-35545-13291-S0/frameworks/20150126-191052-2272962752-35545-13291-0000' for gc 6.99999805755852days in the future
*** SIGSEGV (@0x0) received by PID 13291 (TID 0x7fc0fdb22880) from PID 0; stack trace: ***
    @     0x7fc0da188cbb (unknown)
    @     0x7fc0da18d1a1 (unknown)
    @       0x3aa2a0efa0 (unknown)
    @           0x8f9d48 mesos::internal::tests::Cluster::Slaves::shutdown()
    @           0xe0bfba mesos::internal::tests::MesosTest::ShutdownSlaves()
    @           0xe0bf7e mesos::internal::tests::MesosTest::Shutdown()
    @           0xe0981f mesos::internal::tests::MesosTest::TearDown()
    @           0xe0f2b6 mesos::internal::tests::ContainerizerTest<>::TearDown()
    @          0x10d8180 testing::internal::HandleSehExceptionsInMethodIfSupported<>()
    @          0x10d3356 testing::internal::HandleExceptionsInMethodIfSupported<>()
    @          0x10bb718 testing::Test::Run()
    @          0x10bbdf2 testing::TestInfo::Run()
    @          0x10bc37a testing::TestCase::Run()
    @          0x10c10f6 testing::internal::UnitTestImpl::RunAllTests()
    @          0x10d8ff1 testing::internal::HandleSehExceptionsInMethodIfSupported<>()
    @          0x10d4047 testing::internal::HandleExceptionsInMethodIfSupported<>()
    @          0x10bffa6 testing::UnitTest::Run()
    @           0xce850d main
    @       0x3aa2221b45 (unknown)
    @           0x8d59f9 (unknown)
make[3]: *** [check-local] Segmentation fault (core dumped)
{noformat}",Bug,Major,bmahler,2015-01-28T21:51:22.000+0000,5,Resolved,Complete,SlaveRecoveryTest.ReconcileKillTask is flaky.,2015-01-28T21:51:22.000+0000,MESOS-2283,1.0,mesos,Twitter Mesos Q1 Sprint 1
nfnt,2015-01-27T22:33:03.000+0000,cmaloney,"Currently two formats of credentials are supported: JSON

{code}
  ""credentials"": [
    {
      ""principal"": ""sherman"",
      ""secret"": ""kitesurf""
    }
{code}

And a new line file:
{code}
principal1 secret1
pricipal2 secret2
{code}

We should deprecate the new line format and remove support for the old format.",Improvement,Major,cmaloney,2016-03-25T09:02:19.000+0000,5,Resolved,Complete,Deprecate plain text Credential format.,2016-03-25T09:03:08.000+0000,MESOS-2281,3.0,mesos,Mesosphere Sprint 31
vinodkone,2015-01-27T21:38:10.000+0000,jieyu,"For example, when a future has transitioned into READY state, all onDiscard callbacks should be cleared to avoid potential cyclic dependency and memory leak. For instance:

{noformat}
Promise<Nothing> promise;
Future<Nothing> f = promise.future();
f.onDiscard(lambda::bind(&SomeFunc, f));
promise.set(Nothing());
{noformat}

The above code has a cyclic dependency because f.data has a reference to the future inside an std::function which has a reference to f.data.",Bug,Major,jieyu,2015-01-28T22:27:05.000+0000,5,Resolved,Complete,Future callbacks should be cleared once the future has transitioned.,2015-01-28T22:27:05.000+0000,MESOS-2279,2.0,mesos,Twitter Mesos Q1 Sprint 1
nfnt,2015-01-27T18:56:17.000+0000,nnielsen,"We have several ways of sorting, grouping and ordering headers includes in Mesos. We should agree on a rule set and do a style scan.",Improvement,Trivial,nnielsen,2015-11-23T21:04:12.000+0000,5,Resolved,Complete,Document header include rules in style guide,2015-11-23T21:04:12.000+0000,MESOS-2275,3.0,mesos,Mesosphere Sprint 21
karya,2015-01-26T23:07:55.000+0000,karya,"'make check' allows one to build and run the test suite. However, often we just want to build the tests.  Currently, this is done by setting GTEST_FILTER to an empty string.

It will be nice to have a dedicated target such as 'make tests' that allows one to build the test suite without running it.",Improvement,Major,karya,2015-10-17T20:33:04.000+0000,5,Resolved,Complete,"Add ""tests"" target to Makefile for building-but-not-running tests.",2016-03-04T04:59:51.000+0000,MESOS-2273,1.0,mesos,Mesosphere Sprint 20
,2015-01-23T21:53:16.000+0000,ssk2hd,"As a consumer of the Mesos HTTP API, it is necessary for us to determine the current version of Mesos so that we can parse the JSON documents returned correctly (since they change from version to version). 

Currently we're doing this by fetching state.json, parsing it and pulling out the version field. A more idiomatic way to do this would be to filter on the content-type in the header itself.

To give a more concrete example, currently the JSON documents returned by the HTTP API return the following headers:
{code}
HTTP/1.1 200 OK
Date: Fri, 23 Jan 2015 21:31:37 GMT
Content-Length: 9352
Content-Type: application/json
{code}

Something like the following (e.g. for master/state.json) would be easy to switch upon:
{code}
HTTP/1.1 200 OK
Date: Fri, 23 Jan 2015 21:31:37 GMT
Content-Length: 9352
Content-Type: application/vnd.mesos.master.state.v0.20.1+json; charset=utf-8
{code}

The vnd prefix is typically used for vendor specific file types (see: http://en.wikipedia.org/wiki/Internet_media_type#Prefix_vnd). Charset=utf-8 is required for JSON documents and is currently being omitted.

This content-type would change for each document type, for example:
{code}
application/vnd.mesos.master.state.v0.20.1+json; charset=utf-8
application/vnd.mesos.master.stats.v0.20.1+json; charset=utf-8
application/vnd.mesos.slave.state.v0.20.1+json; charset=utf-8
application/vnd.mesos.slave.stats.v0.20.1+json; charset=utf-8
{code}

Alternatively, the version could be appended as an extra field:
{code}
application/vnd.mesos.master.state+json; charset=utf-8; version=v0.20.1
application/vnd.mesos.master.stats+json; charset=utf-8; version=v0.20.1
application/vnd.mesos.slave.state+json; charset=utf-8; version=v0.20.1
application/vnd.mesos.slave.stats+json; charset=utf-8; version=v0.20.1
{code}

Thanks!",Improvement,Minor,ssk2hd,,10020,Accepted,In Progress,Version the Operator/Admin API,2016-02-26T23:11:59.000+0000,MESOS-2257,13.0,mesos,
jieyu,2015-01-21T22:59:46.000+0000,vinodkone,"Observed this on a local machine running linux w/ sudo.

{code}
[ RUN      ] DiskUsageCollectorTest.SymbolicLink
../../src/tests/disk_quota_tests.cpp:138: Failure
Expected: (usage1.get()) < (Kilobytes(16)), actual: 24KB vs 8-byte object <00-40 00-00 00-00 00-00>
[  FAILED  ] DiskUsageCollectorTest.SymbolicLink (201 ms)
{code}",Bug,Major,vinodkone,2015-02-03T00:28:22.000+0000,5,Resolved,Complete,DiskUsageCollectorTest.SymbolicLink test is flaky,2015-02-03T00:28:22.000+0000,MESOS-2241,1.0,mesos,Twitter Mesos Q1 Sprint 1
vinodkone,2015-01-16T21:59:45.000+0000,vinodkone,"There are several limitations to mesos projects current state of CI, which is run on builds.a.o

--> Only runs on Ubuntu
--> Doesn't run any tests that deal with cgroups
--> Doesn't run any tests that need root permissions

Now that ASF CI supports docker (https://issues.apache.org/jira/browse/BUILDS-25), it would be great for the Mesos project to use it.",Task,Major,vinodkone,2015-04-16T15:49:45.000+0000,5,Resolved,Complete,Run ASF CI mesos builds inside docker,2015-04-16T15:49:45.000+0000,MESOS-2233,5.0,mesos,Twitter Mesos Q1 Sprint 6
bmahler,2015-01-16T15:45:41.000+0000,alex-mesos,"After transforming allocated resources feature was added to allocator, a number of warnings are popping out for allocator tests. Commits leading to this behaviour:
{{dacc88292cc13d4b08fe8cda4df71110a96cb12a}}
{{5a02d5bdc75d3b1149dcda519016374be06ec6bd}}
corresponding reviews:
https://reviews.apache.org/r/29083
https://reviews.apache.org/r/29084

Here is an example:
{code}
[ RUN ] MasterAllocatorTest/0.FrameworkReregistersFirst GMOCK WARNING: Uninteresting mock function call - taking default action specified at: ../../../src/tests/mesos.hpp:719: Function call: transformAllocation(@0x7fd3bb5274d8 20150115-185632-1677764800-59671-44186-0000, @0x7fd3bb5274f8 20150115-185632-1677764800-59671-44186-S0, @0x1119140e0 16-byte object <F0-5E 52-BB D3-7F 00-00 C0-5F 52-BB D3-7F 00-00>) Stack trace: [ OK ] MasterAllocatorTest/0.FrameworkReregistersFirst (204 ms)
{code}",Bug,Minor,alexr,2015-01-29T07:11:10.000+0000,5,Resolved,Complete,Suppress MockAllocator::transformAllocation() warnings.,2015-10-14T10:00:45.000+0000,MESOS-2232,3.0,mesos,Twitter Mesos Q1 Sprint 1
vinodkone,2015-01-16T01:00:53.000+0000,vinodkone,Currently there is no way for the future returned by RateLimiter's acquire() to be discarded by the user of the limiter. This is useful in cases where the user is no longer interested in the permit. See MESOS-1148 for an example use case.,Improvement,Major,vinodkone,2015-01-28T22:27:29.000+0000,5,Resolved,Complete,Update RateLimiter to allow the acquired future to be discarded,2015-01-28T22:27:29.000+0000,MESOS-2230,3.0,mesos,Twitter Mesos Q1 Sprint 1
bmahler,2015-01-15T21:39:55.000+0000,vinodkone,"Observed this on internal CI

{noformat}
[ RUN      ] SlaveTest.MesosExecutorGracefulShutdown
Using temporary directory '/tmp/SlaveTest_MesosExecutorGracefulShutdown_AWdtVJ'
I0124 08:14:04.399211  7926 leveldb.cpp:176] Opened db in 27.364056ms
I0124 08:14:04.402632  7926 leveldb.cpp:183] Compacted db in 3.357646ms
I0124 08:14:04.402691  7926 leveldb.cpp:198] Created db iterator in 23822ns
I0124 08:14:04.402708  7926 leveldb.cpp:204] Seeked to beginning of db in 1913ns
I0124 08:14:04.402716  7926 leveldb.cpp:273] Iterated through 0 keys in the db in 458ns
I0124 08:14:04.402767  7926 replica.cpp:744] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned
I0124 08:14:04.403728  7951 recover.cpp:449] Starting replica recovery
I0124 08:14:04.404011  7951 recover.cpp:475] Replica is in EMPTY status
I0124 08:14:04.407765  7950 replica.cpp:641] Replica in EMPTY status received a broadcasted recover request
I0124 08:14:04.408710  7951 recover.cpp:195] Received a recover response from a replica in EMPTY status
I0124 08:14:04.419666  7951 recover.cpp:566] Updating replica status to STARTING
I0124 08:14:04.429719  7953 master.cpp:262] Master 20150124-081404-16842879-47787-7926 (utopic) started on 127.0.1.1:47787
I0124 08:14:04.429790  7953 master.cpp:308] Master only allowing authenticated frameworks to register
I0124 08:14:04.429802  7953 master.cpp:313] Master only allowing authenticated slaves to register
I0124 08:14:04.429826  7953 credentials.hpp:36] Loading credentials for authentication from '/tmp/SlaveTest_MesosExecutorGracefulShutdown_AWdtVJ/credentials'
I0124 08:14:04.430277  7953 master.cpp:357] Authorization enabled
I0124 08:14:04.432682  7953 master.cpp:1219] The newly elected leader is master@127.0.1.1:47787 with id 20150124-081404-16842879-47787-7926
I0124 08:14:04.432816  7953 master.cpp:1232] Elected as the leading master!
I0124 08:14:04.432894  7953 master.cpp:1050] Recovering from registrar
I0124 08:14:04.433212  7950 registrar.cpp:313] Recovering registrar
I0124 08:14:04.434226  7951 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 14.323302ms
I0124 08:14:04.434270  7951 replica.cpp:323] Persisted replica status to STARTING
I0124 08:14:04.434489  7951 recover.cpp:475] Replica is in STARTING status
I0124 08:14:04.436164  7951 replica.cpp:641] Replica in STARTING status received a broadcasted recover request
I0124 08:14:04.439368  7947 recover.cpp:195] Received a recover response from a replica in STARTING status
I0124 08:14:04.440626  7947 recover.cpp:566] Updating replica status to VOTING
I0124 08:14:04.443667  7947 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 2.698664ms
I0124 08:14:04.443759  7947 replica.cpp:323] Persisted replica status to VOTING
I0124 08:14:04.443925  7947 recover.cpp:580] Successfully joined the Paxos group
I0124 08:14:04.444160  7947 recover.cpp:464] Recover process terminated
I0124 08:14:04.444543  7949 log.cpp:660] Attempting to start the writer
I0124 08:14:04.446331  7949 replica.cpp:477] Replica received implicit promise request with proposal 1
I0124 08:14:04.449329  7949 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 2.690453ms
I0124 08:14:04.449388  7949 replica.cpp:345] Persisted promised to 1
I0124 08:14:04.450637  7947 coordinator.cpp:230] Coordinator attemping to fill missing position
I0124 08:14:04.452271  7949 replica.cpp:378] Replica received explicit promise request for position 0 with proposal 2
I0124 08:14:04.455124  7949 leveldb.cpp:343] Persisting action (8 bytes) to leveldb took 2.593522ms
I0124 08:14:04.455157  7949 replica.cpp:679] Persisted action at 0
I0124 08:14:04.456594  7951 replica.cpp:511] Replica received write request for position 0
I0124 08:14:04.456657  7951 leveldb.cpp:438] Reading position from leveldb took 30358ns
I0124 08:14:04.464860  7951 leveldb.cpp:343] Persisting action (14 bytes) to leveldb took 8.164646ms
I0124 08:14:04.464903  7951 replica.cpp:679] Persisted action at 0
I0124 08:14:04.465947  7949 replica.cpp:658] Replica received learned notice for position 0
I0124 08:14:04.471567  7949 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 5.587838ms
I0124 08:14:04.471601  7949 replica.cpp:679] Persisted action at 0
I0124 08:14:04.471622  7949 replica.cpp:664] Replica learned NOP action at position 0
I0124 08:14:04.472682  7951 log.cpp:676] Writer started with ending position 0
I0124 08:14:04.473919  7951 leveldb.cpp:438] Reading position from leveldb took 28676ns
I0124 08:14:04.491591  7951 registrar.cpp:346] Successfully fetched the registry (0B) in 58.337024ms
I0124 08:14:04.491704  7951 registrar.cpp:445] Applied 1 operations in 28163ns; attempting to update the 'registry'
I0124 08:14:04.493938  7953 log.cpp:684] Attempting to append 118 bytes to the log
I0124 08:14:04.494122  7953 coordinator.cpp:340] Coordinator attempting to write APPEND action at position 1
I0124 08:14:04.495069  7953 replica.cpp:511] Replica received write request for position 1
I0124 08:14:04.500089  7953 leveldb.cpp:343] Persisting action (135 bytes) to leveldb took 4.989356ms
I0124 08:14:04.500123  7953 replica.cpp:679] Persisted action at 1
I0124 08:14:04.501271  7950 replica.cpp:658] Replica received learned notice for position 1
I0124 08:14:04.505698  7950 leveldb.cpp:343] Persisting action (137 bytes) to leveldb took 4.396221ms
I0124 08:14:04.505734  7950 replica.cpp:679] Persisted action at 1
I0124 08:14:04.505755  7950 replica.cpp:664] Replica learned APPEND action at position 1
I0124 08:14:04.507313  7950 registrar.cpp:490] Successfully updated the 'registry' in 15.52896ms
I0124 08:14:04.507478  7953 log.cpp:703] Attempting to truncate the log to 1
I0124 08:14:04.507848  7953 coordinator.cpp:340] Coordinator attempting to write TRUNCATE action at position 2
I0124 08:14:04.508743  7953 replica.cpp:511] Replica received write request for position 2
I0124 08:14:04.509214  7950 registrar.cpp:376] Successfully recovered registrar
I0124 08:14:04.509682  7946 master.cpp:1077] Recovered 0 slaves from the Registry (82B) ; allowing 10mins for slaves to re-register
I0124 08:14:04.514654  7953 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 5.880031ms
I0124 08:14:04.514689  7953 replica.cpp:679] Persisted action at 2
I0124 08:14:04.515736  7953 replica.cpp:658] Replica received learned notice for position 2
I0124 08:14:04.522014  7953 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 6.245138ms
I0124 08:14:04.522086  7953 leveldb.cpp:401] Deleting ~1 keys from leveldb took 37803ns
I0124 08:14:04.522107  7953 replica.cpp:679] Persisted action at 2
I0124 08:14:04.522128  7953 replica.cpp:664] Replica learned TRUNCATE action at position 2
I0124 08:14:04.531460  7926 containerizer.cpp:103] Using isolation: posix/cpu,posix/mem
I0124 08:14:04.547194  7951 slave.cpp:173] Slave started on 208)@127.0.1.1:47787
I0124 08:14:04.555682  7951 credentials.hpp:84] Loading credential for authentication from '/tmp/SlaveTest_MesosExecutorGracefulShutdown_kB74xo/credential'
I0124 08:14:04.556622  7951 slave.cpp:282] Slave using credential for: test-principal
I0124 08:14:04.557052  7951 slave.cpp:300] Slave resources: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I0124 08:14:04.557842  7951 slave.cpp:329] Slave hostname: utopic
I0124 08:14:04.558091  7951 slave.cpp:330] Slave checkpoint: false
W0124 08:14:04.558352  7951 slave.cpp:332] Disabling checkpointing is deprecated and the --checkpoint flag will be removed in a future release. Please avoid using this flag
I0124 08:14:04.566864  7948 state.cpp:33] Recovering state from '/tmp/SlaveTest_MesosExecutorGracefulShutdown_kB74xo/meta'
I0124 08:14:04.575711  7951 status_update_manager.cpp:197] Recovering status update manager
I0124 08:14:04.575904  7951 containerizer.cpp:300] Recovering containerizer
I0124 08:14:04.577112  7951 slave.cpp:3519] Finished recovery
I0124 08:14:04.577374  7926 sched.cpp:151] Version: 0.22.0
I0124 08:14:04.578663  7950 sched.cpp:248] New master detected at master@127.0.1.1:47787
I0124 08:14:04.578759  7950 sched.cpp:304] Authenticating with master master@127.0.1.1:47787
I0124 08:14:04.578781  7950 sched.cpp:311] Using default CRAM-MD5 authenticatee
I0124 08:14:04.579071  7950 authenticatee.hpp:138] Creating new client SASL connection
I0124 08:14:04.579550  7947 master.cpp:4129] Authenticating scheduler-4a6c5cde-c54a-455a-aaad-6fc4e8ee99ef@127.0.1.1:47787
I0124 08:14:04.579582  7947 master.cpp:4140] Using default CRAM-MD5 authenticator
I0124 08:14:04.580031  7947 authenticator.hpp:170] Creating new server SASL connection
I0124 08:14:04.580402  7947 authenticatee.hpp:229] Received SASL authentication mechanisms: CRAM-MD5
I0124 08:14:04.580430  7947 authenticatee.hpp:255] Attempting to authenticate with mechanism 'CRAM-MD5'
I0124 08:14:04.580538  7947 authenticator.hpp:276] Received SASL authentication start
I0124 08:14:04.580581  7947 authenticator.hpp:398] Authentication requires more steps
I0124 08:14:04.580651  7947 authenticatee.hpp:275] Received SASL authentication step
I0124 08:14:04.580746  7947 authenticator.hpp:304] Received SASL authentication step
I0124 08:14:04.580837  7947 authenticator.hpp:390] Authentication success
I0124 08:14:04.580940  7947 authenticatee.hpp:315] Authentication success
I0124 08:14:04.581009  7947 master.cpp:4187] Successfully authenticated principal 'test-principal' at scheduler-4a6c5cde-c54a-455a-aaad-6fc4e8ee99ef@127.0.1.1:47787
I0124 08:14:04.581328  7947 sched.cpp:392] Successfully authenticated with master master@127.0.1.1:47787
I0124 08:14:04.581509  7947 master.cpp:1420] Received registration request for framework 'default' at scheduler-4a6c5cde-c54a-455a-aaad-6fc4e8ee99ef@127.0.1.1:47787
I0124 08:14:04.581585  7947 master.cpp:1298] Authorizing framework principal 'test-principal' to receive offers for role '*'
I0124 08:14:04.582033  7947 master.cpp:1484] Registering framework 20150124-081404-16842879-47787-7926-0000 (default) at scheduler-4a6c5cde-c54a-455a-aaad-6fc4e8ee99ef@127.0.1.1:47787
I0124 08:14:04.582595  7947 hierarchical_allocator_process.hpp:319] Added framework 20150124-081404-16842879-47787-7926-0000
I0124 08:14:04.583051  7947 sched.cpp:442] Framework registered with 20150124-081404-16842879-47787-7926-0000
I0124 08:14:04.584087  7951 slave.cpp:613] New master detected at master@127.0.1.1:47787
I0124 08:14:04.584388  7951 slave.cpp:676] Authenticating with master master@127.0.1.1:47787
I0124 08:14:04.584564  7951 slave.cpp:681] Using default CRAM-MD5 authenticatee
I0124 08:14:04.584951  7951 slave.cpp:649] Detecting new master
I0124 08:14:04.585219  7951 status_update_manager.cpp:171] Pausing sending status updates
I0124 08:14:04.585604  7951 authenticatee.hpp:138] Creating new client SASL connection
I0124 08:14:04.587666  7953 master.cpp:4129] Authenticating slave(208)@127.0.1.1:47787
I0124 08:14:04.587702  7953 master.cpp:4140] Using default CRAM-MD5 authenticator
I0124 08:14:04.588434  7953 authenticator.hpp:170] Creating new server SASL connection
I0124 08:14:04.588764  7953 authenticatee.hpp:229] Received SASL authentication mechanisms: CRAM-MD5
I0124 08:14:04.588790  7953 authenticatee.hpp:255] Attempting to authenticate with mechanism 'CRAM-MD5'
I0124 08:14:04.588896  7953 authenticator.hpp:276] Received SASL authentication start
I0124 08:14:04.588935  7953 authenticator.hpp:398] Authentication requires more steps
I0124 08:14:04.589005  7953 authenticatee.hpp:275] Received SASL authentication step
I0124 08:14:04.589082  7953 authenticator.hpp:304] Received SASL authentication step
I0124 08:14:04.589140  7953 authenticator.hpp:390] Authentication success
I0124 08:14:04.589232  7953 authenticatee.hpp:315] Authentication success
I0124 08:14:04.589300  7953 master.cpp:4187] Successfully authenticated principal 'test-principal' at slave(208)@127.0.1.1:47787
I0124 08:14:04.589587  7953 slave.cpp:747] Successfully authenticated with master master@127.0.1.1:47787
I0124 08:14:04.589913  7953 master.cpp:3275] Registering slave at slave(208)@127.0.1.1:47787 (utopic) with id 20150124-081404-16842879-47787-7926-S0
I0124 08:14:04.590322  7953 registrar.cpp:445] Applied 1 operations in 60404ns; attempting to update the 'registry'
I0124 08:14:04.595336  7948 log.cpp:684] Attempting to append 283 bytes to the log
I0124 08:14:04.595552  7948 coordinator.cpp:340] Coordinator attempting to write APPEND action at position 3
I0124 08:14:04.596535  7948 replica.cpp:511] Replica received write request for position 3
I0124 08:14:04.597846  7951 master.cpp:3263] Ignoring register slave message from slave(208)@127.0.1.1:47787 (utopic) as admission is already in progress
I0124 08:14:04.602326  7948 leveldb.cpp:343] Persisting action (302 bytes) to leveldb took 5.758211ms
I0124 08:14:04.602363  7948 replica.cpp:679] Persisted action at 3
I0124 08:14:04.603492  7951 replica.cpp:658] Replica received learned notice for position 3
I0124 08:14:04.608952  7951 leveldb.cpp:343] Persisting action (304 bytes) to leveldb took 5.427195ms
I0124 08:14:04.608985  7951 replica.cpp:679] Persisted action at 3
I0124 08:14:04.609007  7951 replica.cpp:664] Replica learned APPEND action at position 3
I0124 08:14:04.610643  7951 registrar.cpp:490] Successfully updated the 'registry' in 20.258048ms
I0124 08:14:04.610800  7948 log.cpp:703] Attempting to truncate the log to 3
I0124 08:14:04.611184  7948 coordinator.cpp:340] Coordinator attempting to write TRUNCATE action at position 4
I0124 08:14:04.612076  7948 replica.cpp:511] Replica received write request for position 4
I0124 08:14:04.613061  7946 master.cpp:3329] Registered slave 20150124-081404-16842879-47787-7926-S0 at slave(208)@127.0.1.1:47787 (utopic) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I0124 08:14:04.613299  7946 hierarchical_allocator_process.hpp:453] Added slave 20150124-081404-16842879-47787-7926-S0 (utopic) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (and cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] available)
I0124 08:14:04.613688  7946 slave.cpp:781] Registered with master master@127.0.1.1:47787; given slave ID 20150124-081404-16842879-47787-7926-S0
I0124 08:14:04.614112  7946 master.cpp:4071] Sending 1 offers to framework 20150124-081404-16842879-47787-7926-0000 (default) at scheduler-4a6c5cde-c54a-455a-aaad-6fc4e8ee99ef@127.0.1.1:47787
I0124 08:14:04.614228  7946 status_update_manager.cpp:178] Resuming sending status updates
I0124 08:14:04.617481  7947 master.cpp:2677] Processing ACCEPT call for offers: [ 20150124-081404-16842879-47787-7926-O0 ] on slave 20150124-081404-16842879-47787-7926-S0 at slave(208)@127.0.1.1:47787 (utopic) for framework 20150124-081404-16842879-47787-7926-0000 (default) at scheduler-4a6c5cde-c54a-455a-aaad-6fc4e8ee99ef@127.0.1.1:47787
I0124 08:14:04.617535  7947 master.cpp:2513] Authorizing framework principal 'test-principal' to launch task 7c16772d-4aed-4719-81c4-658a2cc22543 as user 'jenkins'
I0124 08:14:04.618736  7947 master.hpp:782] Adding task 7c16772d-4aed-4719-81c4-658a2cc22543 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave 20150124-081404-16842879-47787-7926-S0 (utopic)
I0124 08:14:04.618854  7947 master.cpp:2885] Launching task 7c16772d-4aed-4719-81c4-658a2cc22543 of framework 20150124-081404-16842879-47787-7926-0000 (default) at scheduler-4a6c5cde-c54a-455a-aaad-6fc4e8ee99ef@127.0.1.1:47787 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave 20150124-081404-16842879-47787-7926-S0 at slave(208)@127.0.1.1:47787 (utopic)
I0124 08:14:04.619209  7947 slave.cpp:1130] Got assigned task 7c16772d-4aed-4719-81c4-658a2cc22543 for framework 20150124-081404-16842879-47787-7926-0000
I0124 08:14:04.619472  7948 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 7.364828ms
I0124 08:14:04.619941  7948 replica.cpp:679] Persisted action at 4
I0124 08:14:04.624851  7953 replica.cpp:658] Replica received learned notice for position 4
I0124 08:14:04.625757  7947 slave.cpp:1245] Launching task 7c16772d-4aed-4719-81c4-658a2cc22543 for framework 20150124-081404-16842879-47787-7926-0000
I0124 08:14:04.630590  7953 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 5.705336ms
I0124 08:14:04.630805  7953 leveldb.cpp:401] Deleting ~2 keys from leveldb took 51263ns
I0124 08:14:04.630828  7953 replica.cpp:679] Persisted action at 4
I0124 08:14:04.630851  7953 replica.cpp:664] Replica learned TRUNCATE action at position 4
I0124 08:14:04.633968  7947 slave.cpp:3921] Launching executor 7c16772d-4aed-4719-81c4-658a2cc22543 of framework 20150124-081404-16842879-47787-7926-0000 in work directory '/tmp/SlaveTest_MesosExecutorGracefulShutdown_kB74xo/slaves/20150124-081404-16842879-47787-7926-S0/frameworks/20150124-081404-16842879-47787-7926-0000/executors/7c16772d-4aed-4719-81c4-658a2cc22543/runs/53887a08-f11d-4a2f-a659-a715d9fcf3d2'
I0124 08:14:04.634963  7951 containerizer.cpp:445] Starting container '53887a08-f11d-4a2f-a659-a715d9fcf3d2' for executor '7c16772d-4aed-4719-81c4-658a2cc22543' of framework '20150124-081404-16842879-47787-7926-0000'
W0124 08:14:04.636931  7951 containerizer.cpp:296] CommandInfo.grace_period flag is not set, using default value: 3secs
I0124 08:14:04.655591  7947 slave.cpp:1368] Queuing task '7c16772d-4aed-4719-81c4-658a2cc22543' for executor 7c16772d-4aed-4719-81c4-658a2cc22543 of framework '20150124-081404-16842879-47787-7926-0000
I0124 08:14:04.656992  7951 launcher.cpp:137] Forked child with pid '11030' for container '53887a08-f11d-4a2f-a659-a715d9fcf3d2'
I0124 08:14:04.673646  7951 slave.cpp:2890] Monitoring executor '7c16772d-4aed-4719-81c4-658a2cc22543' of framework '20150124-081404-16842879-47787-7926-0000' in container '53887a08-f11d-4a2f-a659-a715d9fcf3d2'
I0124 08:14:04.964946 11044 exec.cpp:147] Version: 0.22.0
I0124 08:14:05.113059  7948 slave.cpp:1912] Got registration for executor '7c16772d-4aed-4719-81c4-658a2cc22543' of framework 20150124-081404-16842879-47787-7926-0000 from executor(1)@127.0.1.1:49174
I0124 08:14:05.121086  7948 slave.cpp:2031] Flushing queued task 7c16772d-4aed-4719-81c4-658a2cc22543 for executor '7c16772d-4aed-4719-81c4-658a2cc22543' of framework 20150124-081404-16842879-47787-7926-0000
I0124 08:14:05.266849 11062 exec.cpp:221] Executor registered on slave 20150124-081404-16842879-47787-7926-S0
Shutdown timeout is set to 3secsRegistered executor on utopic
Starting task 7c16772d-4aed-4719-81c4-658a2cc22543
Forked command at 11067
sh -c 'sleep 1000'
I0124 08:14:05.492084  7953 slave.cpp:2265] Handling status update TASK_RUNNING (UUID: 54742a87-ef02-4e72-a19b-83b0eeb62568) for task 7c16772d-4aed-4719-81c4-658a2cc22543 of framework 20150124-081404-16842879-47787-7926-0000 from executor(1)@127.0.1.1:49174
I0124 08:14:05.492805  7953 status_update_manager.cpp:317] Received status update TASK_RUNNING (UUID: 54742a87-ef02-4e72-a19b-83b0eeb62568) for task 7c16772d-4aed-4719-81c4-658a2cc22543 of framework 20150124-081404-16842879-47787-7926-0000
I0124 08:14:05.493762  7953 slave.cpp:2508] Forwarding the update TASK_RUNNING (UUID: 54742a87-ef02-4e72-a19b-83b0eeb62568) for task 7c16772d-4aed-4719-81c4-658a2cc22543 of framework 20150124-081404-16842879-47787-7926-0000 to master@127.0.1.1:47787
I0124 08:14:05.493948  7953 slave.cpp:2441] Sending acknowledgement for status update TASK_RUNNING (UUID: 54742a87-ef02-4e72-a19b-83b0eeb62568) for task 7c16772d-4aed-4719-81c4-658a2cc22543 of framework 20150124-081404-16842879-47787-7926-0000 to executor(1)@127.0.1.1:49174
I0124 08:14:05.495378  7949 master.cpp:3652] Forwarding status update TASK_RUNNING (UUID: 54742a87-ef02-4e72-a19b-83b0eeb62568) for task 7c16772d-4aed-4719-81c4-658a2cc22543 of framework 20150124-081404-16842879-47787-7926-0000
I0124 08:14:05.495584  7949 master.cpp:3624] Status update TASK_RUNNING (UUID: 54742a87-ef02-4e72-a19b-83b0eeb62568) for task 7c16772d-4aed-4719-81c4-658a2cc22543 of framework 20150124-081404-16842879-47787-7926-0000 from slave 20150124-081404-16842879-47787-7926-S0 at slave(208)@127.0.1.1:47787 (utopic)
I0124 08:14:05.495678  7949 master.cpp:4934] Updating the latest state of task 7c16772d-4aed-4719-81c4-658a2cc22543 of framework 20150124-081404-16842879-47787-7926-0000 to TASK_RUNNING
I0124 08:14:05.496422  7949 master.cpp:3125] Forwarding status update acknowledgement 54742a87-ef02-4e72-a19b-83b0eeb62568 for task 7c16772d-4aed-4719-81c4-658a2cc22543 of framework 20150124-081404-16842879-47787-7926-0000 (default) at scheduler-4a6c5cde-c54a-455a-aaad-6fc4e8ee99ef@127.0.1.1:47787 to slave 20150124-081404-16842879-47787-7926-S0 at slave(208)@127.0.1.1:47787 (utopic)
I0124 08:14:05.497735  7946 master.cpp:2961] Asked to kill task 7c16772d-4aed-4719-81c4-658a2cc22543 of framework 20150124-081404-16842879-47787-7926-0000
I0124 08:14:05.497859  7946 master.cpp:3021] Telling slave 20150124-081404-16842879-47787-7926-S0 at slave(208)@127.0.1.1:47787 (utopic) to kill task 7c16772d-4aed-4719-81c4-658a2cc22543 of framework 20150124-081404-16842879-47787-7926-0000 (default) at scheduler-4a6c5cde-c54a-455a-aaad-6fc4e8ee99ef@127.0.1.1:47787
I0124 08:14:05.498589  7947 status_update_manager.cpp:389] Received status update acknowledgement (UUID: 54742a87-ef02-4e72-a19b-83b0eeb62568) for task 7c16772d-4aed-4719-81c4-658a2cc22543 of framework 20150124-081404-16842879-47787-7926-0000
I0124 08:14:05.499006  7953 slave.cpp:1424] Asked to kill task 7c16772d-4aed-4719-81c4-658a2cc22543 of framework 20150124-081404-16842879-47787-7926-0000
Shutting down
Sending SIGTERM to process tree at pid 11067
Killing the following process trees:
[ 
-+- 11067 sh -c sleep 1000 
 \--- 11068 sleep 1000 
]
2015-01-24 08:14:07,295:7926(0x7f30b1b34700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:57753] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
Process 11067 did not terminate after 3secs, sending SIGKILL to process tree at 11067
Killed the following process trees:
[ 
-+- 11067 sh -c sleep 1000 
 \--- 11068 sleep 1000 
]
Command terminated with signal Killed (pid: 11067)
I0124 08:14:09.063453  7953 slave.cpp:2265] Handling status update TASK_KILLED (UUID: 4bd05372-2705-46e5-8182-5cb6907fbab3) for task 7c16772d-4aed-4719-81c4-658a2cc22543 of framework 20150124-081404-16842879-47787-7926-0000 from executor(1)@127.0.1.1:49174
I0124 08:14:09.069545  7953 status_update_manager.cpp:317] Received status update TASK_KILLED (UUID: 4bd05372-2705-46e5-8182-5cb6907fbab3) for task 7c16772d-4aed-4719-81c4-658a2cc22543 of framework 20150124-081404-16842879-47787-7926-0000
I0124 08:14:09.070265  7953 slave.cpp:2508] Forwarding the update TASK_KILLED (UUID: 4bd05372-2705-46e5-8182-5cb6907fbab3) for task 7c16772d-4aed-4719-81c4-658a2cc22543 of framework 20150124-081404-16842879-47787-7926-0000 to master@127.0.1.1:47787
I0124 08:14:09.070996  7947 master.cpp:3652] Forwarding status update TASK_KILLED (UUID: 4bd05372-2705-46e5-8182-5cb6907fbab3) for task 7c16772d-4aed-4719-81c4-658a2cc22543 of framework 20150124-081404-16842879-47787-7926-0000
I0124 08:14:09.071182  7947 master.cpp:3624] Status update TASK_KILLED (UUID: 4bd05372-2705-46e5-8182-5cb6907fbab3) for task 7c16772d-4aed-4719-81c4-658a2cc22543 of framework 20150124-081404-16842879-47787-7926-0000 from slave 20150124-081404-16842879-47787-7926-S0 at slave(208)@127.0.1.1:47787 (utopic)
I0124 08:14:09.071260  7947 master.cpp:4934] Updating the latest state of task 7c16772d-4aed-4719-81c4-658a2cc22543 of framework 20150124-081404-16842879-47787-7926-0000 to TASK_KILLED
I0124 08:14:09.072052  7947 hierarchical_allocator_process.hpp:653] Recovered cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (total allocatable: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]) on slave 20150124-081404-16842879-47787-7926-S0 from framework 20150124-081404-16842879-47787-7926-0000
I0124 08:14:09.072449  7947 master.cpp:4993] Removing task 7c16772d-4aed-4719-81c4-658a2cc22543 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] of framework 20150124-081404-16842879-47787-7926-0000 on slave 20150124-081404-16842879-47787-7926-S0 at slave(208)@127.0.1.1:47787 (utopic)
I0124 08:14:09.072700  7947 master.cpp:3125] Forwarding status update acknowledgement 4bd05372-2705-46e5-8182-5cb6907fbab3 for task 7c16772d-4aed-4719-81c4-658a2cc22543 of framework 20150124-081404-16842879-47787-7926-0000 (default) at scheduler-4a6c5cde-c54a-455a-aaad-6fc4e8ee99ef@127.0.1.1:47787 to slave 20150124-081404-16842879-47787-7926-S0 at slave(208)@127.0.1.1:47787 (utopic)
../../src/tests/slave_tests.cpp:1736: Failure
Expected: (std::string::npos) != (statusKilled.get().message().find(""Terminated"")), actual: 18446744073709551615 vs 18446744073709551615
I0124 08:14:09.073422  7926 sched.cpp:1471] Asked to stop the driver
I0124 08:14:09.073629  7926 master.cpp:654] Master terminating
I0124 08:14:09.075768  7950 sched.cpp:808] Stopping framework '20150124-081404-16842879-47787-7926-0000'
I0124 08:14:09.079352  7953 slave.cpp:2441] Sending acknowledgement for status update TASK_KILLED (UUID: 4bd05372-2705-46e5-8182-5cb6907fbab3) for task 7c16772d-4aed-4719-81c4-658a2cc22543 of framework 20150124-081404-16842879-47787-7926-0000 to executor(1)@127.0.1.1:49174
I0124 08:14:09.085199  7953 slave.cpp:2673] master@127.0.1.1:47787 exited
W0124 08:14:09.085232  7953 slave.cpp:2676] Master disconnected! Waiting for a new master to be elected
I0124 08:14:09.085263  7953 status_update_manager.cpp:389] Received status update acknowledgement (UUID: 4bd05372-2705-46e5-8182-5cb6907fbab3) for task 7c16772d-4aed-4719-81c4-658a2cc22543 of framework 20150124-081404-16842879-47787-7926-0000
I0124 08:14:09.120879  7946 containerizer.cpp:879] Destroying container '53887a08-f11d-4a2f-a659-a715d9fcf3d2'
I0124 08:14:09.216553  7952 containerizer.cpp:1084] Executor for container '53887a08-f11d-4a2f-a659-a715d9fcf3d2' has exited
I0124 08:14:09.218641  7952 slave.cpp:2948] Executor '7c16772d-4aed-4719-81c4-658a2cc22543' of framework 20150124-081404-16842879-47787-7926-0000 terminated with signal Killed
I0124 08:14:09.218855  7952 slave.cpp:3057] Cleaning up executor '7c16772d-4aed-4719-81c4-658a2cc22543' of framework 20150124-081404-16842879-47787-7926-0000
I0124 08:14:09.223268  7947 gc.cpp:56] Scheduling '/tmp/SlaveTest_MesosExecutorGracefulShutdown_kB74xo/slaves/20150124-081404-16842879-47787-7926-S0/frameworks/20150124-081404-16842879-47787-7926-0000/executors/7c16772d-4aed-4719-81c4-658a2cc22543/runs/53887a08-f11d-4a2f-a659-a715d9fcf3d2' for gc 6.99999746482667days in the future
I0124 08:14:09.224205  7947 gc.cpp:56] Scheduling '/tmp/SlaveTest_MesosExecutorGracefulShutdown_kB74xo/slaves/20150124-081404-16842879-47787-7926-S0/frameworks/20150124-081404-16842879-47787-7926-0000/executors/7c16772d-4aed-4719-81c4-658a2cc22543' for gc 6.99999746293926days in the future
I0124 08:14:09.227552  7952 slave.cpp:3136] Cleaning up framework 20150124-081404-16842879-47787-7926-0000
I0124 08:14:09.229786  7949 status_update_manager.cpp:279] Closing status update streams for framework 20150124-081404-16842879-47787-7926-0000
I0124 08:14:09.230849  7952 slave.cpp:495] Slave terminating
I0124 08:14:09.230989  7952 gc.cpp:56] Scheduling '/tmp/SlaveTest_MesosExecutorGracefulShutdown_kB74xo/slaves/20150124-081404-16842879-47787-7926-S0/frameworks/20150124-081404-16842879-47787-7926-0000' for gc 6.99999732935407days in the future
[  FAILED  ] SlaveTest.MesosExecutorGracefulShutdown (4881 ms)
{noformat}",Bug,Major,vinodkone,2015-02-14T02:09:02.000+0000,5,Resolved,Complete,SlaveTest.MesosExecutorGracefulShutdown is flaky,2015-02-14T02:09:02.000+0000,MESOS-2228,3.0,mesos,Twitter Mesos Q1 Sprint 1
karya,2015-01-15T20:25:07.000+0000,vinodkone,"Observed this on internal CI

{code}
[ RUN      ] HookTest.VerifySlaveLaunchExecutorHook
Using temporary directory '/tmp/HookTest_VerifySlaveLaunchExecutorHook_GjBgME'
I0114 18:51:34.659353  4720 leveldb.cpp:176] Opened db in 1.255951ms
I0114 18:51:34.662112  4720 leveldb.cpp:183] Compacted db in 596090ns
I0114 18:51:34.662364  4720 leveldb.cpp:198] Created db iterator in 177877ns
I0114 18:51:34.662719  4720 leveldb.cpp:204] Seeked to beginning of db in 19709ns
I0114 18:51:34.663010  4720 leveldb.cpp:273] Iterated through 0 keys in the db in 18208ns
I0114 18:51:34.663312  4720 replica.cpp:744] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned
I0114 18:51:34.664266  4735 recover.cpp:449] Starting replica recovery
I0114 18:51:34.664908  4735 recover.cpp:475] Replica is in EMPTY status
I0114 18:51:34.667842  4734 replica.cpp:641] Replica in EMPTY status received a broadcasted recover request
I0114 18:51:34.669117  4735 recover.cpp:195] Received a recover response from a replica in EMPTY status
I0114 18:51:34.677913  4735 recover.cpp:566] Updating replica status to STARTING
I0114 18:51:34.683157  4735 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 137939ns
I0114 18:51:34.683507  4735 replica.cpp:323] Persisted replica status to STARTING
I0114 18:51:34.684013  4735 recover.cpp:475] Replica is in STARTING status
I0114 18:51:34.685554  4738 replica.cpp:641] Replica in STARTING status received a broadcasted recover request
I0114 18:51:34.696512  4736 recover.cpp:195] Received a recover response from a replica in STARTING status
I0114 18:51:34.700552  4735 recover.cpp:566] Updating replica status to VOTING
I0114 18:51:34.701128  4735 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 115624ns
I0114 18:51:34.701478  4735 replica.cpp:323] Persisted replica status to VOTING
I0114 18:51:34.701817  4735 recover.cpp:580] Successfully joined the Paxos group
I0114 18:51:34.702569  4735 recover.cpp:464] Recover process terminated
I0114 18:51:34.716439  4736 master.cpp:262] Master 20150114-185134-2272962752-57018-4720 (fedora-19) started on 192.168.122.135:57018
I0114 18:51:34.716913  4736 master.cpp:308] Master only allowing authenticated frameworks to register
I0114 18:51:34.717136  4736 master.cpp:313] Master only allowing authenticated slaves to register
I0114 18:51:34.717488  4736 credentials.hpp:36] Loading credentials for authentication from '/tmp/HookTest_VerifySlaveLaunchExecutorHook_GjBgME/credentials'
I0114 18:51:34.718077  4736 master.cpp:357] Authorization enabled
I0114 18:51:34.719238  4738 whitelist_watcher.cpp:65] No whitelist given
I0114 18:51:34.719755  4737 hierarchical_allocator_process.hpp:285] Initialized hierarchical allocator process
I0114 18:51:34.722584  4736 master.cpp:1219] The newly elected leader is master@192.168.122.135:57018 with id 20150114-185134-2272962752-57018-4720
I0114 18:51:34.722865  4736 master.cpp:1232] Elected as the leading master!
I0114 18:51:34.723310  4736 master.cpp:1050] Recovering from registrar
I0114 18:51:34.723760  4734 registrar.cpp:313] Recovering registrar
I0114 18:51:34.725229  4740 log.cpp:660] Attempting to start the writer
I0114 18:51:34.727893  4739 replica.cpp:477] Replica received implicit promise request with proposal 1
I0114 18:51:34.728425  4739 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 114781ns
I0114 18:51:34.728662  4739 replica.cpp:345] Persisted promised to 1
I0114 18:51:34.731271  4741 coordinator.cpp:230] Coordinator attemping to fill missing position
I0114 18:51:34.733223  4734 replica.cpp:378] Replica received explicit promise request for position 0 with proposal 2
I0114 18:51:34.734076  4734 leveldb.cpp:343] Persisting action (8 bytes) to leveldb took 87441ns
I0114 18:51:34.734441  4734 replica.cpp:679] Persisted action at 0
I0114 18:51:34.740272  4739 replica.cpp:511] Replica received write request for position 0
I0114 18:51:34.740910  4739 leveldb.cpp:438] Reading position from leveldb took 59846ns
I0114 18:51:34.741672  4739 leveldb.cpp:343] Persisting action (14 bytes) to leveldb took 189259ns
I0114 18:51:34.741919  4739 replica.cpp:679] Persisted action at 0
I0114 18:51:34.743000  4739 replica.cpp:658] Replica received learned notice for position 0
I0114 18:51:34.746844  4739 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 328487ns
I0114 18:51:34.747118  4739 replica.cpp:679] Persisted action at 0
I0114 18:51:34.747553  4739 replica.cpp:664] Replica learned NOP action at position 0
I0114 18:51:34.751344  4737 log.cpp:676] Writer started with ending position 0
I0114 18:51:34.753504  4734 leveldb.cpp:438] Reading position from leveldb took 61183ns
I0114 18:51:34.762962  4737 registrar.cpp:346] Successfully fetched the registry (0B) in 38.907904ms
I0114 18:51:34.763610  4737 registrar.cpp:445] Applied 1 operations in 67206ns; attempting to update the 'registry'
I0114 18:51:34.766079  4736 log.cpp:684] Attempting to append 130 bytes to the log
I0114 18:51:34.766769  4736 coordinator.cpp:340] Coordinator attempting to write APPEND action at position 1
I0114 18:51:34.768215  4741 replica.cpp:511] Replica received write request for position 1
I0114 18:51:34.768759  4741 leveldb.cpp:343] Persisting action (149 bytes) to leveldb took 87970ns
I0114 18:51:34.768995  4741 replica.cpp:679] Persisted action at 1
I0114 18:51:34.770691  4736 replica.cpp:658] Replica received learned notice for position 1
I0114 18:51:34.771273  4736 leveldb.cpp:343] Persisting action (151 bytes) to leveldb took 83590ns
I0114 18:51:34.771579  4736 replica.cpp:679] Persisted action at 1
I0114 18:51:34.771917  4736 replica.cpp:664] Replica learned APPEND action at position 1
I0114 18:51:34.773252  4738 log.cpp:703] Attempting to truncate the log to 1
I0114 18:51:34.773756  4735 coordinator.cpp:340] Coordinator attempting to write TRUNCATE action at position 2
I0114 18:51:34.775552  4736 replica.cpp:511] Replica received write request for position 2
I0114 18:51:34.775846  4736 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 71503ns
I0114 18:51:34.776695  4736 replica.cpp:679] Persisted action at 2
I0114 18:51:34.785259  4739 replica.cpp:658] Replica received learned notice for position 2
I0114 18:51:34.786252  4737 registrar.cpp:490] Successfully updated the 'registry' in 22.340864ms
I0114 18:51:34.787094  4737 registrar.cpp:376] Successfully recovered registrar
I0114 18:51:34.787749  4737 master.cpp:1077] Recovered 0 slaves from the Registry (94B) ; allowing 10mins for slaves to re-register
I0114 18:51:34.787282  4739 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 707150ns
I0114 18:51:34.788692  4739 leveldb.cpp:401] Deleting ~1 keys from leveldb took 60262ns
I0114 18:51:34.789048  4739 replica.cpp:679] Persisted action at 2
I0114 18:51:34.789329  4739 replica.cpp:664] Replica learned TRUNCATE action at position 2
I0114 18:51:34.819548  4738 slave.cpp:173] Slave started on 171)@192.168.122.135:57018
I0114 18:51:34.820530  4738 credentials.hpp:84] Loading credential for authentication from '/tmp/HookTest_VerifySlaveLaunchExecutorHook_AYxNqe/credential'
I0114 18:51:34.820952  4738 slave.cpp:282] Slave using credential for: test-principal
I0114 18:51:34.821516  4738 slave.cpp:300] Slave resources: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I0114 18:51:34.822217  4738 slave.cpp:329] Slave hostname: fedora-19
I0114 18:51:34.822502  4738 slave.cpp:330] Slave checkpoint: false
W0114 18:51:34.822857  4738 slave.cpp:332] Disabling checkpointing is deprecated and the --checkpoint flag will be removed in a future release. Please avoid using this flag
I0114 18:51:34.824998  4737 state.cpp:33] Recovering state from '/tmp/HookTest_VerifySlaveLaunchExecutorHook_AYxNqe/meta'
I0114 18:51:34.834015  4738 status_update_manager.cpp:197] Recovering status update manager
I0114 18:51:34.834810  4738 slave.cpp:3519] Finished recovery
I0114 18:51:34.835906  4734 status_update_manager.cpp:171] Pausing sending status updates
I0114 18:51:34.836423  4738 slave.cpp:613] New master detected at master@192.168.122.135:57018
I0114 18:51:34.836908  4738 slave.cpp:676] Authenticating with master master@192.168.122.135:57018
I0114 18:51:34.837190  4738 slave.cpp:681] Using default CRAM-MD5 authenticatee
I0114 18:51:34.837820  4737 authenticatee.hpp:138] Creating new client SASL connection
I0114 18:51:34.838784  4738 slave.cpp:649] Detecting new master
I0114 18:51:34.839306  4740 master.cpp:4130] Authenticating slave(171)@192.168.122.135:57018
I0114 18:51:34.839957  4740 master.cpp:4141] Using default CRAM-MD5 authenticator
I0114 18:51:34.841236  4740 authenticator.hpp:170] Creating new server SASL connection
I0114 18:51:34.842681  4741 authenticatee.hpp:229] Received SASL authentication mechanisms: CRAM-MD5
I0114 18:51:34.843118  4741 authenticatee.hpp:255] Attempting to authenticate with mechanism 'CRAM-MD5'
I0114 18:51:34.843581  4740 authenticator.hpp:276] Received SASL authentication start
I0114 18:51:34.843962  4740 authenticator.hpp:398] Authentication requires more steps
I0114 18:51:34.844357  4740 authenticatee.hpp:275] Received SASL authentication step
I0114 18:51:34.844780  4740 authenticator.hpp:304] Received SASL authentication step
I0114 18:51:34.845113  4740 auxprop.cpp:99] Request to lookup properties for user: 'test-principal' realm: 'fedora-19' server FQDN: 'fedora-19' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0114 18:51:34.845507  4740 auxprop.cpp:171] Looking up auxiliary property '*userPassword'
I0114 18:51:34.845835  4740 auxprop.cpp:171] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0114 18:51:34.846238  4740 auxprop.cpp:99] Request to lookup properties for user: 'test-principal' realm: 'fedora-19' server FQDN: 'fedora-19' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0114 18:51:34.846542  4740 auxprop.cpp:121] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0114 18:51:34.846806  4740 auxprop.cpp:121] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0114 18:51:34.847110  4740 authenticator.hpp:390] Authentication success
I0114 18:51:34.847808  4734 authenticatee.hpp:315] Authentication success
I0114 18:51:34.851029  4734 slave.cpp:747] Successfully authenticated with master master@192.168.122.135:57018
I0114 18:51:34.851608  4737 master.cpp:4188] Successfully authenticated principal 'test-principal' at slave(171)@192.168.122.135:57018
I0114 18:51:34.854962  4720 sched.cpp:151] Version: 0.22.0
I0114 18:51:34.856674  4734 slave.cpp:1075] Will retry registration in 3.085482ms if necessary
I0114 18:51:34.857434  4739 sched.cpp:248] New master detected at master@192.168.122.135:57018
I0114 18:51:34.861433  4739 sched.cpp:304] Authenticating with master master@192.168.122.135:57018
I0114 18:51:34.861693  4739 sched.cpp:311] Using default CRAM-MD5 authenticatee
I0114 18:51:34.857795  4737 master.cpp:3276] Registering slave at slave(171)@192.168.122.135:57018 (fedora-19) with id 20150114-185134-2272962752-57018-4720-S0
I0114 18:51:34.862951  4737 authenticatee.hpp:138] Creating new client SASL connection
I0114 18:51:34.863919  4735 registrar.cpp:445] Applied 1 operations in 120272ns; attempting to update the 'registry'
I0114 18:51:34.864645  4738 master.cpp:4130] Authenticating scheduler-c45273e4-6eb5-44ee-bf45-71b353db648f@192.168.122.135:57018
I0114 18:51:34.865033  4738 master.cpp:4141] Using default CRAM-MD5 authenticator
I0114 18:51:34.866904  4738 authenticator.hpp:170] Creating new server SASL connection
I0114 18:51:34.868840  4737 authenticatee.hpp:229] Received SASL authentication mechanisms: CRAM-MD5
I0114 18:51:34.869125  4737 authenticatee.hpp:255] Attempting to authenticate with mechanism 'CRAM-MD5'
I0114 18:51:34.869523  4737 authenticator.hpp:276] Received SASL authentication start
I0114 18:51:34.869835  4737 authenticator.hpp:398] Authentication requires more steps
I0114 18:51:34.870213  4737 authenticatee.hpp:275] Received SASL authentication step
I0114 18:51:34.870622  4737 authenticator.hpp:304] Received SASL authentication step
I0114 18:51:34.870946  4737 auxprop.cpp:99] Request to lookup properties for user: 'test-principal' realm: 'fedora-19' server FQDN: 'fedora-19' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0114 18:51:34.871219  4737 auxprop.cpp:171] Looking up auxiliary property '*userPassword'
I0114 18:51:34.871554  4737 auxprop.cpp:171] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0114 18:51:34.871968  4737 auxprop.cpp:99] Request to lookup properties for user: 'test-principal' realm: 'fedora-19' server FQDN: 'fedora-19' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0114 18:51:34.872297  4737 auxprop.cpp:121] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0114 18:51:34.872655  4737 auxprop.cpp:121] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0114 18:51:34.873024  4737 authenticator.hpp:390] Authentication success
I0114 18:51:34.873428  4737 authenticatee.hpp:315] Authentication success
I0114 18:51:34.873632  4739 master.cpp:4188] Successfully authenticated principal 'test-principal' at scheduler-c45273e4-6eb5-44ee-bf45-71b353db648f@192.168.122.135:57018
I0114 18:51:34.875006  4740 sched.cpp:392] Successfully authenticated with master master@192.168.122.135:57018
I0114 18:51:34.875319  4740 sched.cpp:515] Sending registration request to master@192.168.122.135:57018
I0114 18:51:34.876200  4740 sched.cpp:548] Will retry registration in 1.952991346secs if necessary
I0114 18:51:34.876729  4738 master.cpp:1417] Received registration request for framework 'default' at scheduler-c45273e4-6eb5-44ee-bf45-71b353db648f@192.168.122.135:57018
I0114 18:51:34.877040  4738 master.cpp:1298] Authorizing framework principal 'test-principal' to receive offers for role '*'
I0114 18:51:34.878059  4738 master.cpp:1481] Registering framework 20150114-185134-2272962752-57018-4720-0000 (default) at scheduler-c45273e4-6eb5-44ee-bf45-71b353db648f@192.168.122.135:57018
I0114 18:51:34.878473  4739 log.cpp:684] Attempting to append 300 bytes to the log
I0114 18:51:34.879464  4737 coordinator.cpp:340] Coordinator attempting to write APPEND action at position 3
I0114 18:51:34.880116  4734 hierarchical_allocator_process.hpp:319] Added framework 20150114-185134-2272962752-57018-4720-0000
I0114 18:51:34.880470  4734 hierarchical_allocator_process.hpp:839] No resources available to allocate!
I0114 18:51:34.882331  4734 hierarchical_allocator_process.hpp:746] Performed allocation for 0 slaves in 1.901284ms
I0114 18:51:34.884024  4741 sched.cpp:442] Framework registered with 20150114-185134-2272962752-57018-4720-0000
I0114 18:51:34.884454  4741 sched.cpp:456] Scheduler::registered took 44320ns
I0114 18:51:34.881965  4737 replica.cpp:511] Replica received write request for position 3
I0114 18:51:34.885218  4737 leveldb.cpp:343] Persisting action (319 bytes) to leveldb took 134480ns
I0114 18:51:34.885716  4737 replica.cpp:679] Persisted action at 3
I0114 18:51:34.886034  4739 slave.cpp:1075] Will retry registration in 22.947772ms if necessary
I0114 18:51:34.886291  4740 master.cpp:3264] Ignoring register slave message from slave(171)@192.168.122.135:57018 (fedora-19) as admission is already in progress
I0114 18:51:34.894690  4736 replica.cpp:658] Replica received learned notice for position 3
I0114 18:51:34.898638  4736 leveldb.cpp:343] Persisting action (321 bytes) to leveldb took 215501ns
I0114 18:51:34.899055  4736 replica.cpp:679] Persisted action at 3
I0114 18:51:34.899416  4736 replica.cpp:664] Replica learned APPEND action at position 3
I0114 18:51:34.911782  4736 registrar.cpp:490] Successfully updated the 'registry' in 46.176768ms
I0114 18:51:34.912286  4740 log.cpp:703] Attempting to truncate the log to 3
I0114 18:51:34.913108  4740 coordinator.cpp:340] Coordinator attempting to write TRUNCATE action at position 4
I0114 18:51:34.915027  4736 master.cpp:3330] Registered slave 20150114-185134-2272962752-57018-4720-S0 at slave(171)@192.168.122.135:57018 (fedora-19) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I0114 18:51:34.915642  4735 hierarchical_allocator_process.hpp:453] Added slave 20150114-185134-2272962752-57018-4720-S0 (fedora-19) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (and cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] available)
I0114 18:51:34.917809  4735 hierarchical_allocator_process.hpp:764] Performed allocation for slave 20150114-185134-2272962752-57018-4720-S0 in 514027ns
I0114 18:51:34.916689  4738 replica.cpp:511] Replica received write request for position 4
I0114 18:51:34.915784  4741 slave.cpp:781] Registered with master master@192.168.122.135:57018; given slave ID 20150114-185134-2272962752-57018-4720-S0
I0114 18:51:34.919293  4741 slave.cpp:2588] Received ping from slave-observer(156)@192.168.122.135:57018
I0114 18:51:34.919775  4740 status_update_manager.cpp:178] Resuming sending status updates
I0114 18:51:34.920374  4736 master.cpp:4072] Sending 1 offers to framework 20150114-185134-2272962752-57018-4720-0000 (default) at scheduler-c45273e4-6eb5-44ee-bf45-71b353db648f@192.168.122.135:57018
I0114 18:51:34.920569  4738 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 1.540136ms
I0114 18:51:34.921092  4738 replica.cpp:679] Persisted action at 4
I0114 18:51:34.927111  4735 replica.cpp:658] Replica received learned notice for position 4
I0114 18:51:34.927299  4734 sched.cpp:605] Scheduler::resourceOffers took 1.335524ms
I0114 18:51:34.930418  4735 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 1.596377ms
I0114 18:51:34.930882  4735 leveldb.cpp:401] Deleting ~2 keys from leveldb took 67578ns
I0114 18:51:34.931115  4735 replica.cpp:679] Persisted action at 4
I0114 18:51:34.931529  4735 replica.cpp:664] Replica learned TRUNCATE action at position 4
I0114 18:51:34.930356  4734 master.cpp:2541] Processing reply for offers: [ 20150114-185134-2272962752-57018-4720-O0 ] on slave 20150114-185134-2272962752-57018-4720-S0 at slave(171)@192.168.122.135:57018 (fedora-19) for framework 20150114-185134-2272962752-57018-4720-0000 (default) at scheduler-c45273e4-6eb5-44ee-bf45-71b353db648f@192.168.122.135:57018
I0114 18:51:34.932834  4734 master.cpp:2647] Authorizing framework principal 'test-principal' to launch task 1 as user 'jenkins'
W0114 18:51:34.934442  4736 master.cpp:2124] Executor default for task 1 uses less CPUs (None) than the minimum required (0.01). Please update your executor, as this will be mandatory in future releases.
W0114 18:51:34.934960  4736 master.cpp:2136] Executor default for task 1 uses less memory (None) than the minimum required (32MB). Please update your executor, as this will be mandatory in future releases.
I0114 18:51:34.935878  4736 master.hpp:766] Adding task 1 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave 20150114-185134-2272962752-57018-4720-S0 (fedora-19)
I0114 18:51:34.939453  4738 hierarchical_allocator_process.hpp:610] Updated allocation of framework 20150114-185134-2272962752-57018-4720-0000 on slave 20150114-185134-2272962752-57018-4720-S0 from cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] to cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I0114 18:51:34.939950  4736 master.cpp:2897] Launching task 1 of framework 20150114-185134-2272962752-57018-4720-0000 (default) at scheduler-c45273e4-6eb5-44ee-bf45-71b353db648f@192.168.122.135:57018 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave 20150114-185134-2272962752-57018-4720-S0 at slave(171)@192.168.122.135:57018 (fedora-19)
I0114 18:51:34.940467  4736 test_hook_module.cpp:52] Executing 'masterLaunchTaskLabelDecorator' hook
I0114 18:51:34.941490  4740 slave.cpp:1130] Got assigned task 1 for framework 20150114-185134-2272962752-57018-4720-0000
I0114 18:51:34.942873  4740 slave.cpp:1245] Launching task 1 for framework 20150114-185134-2272962752-57018-4720-0000
I0114 18:51:34.943469  4740 test_hook_module.cpp:71] Executing 'slaveLaunchExecutorEnvironmentDecorator' hook
I0114 18:51:34.946705  4740 slave.cpp:3921] Launching executor default of framework 20150114-185134-2272962752-57018-4720-0000 in work directory '/tmp/HookTest_VerifySlaveLaunchExecutorHook_AYxNqe/slaves/20150114-185134-2272962752-57018-4720-S0/frameworks/20150114-185134-2272962752-57018-4720-0000/executors/default/runs/d73da0e7-3d52-4a0e-91d0-eaef735fd65d'
I0114 18:51:34.956496  4740 exec.cpp:147] Version: 0.22.0
I0114 18:51:34.960752  4737 exec.cpp:197] Executor started at: executor(56)@192.168.122.135:57018 with pid 4720
I0114 18:51:34.964501  4740 slave.cpp:1368] Queuing task '1' for executor default of framework '20150114-185134-2272962752-57018-4720-0000
I0114 18:51:34.965133  4740 slave.cpp:566] Successfully attached file '/tmp/HookTest_VerifySlaveLaunchExecutorHook_AYxNqe/slaves/20150114-185134-2272962752-57018-4720-S0/frameworks/20150114-185134-2272962752-57018-4720-0000/executors/default/runs/d73da0e7-3d52-4a0e-91d0-eaef735fd65d'
I0114 18:51:34.965605  4740 slave.cpp:1912] Got registration for executor 'default' of framework 20150114-185134-2272962752-57018-4720-0000 from executor(56)@192.168.122.135:57018
I0114 18:51:34.966933  4734 exec.cpp:221] Executor registered on slave 20150114-185134-2272962752-57018-4720-S0
I0114 18:51:34.968889  4740 slave.cpp:2031] Flushing queued task 1 for executor 'default' of framework 20150114-185134-2272962752-57018-4720-0000
I0114 18:51:34.969743  4740 slave.cpp:2890] Monitoring executor 'default' of framework '20150114-185134-2272962752-57018-4720-0000' in container 'd73da0e7-3d52-4a0e-91d0-eaef735fd65d'
I0114 18:51:34.973484  4734 exec.cpp:233] Executor::registered took 4.814445ms
I0114 18:51:34.974081  4734 exec.cpp:308] Executor asked to run task '1'
I0114 18:51:34.974431  4734 exec.cpp:317] Executor::launchTask took 184910ns
I0114 18:51:34.975292  4720 sched.cpp:1471] Asked to stop the driver
I0114 18:51:34.975817  4738 sched.cpp:808] Stopping framework '20150114-185134-2272962752-57018-4720-0000'
I0114 18:51:34.975697  4720 master.cpp:654] Master terminating
W0114 18:51:34.976610  4720 master.cpp:4980] Removing task 1 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] of framework 20150114-185134-2272962752-57018-4720-0000 on slave 20150114-185134-2272962752-57018-4720-S0 at slave(171)@192.168.122.135:57018 (fedora-19) in non-terminal state TASK_STAGING
I0114 18:51:34.977880  4720 master.cpp:5023] Removing executor 'default' with resources  of framework 20150114-185134-2272962752-57018-4720-0000 on slave 20150114-185134-2272962752-57018-4720-S0 at slave(171)@192.168.122.135:57018 (fedora-19)
I0114 18:51:34.978196  4741 hierarchical_allocator_process.hpp:653] Recovered cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (total allocatable: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]) on slave 20150114-185134-2272962752-57018-4720-S0 from framework 20150114-185134-2272962752-57018-4720-0000
I0114 18:51:34.982658  4735 slave.cpp:2673] master@192.168.122.135:57018 exited
W0114 18:51:34.983065  4735 slave.cpp:2676] Master disconnected! Waiting for a new master to be elected
I0114 18:51:35.029485  4720 slave.cpp:495] Slave terminating
I0114 18:51:35.034024  4720 slave.cpp:1585] Asked to shut down framework 20150114-185134-2272962752-57018-4720-0000 by @0.0.0.0:0
I0114 18:51:35.034335  4720 slave.cpp:1610] Shutting down framework 20150114-185134-2272962752-57018-4720-0000
I0114 18:51:35.034857  4720 slave.cpp:3198] Shutting down executor 'default' of framework 20150114-185134-2272962752-57018-4720-0000
tests/hook_tests.cpp:271: Failure
Value of: os::isfile(path.get())
  Actual: true
Expected: false
[  FAILED  ] HookTest.VerifySlaveLaunchExecutorHook (412 ms)

{code}",Bug,Major,vinodkone,2015-07-06T16:09:38.000+0000,5,Resolved,Complete,HookTest.VerifySlaveLaunchExecutorHook is flaky,2015-09-23T22:39:39.000+0000,MESOS-2226,3.0,mesos,Mesosphere Q1 Sprint 2 - 2/6
vinodkone,2015-01-15T20:23:01.000+0000,vinodkone,"Observed this on internal CI.

{code}
[ RUN      ] FaultToleranceTest.ReregisterFrameworkExitedExecutor
Using temporary directory '/tmp/FaultToleranceTest_ReregisterFrameworkExitedExecutor_yNprKi'
I0114 18:50:51.461186  4720 leveldb.cpp:176] Opened db in 4.866948ms
I0114 18:50:51.462057  4720 leveldb.cpp:183] Compacted db in 472256ns
I0114 18:50:51.462514  4720 leveldb.cpp:198] Created db iterator in 42905ns
I0114 18:50:51.462784  4720 leveldb.cpp:204] Seeked to beginning of db in 21630ns
I0114 18:50:51.463068  4720 leveldb.cpp:273] Iterated through 0 keys in the db in 19967ns
I0114 18:50:51.463485  4720 replica.cpp:744] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned
I0114 18:50:51.464555  4737 recover.cpp:449] Starting replica recovery
I0114 18:50:51.465188  4737 recover.cpp:475] Replica is in EMPTY status
I0114 18:50:51.467324  4741 replica.cpp:641] Replica in EMPTY status received a broadcasted recover request
I0114 18:50:51.470118  4736 recover.cpp:195] Received a recover response from a replica in EMPTY status
I0114 18:50:51.475424  4739 recover.cpp:566] Updating replica status to STARTING
I0114 18:50:51.476553  4739 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 107545ns
I0114 18:50:51.476862  4739 replica.cpp:323] Persisted replica status to STARTING
I0114 18:50:51.477309  4739 recover.cpp:475] Replica is in STARTING status
I0114 18:50:51.479109  4734 replica.cpp:641] Replica in STARTING status received a broadcasted recover request
I0114 18:50:51.481274  4738 recover.cpp:195] Received a recover response from a replica in STARTING status
I0114 18:50:51.482324  4738 recover.cpp:566] Updating replica status to VOTING
I0114 18:50:51.482913  4738 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 66011ns
I0114 18:50:51.483186  4738 replica.cpp:323] Persisted replica status to VOTING
I0114 18:50:51.483608  4738 recover.cpp:580] Successfully joined the Paxos group
I0114 18:50:51.484031  4738 recover.cpp:464] Recover process terminated
I0114 18:50:51.554949  4734 master.cpp:262] Master 20150114-185051-2272962752-57018-4720 (fedora-19) started on 192.168.122.135:57018
I0114 18:50:51.555785  4734 master.cpp:308] Master only allowing authenticated frameworks to register
I0114 18:50:51.556046  4734 master.cpp:313] Master only allowing authenticated slaves to register
I0114 18:50:51.556426  4734 credentials.hpp:36] Loading credentials for authentication from '/tmp/FaultToleranceTest_ReregisterFrameworkExitedExecutor_yNprKi/credentials'
I0114 18:50:51.557003  4734 master.cpp:357] Authorization enabled
I0114 18:50:51.558007  4737 hierarchical_allocator_process.hpp:285] Initialized hierarchical allocator process
I0114 18:50:51.558521  4741 whitelist_watcher.cpp:65] No whitelist given
I0114 18:50:51.562185  4734 master.cpp:1219] The newly elected leader is master@192.168.122.135:57018 with id 20150114-185051-2272962752-57018-4720
I0114 18:50:51.562680  4734 master.cpp:1232] Elected as the leading master!
I0114 18:50:51.562950  4734 master.cpp:1050] Recovering from registrar
I0114 18:50:51.564506  4736 registrar.cpp:313] Recovering registrar
I0114 18:50:51.566162  4737 log.cpp:660] Attempting to start the writer
I0114 18:50:51.568691  4741 replica.cpp:477] Replica received implicit promise request with proposal 1
I0114 18:50:51.569154  4741 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 106885ns
I0114 18:50:51.569504  4741 replica.cpp:345] Persisted promised to 1
I0114 18:50:51.573277  4740 coordinator.cpp:230] Coordinator attemping to fill missing position
I0114 18:50:51.575623  4739 replica.cpp:378] Replica received explicit promise request for position 0 with proposal 2
I0114 18:50:51.576133  4739 leveldb.cpp:343] Persisting action (8 bytes) to leveldb took 86360ns
I0114 18:50:51.576449  4739 replica.cpp:679] Persisted action at 0
I0114 18:50:51.586966  4736 replica.cpp:511] Replica received write request for position 0
I0114 18:50:51.587666  4736 leveldb.cpp:438] Reading position from leveldb took 60621ns
I0114 18:50:51.588043  4736 leveldb.cpp:343] Persisting action (14 bytes) to leveldb took 81094ns
I0114 18:50:51.588374  4736 replica.cpp:679] Persisted action at 0
I0114 18:50:51.589418  4736 replica.cpp:658] Replica received learned notice for position 0
I0114 18:50:51.590428  4736 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 106648ns
I0114 18:50:51.590840  4736 replica.cpp:679] Persisted action at 0
I0114 18:50:51.591104  4736 replica.cpp:664] Replica learned NOP action at position 0
I0114 18:50:51.592260  4734 log.cpp:676] Writer started with ending position 0
I0114 18:50:51.594172  4739 leveldb.cpp:438] Reading position from leveldb took 52163ns
I0114 18:50:51.600744  4736 registrar.cpp:346] Successfully fetched the registry (0B) in 35968us
I0114 18:50:51.601646  4736 registrar.cpp:445] Applied 1 operations in 184502ns; attempting to update the 'registry'
I0114 18:50:51.604329  4737 log.cpp:684] Attempting to append 130 bytes to the log
I0114 18:50:51.604966  4737 coordinator.cpp:340] Coordinator attempting to write APPEND action at position 1
I0114 18:50:51.606449  4737 replica.cpp:511] Replica received write request for position 1
I0114 18:50:51.606937  4737 leveldb.cpp:343] Persisting action (149 bytes) to leveldb took 84877ns
I0114 18:50:51.607199  4737 replica.cpp:679] Persisted action at 1
I0114 18:50:51.611934  4741 replica.cpp:658] Replica received learned notice for position 1
I0114 18:50:51.612423  4741 leveldb.cpp:343] Persisting action (151 bytes) to leveldb took 113059ns
I0114 18:50:51.612794  4741 replica.cpp:679] Persisted action at 1
I0114 18:50:51.613056  4741 replica.cpp:664] Replica learned APPEND action at position 1
I0114 18:50:51.614598  4741 log.cpp:703] Attempting to truncate the log to 1
I0114 18:50:51.615157  4741 coordinator.cpp:340] Coordinator attempting to write TRUNCATE action at position 2
I0114 18:50:51.616458  4737 replica.cpp:511] Replica received write request for position 2
I0114 18:50:51.616902  4737 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 71716ns
I0114 18:50:51.617168  4737 replica.cpp:679] Persisted action at 2
I0114 18:50:51.618505  4740 replica.cpp:658] Replica received learned notice for position 2
I0114 18:50:51.619031  4740 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 78481ns
I0114 18:50:51.619567  4740 leveldb.cpp:401] Deleting ~1 keys from leveldb took 59638ns
I0114 18:50:51.619832  4740 replica.cpp:679] Persisted action at 2
I0114 18:50:51.620101  4740 replica.cpp:664] Replica learned TRUNCATE action at position 2
I0114 18:50:51.621757  4736 registrar.cpp:490] Successfully updated the 'registry' in 19.78496ms
I0114 18:50:51.622658  4736 registrar.cpp:376] Successfully recovered registrar
I0114 18:50:51.623261  4736 master.cpp:1077] Recovered 0 slaves from the Registry (94B) ; allowing 10mins for slaves to re-register
I0114 18:50:51.670349  4739 slave.cpp:173] Slave started on 115)@192.168.122.135:57018
I0114 18:50:51.671133  4739 credentials.hpp:84] Loading credential for authentication from '/tmp/FaultToleranceTest_ReregisterFrameworkExitedExecutor_ONrVug/credential'
I0114 18:50:51.671685  4739 slave.cpp:282] Slave using credential for: test-principal
I0114 18:50:51.672245  4739 slave.cpp:300] Slave resources: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I0114 18:50:51.673360  4739 slave.cpp:329] Slave hostname: fedora-19
I0114 18:50:51.673660  4739 slave.cpp:330] Slave checkpoint: false
W0114 18:50:51.674052  4739 slave.cpp:332] Disabling checkpointing is deprecated and the --checkpoint flag will be removed in a future release. Please avoid using this flag
I0114 18:50:51.677234  4737 state.cpp:33] Recovering state from '/tmp/FaultToleranceTest_ReregisterFrameworkExitedExecutor_ONrVug/meta'
I0114 18:50:51.684973  4739 status_update_manager.cpp:197] Recovering status update manager
I0114 18:50:51.687644  4739 slave.cpp:3519] Finished recovery
I0114 18:50:51.688698  4737 slave.cpp:613] New master detected at master@192.168.122.135:57018
I0114 18:50:51.688902  4734 status_update_manager.cpp:171] Pausing sending status updates
I0114 18:50:51.689482  4737 slave.cpp:676] Authenticating with master master@192.168.122.135:57018
I0114 18:50:51.689910  4737 slave.cpp:681] Using default CRAM-MD5 authenticatee
I0114 18:50:51.690577  4741 authenticatee.hpp:138] Creating new client SASL connection
I0114 18:50:51.691453  4737 slave.cpp:649] Detecting new master
I0114 18:50:51.691864  4741 master.cpp:4130] Authenticating slave(115)@192.168.122.135:57018
I0114 18:50:51.692369  4741 master.cpp:4141] Using default CRAM-MD5 authenticator
I0114 18:50:51.693208  4741 authenticator.hpp:170] Creating new server SASL connection
I0114 18:50:51.694598  4738 authenticatee.hpp:229] Received SASL authentication mechanisms: CRAM-MD5
I0114 18:50:51.694893  4738 authenticatee.hpp:255] Attempting to authenticate with mechanism 'CRAM-MD5'
I0114 18:50:51.695329  4741 authenticator.hpp:276] Received SASL authentication start
I0114 18:50:51.695641  4741 authenticator.hpp:398] Authentication requires more steps
I0114 18:50:51.696028  4736 authenticatee.hpp:275] Received SASL authentication step
I0114 18:50:51.696486  4741 authenticator.hpp:304] Received SASL authentication step
I0114 18:50:51.696753  4741 auxprop.cpp:99] Request to lookup properties for user: 'test-principal' realm: 'fedora-19' server FQDN: 'fedora-19' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0114 18:50:51.697041  4741 auxprop.cpp:171] Looking up auxiliary property '*userPassword'
I0114 18:50:51.697343  4741 auxprop.cpp:171] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0114 18:50:51.697685  4741 auxprop.cpp:99] Request to lookup properties for user: 'test-principal' realm: 'fedora-19' server FQDN: 'fedora-19' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0114 18:50:51.697998  4741 auxprop.cpp:121] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0114 18:50:51.698251  4741 auxprop.cpp:121] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0114 18:50:51.698580  4741 authenticator.hpp:390] Authentication success
I0114 18:50:51.698927  4735 authenticatee.hpp:315] Authentication success
I0114 18:50:51.705123  4741 master.cpp:4188] Successfully authenticated principal 'test-principal' at slave(115)@192.168.122.135:57018
I0114 18:50:51.705847  4720 sched.cpp:151] Version: 0.22.0
I0114 18:50:51.707159  4736 sched.cpp:248] New master detected at master@192.168.122.135:57018
I0114 18:50:51.707523  4736 sched.cpp:304] Authenticating with master master@192.168.122.135:57018
I0114 18:50:51.707792  4736 sched.cpp:311] Using default CRAM-MD5 authenticatee
I0114 18:50:51.708412  4736 authenticatee.hpp:138] Creating new client SASL connection
I0114 18:50:51.709316  4735 slave.cpp:747] Successfully authenticated with master master@192.168.122.135:57018
I0114 18:50:51.709723  4737 master.cpp:4130] Authenticating scheduler-092fbbec-0938-4355-8187-fb92e5174c64@192.168.122.135:57018
I0114 18:50:51.710274  4737 master.cpp:4141] Using default CRAM-MD5 authenticator
I0114 18:50:51.710739  4735 slave.cpp:1075] Will retry registration in 17.028024ms if necessary
I0114 18:50:51.711304  4737 master.cpp:3276] Registering slave at slave(115)@192.168.122.135:57018 (fedora-19) with id 20150114-185051-2272962752-57018-4720-S0
I0114 18:50:51.711459  4738 authenticator.hpp:170] Creating new server SASL connection
I0114 18:50:51.713142  4739 registrar.cpp:445] Applied 1 operations in 100530ns; attempting to update the 'registry'
I0114 18:50:51.713465  4738 authenticatee.hpp:229] Received SASL authentication mechanisms: CRAM-MD5
I0114 18:50:51.715435  4738 authenticatee.hpp:255] Attempting to authenticate with mechanism 'CRAM-MD5'
I0114 18:50:51.715963  4740 authenticator.hpp:276] Received SASL authentication start
I0114 18:50:51.716258  4740 authenticator.hpp:398] Authentication requires more steps
I0114 18:50:51.716524  4740 authenticatee.hpp:275] Received SASL authentication step
I0114 18:50:51.716784  4740 authenticator.hpp:304] Received SASL authentication step
I0114 18:50:51.716979  4740 auxprop.cpp:99] Request to lookup properties for user: 'test-principal' realm: 'fedora-19' server FQDN: 'fedora-19' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0114 18:50:51.717139  4740 auxprop.cpp:171] Looking up auxiliary property '*userPassword'
I0114 18:50:51.717315  4740 auxprop.cpp:171] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0114 18:50:51.717542  4740 auxprop.cpp:99] Request to lookup properties for user: 'test-principal' realm: 'fedora-19' server FQDN: 'fedora-19' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0114 18:50:51.717703  4740 auxprop.cpp:121] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0114 18:50:51.717864  4740 auxprop.cpp:121] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0114 18:50:51.718040  4740 authenticator.hpp:390] Authentication success
I0114 18:50:51.718292  4740 authenticatee.hpp:315] Authentication success
I0114 18:50:51.718454  4738 master.cpp:4188] Successfully authenticated principal 'test-principal' at scheduler-092fbbec-0938-4355-8187-fb92e5174c64@192.168.122.135:57018
I0114 18:50:51.719012  4740 sched.cpp:392] Successfully authenticated with master master@192.168.122.135:57018
I0114 18:50:51.719364  4740 sched.cpp:515] Sending registration request to master@192.168.122.135:57018
I0114 18:50:51.719702  4740 sched.cpp:548] Will retry registration in 746.539282ms if necessary
I0114 18:50:51.719902  4735 master.cpp:1417] Received registration request for framework 'default' at scheduler-092fbbec-0938-4355-8187-fb92e5174c64@192.168.122.135:57018
I0114 18:50:51.720232  4735 master.cpp:1298] Authorizing framework principal 'test-principal' to receive offers for role '*'
I0114 18:50:51.722206  4735 master.cpp:1481] Registering framework 20150114-185051-2272962752-57018-4720-0000 (default) at scheduler-092fbbec-0938-4355-8187-fb92e5174c64@192.168.122.135:57018
I0114 18:50:51.720927  4737 log.cpp:684] Attempting to append 300 bytes to the log
I0114 18:50:51.722924  4737 coordinator.cpp:340] Coordinator attempting to write APPEND action at position 3
I0114 18:50:51.724269  4737 replica.cpp:511] Replica received write request for position 3
I0114 18:50:51.724817  4737 leveldb.cpp:343] Persisting action (319 bytes) to leveldb took 116638ns
I0114 18:50:51.728560  4737 replica.cpp:679] Persisted action at 3
I0114 18:50:51.726066  4736 sched.cpp:442] Framework registered with 20150114-185051-2272962752-57018-4720-0000
I0114 18:50:51.728879  4736 sched.cpp:456] Scheduler::registered took 34885ns
I0114 18:50:51.725520  4735 hierarchical_allocator_process.hpp:319] Added framework 20150114-185051-2272962752-57018-4720-0000
I0114 18:50:51.731864  4735 hierarchical_allocator_process.hpp:839] No resources available to allocate!
I0114 18:50:51.732038  4735 hierarchical_allocator_process.hpp:746] Performed allocation for 0 slaves in 214728ns
I0114 18:50:51.733106  4738 replica.cpp:658] Replica received learned notice for position 3
I0114 18:50:51.733340  4738 leveldb.cpp:343] Persisting action (321 bytes) to leveldb took 83165ns
I0114 18:50:51.733538  4738 replica.cpp:679] Persisted action at 3
I0114 18:50:51.733705  4738 replica.cpp:664] Replica learned APPEND action at position 3
I0114 18:50:51.735610  4738 registrar.cpp:490] Successfully updated the 'registry' in 21.936128ms
I0114 18:50:51.735805  4739 log.cpp:703] Attempting to truncate the log to 3
I0114 18:50:51.736445  4739 coordinator.cpp:340] Coordinator attempting to write TRUNCATE action at position 4
I0114 18:50:51.737664  4739 replica.cpp:511] Replica received write request for position 4
I0114 18:50:51.738013  4739 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 72906ns
I0114 18:50:51.738255  4739 replica.cpp:679] Persisted action at 4
I0114 18:50:51.743397  4734 replica.cpp:658] Replica received learned notice for position 4
I0114 18:50:51.743628  4734 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 78832ns
I0114 18:50:51.743837  4734 leveldb.cpp:401] Deleting ~2 keys from leveldb took 63991ns
I0114 18:50:51.744004  4734 replica.cpp:679] Persisted action at 4
I0114 18:50:51.744168  4734 replica.cpp:664] Replica learned TRUNCATE action at position 4
I0114 18:50:51.745537  4738 master.cpp:3330] Registered slave 20150114-185051-2272962752-57018-4720-S0 at slave(115)@192.168.122.135:57018 (fedora-19) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I0114 18:50:51.745968  4734 hierarchical_allocator_process.hpp:453] Added slave 20150114-185051-2272962752-57018-4720-S0 (fedora-19) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (and cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] available)
I0114 18:50:51.746070  4735 slave.cpp:781] Registered with master master@192.168.122.135:57018; given slave ID 20150114-185051-2272962752-57018-4720-S0
I0114 18:50:51.751437  4741 status_update_manager.cpp:178] Resuming sending status updates
I0114 18:50:51.752428  4740 master.cpp:4072] Sending 1 offers to framework 20150114-185051-2272962752-57018-4720-0000 (default) at scheduler-092fbbec-0938-4355-8187-fb92e5174c64@192.168.122.135:57018
I0114 18:50:51.753764  4740 sched.cpp:605] Scheduler::resourceOffers took 751714ns
I0114 18:50:51.754812  4740 master.cpp:2541] Processing reply for offers: [ 20150114-185051-2272962752-57018-4720-O0 ] on slave 20150114-185051-2272962752-57018-4720-S0 at slave(115)@192.168.122.135:57018 (fedora-19) for framework 20150114-185051-2272962752-57018-4720-0000 (default) at scheduler-092fbbec-0938-4355-8187-fb92e5174c64@192.168.122.135:57018
I0114 18:50:51.755040  4740 master.cpp:2647] Authorizing framework principal 'test-principal' to launch task 0 as user 'jenkins'
W0114 18:50:51.756431  4741 master.cpp:2124] Executor default for task 0 uses less CPUs (None) than the minimum required (0.01). Please update your executor, as this will be mandatory in future releases.
W0114 18:50:51.756652  4741 master.cpp:2136] Executor default for task 0 uses less memory (None) than the minimum required (32MB). Please update your executor, as this will be mandatory in future releases.
I0114 18:50:51.757284  4741 master.hpp:766] Adding task 0 with resources cpus(*):1; mem(*):16 on slave 20150114-185051-2272962752-57018-4720-S0 (fedora-19)
I0114 18:50:51.757733  4734 hierarchical_allocator_process.hpp:764] Performed allocation for slave 20150114-185051-2272962752-57018-4720-S0 in 9.535066ms
I0114 18:50:51.758117  4735 slave.cpp:2588] Received ping from slave-observer(95)@192.168.122.135:57018
I0114 18:50:51.758630  4741 master.cpp:2897] Launching task 0 of framework 20150114-185051-2272962752-57018-4720-0000 (default) at scheduler-092fbbec-0938-4355-8187-fb92e5174c64@192.168.122.135:57018 with resources cpus(*):1; mem(*):16 on slave 20150114-185051-2272962752-57018-4720-S0 at slave(115)@192.168.122.135:57018 (fedora-19)
I0114 18:50:51.759526  4741 hierarchical_allocator_process.hpp:610] Updated allocation of framework 20150114-185051-2272962752-57018-4720-0000 on slave 20150114-185051-2272962752-57018-4720-S0 from cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] to cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I0114 18:50:51.759796  4737 slave.cpp:1130] Got assigned task 0 for framework 20150114-185051-2272962752-57018-4720-0000
I0114 18:50:51.761184  4737 slave.cpp:1245] Launching task 0 for framework 20150114-185051-2272962752-57018-4720-0000
I0114 18:50:51.763586  4741 hierarchical_allocator_process.hpp:653] Recovered cpus(*):1; mem(*):1008; disk(*):1024; ports(*):[31000-32000] (total allocatable: cpus(*):1; mem(*):1008; disk(*):1024; ports(*):[31000-32000]) on slave 20150114-185051-2272962752-57018-4720-S0 from framework 20150114-185051-2272962752-57018-4720-0000
I0114 18:50:51.764034  4741 hierarchical_allocator_process.hpp:689] Framework 20150114-185051-2272962752-57018-4720-0000 filtered slave 20150114-185051-2272962752-57018-4720-S0 for 5secs
I0114 18:50:51.764984  4737 slave.cpp:3921] Launching executor default of framework 20150114-185051-2272962752-57018-4720-0000 in work directory '/tmp/FaultToleranceTest_ReregisterFrameworkExitedExecutor_ONrVug/slaves/20150114-185051-2272962752-57018-4720-S0/frameworks/20150114-185051-2272962752-57018-4720-0000/executors/default/runs/dd104b76-b838-431e-ada9-ff7a2b07e694'
I0114 18:50:51.775048  4737 exec.cpp:147] Version: 0.22.0
I0114 18:50:51.778069  4736 exec.cpp:197] Executor started at: executor(29)@192.168.122.135:57018 with pid 4720
I0114 18:50:51.778722  4737 slave.cpp:1368] Queuing task '0' for executor default of framework '20150114-185051-2272962752-57018-4720-0000
I0114 18:50:51.779103  4737 slave.cpp:566] Successfully attached file '/tmp/FaultToleranceTest_ReregisterFrameworkExitedExecutor_ONrVug/slaves/20150114-185051-2272962752-57018-4720-S0/frameworks/20150114-185051-2272962752-57018-4720-0000/executors/default/runs/dd104b76-b838-431e-ada9-ff7a2b07e694'
I0114 18:50:51.779470  4737 slave.cpp:1912] Got registration for executor 'default' of framework 20150114-185051-2272962752-57018-4720-0000 from executor(29)@192.168.122.135:57018
I0114 18:50:51.780288  4740 exec.cpp:221] Executor registered on slave 20150114-185051-2272962752-57018-4720-S0
I0114 18:50:51.782098  4740 exec.cpp:233] Executor::registered took 61371ns
I0114 18:50:51.782616  4737 slave.cpp:2031] Flushing queued task 0 for executor 'default' of framework 20150114-185051-2272962752-57018-4720-0000
I0114 18:50:51.783262  4741 exec.cpp:308] Executor asked to run task '0'
I0114 18:50:51.783614  4741 exec.cpp:317] Executor::launchTask took 97020ns
I0114 18:50:51.785373  4741 exec.cpp:540] Executor sending status update TASK_RUNNING (UUID: 3f6824a3-8a23-4029-8505-8eb5f72e472b) for task 0 of framework 20150114-185051-2272962752-57018-4720-0000
I0114 18:50:51.785995  4737 slave.cpp:2890] Monitoring executor 'default' of framework '20150114-185051-2272962752-57018-4720-0000' in container 'dd104b76-b838-431e-ada9-ff7a2b07e694'
I0114 18:50:51.789064  4737 slave.cpp:2265] Handling status update TASK_RUNNING (UUID: 3f6824a3-8a23-4029-8505-8eb5f72e472b) for task 0 of framework 20150114-185051-2272962752-57018-4720-0000 from executor(29)@192.168.122.135:57018
I0114 18:50:51.789553  4735 status_update_manager.cpp:317] Received status update TASK_RUNNING (UUID: 3f6824a3-8a23-4029-8505-8eb5f72e472b) for task 0 of framework 20150114-185051-2272962752-57018-4720-0000
I0114 18:50:51.789827  4735 status_update_manager.cpp:494] Creating StatusUpdate stream for task 0 of framework 20150114-185051-2272962752-57018-4720-0000
I0114 18:50:51.790329  4735 status_update_manager.cpp:371] Forwarding update TASK_RUNNING (UUID: 3f6824a3-8a23-4029-8505-8eb5f72e472b) for task 0 of framework 20150114-185051-2272962752-57018-4720-0000 to the slave
I0114 18:50:51.790875  4737 slave.cpp:2508] Forwarding the update TASK_RUNNING (UUID: 3f6824a3-8a23-4029-8505-8eb5f72e472b) for task 0 of framework 20150114-185051-2272962752-57018-4720-0000 to master@192.168.122.135:57018
I0114 18:50:51.791442  4736 master.cpp:3653] Forwarding status update TASK_RUNNING (UUID: 3f6824a3-8a23-4029-8505-8eb5f72e472b) for task 0 of framework 20150114-185051-2272962752-57018-4720-0000
I0114 18:50:51.791813  4736 master.cpp:3625] Status update TASK_RUNNING (UUID: 3f6824a3-8a23-4029-8505-8eb5f72e472b) for task 0 of framework 20150114-185051-2272962752-57018-4720-0000 from slave 20150114-185051-2272962752-57018-4720-S0 at slave(115)@192.168.122.135:57018 (fedora-19)
I0114 18:50:51.792140  4736 master.cpp:4935] Updating the latest state of task 0 of framework 20150114-185051-2272962752-57018-4720-0000 to TASK_RUNNING
I0114 18:50:51.792690  4736 sched.cpp:696] Scheduler::statusUpdate took 70266ns
I0114 18:50:51.793184  4739 master.cpp:3126] Forwarding status update acknowledgement 3f6824a3-8a23-4029-8505-8eb5f72e472b for task 0 of framework 20150114-185051-2272962752-57018-4720-0000 (default) at scheduler-092fbbec-0938-4355-8187-fb92e5174c64@192.168.122.135:57018 to slave 20150114-185051-2272962752-57018-4720-S0 at slave(115)@192.168.122.135:57018 (fedora-19)
I0114 18:50:51.794311  4720 master.cpp:654] Master terminating
W0114 18:50:51.794908  4720 master.cpp:4980] Removing task 0 with resources cpus(*):1; mem(*):16 of framework 20150114-185051-2272962752-57018-4720-0000 on slave 20150114-185051-2272962752-57018-4720-S0 at slave(115)@192.168.122.135:57018 (fedora-19) in non-terminal state TASK_RUNNING
I0114 18:50:51.795251  4739 slave.cpp:2435] Status update manager successfully handled status update TASK_RUNNING (UUID: 3f6824a3-8a23-4029-8505-8eb5f72e472b) for task 0 of framework 20150114-185051-2272962752-57018-4720-0000
I0114 18:50:51.795881  4739 slave.cpp:2441] Sending acknowledgement for status update TASK_RUNNING (UUID: 3f6824a3-8a23-4029-8505-8eb5f72e472b) for task 0 of framework 20150114-185051-2272962752-57018-4720-0000 to executor(29)@192.168.122.135:57018
I0114 18:50:51.796308  4739 exec.cpp:354] Executor received status update acknowledgement 3f6824a3-8a23-4029-8505-8eb5f72e472b for task 0 of framework 20150114-185051-2272962752-57018-4720-0000
I0114 18:50:51.795326  4741 status_update_manager.cpp:389] Received status update acknowledgement (UUID: 3f6824a3-8a23-4029-8505-8eb5f72e472b) for task 0 of framework 20150114-185051-2272962752-57018-4720-0000
I0114 18:50:51.797854  4741 slave.cpp:1852] Status update manager successfully handled status update acknowledgement (UUID: 3f6824a3-8a23-4029-8505-8eb5f72e472b) for task 0 of framework 20150114-185051-2272962752-57018-4720-0000
I0114 18:50:51.797144  4720 master.cpp:5023] Removing executor 'default' with resources  of framework 20150114-185051-2272962752-57018-4720-0000 on slave 20150114-185051-2272962752-57018-4720-S0 at slave(115)@192.168.122.135:57018 (fedora-19)
I0114 18:50:51.796748  4734 hierarchical_allocator_process.hpp:653] Recovered cpus(*):1; mem(*):16 (total allocatable: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]) on slave 20150114-185051-2272962752-57018-4720-S0 from framework 20150114-185051-2272962752-57018-4720-0000
I0114 18:50:51.802438  4739 slave.cpp:2673] master@192.168.122.135:57018 exited
W0114 18:50:51.802707  4739 slave.cpp:2676] Master disconnected! Waiting for a new master to be elected
I0114 18:50:51.849522  4720 leveldb.cpp:176] Opened db in 1.773376ms
I0114 18:50:51.860327  4720 leveldb.cpp:183] Compacted db in 1.475626ms
I0114 18:50:51.860661  4720 leveldb.cpp:198] Created db iterator in 58499ns
I0114 18:50:51.861027  4720 leveldb.cpp:204] Seeked to beginning of db in 53681ns
I0114 18:50:51.861476  4720 leveldb.cpp:273] Iterated through 3 keys in the db in 195975ns
I0114 18:50:51.861803  4720 replica.cpp:744] Replica recovered with log positions 3 -> 4 with 0 holes and 0 unlearned
I0114 18:50:51.862931  4737 recover.cpp:449] Starting replica recovery
I0114 18:50:51.863837  4737 recover.cpp:475] Replica is in VOTING status
I0114 18:50:51.864320  4737 recover.cpp:464] Recover process terminated
I0114 18:50:51.912767  4734 master.cpp:262] Master 20150114-185051-2272962752-57018-4720 (fedora-19) started on 192.168.122.135:57018
I0114 18:50:51.913460  4734 master.cpp:308] Master only allowing authenticated frameworks to register
I0114 18:50:51.913712  4734 master.cpp:313] Master only allowing authenticated slaves to register
I0114 18:50:51.914023  4734 credentials.hpp:36] Loading credentials for authentication from '/tmp/FaultToleranceTest_ReregisterFrameworkExitedExecutor_yNprKi/credentials'
I0114 18:50:51.914626  4734 master.cpp:357] Authorization enabled
I0114 18:50:51.915576  4739 hierarchical_allocator_process.hpp:285] Initialized hierarchical allocator process
I0114 18:50:51.916064  4735 whitelist_watcher.cpp:65] No whitelist given
I0114 18:50:51.919319  4734 master.cpp:1219] The newly elected leader is master@192.168.122.135:57018 with id 20150114-185051-2272962752-57018-4720
I0114 18:50:51.921718  4734 master.cpp:1232] Elected as the leading master!
I0114 18:50:51.921975  4734 master.cpp:1050] Recovering from registrar
I0114 18:50:51.922523  4738 registrar.cpp:313] Recovering registrar
I0114 18:50:51.924142  4738 log.cpp:660] Attempting to start the writer
I0114 18:50:51.926363  4739 replica.cpp:477] Replica received implicit promise request with proposal 2
I0114 18:50:51.927110  4739 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 147268ns
I0114 18:50:51.927486  4739 replica.cpp:345] Persisted promised to 2
I0114 18:50:51.935008  4741 coordinator.cpp:230] Coordinator attemping to fill missing position
I0114 18:50:51.935816  4741 log.cpp:676] Writer started with ending position 4
I0114 18:50:51.937769  4739 leveldb.cpp:438] Reading position from leveldb took 108522ns
I0114 18:50:51.938480  4739 leveldb.cpp:438] Reading position from leveldb took 171418ns
I0114 18:50:51.942811  4740 registrar.cpp:346] Successfully fetched the registry (261B) in 19.91296ms
I0114 18:50:51.943493  4740 registrar.cpp:445] Applied 1 operations in 96988ns; attempting to update the 'registry'
I0114 18:50:51.946138  4737 log.cpp:684] Attempting to append 300 bytes to the log
I0114 18:50:51.950773  4737 coordinator.cpp:340] Coordinator attempting to write APPEND action at position 5
I0114 18:50:51.954259  4739 replica.cpp:511] Replica received write request for position 5
I0114 18:50:51.958901  4739 leveldb.cpp:343] Persisting action (319 bytes) to leveldb took 351525ns
I0114 18:50:51.959277  4739 replica.cpp:679] Persisted action at 5
I0114 18:50:51.966125  4736 replica.cpp:658] Replica received learned notice for position 5
I0114 18:50:51.966882  4736 leveldb.cpp:343] Persisting action (321 bytes) to leveldb took 114790ns
I0114 18:50:51.967159  4736 replica.cpp:679] Persisted action at 5
I0114 18:50:51.967515  4736 replica.cpp:664] Replica learned APPEND action at position 5
I0114 18:50:51.971989  4739 registrar.cpp:490] Successfully updated the 'registry' in 28.18304ms
I0114 18:50:51.972854  4739 registrar.cpp:376] Successfully recovered registrar
I0114 18:50:51.973675  4737 master.cpp:1077] Recovered 1 slaves from the Registry (261B) ; allowing 10mins for slaves to re-register
I0114 18:50:51.974957  4737 log.cpp:703] Attempting to truncate the log to 5
I0114 18:50:51.975620  4740 coordinator.cpp:340] Coordinator attempting to write TRUNCATE action at position 6
I0114 18:50:51.977298  4740 replica.cpp:511] Replica received write request for position 6
I0114 18:50:51.978060  4740 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 108071ns
I0114 18:50:51.978374  4740 replica.cpp:679] Persisted action at 6
I0114 18:50:51.982532  4737 replica.cpp:658] Replica received learned notice for position 6
I0114 18:50:51.983160  4737 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 89982ns
I0114 18:50:51.983505  4737 leveldb.cpp:401] Deleting ~2 keys from leveldb took 64662ns
I0114 18:50:51.983806  4737 replica.cpp:679] Persisted action at 6
I0114 18:50:51.984136  4737 replica.cpp:664] Replica learned TRUNCATE action at position 6
I0114 18:50:51.997160  4740 slave.cpp:613] New master detected at master@192.168.122.135:57018
I0114 18:50:51.998111  4740 slave.cpp:676] Authenticating with master master@192.168.122.135:57018
I0114 18:50:51.998437  4740 slave.cpp:681] Using default CRAM-MD5 authenticatee
I0114 18:50:51.999161  4734 authenticatee.hpp:138] Creating new client SASL connection
I0114 18:50:51.997766  4735 status_update_manager.cpp:171] Pausing sending status updates
I0114 18:50:52.000628  4740 slave.cpp:649] Detecting new master
I0114 18:50:52.001258  4734 master.cpp:4130] Authenticating slave(115)@192.168.122.135:57018
I0114 18:50:52.002085  4734 master.cpp:4141] Using default CRAM-MD5 authenticator
I0114 18:50:52.003057  4734 authenticator.hpp:170] Creating new server SASL connection
I0114 18:50:52.004458  4735 authenticatee.hpp:229] Received SASL authentication mechanisms: CRAM-MD5
I0114 18:50:52.004762  4735 authenticatee.hpp:255] Attempting to authenticate with mechanism 'CRAM-MD5'
I0114 18:50:52.005210  4734 authenticator.hpp:276] Received SASL authentication start
I0114 18:50:52.005544  4734 authenticator.hpp:398] Authentication requires more steps
I0114 18:50:52.006116  4736 authenticatee.hpp:275] Received SASL authentication step
I0114 18:50:52.006676  4734 authenticator.hpp:304] Received SASL authentication step
I0114 18:50:52.007045  4734 auxprop.cpp:99] Request to lookup properties for user: 'test-principal' realm: 'fedora-19' server FQDN: 'fedora-19' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0114 18:50:52.007340  4734 auxprop.cpp:171] Looking up auxiliary property '*userPassword'
I0114 18:50:52.007733  4734 auxprop.cpp:171] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0114 18:50:52.008149  4734 auxprop.cpp:99] Request to lookup properties for user: 'test-principal' realm: 'fedora-19' server FQDN: 'fedora-19' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0114 18:50:52.008437  4734 auxprop.cpp:121] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0114 18:50:52.008714  4734 auxprop.cpp:121] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0114 18:50:52.009009  4734 authenticator.hpp:390] Authentication success
I0114 18:50:52.009459  4741 authenticatee.hpp:315] Authentication success
I0114 18:50:52.018327  4738 master.cpp:4188] Successfully authenticated principal 'test-principal' at slave(115)@192.168.122.135:57018
I0114 18:50:52.018959  4741 slave.cpp:747] Successfully authenticated with master master@192.168.122.135:57018
I0114 18:50:52.020071  4739 master.cpp:3453] Re-registering slave 20150114-185051-2272962752-57018-4720-S0 at slave(115)@192.168.122.135:57018 (fedora-19)
I0114 18:50:52.021256  4739 registrar.cpp:445] Applied 1 operations in 109203ns; attempting to update the 'registry'
I0114 18:50:52.023926  4737 log.cpp:684] Attempting to append 300 bytes to the log
I0114 18:50:52.024710  4735 coordinator.cpp:340] Coordinator attempting to write APPEND action at position 7
I0114 18:50:52.026480  4734 replica.cpp:511] Replica received write request for position 7
I0114 18:50:52.027065  4734 leveldb.cpp:343] Persisting action (319 bytes) to leveldb took 109150ns
I0114 18:50:52.027524  4734 replica.cpp:679] Persisted action at 7
I0114 18:50:52.028818  4738 replica.cpp:658] Replica received learned notice for position 7
I0114 18:50:52.029525  4738 leveldb.cpp:343] Persisting action (321 bytes) to leveldb took 185197ns
I0114 18:50:52.029930  4738 replica.cpp:679] Persisted action at 7
I0114 18:50:52.030205  4738 replica.cpp:664] Replica learned APPEND action at position 7
I0114 18:50:52.031692  4735 log.cpp:703] Attempting to truncate the log to 7
I0114 18:50:52.032083  4740 coordinator.cpp:340] Coordinator attempting to write TRUNCATE action at position 8
I0114 18:50:52.033411  4740 replica.cpp:511] Replica received write request for position 8
I0114 18:50:52.033768  4740 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 78202ns
I0114 18:50:52.034054  4740 replica.cpp:679] Persisted action at 8
I0114 18:50:52.035274  4740 replica.cpp:658] Replica received learned notice for position 8
I0114 18:50:52.035912  4740 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 80144ns
I0114 18:50:52.036262  4740 leveldb.cpp:401] Deleting ~2 keys from leveldb took 93273ns
I0114 18:50:52.036558  4740 replica.cpp:679] Persisted action at 8
I0114 18:50:52.036883  4740 replica.cpp:664] Replica learned TRUNCATE action at position 8
I0114 18:50:52.038254  4741 slave.cpp:1075] Will retry registration in 5.240065ms if necessary
I0114 18:50:52.044471  4739 registrar.cpp:490] Successfully updated the 'registry' in 22.825984ms
I0114 18:50:52.045918  4740 master.hpp:766] Adding task 0 with resources cpus(*):1; mem(*):16 on slave 20150114-185051-2272962752-57018-4720-S0 (fedora-19)
W0114 18:50:52.052153  4740 master.cpp:4697] Possibly orphaned task 0 of framework 20150114-185051-2272962752-57018-4720-0000 running on slave 20150114-185051-2272962752-57018-4720-S0 at slave(115)@192.168.122.135:57018 (fedora-19)
I0114 18:50:52.053467  4738 hierarchical_allocator_process.hpp:453] Added slave 20150114-185051-2272962752-57018-4720-S0 (fedora-19) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (and cpus(*):1; mem(*):1008; disk(*):1024; ports(*):[31000-32000] available)
I0114 18:50:52.054124  4738 hierarchical_allocator_process.hpp:839] No resources available to allocate!
I0114 18:50:52.054733  4738 hierarchical_allocator_process.hpp:764] Performed allocation for slave 20150114-185051-2272962752-57018-4720-S0 in 795150ns
I0114 18:50:52.055675  4736 slave.cpp:1075] Will retry registration in 4.9981ms if necessary
I0114 18:50:52.056367  4736 slave.cpp:2588] Received ping from slave-observer(96)@192.168.122.135:57018
I0114 18:50:52.056958  4740 master.cpp:3521] Re-registered slave 20150114-185051-2272962752-57018-4720-S0 at slave(115)@192.168.122.135:57018 (fedora-19) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I0114 18:50:52.057782  4740 master.cpp:3402] Re-registering slave 20150114-185051-2272962752-57018-4720-S0 at slave(115)@192.168.122.135:57018 (fedora-19)
I0114 18:50:52.058290  4734 slave.cpp:849] Re-registered with master master@192.168.122.135:57018
I0114 18:50:52.061352  4734 slave.cpp:2948] Executor 'default' of framework 20150114-185051-2272962752-57018-4720-0000 exited with status 0
I0114 18:50:52.061640  4737 status_update_manager.cpp:178] Resuming sending status updates
I0114 18:50:52.064230  4734 slave.cpp:2265] Handling status update TASK_LOST (UUID: 9b52bc70-76aa-4923-be0e-f14669185255) for task 0 of framework 20150114-185051-2272962752-57018-4720-0000 from @0.0.0.0:0
I0114 18:50:52.064846  4734 slave.cpp:4229] Terminating task 0
W0114 18:50:52.065830  4734 slave.cpp:856] Already re-registered with master master@192.168.122.135:57018
I0114 18:50:52.067150  4739 master.cpp:3705] Executor default of framework 20150114-185051-2272962752-57018-4720-0000 on slave 20150114-185051-2272962752-57018-4720-S0 at slave(115)@192.168.122.135:57018 (fedora-19) exited with status 0
I0114 18:50:52.070163  4737 status_update_manager.cpp:317] Received status update TASK_LOST (UUID: 9b52bc70-76aa-4923-be0e-f14669185255) for task 0 of framework 20150114-185051-2272962752-57018-4720-0000
I0114 18:50:52.070940  4737 status_update_manager.cpp:371] Forwarding update TASK_LOST (UUID: 9b52bc70-76aa-4923-be0e-f14669185255) for task 0 of framework 20150114-185051-2272962752-57018-4720-0000 to the slave
I0114 18:50:52.071951  4736 sched.cpp:242] Scheduler::disconnected took 43823ns
I0114 18:50:52.072419  4736 sched.cpp:248] New master detected at master@192.168.122.135:57018
I0114 18:50:52.072935  4736 sched.cpp:304] Authenticating with master master@192.168.122.135:57018
I0114 18:50:52.073321  4736 sched.cpp:311] Using default CRAM-MD5 authenticatee
I0114 18:50:52.074064  4736 authenticatee.hpp:138] Creating new client SASL connection
I0114 18:50:52.076202  4734 slave.cpp:2508] Forwarding the update TASK_LOST (UUID: 9b52bc70-76aa-4923-be0e-f14669185255) for task 0 of framework 20150114-185051-2272962752-57018-4720-0000 to master@192.168.122.135:57018
I0114 18:50:52.077155  4734 slave.cpp:2435] Status update manager successfully handled status update TASK_LOST (UUID: 9b52bc70-76aa-4923-be0e-f14669185255) for task 0 of framework 20150114-185051-2272962752-57018-4720-0000
I0114 18:50:52.076659  4739 master.cpp:5023] Removing executor 'default' with resources  of framework 20150114-185051-2272962752-57018-4720-0000 on slave 20150114-185051-2272962752-57018-4720-S0 at slave(115)@192.168.122.135:57018 (fedora-19)
I0114 18:50:52.080638  4739 master.cpp:4130] Authenticating scheduler-092fbbec-0938-4355-8187-fb92e5174c64@192.168.122.135:57018
I0114 18:50:52.081056  4739 master.cpp:4141] Using default CRAM-MD5 authenticator
I0114 18:50:52.081892  4741 authenticator.hpp:170] Creating new server SASL connection
I0114 18:50:52.083005  4741 authenticatee.hpp:229] Received SASL authentication mechanisms: CRAM-MD5
I0114 18:50:52.083470  4741 authenticatee.hpp:255] Attempting to authenticate with mechanism 'CRAM-MD5'
I0114 18:50:52.083953  4741 authenticator.hpp:276] Received SASL authentication start
I0114 18:50:52.084355  4741 authenticator.hpp:398] Authentication requires more steps
I0114 18:50:52.084794  4741 authenticatee.hpp:275] Received SASL authentication step
I0114 18:50:52.085310  4737 authenticator.hpp:304] Received SASL authentication step
I0114 18:50:52.085654  4737 auxprop.cpp:99] Request to lookup properties for user: 'test-principal' realm: 'fedora-19' server FQDN: 'fedora-19' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0114 18:50:52.085969  4737 auxprop.cpp:171] Looking up auxiliary property '*userPassword'
I0114 18:50:52.086297  4737 auxprop.cpp:171] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0114 18:50:52.086642  4737 auxprop.cpp:99] Request to lookup properties for user: 'test-principal' realm: 'fedora-19' server FQDN: 'fedora-19' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0114 18:50:52.086942  4737 auxprop.cpp:121] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0114 18:50:52.087226  4737 auxprop.cpp:121] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0114 18:50:52.087550  4737 authenticator.hpp:390] Authentication success
I0114 18:50:52.087934  4741 authenticatee.hpp:315] Authentication success
I0114 18:50:52.088513  4741 sched.cpp:392] Successfully authenticated with master master@192.168.122.135:57018
I0114 18:50:52.088899  4741 sched.cpp:515] Sending registration request to master@192.168.122.135:57018
I0114 18:50:52.089360  4741 sched.cpp:548] Will retry registration in 1.858884079secs if necessary
I0114 18:50:52.090150  4739 master.cpp:1522] Queuing up re-registration request for framework 20150114-185051-2272962752-57018-4720-0000 (default) at scheduler-092fbbec-0938-4355-8187-fb92e5174c64@192.168.122.135:57018 because authentication is still in progress
I0114 18:50:52.095142  4739 master.cpp:4188] Successfully authenticated principal 'test-principal' at scheduler-092fbbec-0938-4355-8187-fb92e5174c64@192.168.122.135:57018
I0114 18:50:52.108275  4739 master.cpp:1554] Received re-registration request from framework 20150114-185051-2272962752-57018-4720-0000 (default) at scheduler-092fbbec-0938-4355-8187-fb92e5174c64@192.168.122.135:57018
I0114 18:50:52.108742  4739 master.cpp:1298] Authorizing framework principal 'test-principal' to receive offers for role '*'
I0114 18:50:52.109735  4739 master.cpp:1607] Re-registering framework 20150114-185051-2272962752-57018-4720-0000 (default)  at scheduler-092fbbec-0938-4355-8187-fb92e5174c64@192.168.122.135:57018
I0114 18:50:52.110985  4735 hierarchical_allocator_process.hpp:319] Added framework 20150114-185051-2272962752-57018-4720-0000
I0114 18:50:52.120640  4735 hierarchical_allocator_process.hpp:746] Performed allocation for 1 slaves in 9.254989ms
I0114 18:50:52.121709  4734 slave.cpp:1762] Updating framework 20150114-185051-2272962752-57018-4720-0000 pid to scheduler-092fbbec-0938-4355-8187-fb92e5174c64@192.168.122.135:57018
I0114 18:50:52.122190  4734 status_update_manager.cpp:178] Resuming sending status updates
W0114 18:50:52.122694  4734 status_update_manager.cpp:185] Resending status update TASK_LOST (UUID: 9b52bc70-76aa-4923-be0e-f14669185255) for task 0 of framework 20150114-185051-2272962752-57018-4720-0000
I0114 18:50:52.123072  4734 status_update_manager.cpp:371] Forwarding update TASK_LOST (UUID: 9b52bc70-76aa-4923-be0e-f14669185255) for task 0 of framework 20150114-185051-2272962752-57018-4720-0000 to the slave
I0114 18:50:52.123733  4734 slave.cpp:2508] Forwarding the update TASK_LOST (UUID: 9b52bc70-76aa-4923-be0e-f14669185255) for task 0 of framework 20150114-185051-2272962752-57018-4720-0000 to master@192.168.122.135:57018
I0114 18:50:52.124590  4720 sched.cpp:1471] Asked to stop the driver
I0114 18:50:52.125461  4739 master.cpp:4072] Sending 1 offers to framework 20150114-185051-2272962752-57018-4720-0000 (default) at scheduler-092fbbec-0938-4355-8187-fb92e5174c64@192.168.122.135:57018
I0114 18:50:52.126096  4739 master.cpp:654] Master terminating
W0114 18:50:52.126626  4739 master.cpp:4980] Removing task 0 with resources cpus(*):1; mem(*):16 of framework 20150114-185051-2272962752-57018-4720-0000 on slave 20150114-185051-2272962752-57018-4720-S0 at slave(115)@192.168.122.135:57018 (fedora-19) in non-terminal state TASK_RUNNING
I0114 18:50:52.125669  4735 sched.cpp:423] Ignoring framework registered message because the driver is not running!
I0114 18:50:52.127410  4735 sched.cpp:808] Stopping framework '20150114-185051-2272962752-57018-4720-0000'
I0114 18:50:52.128592  4735 hierarchical_allocator_process.hpp:653] Recovered cpus(*):1; mem(*):16 (total allocatable: cpus(*):1; mem(*):16) on slave 20150114-185051-2272962752-57018-4720-S0 from framework 20150114-185051-2272962752-57018-4720-0000
I0114 18:50:52.132880  4740 slave.cpp:2673] master@192.168.122.135:57018 exited
W0114 18:50:52.133318  4740 slave.cpp:2676] Master disconnected! Waiting for a new master to be elected
I0114 18:50:52.173943  4720 slave.cpp:495] Slave terminating
I0114 18:50:52.174928  4720 slave.cpp:1585] Asked to shut down framework 20150114-185051-2272962752-57018-4720-0000 by @0.0.0.0:0
I0114 18:50:52.175448  4720 slave.cpp:1610] Shutting down framework 20150114-185051-2272962752-57018-4720-0000
I0114 18:50:52.175858  4720 slave.cpp:3057] Cleaning up executor 'default' of framework 20150114-185051-2272962752-57018-4720-0000
I0114 18:50:52.176615  4740 gc.cpp:56] Scheduling '/tmp/FaultToleranceTest_ReregisterFrameworkExitedExecutor_ONrVug/slaves/20150114-185051-2272962752-57018-4720-S0/frameworks/20150114-185051-2272962752-57018-4720-0000/executors/default/runs/dd104b76-b838-431e-ada9-ff7a2b07e694' for gc 6.99999795726815days in the future
I0114 18:50:52.177549  4734 gc.cpp:56] Scheduling '/tmp/FaultToleranceTest_ReregisterFrameworkExitedExecutor_ONrVug/slaves/20150114-185051-2272962752-57018-4720-S0/frameworks/20150114-185051-2272962752-57018-4720-0000/executors/default' for gc 6.99999794655111days in the future
I0114 18:50:52.178169  4720 slave.cpp:3136] Cleaning up framework 20150114-185051-2272962752-57018-4720-0000
I0114 18:50:52.178683  4741 status_update_manager.cpp:279] Closing status update streams for framework 20150114-185051-2272962752-57018-4720-0000
I0114 18:50:52.179054  4741 status_update_manager.cpp:525] Cleaning up status update stream for task 0 of framework 20150114-185051-2272962752-57018-4720-0000
I0114 18:50:52.179730  4737 gc.cpp:56] Scheduling '/tmp/FaultToleranceTest_ReregisterFrameworkExitedExecutor_ONrVug/slaves/20150114-185051-2272962752-57018-4720-S0/frameworks/20150114-185051-2272962752-57018-4720-0000' for gc 6.9999979210637days in the future
tests/fault_tolerance_tests.cpp:1213: Failure
Actual function call count doesn't match EXPECT_CALL(sched, registered(&driver, _, _))...
         Expected: to be called once
           Actual: never called - unsatisfied and active
[  FAILED  ] FaultToleranceTest.ReregisterFrameworkExitedExecutor (776 ms)
{code}",Bug,Major,vinodkone,2015-01-20T19:53:48.000+0000,5,Resolved,Complete,FaultToleranceTest.ReregisterFrameworkExitedExecutor is flaky,2015-01-20T19:53:48.000+0000,MESOS-2225,2.0,mesos,Twitter Mesos Q1 Sprint 1
,2015-01-15T02:43:30.000+0000,bmahler,"In order to authorize the HTTP endpoints for maintenance (to be added in MESOS-2067), we will need to add an ACL definition for performing maintenance operations.",Task,Major,bmahler,,10020,Accepted,In Progress,Add ACLs for the maintenance HTTP endpoints.,2016-04-27T06:00:21.000+0000,MESOS-2222,3.0,mesos,
tnachen,2015-01-13T16:06:16.000+0000,SteveNiemitz,"Once the slave restarts and recovers the task, I see this error in the log for all tasks that were recovered every second or so.  Note, these were NOT docker tasks:

W0113 16:01:00.790323 773142 monitor.cpp:213] Failed to get resource usage for  container 7b729b89-dc7e-4d08-af97-8cd1af560a21 for executor thermos-1421085237813-slipstream-prod-agent-3-8f769514-1835-4151-90d0-3f55dcc940dd of framework 20150109-161713-715350282-5050-290797-0000: Failed to 'docker inspect mesos-7b729b89-dc7e-4d08-af97-8cd1af560a21': exit status = exited with status 1 stderr = Error: No such image or container: mesos-7b729b89-dc7e-4d08-af97-8cd1af560a21
However the tasks themselves are still healthy and running.

The slave was launched with --containerizers=mesos,docker

-----
More info: it looks like the docker containerizer is a little too ambitious about recovering containers, again this was not a docker task:
I0113 15:59:59.476145 773142 docker.cpp:814] Recovering container '7b729b89-dc7e-4d08-af97-8cd1af560a21' for executor 'thermos-1421085237813-slipstream-prod-agent-3-8f769514-1835-4151-90d0-3f55dcc940dd' of framework 20150109-161713-715350282-5050-290797-0000

Looking into the source, it looks like the problem is that the ComposingContainerize runs recover in parallel, but neither the docker containerizer nor mesos containerizer check if they should recover the task or not (because they were the ones that launched it).  Perhaps this needs to be written into the checkpoint somewhere?",Bug,Major,SteveNiemitz,2015-05-25T21:44:18.000+0000,5,Resolved,Complete,"The Docker containerizer attempts to recover any task when checkpointing is enabled, not just docker tasks.",2015-07-02T18:43:15.000+0000,MESOS-2215,8.0,mesos,Mesosphere Q1 Sprint 3 - 2/20
haosdent@gmail.com,2015-01-08T23:01:25.000+0000,jieyu,"As we introduce persistent volumes in MESOS-1524, we will use roles as directory names on the slave (https://reviews.apache.org/r/28562/). As a result, the master should disallow special characters (like space and slash) in role.",Task,Major,jieyu,2016-01-14T10:00:56.000+0000,5,Resolved,Complete,Disallow special characters in role.,2016-01-15T07:43:13.000+0000,MESOS-2210,2.0,mesos,Mesosphere Sprint 22
mcypark,2014-12-30T02:01:38.000+0000,mcypark,"Add a user guide for reservations which describes basic usage of them, how ACLs are used to specify who can unreserve whose resources, and few advanced usage cases.",Documentation,Critical,mcypark,2015-07-24T01:15:28.000+0000,5,Resolved,Complete,Add user documentation for reservations,2015-07-24T01:15:28.000+0000,MESOS-2205,2.0,mesos,Mesosphere Q1 Sprint 5 - 3/20
js84,2014-12-19T20:49:02.000+0000,jaybuff,"When a scheduler specifies a bogus image in ContainerInfo mesos doesn't tell the scheduler that the docker pull failed or why.

This error is logged in the mesos-slave log, but it isn't given to the scheduler (as far as I can tell):

{noformat}
E1218 23:50:55.406230  8123 slave.cpp:2730] Container '8f70784c-3e40-4072-9ca2-9daed23f15ff' for executor 'thermos-1418946354013-xxx-xxx-curl-0-f500cc41-dd0a-4338-8cbc-d631cb588bb1' of framework '20140522-213145-1749004561-5050-29512-0000' failed to start: Failed to 'docker pull docker-registry.example.com/doesntexist/hello1.1:latest': exit status = exited with status 1 stderr = 2014/12/18 23:50:55 Error: image doesntexist/hello1.1 not found
{noformat}

If the docker image is not in the registry, the scheduler should give the user an error message.  If docker pull failed because of networking issues, it should be retried.  Mesos should give the scheduler enough information to be able to make that decision.",Bug,Major,jaybuff,,10020,Accepted,In Progress,bogus docker images result in bad error message to scheduler,2015-08-17T23:42:43.000+0000,MESOS-2200,2.0,mesos,Mesosphere Sprint 15
haosdent@gmail.com,2014-12-19T01:31:56.000+0000,idownes,"Appears that running the executor as {{nobody}} is not supported.

[~nnielsen] can you take a look?

Executor log:
{noformat}
[root@hostname build]# cat /tmp/SlaveTest_ROOT_RunTaskWithCommandInfoWithUser_cxF1dY/slaves/20141219-005206-2081170186-60487-11862-S0/frameworks/20141219-005206-2081170186-60
487-11862-0000/executors/1/runs/latest/std*
sh: /home/idownes/workspace/mesos/build/src/mesos-executor: Permission denied
{noformat}

Test output:
{noformat}
[==========] Running 1 test from 1 test case.
[----------] Global test environment set-up.
[----------] 1 test from SlaveTest
[ RUN      ] SlaveTest.ROOT_RunTaskWithCommandInfoWithUser
../../src/tests/slave_tests.cpp:680: Failure
Value of: statusRunning.get().state()
  Actual: TASK_FAILED
Expected: TASK_RUNNING
../../src/tests/slave_tests.cpp:682: Failure
Failed to wait 10secs for statusFinished
../../src/tests/slave_tests.cpp:673: Failure
Actual function call count doesn't match EXPECT_CALL(sched, statusUpdate(&driver, _))...
         Expected: to be called twice
           Actual: called once - unsatisfied and active
[  FAILED  ] SlaveTest.ROOT_RunTaskWithCommandInfoWithUser (10641 ms)
[----------] 1 test from SlaveTest (10641 ms total)

[----------] Global test environment tear-down
[==========] 1 test from 1 test case ran. (10658 ms total)
{noformat}",Bug,Major,idownes,,10020,Accepted,In Progress,Failing test: SlaveTest.ROOT_RunTaskWithCommandInfoWithUser,2016-03-03T07:10:45.000+0000,MESOS-2199,2.0,mesos,
tnachen,2014-12-12T05:12:22.000+0000,mneuhaus,"{{TaskStatus}} provides the frameworks with certain information ({{executorId}}, {{slaveId}}, etc.) which is useful when collecting statistics about cluster performance; however, it is difficult to associate tasks to the container it is executed since this information stays always within mesos itself. Therefore it would be good to provide the framework scheduler with this information, adding a new field in the {{TaskStatus}} message.

See comments for a use case.",Wish,Major,mneuhaus,2015-06-09T07:36:26.000+0000,5,Resolved,Complete,Add ContainerId to the TaskStatus message,2015-07-02T07:50:18.000+0000,MESOS-2191,3.0,mesos,Mesosphere Sprint 12
idownes,2014-12-09T22:24:36.000+0000,dhamon,cgroups_subsystems is a slave flag that is no longer used and should be deprecated.,Task,Minor,dhamon,2015-01-26T22:55:31.000+0000,5,Resolved,Complete,deprecate unused flag 'cgroups_subsystems',2015-01-26T22:55:31.000+0000,MESOS-2184,1.0,mesos,Twitter Mesos Q1 Sprint 1
bmahler,2014-12-09T01:04:03.000+0000,bmahler,"Noticed an issue in production under which the master is slow to respond after failover for ~15 minutes.

After looking at some perf data, the top offender is:

{noformat}
    12.02%  mesos-master  libmesos-0.21.0-rc3.so  [.] std::_Rb_tree<process::ProcessBase*, process::ProcessBase*, std::_Identity<process::ProcessBase*>, std::less<process::ProcessBase*>, std::allocator<process::ProcessBase*> >::erase(process::ProcessBase* const&)
...
     3.29%  mesos-master  libmesos-0.21.0-rc3.so  [.] process::SocketManager::exited(process::ProcessBase*)
{noformat}

It appears that in the SocketManager, whenever an internal Process exits, we loop over all the links unnecessarily:

{code}
void SocketManager::exited(ProcessBase* process)
{
  // An exited event is enough to cause the process to get deleted
  // (e.g., by the garbage collector), which means we can't
  // dereference process (or even use the address) after we enqueue at
  // least one exited event. Thus, we save the process pid.
  const UPID pid = process->pid;

  // Likewise, we need to save the current time of the process so we
  // can update the clocks of linked processes as appropriate.
  const Time time = Clock::now(process);

  synchronized (this) {
    // Iterate through the links, removing any links the process might
    // have had and creating exited events for any linked processes.
    foreachpair (const UPID& linkee, set<ProcessBase*>& processes, links) {
      processes.erase(process);

      if (linkee == pid) {
        foreach (ProcessBase* linker, processes) {
          CHECK(linker != process) << ""Process linked with itself"";
          synchronized (timeouts) {
            if (Clock::paused()) {
              Clock::update(linker, time);
            }
          }
          linker->enqueue(new ExitedEvent(linkee));
        }
      }
    }

    links.erase(pid);
  }
}
{code}

On clusters with 10,000s of slaves, this means we hold the socket manager lock for a very expensive loop erasing nothing from a set! This is because, the master contains links from the Master Process to each slave. However, when a random ephemeral Process terminates, we don't need to loop over each slave link.

While we hold this lock, the following calls will block:

{code}
class SocketManager
{
public:
  Socket accepted(int s);
  void link(ProcessBase* process, const UPID& to);
  PID<HttpProxy> proxy(const Socket& socket);
  void send(Encoder* encoder, bool persist);
  void send(const Response& response,
            const Request& request,
            const Socket& socket);
  void send(Message* message);
  Encoder* next(int s);
  void close(int s);
  void exited(const Node& node);
  void exited(ProcessBase* process);
...
{code}

As a result, the slave observers and the master can block calling send()!

Short term, we will try to fix this issue by removing the unnecessary looping. Longer term, it would be nice to avoid all this locking when sending on independent sockets.",Bug,Blocker,bmahler,2014-12-10T07:17:29.000+0000,5,Resolved,Complete,Performance issue in libprocess SocketManager.,2014-12-19T20:21:54.000+0000,MESOS-2182,3.0,mesos,Twitter Mesos Q4 Sprint 5
bmahler,2014-12-05T19:36:38.000+0000,bmahler,"Looking through the allocator code for MESOS-2099, I see an issue with respect to accounting reserved resources in the sorters:

Within {{HierarchicalAllocatorProcess::allocate}}, only unreserved resources are accounted for in the sorters, whereas everywhere else (add/remove framework, add/remove slave) we account for both reserved and unreserved.

From git blame, it looks like this issue was introduced over a long course of refactoring and fixes to the allocator. My guess is that this was never caught due to the lack of unit-testability of the allocator (unnecessarily requires a master PID to use an allocator).

From my understanding, the two levels of the hierarchical sorter should have the following semantics:

# Level 1 sorts across roles. Only unreserved resources are shared across roles, and therefore the ""role sorter"" for level 1 should only account for the unreserved resource pool.
# Level 2 sorts across frameworks, within a role. Both unreserved and reserved resources are shared across frameworks within a role, and therefore the ""framework sorters"" for level 2 should each account for the reserved resource pool for the role, as well as the unreserved resources _allocated_ inside the role.",Bug,Major,bmahler,2014-12-12T22:00:44.000+0000,5,Resolved,Complete,Hierarchical allocator inconsistently accounts for reserved resources. ,2014-12-12T22:00:45.000+0000,MESOS-2176,5.0,mesos,Twitter Mesos Q4 Sprint 5
bernd-mesos,2014-12-04T02:03:08.000+0000,bernd-mesos,"There are 5 env vars for that transport parameters into the fetcher program. Once we have a fetcher cache, it might be 7. The env var holding the CommandInfo  already uses JSON parsing to simplify the source code. Let's extend this benefit to all of them and bundle them together for simpler processing at both ends.",Improvement,Minor,bernd-mesos,2014-12-30T14:17:18.000+0000,5,Resolved,Complete,Consolidate all fetcher env vars into one that holds a JSON object,2014-12-30T14:17:18.000+0000,MESOS-2173,0.2,mesos,Mesosphere Q4 Sprint 3 - 12/7
bernd-mesos,2014-12-04T01:58:21.000+0000,bernd-mesos,"Fetching will have state once we introduce caching. In preparation for that, let's refactor the fetcher routines into a class for a singleton fetcher object per slave that will be able to hold the cache state.",Improvement,Minor,bernd-mesos,2014-12-30T14:16:38.000+0000,5,Resolved,Complete,Refactor fetcher namespace into a class,2014-12-30T14:16:38.000+0000,MESOS-2172,0.1,mesos,Mesosphere Q4 Sprint 3 - 12/7
ijimenez,2014-12-02T23:51:46.000+0000,cmaloney,The perf::valid() relies on the 'perf' command being installed. This isn't always the case. Configure should probably check for the perf command exists.,Bug,Major,cmaloney,2015-07-06T03:44:32.000+0000,5,Resolved,Complete, PerfEventIsolatorTest.ROOT_CGROUPS_Sample requires 'perf' to be installed,2015-07-06T03:44:32.000+0000,MESOS-2166,1.0,mesos,Mesosphere Sprint 13
,2014-11-25T19:11:42.000+0000,nnielsen,"master/state.json exports the entire state of the cluster and can, for large clusters, become massive (tens of megabytes of JSON).
Often, a client only need information about subsets of the entire state, for example all connected slaves, or information (registration info, tasks, etc) belonging to a particular framework.

We can partition state.json into many smaller endpoints, but for starters, being able to get slave information and tasks information per framework would be useful.",Task,Trivial,nnielsen,,10020,Accepted,In Progress,Add /master/slaves and /master/frameworks/{framework}/tasks/{task} endpoints,2015-11-23T10:03:30.000+0000,MESOS-2157,5.0,mesos,Mesosphere Q1 Sprint 2 - 2/6
jieyu,2014-11-20T22:19:53.000+0000,jieyu,"We observed that in our production environment with network monitoring being turned on.

If there are many connections (> 10^4) in a container, getting socket information is expensive. It might take 1min to process all the socket information.

One of the reason is that the library we are using (libnl) is not so optimized. Cong Wang has already submitted a patch:
http://lists.infradead.org/pipermail/libnl/2014-November/001715.html",Bug,Major,jieyu,2014-12-06T17:44:54.000+0000,5,Resolved,Complete,Large number of connections slows statistics.json responses.,2014-12-06T17:45:52.000+0000,MESOS-2147,2.0,mesos,Twitter Mesos Q4 Sprint 4
vinodkone,2014-11-20T18:56:19.000+0000,cmaloney,"Occured on review bot review of: https://reviews.apache.org/r/28262/#review62333

The review doesn't touch code related to the test (And doesn't break libprocess in general)

[ RUN      ] ExamplesTest.LowLevelSchedulerPthread
../../src/tests/script.cpp:83: Failure
Failed
low_level_scheduler_pthread_test.sh terminated with signal Segmentation fault
[  FAILED  ] ExamplesTest.LowLevelSchedulerPthread (7561 ms)

The test ",Bug,Minor,cmaloney,2016-01-14T21:42:52.000+0000,5,Resolved,Complete,Segmentation Fault in ExamplesTest.LowLevelSchedulerPthread,2016-01-14T21:42:52.000+0000,MESOS-2144,8.0,mesos,Twitter Mesos Q1 Sprint 2
mcypark,2014-11-20T00:13:59.000+0000,mcypark,"master's {{_accept}} function currently only handles {{Create}} and {{Destroy}} operations which exist for persistent volumes. We need to handle the {{Reserve}} and {{Unreserve}} operations for dynamic reservations as well.

In addition, we need to add {{validate}} functions for the reservation operations.",Task,Major,mcypark,2015-05-12T20:41:36.000+0000,5,Resolved,Complete,Enable the master to handle reservation operations,2015-07-02T20:41:12.000+0000,MESOS-2139,5.0,mesos,Mesosphere Q4 Sprint 3 - 12/7
chzhcn,2014-11-19T22:21:22.000+0000,idownes,"The cgroup memory controller can provide information on the memory pressure of a cgroup. This is in the form of an event based notification where events of (low, medium, critical) are generated when the kernel makes specific actions to allocate memory. This signal is probably more informative than comparing memory usage to memory limit.
",Improvement,Major,idownes,2015-03-27T20:02:29.000+0000,5,Resolved,Complete,Expose per-cgroup memory pressure,2015-03-30T18:03:37.000+0000,MESOS-2136,5.0,mesos,Twitter Mesos Q4 Sprint 5
jieyu,2014-11-19T19:57:17.000+0000,jieyu,"We need to change the following functions:
1) addable
2) subtractable
3) validate

We probably shouldn't add two disk resources with the same persistence id because they must come from different ""namespaces"". We can add more checks in the validate functions (for protobufs).",Task,Major,jieyu,2014-11-21T00:41:15.000+0000,5,Resolved,Complete,Support DiskInfo in C++ Resources,2014-11-21T00:41:15.000+0000,MESOS-2135,3.0,mesos,Twitter Mesos Q4 Sprint 4
idownes,2014-11-19T01:18:43.000+0000,bmetzdorf,"Our test runs show that enabling cgroups_limit_swap effectively disables memory isolation altogether.

Per: https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux/6/html/Resource_Management_Guide/sec-memory.html

""It is important to set the memory.limit_in_bytes parameter before setting the memory.memsw.limit_in_bytes parameter: attempting to do so in the reverse order results in an error. This is because memory.memsw.limit_in_bytes becomes available only after all memory limitations (previously set in memory.limit_in_bytes) are exhausted.""

Looks like the flag sets ""memory.memsw.limit_in_bytes"" if true and ""memory.limit_in_bytes"" if false, but should always set ""memory.limit_in_bytes"" and in addition set ""memory.memsw.limit_in_bytes"" if true. Otherwise the limits won't be set and enforced.

See: https://github.com/apache/mesos/blob/c8598f7f5a24a01b6a68e0f060b79662ee97af89/src/slave/containerizer/isolators/cgroups/mem.cpp#L365
",Bug,Major,bmetzdorf,2014-11-21T19:21:48.000+0000,5,Resolved,Complete,Turning on cgroups_limit_swap effectively disables memory isolation,2014-11-21T19:21:48.000+0000,MESOS-2128,2.0,mesos,Twitter Mesos Q4 Sprint 4
bmahler,2014-11-18T22:56:01.000+0000,vinodkone,"Currently, {{killTask}} uses its own reconciliation logic, which has diverged from the {{reconcileTasks}} logic. Specifically, when the task is unknown and a non-strict registry is in use, {{killTask}} will not send TASK_LOST whereas {{reconcileTask}} will.

We should make these consistent.
",Improvement,Major,vinodkone,2014-11-20T20:53:04.000+0000,5,Resolved,Complete,killTask() should perform reconciliation for unknown tasks.,2014-11-20T20:53:04.000+0000,MESOS-2127,3.0,mesos,Twitter Mesos Q4 Sprint 4
jieyu,2014-11-18T01:20:50.000+0000,jieyu,"With the refactor introduced in MESOS-1974, we need to document those API changes in CHANGELOG.
",Task,Major,jieyu,2015-07-24T01:15:55.000+0000,5,Resolved,Complete,Document changes in C++ Resources API in CHANGELOG.,2015-07-24T01:15:55.000+0000,MESOS-2123,2.0,mesos,Twitter Mesos Q1 Sprint 2
,2014-11-17T19:42:35.000+0000,nnielsen,Add more Socket specific tests to get coverage while doing libev to libevent (w and wo SSL) move,Task,Major,nnielsen,,10020,Accepted,In Progress,Add Socket tests,2015-10-02T15:21:29.000+0000,MESOS-2119,5.0,mesos,Mesosphere Q4 Sprint 3 - 12/7
adam-mesos,2014-11-14T02:11:07.000+0000,adam-mesos,"After a series of ping-failures, the master considers the slave lost and calls shutdownSlave, requiring such a slave that reconnects to kill its tasks and re-register as a new slaveId. On the other side, after a similar timeout, the slave will consider the master lost and try to detect a new master. These timeouts are currently hardcoded constants (5 * 15s), which may not be well-suited for all scenarios.
- Some clusters may tolerate a longer slave process restart period, and wouldn't want tasks to be killed upon reconnect.
- Some clusters may have higher-latency networks (e.g. cross-datacenter, or for volunteer computing efforts), and would like to tolerate longer periods without communication.

We should provide flags/mechanisms on the master to control its tolerance for non-communicative slaves, and (less importantly?) on the slave to tolerate missing masters.",Improvement,Major,adam-mesos,2015-07-06T18:55:39.000+0000,5,Resolved,Complete,Configurable Ping Timeouts,2015-07-06T18:55:39.000+0000,MESOS-2110,8.0,mesos,Mesosphere Q4 Sprint 3 - 12/7
chzhcn,2014-11-13T22:50:21.000+0000,idownes,"mem_rss_bytes is *not* RSS but is the total memory usage (memory.usage_in_bytes) of the cgroup, including file cache etc. Actual RSS is reported as mem_anon_bytes. These, and others, should be consistently named.",Improvement,Major,idownes,2015-06-21T03:06:48.000+0000,5,Resolved,Complete,Correct naming of cgroup memory statistics,2015-06-21T03:06:48.000+0000,MESOS-2104,3.0,mesos,Twitter Mesos Q1 Sprint 4
chzhcn,2014-11-13T19:17:47.000+0000,idownes,"The CFS cpu statistics (cpus_nr_throttled, cpus_nr_periods, cpus_throttled_time) are difficult to interpret.
1) nr_throttled is the number of intervals where *any* throttling occurred
2) throttled_time is the aggregate time *across all runnable tasks* (tasks in the Linux sense).

For example, in a typical 60 second sampling interval: nr_periods = 600, nr_throttled could be 60, i.e., 10% of intervals, but throttled_time could be much higher than (60/600) * 60 = 6 seconds if there is more than one task that is runnable but throttled. *Each* throttled task contributes to the total throttled time.

Small test to demonstrate throttled_time > nr_periods * quota_interval:

5 x {{'openssl speed'}} running with quota=100ms:
{noformat}
cat cpu.stat && sleep 1 && cat cpu.stat
nr_periods 3228
nr_throttled 1276
throttled_time 528843772540
nr_periods 3238
nr_throttled 1286
throttled_time 531668964667
{noformat}
All 10 intervals throttled (100%) for total time of 2.8 seconds in 1 second (""more than 100%"" of the time interval)


It would be helpful to expose the number of processes and tasks in the container cgroup. This would be at a very coarse granularity but would give some guidance.",Improvement,Major,idownes,2015-03-16T20:03:48.000+0000,5,Resolved,Complete,Expose number of processes and threads in a container,2015-07-02T07:50:17.000+0000,MESOS-2103,2.0,mesos,Twitter Mesos Q1 Sprint 2
jieyu,2014-11-13T01:06:40.000+0000,jieyu,"We are thinking about introducing a Release protobuf message which specifies persistent disk resources (w/ DiskInfo) to release. The Release message could be piggybacked on the Launch/Decline message.

This probably will overlap with the dynamic reservation work (MESOS-2018).",Task,Major,jieyu,2015-02-09T17:27:05.000+0000,5,Resolved,Complete,Add the persistent resources release primitive to the framework API,2015-02-09T18:10:37.000+0000,MESOS-2101,3.0,mesos,Twitter Mesos Q1 Sprint 2
jieyu,2014-11-13T01:00:36.000+0000,jieyu,"We need to do the following:
1) Slave needs to send persisted resources when registering (or re-registering).
2) Master needs to send total persisted resources to slave by either re-using RunTask/UpdateFrameworkInfo or introduce new type of messages (like UpdateResources).",Task,Major,jieyu,2015-02-09T19:40:26.000+0000,5,Resolved,Complete,Implement master to slave protocol for persistent disk resources.,2015-02-09T19:40:26.000+0000,MESOS-2100,8.0,mesos,Twitter Mesos Q4 Sprint 4
bmahler,2014-11-13T00:57:06.000+0000,jieyu,"The allocator needs to be changed because the resources are changing while we acquiring or releasing persistent disk resources (resources with DiskInfo). For example, when we release a persistent disk resource, we are changing the release with DiskInfo to a resource with the DiskInfo.",Task,Major,jieyu,2014-12-18T00:58:02.000+0000,5,Resolved,Complete,Support acquiring/releasing resources with DiskInfo in allocator.,2014-12-18T00:58:02.000+0000,MESOS-2099,8.0,mesos,Twitter Mesos Q4 Sprint 5
jieyu,2014-11-13T00:46:14.000+0000,jieyu,So that we can simply the task validation because we no longer need to check with pendingTasks.,Task,Major,jieyu,2014-11-13T18:48:06.000+0000,5,Resolved,Complete,Update task validation to be after task authorization.,2015-07-02T18:43:04.000+0000,MESOS-2098,3.0,mesos,Twitter Mesos Q4 Sprint 3
jieyu,2014-11-13T00:40:04.000+0000,jieyu,"{noformat}
message Resource {
  required string name = 1;
  required Value.Type type = 2;
  optional Value.Scalar scalar = 3;
  optional Value.Ranges ranges = 4;
  optional Value.Set set = 5;
  optional string role = 6 [default = ""*""];

  // Used for describing persistent disk resource.
  message DiskInfo {
    // A unique identifier for the persistent disk resource. The id
    // needs to be unique within a role for a slave.
    required string id = 1;

    // The volume mapping for the persistent disk resource.
    required Volume volume = 2;
  }

  optional DiskInfo disk = 8;
}
{noformat}",Task,Major,jieyu,2014-11-20T20:42:40.000+0000,5,Resolved,Complete,Update Resource protobuf with DiskInfo,2015-07-02T21:56:05.000+0000,MESOS-2097,1.0,mesos,Twitter Mesos Q4 Sprint 4
jvanremoortere,2014-11-12T01:04:22.000+0000,nnielsen,"During cluster upgrade from non-encrypted to encrypted communication, we need to support an interim where:
1) A master can have connections to both encrypted and non-encrypted slaves
2) A slave that supports encrypted communication connects to a master that has not yet been upgraded.
3) Frameworks are encrypted but the master has not been upgraded yet.
4) Master has been upgraded but frameworks haven't.
5) A slave process has upgraded but running executor processes haven't.",Task,Critical,nnielsen,2015-07-06T18:54:57.000+0000,5,Resolved,Complete,Add support encrypted and non-encrypted communication in parallel for cluster upgrade,2015-07-06T18:54:57.000+0000,MESOS-2085,13.0,mesos,Mesosphere Q4 Sprint 3 - 12/7
kaysoky,2014-11-12T00:43:52.000+0000,bmahler,"We should provide some guiding documentation around the upcoming maintenance primitives in Mesos.

Specifically, we should ensure that general users, framework developers, and operators understand the notion of maintenance in Mesos. Some guidance and recommendations for the latter two audiences will be necessary.",Documentation,Major,bmahler,2015-09-20T19:31:31.000+0000,5,Resolved,Complete,Add documentation for maintenance primitives.,2015-09-25T22:46:29.000+0000,MESOS-2083,8.0,mesos,Mesosphere Sprint 17
,2014-11-12T00:01:18.000+0000,bmahler,"The simplest thing here would probably be to include another tab in the header for maintenance information.

We could also consider adding maintenance information inline to the slaves table. Depending on how this is done, the maintenance tab could actually be a subset of the slaves table; only those slaves for which there is maintenance information.",Task,Major,bmahler,,10020,Accepted,In Progress,Update the webui to include maintenance information.,2015-07-17T16:05:17.000+0000,MESOS-2082,5.0,mesos,
,2014-11-11T23:49:03.000+0000,bmahler,"In order to ensure that the maintenance primitives can be used safely by operators, we want to put a few safety mechanisms in place. Some ideas from the [design doc|https://docs.google.com/a/twitter.com/document/d/16k0lVwpSGVOyxPSyXKmGC-gbNmRlisNEe4p-fAUSojk/]:

# Prevent bad schedules from being constructed: schedules with more than x% overlap in slaves are rejected.
# Prevent bad maintenance from proceeding unchecked: if x% of the slaves are not being unscheduled, or are not re-registering, cancel the schedule.

These will likely be configurable via flags.",Task,Major,bmahler,,10020,Accepted,In Progress,Add safety constraints for maintenance primitives.,2015-11-20T02:12:11.000+0000,MESOS-2081,8.0,mesos,
JamesYongQiaoWang,2014-11-11T23:41:56.000+0000,bmahler,"We'll need metrics in order to gain visibility into the maintenance functionality. This will also allow operators to add alerting on these metrics, in particular:

# Number of scheduled hosts.
# Number of active windows.
# Number of expired windows.
# Number of successful drains.
# Number of failed drains.

As an example of an alert guideline, we would want to know the number of expired windows as a gauge to ensure that it is not growing excessively. This allows alerting to catch when operators are not properly unscheduling maintenance once it is complete.",Task,Major,bmahler,,3,In Progress,In Progress,Add master metrics for maintenance.,2015-11-09T21:29:47.000+0000,MESOS-2080,3.0,mesos,
vinodkone,2014-11-11T21:07:19.000+0000,wfarner,"[~vinodkone] discovered that this can happen if the scheduler calls {{SchedulerDriver#stop}} before or while handling {{Scheduler#statusUpdate}}.

In src/sched/sched.cpp:
The driver invokes {{statusUpdate}} and later checks the {{aborted}} flag to determine whether to send an ACK.
{code}
  void statusUpdate(
      const UPID& from,
      const StatusUpdate& update,
      const UPID& pid)
  {

...

    scheduler->statusUpdate(driver, status);

    VLOG(1) << ""Scheduler::statusUpdate took "" << stopwatch.elapsed();

    // Note that we need to look at the volatile 'aborted' here to
    // so that we don't acknowledge the update if the driver was
    // aborted during the processing of the update.
    if (aborted) {
      VLOG(1) << ""Not sending status update acknowledgment message because ""
              << ""the driver is aborted!"";
      return;
    }
...
{code}

In src/java/jni/org_apache_mesos_MesosSchedulerDriver.cpp:
The {{statusUpdate}} implementation checks for an exception and invokes {{driver->abort()}}.
{code}
void JNIScheduler::statusUpdate(SchedulerDriver* driver,
                                const TaskStatus& status)
{
  jvm->AttachCurrentThread(JNIENV_CAST(&env), NULL);

  jclass clazz = env->GetObjectClass(jdriver);

  jfieldID scheduler = env->GetFieldID(clazz, ""scheduler"", ""Lorg/apache/mesos/Scheduler;"");
  jobject jscheduler = env->GetObjectField(jdriver, scheduler);

  clazz = env->GetObjectClass(jscheduler);

  // scheduler.statusUpdate(driver, status);
  jmethodID statusUpdate =
    env->GetMethodID(clazz, ""statusUpdate"",
                     ""(Lorg/apache/mesos/SchedulerDriver;""
                     ""Lorg/apache/mesos/Protos$TaskStatus;)V"");

  jobject jstatus = convert<TaskStatus>(env, status);

  env->ExceptionClear();

  env->CallVoidMethod(jscheduler, statusUpdate, jdriver, jstatus);

  if (env->ExceptionCheck()) {
    env->ExceptionDescribe();
    env->ExceptionClear();
    jvm->DetachCurrentThread();
    driver->abort();
    return;
  }

  jvm->DetachCurrentThread();
}
{code}

In src/sched/sched.cpp:
The {{abort()}} implementation exits early if {{status != DRIVER_RUNNING}}, and *does not set the aborted flag*.
{code}
Status MesosSchedulerDriver::abort()
{
  Lock lock(&mutex);

  if (status != DRIVER_RUNNING) {
    return status;
  }

  CHECK(process != NULL);

  // We set the volatile aborted to true here to prevent any further
  // messages from being processed in the SchedulerProcess. However,
  // if abort() is called from another thread as the SchedulerProcess,
  // there may be at most one additional message processed.
  // TODO(bmahler): Use an atomic boolean.
  process->aborted = true;

  // Dispatching here ensures that we still process the outstanding
  // requests *from* the scheduler, since those do proceed when
  // aborted is true.
  dispatch(process, &SchedulerProcess::abort);

  return status = DRIVER_ABORTED;
}
{code}

As a result, the code will ACK despite an exception being thrown.",Bug,Critical,wfarner,2014-11-13T00:31:44.000+0000,5,Resolved,Complete,Scheduler driver may ACK status updates when the scheduler threw an exception,2014-11-13T21:39:31.000+0000,MESOS-2078,3.0,mesos,Twitter Mesos Q4 Sprint 3
gyliu,2014-11-11T20:52:23.000+0000,bmahler,"For maintenance, sometimes operators will force the drain of a slave (via SIGUSR1), when deemed safe (e.g. non-critical tasks running) and/or necessary (e.g. bad hardware).

To eliminate alerting noise, we'd like to add a 'Reason' that expresses the forced drain of the slave, so that these are not considered to be a generic slave removal TASK_LOST.",Improvement,Major,bmahler,,10006,Reviewable,New,Ensure that TASK_LOSTs for a hard slave drain (SIGUSR1) include a Reason.,2015-11-09T18:22:17.000+0000,MESOS-2077,3.0,mesos,
jvanremoortere,2014-11-11T20:42:54.000+0000,bmahler,"The master will need to do a number of things to implement the maintenance primitives:

# For machines that have a maintenance window:
#* Disambiguate machines to agents.
#* For unused resources, offers must be augmented with an Unavailability.
#* For used resources, inverse offers must be sent.
# For inverse offers:
#* Filter them before sending them again.
#* For declined inverse offers, do something with the reason (store or log).
# Recover the maintenance information upon failover.

Note: Some amount of this logic will need to be placed in the allocator.",Task,Major,bmahler,2015-09-25T04:33:21.000+0000,5,Resolved,Complete,Implement maintenance primitives in the Master.,2015-09-25T04:33:21.000+0000,MESOS-2076,13.0,mesos,
kaysoky,2014-11-11T20:14:42.000+0000,bmahler,"To achieve fault-tolerance for the maintenance primitives, we will need to add the maintenance information to the registry.

The registry currently stores all of the slave information, which is quite large (~ 17MB for 50,000 slaves from my testing), which results in a protobuf object that is extremely expensive to copy.

As far as I can tell, reads / writes to maintenance information is independent of reads / writes to the existing 'registry' information. So there are two approach here:

h4. Add maintenance information to 'maintenance' key: 
# The advantage of this approach is that we don't further grow the large Registry object.
# This approach assumes that writes to 'maintenance' are independent of writes to the 'registry'. -If these writes are not independent, this approach requires that we add transactional support to the State abstraction.-
# -This approach requires adding compaction to LogStorage.-
# This approach likely requires some refactoring to the Registrar.

h4. Add maintenance information to 'registry' key: (This is the chosen method.)
# The advantage of this approach is that it's the easiest to implement.
# This will further grow the single 'registry' object, but doesn't preclude it being split apart in the future.
# This approach may require using the diff support in LogStorage and/or adding compression support to LogStorage snapshots to deal with the increased size of the registry.",Task,Major,bmahler,2015-08-31T18:00:32.000+0000,5,Resolved,Complete,Add maintenance information to the replicated registry.,2015-08-31T18:00:32.000+0000,MESOS-2075,13.0,mesos,
bernd-mesos,2014-11-11T16:59:11.000+0000,bernd-mesos,"To accelerate providing good test coverage for the fetcher cache (MESOS-336), we can provide a framework that canonicalizes creating and running a number of tasks and allows easy parametrization with combinations of the following:
- whether to cache or not
- whether make what has been downloaded executable or not
- whether to extract from an archive or not
- whether to download from a file system, http, or...

We can create a simple HHTP server in the test fixture to support the latter.

Furthermore, the tests need to be robust wrt. varying numbers of StatusUpdate messages. An accumulating update message sink that reports the final state is needed.

All this has already been programmed in this patch, just needs to be rebased:
https://reviews.apache.org/r/21316/",Improvement,Major,bernd-mesos,2015-06-01T13:49:52.000+0000,5,Resolved,Complete,Fetcher cache test fixture,2015-07-02T06:51:16.000+0000,MESOS-2074,5.0,mesos,Mesosphere Q1 Sprint 3 - 2/20
bernd-mesos,2014-11-11T15:00:33.000+0000,bernd-mesos,"Delete files from the fetcher cache so that a given cache size is never exceeded. Succeed in doing so while concurrent downloads are on their way and new requests are pouring in.

Idea: measure the size of each download before it begins, make enough room before the download. This means that only download mechanisms that divulge the size before the main download will be supported. AFAWK, those in use so far have this property. 

The calculation of how much space to free needs to be under concurrency control, accumulating all space needed for competing, incomplete download requests. (The Python script that performs fetcher caching for Aurora does not seem to implement this. See https://gist.github.com/zmanji/f41df77510ef9d00265a, imagine several of these programs running concurrently, each one's _cache_eviction() call succeeding, each perceiving the SAME free space being available.)

Ultimately, a conflict resolution strategy is needed if just the downloads underway already exceed the cache capacity. Then, as a fallback, direct download into the work directory will be used for some tasks. TBD how to pick which task gets treated how. 

At first, only support copying of any downloaded files to the work directory for task execution. This isolates the task life cycle after starting a task from cache eviction considerations. 

(Later, we can add symbolic links that avoid copying. But then eviction of fetched files used by ongoing tasks must be blocked, which adds complexity. another future extension is MESOS-1667 ""Extract from URI while downloading into work dir"").
",Improvement,Major,bernd-mesos,2015-06-01T13:51:05.000+0000,5,Resolved,Complete,Fetcher cache eviction,2015-07-02T07:21:46.000+0000,MESOS-2072,8.0,mesos,Mesosphere Q1 Sprint 2 - 2/6
bernd-mesos,2014-11-11T11:58:34.000+0000,bernd-mesos,"Clean the fetcher cache completely upon slave restart/recovery. This implements correct, albeit not ideal behavior. More efficient schemes that restore knowledge about cached files or even resume downloads can be added later. ",Improvement,Major,bernd-mesos,2015-06-01T13:52:28.000+0000,5,Resolved,Complete,Implement simple slave recovery behavior for fetcher cache,2015-07-02T06:51:17.000+0000,MESOS-2070,2.0,mesos,Mesosphere Q1 Sprint 1 - 1/23
bernd-mesos,2014-11-11T11:58:20.000+0000,bernd-mesos,"Add a flag to CommandInfo URI protobufs that indicates that files downloaded by the fetcher shall be cached in a repository. To be followed by MESOS-2057 for concurrency control.

Also see MESOS-336 for the overall goals for the fetcher cache.",Improvement,Major,bernd-mesos,2015-06-01T13:50:47.000+0000,5,Resolved,Complete,Basic fetcher cache functionality,2015-07-02T07:21:45.000+0000,MESOS-2069,8.0,mesos,Mesosphere Q4 Sprint 3 - 12/7
bernd-mesos,2014-11-11T09:32:30.000+0000,bernd-mesos,"Fixing MESOS-947 was relatively difficult because the source code is mostly the only source of information with regard to the life cycle of frameworks, executors, and tasks in the slave. In particular this leads to confusion about whether there could be a task lost state  at the beginning of _runTask() when the framework is NULL. This shall be explained to the best of the assignees knowledge.

For context see https://reviews.apache.org/r/27567
with these comments:

On Nov. 5, 2014, 7:50 p.m., Ben Mahler wrote:
src/slave/slave.cpp, lines 1195-1200
<https://reviews.apache.org/r/27567/diff/1/?file=748326#file748326line1195>

   A comment here as to why we don't need to send TASK_LOST would be much appreciated! It's not obvious so someone might come along and add a TASK_LOST to make sure we're not dropping the task on the floor, so context here would be great!

Bernd Mathiske wrote:
   Hah, thanks for sharing - I am not alone! :-) None of this was obvious to me either, because there is no comment explaining the general life cycle of anything. Once you understand the intended life cycle, there is now way there can be a TASK_LOST situation here, though. Therefore I propose adding comments describing the overall picture regarding frameworks, executor IDs and task creation in the appropriate places, instead. I'll file a ticket if you agree.

Once you understand the intended life cycle, there is now way there can be a TASK_LOST situation here, though.

Phew! :)

Could you distill your learnings into a comment here, and maybe make the log message more informative? Even with an overall description as you mentioned, dummies like me would still get confused here given the lack of _local_ context. ;)

- Ben
",Improvement,Minor,bernd-mesos,,10020,Accepted,In Progress,"Add comments that explain framework, executor ID, and task life cycle in slave",2016-01-15T17:10:06.000+0000,MESOS-2068,0.25,mesos,
kaysoky,2014-11-11T02:19:30.000+0000,bmahler,"Based on MESOS-1474, we'd like to provide an HTTP API on the master for the maintenance primitives in mesos.

For the MVP, we'll want something like this for manipulating the schedule:
{code}
/maintenance/schedule
  GET - returns the schedule, which will include the various maintenance windows.
  POST - create or update the schedule with a JSON blob (see below).

/maintenance/status
  GET - returns a list of machines and their maintenance mode.

/maintenance/start
  POST - Transition a set of machines from Draining into Deactivated mode.

/maintenance/stop
  POST - Transition a set of machines from Deactivated into Normal mode.

/maintenance/consensus <- (Not sure what the right name is.  matrix?  acceptance?)
  GET - Returns the latest info on which frameworks have accepted or declined the maintenance schedule.
{code}
(Note: The slashes in URLs might not be supported yet.)

A schedule might look like:
{code}
{
  ""windows"" : [
    {
      ""machines"" : [
          { ""ip"" : ""192.168.0.1"" },
          { ""hostname"" : ""localhost"" },
          ...
        ], 
      ""unavailability"" : {
        ""start"" : 12345, // Epoch seconds.
        ""duration"" : 1000 // Seconds.
      }
    },
    ...
  ]
}
{code}

There should be firewall settings such that only those with access to master can use these endpoints.",Task,Major,bmahler,2015-08-31T17:27:47.000+0000,5,Resolved,Complete,Add HTTP API to the master for maintenance operations.,2015-09-25T22:46:26.000+0000,MESOS-2067,8.0,mesos,Mesosphere Sprint 16
kaysoky,2014-11-11T00:36:09.000+0000,bmahler,"In order to inform frameworks about upcoming maintenance on offered resources, per MESOS-1474, we'd like to add an optional 'Unavailability' information to offers:

{code}
message Interval {
  optional double start = 1; // Time, in seconds since the Epoch.
  optional double duration = 2; // Time, in seconds.
}

message Offer {
  // Existing fields
  ...
 
  // Signifies that the resources in this Offer are part of a planned
  // maintenance schedule in the specified window.  Any tasks launched
  // using these resources may be killed when the window arrives.
  // This field gives additional information about the maintenance.
  // The maintenance may not necessarily start at exactly at this interval,
  // nor last for exactly the duration of this interval.
  optional Interval unavailability = 9;
}
{code}",Task,Major,bmahler,2015-08-31T17:21:31.000+0000,5,Resolved,Complete,Add optional 'Unavailability' to resource offers to provide maintenance awareness.,2015-09-25T22:46:27.000+0000,MESOS-2066,3.0,mesos,Mesosphere Sprint 15
JamesYongQiaoWang,2014-11-11T00:07:27.000+0000,bmahler,"The initial use case for InverseOffer in the framework API will be the maintenance primitives in mesos: MESOS-1474.

One way to add these to the Python Scheduler API is to add a new callback:

{code}
  def inverseResourceOffers(self, driver, inverse_offers):
{code}

Egg / libmesos compatibility will need to be figured out here.

We may want to leave the Python binding untouched in favor of Event/Call, in order to not break API compatibility for schedulers.",Task,Major,bmahler,2015-09-17T17:01:46.000+0000,5,Resolved,Complete,Add InverseOffer to Python Scheduler API.,2015-09-17T17:01:46.000+0000,MESOS-2065,5.0,mesos,
JamesYongQiaoWang,2014-11-11T00:07:25.000+0000,bmahler,"The initial use case for InverseOffer in the framework API will be the maintenance primitives in mesos: MESOS-1474.

One way to add these to the Java Scheduler API is to add a new callback:

{code}
  void inverseResourceOffers(
      SchedulerDriver driver,
      List<InverseOffer> inverseOffers);
{code}

JAR / libmesos compatibility will need to be figured out here.

We may want to leave the Java binding untouched in favor of Event/Call, in order to not break API compatibility for schedulers.",Task,Major,bmahler,2015-09-17T17:01:37.000+0000,5,Resolved,Complete,Add InverseOffer to Java Scheduler API.,2015-09-17T17:01:37.000+0000,MESOS-2064,5.0,mesos,
qianzhang,2014-11-11T00:07:23.000+0000,bmahler,"The initial use case for InverseOffer in the framework API will be the maintenance primitives in mesos: MESOS-1474.

One way to add these to the C++ Scheduler API is to add a new callback:

{code}
  virtual void inverseResourceOffers(
      SchedulerDriver* driver,
      const std::vector<InverseOffer>& inverseOffers) = 0;
{code}

libmesos compatibility will need to be figured out here.

We may want to leave the C++ binding untouched in favor of Event/Call, in order to not break API compatibility for schedulers.",Task,Major,bmahler,2015-09-17T17:01:14.000+0000,5,Resolved,Complete,Add InverseOffer to C++ Scheduler API.,2015-09-17T17:01:14.000+0000,MESOS-2063,5.0,mesos,
jvanremoortere,2014-11-10T23:57:08.000+0000,bmahler,"The initial use case for InverseOffer in the framework API will be the maintenance primitives in mesos: MESOS-1474.

One way to add this is to tack it on to the OFFERS Event:

{code}
message Offers {
  repeated Offer offers = 1;
  repeated InverseOffer inverse_offers = 2;
}
{code}",Task,Major,bmahler,2015-09-14T18:05:04.000+0000,5,Resolved,Complete,Add InverseOffer to Event/Call API.,2015-09-25T22:46:30.000+0000,MESOS-2062,3.0,mesos,Mesosphere Sprint 17
kaysoky,2014-11-10T23:51:28.000+0000,bmahler,"InverseOffer was defined as part of the maintenance work in MESOS-1474, design doc here: https://docs.google.com/document/d/16k0lVwpSGVOyxPSyXKmGC-gbNmRlisNEe4p-fAUSojk/edit?usp=sharing

{code}
/**
 * A request to return some resources occupied by a framework.
 */
message InverseOffer {
  required OfferID id = 1;
  required FrameworkID framework_id = 2;

  // A list of resources being requested back from the framework.
  repeated Resource resources = 3;

  // Specified if the resources need to be released from a particular slave.
  optional SlaveID slave_id = 4;

  // The resources in this InverseOffer are part of a planned maintenance
  // schedule in the specified window.  Any tasks running using these
  // resources may be killed when the window arrives.
  optional Interval unavailability = 5;
}
{code}

This ticket is to capture the addition of the InverseOffer protobuf to mesos.proto, the necessary API changes for Event/Call and the language bindings will be tracked separately.",Task,Major,bmahler,2015-08-31T17:21:08.000+0000,5,Resolved,Complete,Add InverseOffer protobuf message.,2015-09-25T22:46:27.000+0000,MESOS-2061,3.0,mesos,Mesosphere Sprint 15
dhamon,2014-11-10T21:29:03.000+0000,dhamon,"With the introduction of the libprocess {{/metrics/snapshot}} endpoint, metrics are now duplicated in the Master and Slave between this and {{stats.json}}. We should deprecate the {{stats.json}} endpoints.

Manual inspection of {{stats.json}} shows that all metrics are now covered by the new endpoint for Master and Slave.",Task,Major,dhamon,2015-03-03T20:27:35.000+0000,5,Resolved,Complete,Deprecate stats.json endpoints for Master and Slave,2016-04-21T20:46:33.000+0000,MESOS-2058,1.0,mesos,Twitter Mesos Q1 Sprint 1
bernd-mesos,2014-11-10T17:32:28.000+0000,bernd-mesos,"Having added a URI flag to CommandInfo messages (in MESOS-2069) that indicates caching, caching files downloaded by the fetcher in a repository, now ensure that when a URI is ""cached"", it is only ever downloaded once for the same user on the same slave as long as the slave keeps running. 

This even holds if multiple tasks request the same URI concurrently. If multiple requests for the same URI occur, perform only one of them and reuse the result. Make concurrent requests for the same URI wait for the one download. 

Different URIs from different CommandInfos can be downloaded concurrently.

No cache eviction, cleanup or failover will be handled for now. Additional tickets will be filed for these enhancements. (So don't use this feature in production until the whole epic is complete.)

Note that implementing this does not suffice for production use. This ticket contains the main part of the fetcher logic, though. See the epic MESOS-336 for the rest of the features that lead to a fully functional fetcher cache.

The proposed general approach is to keep all bookkeeping about what is in which stage of being fetched and where it resides in the slave's MesosContainerizerProcess, so that all concurrent access is disambiguated and controlled by an ""actor"" (aka libprocess ""process"").

Depends on MESOS-2056 and MESOS-2069.
",Improvement,Major,bernd-mesos,2015-06-01T13:50:17.000+0000,5,Resolved,Complete,Concurrency control for fetcher cache,2015-07-02T06:51:16.000+0000,MESOS-2057,8.0,mesos,Mesosphere Q4 Sprint 3 - 12/7
bernd-mesos,2014-11-10T17:30:20.000+0000,bernd-mesos,"Refactor/rearrange fetcher-related code so that cache functionality can be dropped in. One could do both together in one go. This is splitting up reviews into smaller chunks. It will not immediately be obvious how this change will be used later, but it will look better-factored and still do the exact same thing as before. In particular, a download routine to be reused several times in launcher/fetcher will be factored out and the remainder of fetcher-related code can be moved from the containerizer realm into fetcher.cpp.",Improvement,Minor,bernd-mesos,2014-11-18T13:56:43.000+0000,5,Resolved,Complete,Refactor fetcher code in preparation for fetcher cache,2014-11-18T13:56:43.000+0000,MESOS-2056,1.0,mesos,Mesosphere Q4 Sprint 2 - 11/14
,2014-11-08T01:27:57.000+0000,vinodkone,"Observed this on ASF CI:

{code}
[ RUN      ] MesosContainerizerExecuteTest.IoRedirection
Using temporary directory '/tmp/MesosContainerizerExecuteTest_IoRedirection_PbBn8a'
I1108 00:34:25.820514 30391 containerizer.cpp:100] Using isolation: posix/cpu,posix/mem
I1108 00:34:25.821048 30411 containerizer.cpp:424] Starting container 'test_container' for executor 'executor' of framework ''
I1108 00:34:25.824015 30411 launcher.cpp:137] Forked child with pid '4221' for container 'test_container'
I1108 00:34:25.825438 30408 containerizer.cpp:571] Fetching URIs for container 'test_container' using command '/home/jenkins/jenkins-slave/workspace/Mesos-Trunk-Ubuntu-Build-Out-Of-Src-Disable-Java-Disable-Python-Disable-Webui/build/src/mesos-fetcher'
I1108 00:34:25.984254 30419 containerizer.cpp:1117] Executor for container 'test_container' has exited
I1108 00:34:25.984341 30419 containerizer.cpp:946] Destroying container 'test_container'
../../src/tests/containerizer_tests.cpp:487: Failure
Value of: (os::read(path::join(directory, ""stderr""))).get()
  Actual: ""I1108 00:34:25.872990  4224 logging.cpp:177] Logging to STDERR\nthis is stderr\n""
Expected: errMsg + ""\n""
Which is: ""this is stderr\n""
[  FAILED  ] MesosContainerizerExecuteTest.IoRedirection (185 ms)
[----------] 1 test from MesosContainerizerExecuteTest (185 ms total)
{code}",Bug,Major,vinodkone,2014-11-08T01:34:49.000+0000,5,Resolved,Complete,MesosContainerizerExecuteTest.IoRedirection test is flaky,2014-11-08T01:34:49.000+0000,MESOS-2055,1.0,mesos,Twitter Mesos Q4 Sprint 3
idownes,2014-11-07T19:16:53.000+0000,idownes,"RunState::recover() will return partial state if it cannot find or open the libprocess pid file. Specifically, it does not recover the 'completed' flag.

However, if the slave has removed the executor (because launch failed or the executor failed to register) the sentinel flag will be set and this fact should be recovered. This ensures that container recovery is not attempted later.

This was discovered when the LinuxLauncher failed to recover because it was asked to recover two containers with the same forkedPid. Investigation showed the executors both OOM'ed before registering, i.e., no libprocess pid file was present. However, the containerizer had detected the OOM, destroyed the container, and notified the slave which cleaned everything up: failing the task and calling removeExecutor (which writes the completed sentinel file.)",Bug,Major,idownes,2014-11-10T18:58:23.000+0000,5,Resolved,Complete,RunState::recover should always recover 'completed',2014-11-10T20:11:12.000+0000,MESOS-2052,1.0,mesos,Twitter Mesos Q4 Sprint 3
marco-mesos,2014-11-04T22:39:48.000+0000,wangcong,"If there are enough IP addresses, either IPv4 or IPv6, we should use one IP address per container, instead of the ugly port range based solution. One problem with this is the IP address management, usually it is managed by a DHCP server, maybe we need to manage them in mesos master/slave.

Also, maybe use macvlan instead of veth for better isolation.",Epic,Major,wangcong,,3,In Progress,In Progress,Use one IP address per container for network isolation,2015-11-24T21:21:43.000+0000,MESOS-2044,40.0,mesos,
greggomann,2014-11-04T20:16:11.000+0000,bhuvan,"I'm facing this issue in master as of https://github.com/apache/mesos/commit/74ea59e144d131814c66972fb0cc14784d3503d4

As [~adam-mesos] mentioned in IRC, this sounds similar to MESOS-1866. I'm running 1 master and 1 scheduler (aurora). The framework authentication fail due to time out:

error on mesos master:

{code}
I1104 19:37:17.741449  8329 master.cpp:3874] Authenticating scheduler-d2d4437b-d375-4467-a583-362152fe065a@SCHEDULER_IP:8083
I1104 19:37:17.741585  8329 master.cpp:3885] Using default CRAM-MD5 authenticator
I1104 19:37:17.742106  8336 authenticator.hpp:169] Creating new server SASL connection
W1104 19:37:22.742959  8329 master.cpp:3953] Authentication timed out
W1104 19:37:22.743548  8329 master.cpp:3930] Failed to authenticate scheduler-d2d4437b-d375-4467-a583-362152fe065a@SCHEDULER_IP:8083: Authentication discarded
{code}

scheduler error:
{code}
I1104 19:38:57.885486 49012 sched.cpp:283] Authenticating with master master@MASTER_IP:PORT
I1104 19:38:57.885928 49002 authenticatee.hpp:133] Creating new client SASL connection
I1104 19:38:57.890581 49007 authenticatee.hpp:224] Received SASL authentication mechanisms: CRAM-MD5
I1104 19:38:57.890656 49007 authenticatee.hpp:250] Attempting to authenticate with mechanism 'CRAM-MD5'
W1104 19:39:02.891196 49005 sched.cpp:378] Authentication timed out
I1104 19:39:02.891850 49018 sched.cpp:338] Failed to authenticate with master master@MASTER_IP:PORT: Authentication discarded
{code}

Looks like 2 instances {{scheduler-20f88a53-5945-4977-b5af-28f6c52d3c94}} & {{scheduler-d2d4437b-d375-4467-a583-362152fe065a}} of same framework is trying to authenticate and fail.
{code}
W1104 19:36:30.769420  8319 master.cpp:3930] Failed to authenticate scheduler-20f88a53-5945-4977-b5af-28f6c52d3c94@SCHEDULER_IP:8083: Failed to communicate with authenticatee
I1104 19:36:42.701441  8328 master.cpp:3860] Queuing up authentication request from scheduler-d2d4437b-d375-4467-a583-362152fe065a@SCHEDULER_IP:8083 because authentication is still in progress
{code}

Restarting master and scheduler didn't fix it. 

This particular issue happen with 1 master and 1 scheduler after MESOS-1866 is fixed.",Bug,Critical,bhuvan,,10020,Accepted,In Progress,framework auth fail with timeout error and never get authenticated,2016-05-02T11:45:12.000+0000,MESOS-2043,5.0,mesos,
jieyu,2014-11-03T22:29:26.000+0000,dhamon,"When an isolator kills a task, the reason is unknown. As part of MESOS-1830, the reason is set to a general one but ideally we would have the termination reason to pass through to the status update.",Improvement,Major,dhamon,2015-10-13T00:14:41.000+0000,5,Resolved,Complete,Add reason to containerizer proto Termination,2015-12-21T21:24:12.000+0000,MESOS-2035,5.0,mesos,Twitter Mesos Q3 Sprint 5
bmahler,2014-11-03T19:29:08.000+0000,bmahler,"With persistent resources and dynamic reservations, frameworks need to know how long the resources will be unavailable for maintenance operations.

This is because for persistent resources, the framework needs to understand how long the persistent resource will be unavailable. For example, if there will be a 10 minute reboot for a kernel upgrade, the framework will not want to re-replicate all of it's persistent data on the machine. Rather, tolerating one unavailable replica for the maintenance window would be preferred.

I'd like to do a revisit of the design to ensure it works well for persistent resources as well.",Task,Major,bmahler,2014-11-13T22:05:36.000+0000,5,Resolved,Complete,Update Maintenance design to account for persistent resources.,2014-11-13T22:05:36.000+0000,MESOS-2032,13.0,mesos,Twitter Mesos Q4 Sprint 3
jieyu,2014-11-03T18:25:59.000+0000,jieyu,"Whenever a slave sees a persistent disk resource (in ExecutorInfo or TaskInfo) that is new to it, it will create a persistent directory which is for tasks to store persistent data.

The slave needs to do the following after it's created:
1) symlink into the executor sandbox so that tasks/executor can see it
2) garbage collect it once it is released by the framework",Task,Major,jieyu,2015-02-26T00:40:26.000+0000,5,Resolved,Complete,Manage persistent directories on slave.,2015-02-26T00:40:26.000+0000,MESOS-2031,5.0,mesos,Twitter Mesos Q4 Sprint 3
jieyu,2014-11-03T18:16:30.000+0000,jieyu,"Maintain an in-memory data structure to track persistent disk resources on each slave. Update this data structure when slaves register/re-register/disconnect, etc.",Task,Major,jieyu,2015-01-25T10:53:10.000+0000,5,Resolved,Complete,Maintain persistent disk resources in master memory.,2015-01-25T10:53:11.000+0000,MESOS-2030,3.0,mesos,Twitter Mesos Q4 Sprint 3
jieyu,2014-11-03T18:05:29.000+0000,jieyu,"The checkpointed resources are independent of the slave lifecycle. In other words, even if the slave host reboots, it'll still recover the checkpointed resources (unlike other checkpointed data). The slave needs to verify during startup that the checkpointed resources are compatible with the resources of the slave (specified using --resources flag).",Task,Major,jieyu,2014-11-22T01:22:31.000+0000,5,Resolved,Complete,Allow slave to checkpoint resources.,2014-11-22T01:22:31.000+0000,MESOS-2029,5.0,mesos,Twitter Mesos Q4 Sprint 3
klueska,2014-10-30T17:16:19.000+0000,xujyan,"The most recent one:

{noformat:title=DRFAllocatorTest.DRFAllocatorProcess}
[ RUN      ] DRFAllocatorTest.DRFAllocatorProcess
Using temporary directory '/tmp/DRFAllocatorTest_DRFAllocatorProcess_BI905j'
I1030 05:55:06.934813 24459 leveldb.cpp:176] Opened db in 3.175202ms
I1030 05:55:06.935925 24459 leveldb.cpp:183] Compacted db in 1.077924ms
I1030 05:55:06.935976 24459 leveldb.cpp:198] Created db iterator in 16460ns
I1030 05:55:06.935995 24459 leveldb.cpp:204] Seeked to beginning of db in 2018ns
I1030 05:55:06.936005 24459 leveldb.cpp:273] Iterated through 0 keys in the db in 335ns
I1030 05:55:06.936039 24459 replica.cpp:741] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned
I1030 05:55:06.936705 24480 recover.cpp:437] Starting replica recovery
I1030 05:55:06.937023 24480 recover.cpp:463] Replica is in EMPTY status
I1030 05:55:06.938158 24475 replica.cpp:638] Replica in EMPTY status received a broadcasted recover request
I1030 05:55:06.938859 24482 recover.cpp:188] Received a recover response from a replica in EMPTY status
I1030 05:55:06.939486 24474 recover.cpp:554] Updating replica status to STARTING
I1030 05:55:06.940249 24489 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 591981ns
I1030 05:55:06.940274 24489 replica.cpp:320] Persisted replica status to STARTING
I1030 05:55:06.940752 24481 recover.cpp:463] Replica is in STARTING status
I1030 05:55:06.940820 24489 master.cpp:312] Master 20141030-055506-3142697795-40429-24459 (pomona.apache.org) started on 67.195.81.187:40429
I1030 05:55:06.940871 24489 master.cpp:358] Master only allowing authenticated frameworks to register
I1030 05:55:06.940891 24489 master.cpp:363] Master only allowing authenticated slaves to register
I1030 05:55:06.940908 24489 credentials.hpp:36] Loading credentials for authentication from '/tmp/DRFAllocatorTest_DRFAllocatorProcess_BI905j/credentials'
I1030 05:55:06.941215 24489 master.cpp:392] Authorization enabled
I1030 05:55:06.941751 24475 master.cpp:120] No whitelist given. Advertising offers for all slaves
I1030 05:55:06.942227 24474 replica.cpp:638] Replica in STARTING status received a broadcasted recover request
I1030 05:55:06.942401 24476 hierarchical_allocator_process.hpp:299] Initializing hierarchical allocator process with master : master@67.195.81.187:40429
I1030 05:55:06.942895 24483 recover.cpp:188] Received a recover response from a replica in STARTING status
I1030 05:55:06.943035 24474 master.cpp:1242] The newly elected leader is master@67.195.81.187:40429 with id 20141030-055506-3142697795-40429-24459
I1030 05:55:06.943063 24474 master.cpp:1255] Elected as the leading master!
I1030 05:55:06.943079 24474 master.cpp:1073] Recovering from registrar
I1030 05:55:06.943313 24480 registrar.cpp:313] Recovering registrar
I1030 05:55:06.943455 24475 recover.cpp:554] Updating replica status to VOTING
I1030 05:55:06.944144 24474 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 536365ns
I1030 05:55:06.944172 24474 replica.cpp:320] Persisted replica status to VOTING
I1030 05:55:06.944355 24489 recover.cpp:568] Successfully joined the Paxos group
I1030 05:55:06.944576 24489 recover.cpp:452] Recover process terminated
I1030 05:55:06.945155 24486 log.cpp:656] Attempting to start the writer
I1030 05:55:06.947013 24473 replica.cpp:474] Replica received implicit promise request with proposal 1
I1030 05:55:06.947854 24473 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 806463ns
I1030 05:55:06.947883 24473 replica.cpp:342] Persisted promised to 1
I1030 05:55:06.948547 24481 coordinator.cpp:230] Coordinator attemping to fill missing position
I1030 05:55:06.950269 24479 replica.cpp:375] Replica received explicit promise request for position 0 with proposal 2
I1030 05:55:06.950933 24479 leveldb.cpp:343] Persisting action (8 bytes) to leveldb took 603843ns
I1030 05:55:06.950961 24479 replica.cpp:676] Persisted action at 0
I1030 05:55:06.952180 24476 replica.cpp:508] Replica received write request for position 0
I1030 05:55:06.952239 24476 leveldb.cpp:438] Reading position from leveldb took 28437ns
I1030 05:55:06.952896 24476 leveldb.cpp:343] Persisting action (14 bytes) to leveldb took 623980ns
I1030 05:55:06.952926 24476 replica.cpp:676] Persisted action at 0
I1030 05:55:06.953543 24485 replica.cpp:655] Replica received learned notice for position 0
I1030 05:55:06.954082 24485 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 511807ns
I1030 05:55:06.954107 24485 replica.cpp:676] Persisted action at 0
I1030 05:55:06.954128 24485 replica.cpp:661] Replica learned NOP action at position 0
I1030 05:55:06.954710 24473 log.cpp:672] Writer started with ending position 0
I1030 05:55:06.956215 24478 leveldb.cpp:438] Reading position from leveldb took 33085ns
I1030 05:55:06.959481 24475 registrar.cpp:346] Successfully fetched the registry (0B) in 16.11904ms
I1030 05:55:06.959616 24475 registrar.cpp:445] Applied 1 operations in 28239ns; attempting to update the 'registry'
I1030 05:55:06.962514 24487 log.cpp:680] Attempting to append 139 bytes to the log
I1030 05:55:06.962646 24474 coordinator.cpp:340] Coordinator attempting to write APPEND action at position 1
I1030 05:55:06.964146 24486 replica.cpp:508] Replica received write request for position 1
I1030 05:55:06.964962 24486 leveldb.cpp:343] Persisting action (158 bytes) to leveldb took 743389ns
I1030 05:55:06.964993 24486 replica.cpp:676] Persisted action at 1
I1030 05:55:06.965895 24473 replica.cpp:655] Replica received learned notice for position 1
I1030 05:55:06.966531 24473 leveldb.cpp:343] Persisting action (160 bytes) to leveldb took 607242ns
I1030 05:55:06.966555 24473 replica.cpp:676] Persisted action at 1
I1030 05:55:06.966578 24473 replica.cpp:661] Replica learned APPEND action at position 1
I1030 05:55:06.967706 24481 registrar.cpp:490] Successfully updated the 'registry' in 8.036096ms
I1030 05:55:06.967895 24481 registrar.cpp:376] Successfully recovered registrar
I1030 05:55:06.967993 24482 log.cpp:699] Attempting to truncate the log to 1
I1030 05:55:06.968258 24479 coordinator.cpp:340] Coordinator attempting to write TRUNCATE action at position 2
I1030 05:55:06.968268 24475 master.cpp:1100] Recovered 0 slaves from the Registry (101B) ; allowing 10mins for slaves to re-register
I1030 05:55:06.969156 24476 replica.cpp:508] Replica received write request for position 2
I1030 05:55:06.969678 24476 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 491913ns
I1030 05:55:06.969703 24476 replica.cpp:676] Persisted action at 2
I1030 05:55:06.970459 24478 replica.cpp:655] Replica received learned notice for position 2
I1030 05:55:06.971060 24478 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 573076ns
I1030 05:55:06.971124 24478 leveldb.cpp:401] Deleting ~1 keys from leveldb took 35339ns
I1030 05:55:06.971145 24478 replica.cpp:676] Persisted action at 2
I1030 05:55:06.971168 24478 replica.cpp:661] Replica learned TRUNCATE action at position 2
I1030 05:55:06.980211 24459 containerizer.cpp:100] Using isolation: posix/cpu,posix/mem
I1030 05:55:06.984153 24473 slave.cpp:169] Slave started on 203)@67.195.81.187:40429
I1030 05:55:07.055308 24473 credentials.hpp:84] Loading credential for authentication from '/tmp/DRFAllocatorTest_DRFAllocatorProcess_wULx31/credential'
I1030 05:55:06.988750 24459 sched.cpp:137] Version: 0.21.0
I1030 05:55:07.055521 24473 slave.cpp:276] Slave using credential for: test-principal
I1030 05:55:07.055726 24473 slave.cpp:289] Slave resources: cpus(*):2; mem(*):1024; disk(*):0; ports(*):[31000-32000]
I1030 05:55:07.055865 24473 slave.cpp:318] Slave hostname: pomona.apache.org
I1030 05:55:07.055881 24473 slave.cpp:319] Slave checkpoint: false
W1030 05:55:07.055889 24473 slave.cpp:321] Disabling checkpointing is deprecated and the --checkpoint flag will be removed in a future release. Please avoid using this flag
I1030 05:55:07.056172 24485 sched.cpp:233] New master detected at master@67.195.81.187:40429
I1030 05:55:07.056222 24485 sched.cpp:283] Authenticating with master master@67.195.81.187:40429
I1030 05:55:07.056717 24485 state.cpp:33] Recovering state from '/tmp/DRFAllocatorTest_DRFAllocatorProcess_wULx31/meta'
I1030 05:55:07.056851 24475 authenticatee.hpp:133] Creating new client SASL connection
I1030 05:55:07.057003 24473 status_update_manager.cpp:197] Recovering status update manager
I1030 05:55:07.057252 24488 master.cpp:3853] Authenticating scheduler-c98e7aac-d03f-464a-aa75-61208600e196@67.195.81.187:40429
I1030 05:55:07.057502 24489 containerizer.cpp:281] Recovering containerizer
I1030 05:55:07.057524 24475 authenticator.hpp:161] Creating new server SASL connection
I1030 05:55:07.057688 24475 authenticatee.hpp:224] Received SASL authentication mechanisms: CRAM-MD5
I1030 05:55:07.057719 24475 authenticatee.hpp:250] Attempting to authenticate with mechanism 'CRAM-MD5'
I1030 05:55:07.057919 24481 authenticator.hpp:267] Received SASL authentication start
I1030 05:55:07.057968 24481 authenticator.hpp:389] Authentication requires more steps
I1030 05:55:07.058070 24473 authenticatee.hpp:270] Received SASL authentication step
I1030 05:55:07.058199 24485 authenticator.hpp:295] Received SASL authentication step
I1030 05:55:07.058223 24485 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'pomona.apache.org' server FQDN: 'pomona.apache.org' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I1030 05:55:07.058233 24485 auxprop.cpp:153] Looking up auxiliary property '*userPassword'
I1030 05:55:07.058259 24485 auxprop.cpp:153] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I1030 05:55:07.058290 24485 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'pomona.apache.org' server FQDN: 'pomona.apache.org' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I1030 05:55:07.058302 24485 auxprop.cpp:103] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I1030 05:55:07.058307 24485 auxprop.cpp:103] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I1030 05:55:07.058320 24485 authenticator.hpp:381] Authentication success
I1030 05:55:07.058467 24480 master.cpp:3893] Successfully authenticated principal 'test-principal' at scheduler-c98e7aac-d03f-464a-aa75-61208600e196@67.195.81.187:40429
I1030 05:55:07.058493 24485 slave.cpp:3456] Finished recovery
I1030 05:55:07.058593 24478 authenticatee.hpp:310] Authentication success
I1030 05:55:07.058838 24478 sched.cpp:357] Successfully authenticated with master master@67.195.81.187:40429
I1030 05:55:07.058861 24478 sched.cpp:476] Sending registration request to master@67.195.81.187:40429
I1030 05:55:07.058969 24475 slave.cpp:602] New master detected at master@67.195.81.187:40429
I1030 05:55:07.058969 24487 status_update_manager.cpp:171] Pausing sending status updates
I1030 05:55:07.059026 24475 slave.cpp:665] Authenticating with master master@67.195.81.187:40429
I1030 05:55:07.059061 24481 master.cpp:1362] Received registration request for framework 'framework1' at scheduler-c98e7aac-d03f-464a-aa75-61208600e196@67.195.81.187:40429
I1030 05:55:07.059131 24481 master.cpp:1321] Authorizing framework principal 'test-principal' to receive offers for role 'role1'
I1030 05:55:07.059171 24475 slave.cpp:638] Detecting new master
I1030 05:55:07.059214 24482 authenticatee.hpp:133] Creating new client SASL connection
I1030 05:55:07.059550 24481 master.cpp:3853] Authenticating slave(203)@67.195.81.187:40429
I1030 05:55:07.059787 24487 authenticator.hpp:161] Creating new server SASL connection
I1030 05:55:07.059922 24481 master.cpp:1426] Registering framework 20141030-055506-3142697795-40429-24459-0000 (framework1) at scheduler-c98e7aac-d03f-464a-aa75-61208600e196@67.195.81.187:40429
I1030 05:55:07.059996 24474 authenticatee.hpp:224] Received SASL authentication mechanisms: CRAM-MD5
I1030 05:55:07.060034 24474 authenticatee.hpp:250] Attempting to authenticate with mechanism 'CRAM-MD5'
I1030 05:55:07.060117 24474 authenticator.hpp:267] Received SASL authentication start
I1030 05:55:07.060165 24474 authenticator.hpp:389] Authentication requires more steps
I1030 05:55:07.060377 24476 hierarchical_allocator_process.hpp:329] Added framework 20141030-055506-3142697795-40429-24459-0000
I1030 05:55:07.060394 24488 sched.cpp:407] Framework registered with 20141030-055506-3142697795-40429-24459-0000
I1030 05:55:07.060403 24476 hierarchical_allocator_process.hpp:697] No resources available to allocate!
I1030 05:55:07.060431 24476 hierarchical_allocator_process.hpp:659] Performed allocation for 0 slaves in 29857ns
I1030 05:55:07.060443 24488 sched.cpp:421] Scheduler::registered took 19407ns
I1030 05:55:07.060545 24478 authenticatee.hpp:270] Received SASL authentication step
I1030 05:55:07.060645 24478 authenticator.hpp:295] Received SASL authentication step
I1030 05:55:07.060673 24478 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'pomona.apache.org' server FQDN: 'pomona.apache.org' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I1030 05:55:07.060685 24478 auxprop.cpp:153] Looking up auxiliary property '*userPassword'
I1030 05:55:07.060714 24478 auxprop.cpp:153] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I1030 05:55:07.060740 24478 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'pomona.apache.org' server FQDN: 'pomona.apache.org' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I1030 05:55:07.060760 24478 auxprop.cpp:103] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I1030 05:55:07.060770 24478 auxprop.cpp:103] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I1030 05:55:07.060788 24478 authenticator.hpp:381] Authentication success
I1030 05:55:07.060920 24474 authenticatee.hpp:310] Authentication success
I1030 05:55:07.060945 24485 master.cpp:3893] Successfully authenticated principal 'test-principal' at slave(203)@67.195.81.187:40429
I1030 05:55:07.061388 24489 slave.cpp:722] Successfully authenticated with master master@67.195.81.187:40429
I1030 05:55:07.061504 24489 slave.cpp:1050] Will retry registration in 4.778336ms if necessary
I1030 05:55:07.061718 24480 master.cpp:3032] Registering slave at slave(203)@67.195.81.187:40429 (pomona.apache.org) with id 20141030-055506-3142697795-40429-24459-S0
I1030 05:55:07.062119 24489 registrar.cpp:445] Applied 1 operations in 53691ns; attempting to update the 'registry'
I1030 05:55:07.065182 24479 log.cpp:680] Attempting to append 316 bytes to the log
I1030 05:55:07.065337 24487 coordinator.cpp:340] Coordinator attempting to write APPEND action at position 3
I1030 05:55:07.066359 24474 replica.cpp:508] Replica received write request for position 3
I1030 05:55:07.066643 24474 leveldb.cpp:343] Persisting action (335 bytes) to leveldb took 249579ns
I1030 05:55:07.066671 24474 replica.cpp:676] Persisted action at 3
I../../src/tests/allocator_tests.cpp:120: Failure
Failed to wait 10secs for offers1
1030 05:55:07.067101 24477 slave.cpp:1050] Will retry registration in 24.08243ms if necessary
I1030 05:55:07.067140 24473 master.cpp:3020] Ignoring register slave message from slave(203)@67.195.81.187:40429 (pomona.apache.org) as admission is already in progress
I1030 05:55:07.067395 24488 replica.cpp:655] Replica received learned notice for position 3
I1030 05:55:07.943416 24478 hierarchical_allocator_process.hpp:697] No resources available to allocate!
I1030 05:55:19.804687 24478 hierarchical_allocator_process.hpp:659] Performed allocation for 0 slaves in 11.861261123secs
I1030 05:55:11.942713 24474 master.cpp:120] No whitelist given. Advertising offers for all slaves
I1030 05:55:19.805850 24488 leveldb.cpp:343] Persisting action (337 bytes) to leveldb took 1.067224ms
I1030 05:55:19.806012 24488 replica.cpp:676] Persisted action at 3
../../src/tests/allocator_tests.cpp:115: Failure
Actual function call count doesn't match EXPECT_CALL(sched1, resourceOffers(_, _))...
         Expected: to be called once
           Actual: never called - unsatisfied and active
I1030 05:55:19.806144 24488 replica.cpp:661] Replica learned APPEND action at position 3
I1030 05:55:19.806695 24473 master.cpp:768] Framework 20141030-055506-3142697795-40429-24459-0000 (framework1) at scheduler-c98e7aac-d03f-464a-aa75-61208600e196@67.195.81.187:40429 disconnected
I1030 05:55:19.806726 24473 master.cpp:1731] Disconnecting framework 20141030-055506-3142697795-40429-24459-0000 (framework1) at scheduler-c98e7aac-d03f-464a-aa75-61208600e196@67.195.81.187:40429
I1030 05:55:19.806751 24473 master.cpp:1747] Deactivating framework 20141030-055506-3142697795-40429-24459-0000 (framework1) at scheduler-c98e7aac-d03f-464a-aa75-61208600e196@67.195.81.187:40429
I1030 05:55:19.806967 24473 master.cpp:790] Giving framework 20141030-055506-3142697795-40429-24459-0000 (framework1) at scheduler-c98e7aac-d03f-464a-aa75-61208600e196@67.195.81.187:40429 0ns to failover
../../src/tests/allocator_tests.cpp:94: Failure
Actual function call count doesn't match EXPECT_CALL(allocator, slaveAdded(_, _, _))...
         Expected: to be called once
           Actual: never called - unsatisfied and active
F1030 05:55:19.806967 24480 logging.cpp:57] RAW: Pure virtual method called
I1030 05:55:19.807348 24488 master.cpp:3665] Framework failover timeout, removing framework 20141030-055506-3142697795-40429-24459-0000 (framework1) at scheduler-c98e7aac-d03f-464a-aa75-61208600e196@67.195.81.187:40429
I1030 05:55:19.807370 24488 master.cpp:4201] Removing framework 20141030-055506-3142697795-40429-24459-0000 (framework1) at scheduler-c98e7aac-d03f-464a-aa75-61208600e196@67.195.81.187:40429
*** Aborted at 1414648519 (unix time) try ""date -d @1414648519"" if you are using GNU date ***
PC: @           0x91bc86 process::PID<>::PID()
*** SIGSEGV (@0x0) received by PID 24459 (TID 0x2b86c919a700) from PID 0; stack trace: ***
I1030 05:55:19.808631 24489 registrar.cpp:490] Successfully updated the 'registry' in 12.746377984secs
    @     0x2b86c55fc340 (unknown)
I1030 05:55:19.808938 24473 log.cpp:699] Attempting to truncate the log to 3
    @     0x2b86c3327174  google::LogMessage::Fail()
I1030 05:55:19.809084 24481 coordinator.cpp:340] Coordinator attempting to write TRUNCATE action at position 4
    @           0x91bc86 process::PID<>::PID()
    @     0x2b86c332c868  google::RawLog__()
I1030 05:55:19.810191 24479 replica.cpp:508] Replica received write request for position 4
I1030 05:55:19.810899 24479 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 678090ns
I1030 05:55:19.810919 24479 replica.cpp:676] Persisted action at 4
    @           0x91bf24 process::Process<>::self()
I1030 05:55:19.811635 24485 replica.cpp:655] Replica received learned notice for position 4
I1030 05:55:19.812180 24485 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 523927ns
I1030 05:55:19.812228 24485 leveldb.cpp:401] Deleting ~2 keys from leveldb took 29523ns
I1030 05:55:19.812242 24485 replica.cpp:676] Persisted action at 4
I    @     0x2b86c29d2a36  __cxa_pure_virtual
1030 05:55:19.812258 24485 replica.cpp:661] Replica learned TRUNCATE action at position 4
    @          0x1046936  testing::internal::UntypedFunctionMockerBase::UntypedInvokeWith()
I1030 05:55:19.829655 24474 slave.cpp:1050] Will retry registration in 31.785967ms if necessary
    @           0x9c0633  testing::internal::FunctionMockerBase<>::InvokeWith()
    @           0x9b6152  testing::internal::FunctionMocker<>::Invoke()
    @           0x9abdeb  mesos::internal::tests::MockAllocatorProcess<>::frameworkDeactivated()
    @           0x91c78f  _ZZN7process8dispatchIN5mesos8internal6master9allocator16AllocatorProcessERKNS1_11FrameworkIDES6_EEvRKNS_3PIDIT_EEMSA_FvT0_ET1_ENKUlPNS_11ProcessBaseEE_clESJ_
    @           0x959ad7  _ZNSt17_Function_handlerIFvPN7process11ProcessBaseEEZNS0_8dispatchIN5mesos8internal6master9allocator16AllocatorProcessERKNS5_11FrameworkIDESA_EEvRKNS0_3PIDIT_EEMSE_FvT0_ET1_EUlS2_E_E9_M_invokeERKSt9_Any_dataS2_
    @     0x2b86c32d174f  std::function<>::operator()()
    @     0x2b86c32b2a17  process::ProcessBase::visit()
    @     0x2b86c32bd34c  process::DispatchEvent::visit()
    @           0x8e0812  process::ProcessBase::serve()
    @     0x2b86c32aec8c  process::ProcessManager::resume()
I1030 05:55:22.050081 24478 slave.cpp:1050] Will retry registration in 25.327301ms if necessary
    @     0x2b86c32a5351  process::schedule()
    @     0x2b86c55f4182  start_thread
    @     0x2b86c5904fbd  (unknown)
{noformat}",Bug,Major,xujyan,2016-02-10T19:35:15.000+0000,5,Resolved,Complete,"Segfault with ""Pure virtual method called"" when tests fail",2016-02-10T19:35:15.000+0000,MESOS-2017,5.0,mesos,Twitter Mesos Q4 Sprint 3
vinodkone,2014-10-29T18:17:52.000+0000,xujyan,"{noformat:title=}
[ RUN      ] MasterAuthorizationTest.DuplicateReregistration
Using temporary directory '/tmp/MasterAuthorizationTest_DuplicateReregistration_DLOmYX'
I1029 08:25:26.021766 32232 leveldb.cpp:176] Opened db in 3.066621ms
I1029 08:25:26.022734 32232 leveldb.cpp:183] Compacted db in 935019ns
I1029 08:25:26.022766 32232 leveldb.cpp:198] Created db iterator in 4350ns
I1029 08:25:26.022785 32232 leveldb.cpp:204] Seeked to beginning of db in 902ns
I1029 08:25:26.022799 32232 leveldb.cpp:273] Iterated through 0 keys in the db in 387ns
I1029 08:25:26.022831 32232 replica.cpp:741] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned
I1029 08:25:26.023305 32248 recover.cpp:437] Starting replica recovery
I1029 08:25:26.023598 32248 recover.cpp:463] Replica is in EMPTY status
I1029 08:25:26.025059 32260 replica.cpp:638] Replica in EMPTY status received a broadcasted recover request
I1029 08:25:26.025320 32247 recover.cpp:188] Received a recover response from a replica in EMPTY status
I1029 08:25:26.025585 32256 recover.cpp:554] Updating replica status to STARTING
I1029 08:25:26.026546 32249 master.cpp:312] Master 20141029-082526-3142697795-40696-32232 (pomona.apache.org) started on 67.195.81.187:40696
I1029 08:25:26.026561 32261 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 694444ns
I1029 08:25:26.026592 32249 master.cpp:358] Master only allowing authenticated frameworks to register
I1029 08:25:26.026592 32261 replica.cpp:320] Persisted replica status to STARTING
I1029 08:25:26.026605 32249 master.cpp:363] Master only allowing authenticated slaves to register
I1029 08:25:26.026639 32249 credentials.hpp:36] Loading credentials for authentication from '/tmp/MasterAuthorizationTest_DuplicateReregistration_DLOmYX/credentials'
I1029 08:25:26.026877 32249 master.cpp:392] Authorization enabled
I1029 08:25:26.026901 32260 recover.cpp:463] Replica is in STARTING status
I1029 08:25:26.027498 32261 master.cpp:120] No whitelist given. Advertising offers for all slaves
I1029 08:25:26.027541 32248 hierarchical_allocator_process.hpp:299] Initializing hierarchical allocator process with master : master@67.195.81.187:40696
I1029 08:25:26.028055 32252 replica.cpp:638] Replica in STARTING status received a broadcasted recover request
I1029 08:25:26.028451 32247 recover.cpp:188] Received a recover response from a replica in STARTING status
I1029 08:25:26.028733 32249 master.cpp:1242] The newly elected leader is master@67.195.81.187:40696 with id 20141029-082526-3142697795-40696-32232
I1029 08:25:26.028764 32249 master.cpp:1255] Elected as the leading master!
I1029 08:25:26.028781 32249 master.cpp:1073] Recovering from registrar
I1029 08:25:26.028904 32246 recover.cpp:554] Updating replica status to VOTING
I1029 08:25:26.029163 32257 registrar.cpp:313] Recovering registrar
I1029 08:25:26.029556 32251 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 485711ns
I1029 08:25:26.029588 32251 replica.cpp:320] Persisted replica status to VOTING
I1029 08:25:26.029726 32253 recover.cpp:568] Successfully joined the Paxos group
I1029 08:25:26.029932 32253 recover.cpp:452] Recover process terminated
I1029 08:25:26.030436 32250 log.cpp:656] Attempting to start the writer
I1029 08:25:26.032152 32248 replica.cpp:474] Replica received implicit promise request with proposal 1
I1029 08:25:26.032778 32248 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 597030ns
I1029 08:25:26.032807 32248 replica.cpp:342] Persisted promised to 1
I1029 08:25:26.033481 32254 coordinator.cpp:230] Coordinator attemping to fill missing position
I1029 08:25:26.035429 32247 replica.cpp:375] Replica received explicit promise request for position 0 with proposal 2
I1029 08:25:26.036154 32247 leveldb.cpp:343] Persisting action (8 bytes) to leveldb took 690208ns
I1029 08:25:26.036181 32247 replica.cpp:676] Persisted action at 0
I1029 08:25:26.037344 32249 replica.cpp:508] Replica received write request for position 0
I1029 08:25:26.037395 32249 leveldb.cpp:438] Reading position from leveldb took 22607ns
I1029 08:25:26.038074 32249 leveldb.cpp:343] Persisting action (14 bytes) to leveldb took 647429ns
I1029 08:25:26.038105 32249 replica.cpp:676] Persisted action at 0
I1029 08:25:26.038683 32247 replica.cpp:655] Replica received learned notice for position 0
I1029 08:25:26.039378 32247 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 664911ns
I1029 08:25:26.039407 32247 replica.cpp:676] Persisted action at 0
I1029 08:25:26.039433 32247 replica.cpp:661] Replica learned NOP action at position 0
I1029 08:25:26.040045 32252 log.cpp:672] Writer started with ending position 0
I1029 08:25:26.041378 32251 leveldb.cpp:438] Reading position from leveldb took 25625ns
I1029 08:25:26.044642 32246 registrar.cpp:346] Successfully fetched the registry (0B) in 15.433984ms
I1029 08:25:26.044742 32246 registrar.cpp:445] Applied 1 operations in 16444ns; attempting to update the 'registry'
I1029 08:25:26.047538 32256 log.cpp:680] Attempting to append 139 bytes to the log
I1029 08:25:26.156330 32247 coordinator.cpp:340] Coordinator attempting to write APPEND action at position 1
I1029 08:25:26.158460 32261 replica.cpp:508] Replica received write request for position 1
I1029 08:25:26.159277 32261 leveldb.cpp:343] Persisting action (158 bytes) to leveldb took 782308ns
I1029 08:25:26.159328 32261 replica.cpp:676] Persisted action at 1
I1029 08:25:26.160267 32255 replica.cpp:655] Replica received learned notice for position 1
I1029 08:25:26.161070 32255 leveldb.cpp:343] Persisting action (160 bytes) to leveldb took 750259ns
I1029 08:25:26.161100 32255 replica.cpp:676] Persisted action at 1
I1029 08:25:26.161125 32255 replica.cpp:661] Replica learned APPEND action at position 1
I1029 08:25:26.162199 32253 registrar.cpp:490] Successfully updated the 'registry' in 117.40416ms
I1029 08:25:26.162400 32253 registrar.cpp:376] Successfully recovered registrar
I1029 08:25:26.162724 32249 master.cpp:1100] Recovered 0 slaves from the Registry (101B) ; allowing 10mins for slaves to re-register
I1029 08:25:26.162757 32253 log.cpp:699] Attempting to truncate the log to 1
I1029 08:25:26.162919 32256 coordinator.cpp:340] Coordinator attempting to write TRUNCATE action at position 2
I1029 08:25:26.163949 32250 replica.cpp:508] Replica received write request for position 2
I1029 08:25:26.164589 32250 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 603175ns
I1029 08:25:26.164618 32250 replica.cpp:676] Persisted action at 2
I1029 08:25:26.165385 32251 replica.cpp:655] Replica received learned notice for position 2
I1029 08:25:26.166007 32251 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 594003ns
I1029 08:25:26.166056 32251 leveldb.cpp:401] Deleting ~1 keys from leveldb took 23309ns
I1029 08:25:26.166077 32251 replica.cpp:676] Persisted action at 2
I1029 08:25:26.166100 32251 replica.cpp:661] Replica learned TRUNCATE action at position 2
I1029 08:25:26.178493 32232 sched.cpp:137] Version: 0.21.0
I1029 08:25:26.179029 32256 sched.cpp:233] New master detected at master@67.195.81.187:40696
I1029 08:25:26.179078 32256 sched.cpp:283] Authenticating with master master@67.195.81.187:40696
I1029 08:25:26.179424 32246 authenticatee.hpp:133] Creating new client SASL connection
I1029 08:25:26.179678 32259 master.cpp:3853] Authenticating scheduler-9ba6b803-40b4-48b9-bcef-45a329f6b2a4@67.195.81.187:40696
I1029 08:25:26.179970 32250 authenticator.hpp:161] Creating new server SASL connection
I1029 08:25:26.180165 32250 authenticatee.hpp:224] Received SASL authentication mechanisms: CRAM-MD5
I1029 08:25:26.180191 32250 authenticatee.hpp:250] Attempting to authenticate with mechanism 'CRAM-MD5'
I1029 08:25:26.180272 32250 authenticator.hpp:267] Received SASL authentication start
I1029 08:25:26.180378 32250 authenticator.hpp:389] Authentication requires more steps
I1029 08:25:26.180557 32260 authenticatee.hpp:270] Received SASL authentication step
I1029 08:25:26.180704 32254 authenticator.hpp:295] Received SASL authentication step
I1029 08:25:26.180737 32254 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'pomona.apache.org' server FQDN: 'pomona.apache.org' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I1029 08:25:26.180748 32254 auxprop.cpp:153] Looking up auxiliary property '*userPassword'
I1029 08:25:26.180780 32254 auxprop.cpp:153] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I1029 08:25:26.180804 32254 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'pomona.apache.org' server FQDN: 'pomona.apache.org' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I1029 08:25:26.180816 32254 auxprop.cpp:103] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I1029 08:25:26.180824 32254 auxprop.cpp:103] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I1029 08:25:26.180841 32254 authenticator.hpp:381] Authentication success
I1029 08:25:26.180937 32259 authenticatee.hpp:310] Authentication success
I1029 08:25:26.180991 32260 master.cpp:3893] Successfully authenticated principal 'test-principal' at scheduler-9ba6b803-40b4-48b9-bcef-45a329f6b2a4@67.195.81.187:40696
I1029 08:25:26.181422 32259 sched.cpp:357] Successfully authenticated with master master@67.195.81.187:40696
I1029 08:25:26.181449 32259 sched.cpp:476] Sending registration request to master@67.195.81.187:40696
I1029 08:25:26.181697 32260 master.cpp:1362] Received registration request for framework 'default' at scheduler-9ba6b803-40b4-48b9-bcef-45a329f6b2a4@67.195.81.187:40696
I1029 08:25:26.181758 32260 master.cpp:1321] Authorizing framework principal 'test-principal' to receive offers for role '*'
I1029 08:25:26.182063 32260 master.cpp:1426] Registering framework 20141029-082526-3142697795-40696-32232-0000 (default) at scheduler-9ba6b803-40b4-48b9-bcef-45a329f6b2a4@67.195.81.187:40696
I1029 08:25:26.182430 32248 hierarchical_allocator_process.hpp:329] Added framework 20141029-082526-3142697795-40696-32232-0000
I1029 08:25:26.182462 32248 hierarchical_allocator_process.hpp:697] No resources available to allocate!
I1029 08:25:26.182462 32261 sched.cpp:407] Framework registered with 20141029-082526-3142697795-40696-32232-0000
I1029 08:25:26.182473 32248 hierarchical_allocator_process.hpp:659] Performed allocation for 0 slaves in 15372ns
I1029 08:25:26.182554 32261 sched.cpp:421] Scheduler::registered took 60059ns
I1029 08:25:26.185515 32260 sched.cpp:227] Scheduler::disconnected took 16607ns
I1029 08:25:26.185538 32260 sched.cpp:233] New master detected at master@67.195.81.187:40696
I1029 08:25:26.185567 32260 sched.cpp:283] Authenticating with master master@67.195.81.187:40696
I1029 08:25:26.185783 32246 authenticatee.hpp:133] Creating new client SASL connection
I1029 08:25:26.186218 32250 master.cpp:3853] Authenticating scheduler-9ba6b803-40b4-48b9-bcef-45a329f6b2a4@67.195.81.187:40696
I1029 08:25:26.186456 32247 authenticator.hpp:161] Creating new server SASL connection
I1029 08:25:26.186594 32250 authenticatee.hpp:224] Received SASL authentication mechanisms: CRAM-MD5
I1029 08:25:26.186621 32250 authenticatee.hpp:250] Attempting to authenticate with mechanism 'CRAM-MD5'
I1029 08:25:26.186745 32259 authenticator.hpp:267] Received SASL authentication start
I1029 08:25:26.186800 32259 authenticator.hpp:389] Authentication requires more steps
I1029 08:25:26.186936 32260 authenticatee.hpp:270] Received SASL authentication step
I1029 08:25:26.187062 32249 authenticator.hpp:295] Received SASL authentication step
I1029 08:25:26.187095 32249 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'pomona.apache.org' server FQDN: 'pomona.apache.org' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I1029 08:25:26.187108 32249 auxprop.cpp:153] Looking up auxiliary property '*userPassword'
I1029 08:25:26.187137 32249 auxprop.cpp:153] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I1029 08:25:26.187162 32249 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'pomona.apache.org' server FQDN: 'pomona.apache.org' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I1029 08:25:26.187175 32249 auxprop.cpp:103] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I1029 08:25:26.187182 32249 auxprop.cpp:103] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I1029 08:25:26.187199 32249 authenticator.hpp:381] Authentication success
I1029 08:25:26.187327 32249 authenticatee.hpp:310] Authentication success
I1029 08:25:26.187366 32260 master.cpp:3893] Successfully authenticated principal 'test-principal' at scheduler-9ba6b803-40b4-48b9-bcef-45a329f6b2a4@67.195.81.187:40696
I1029 08:25:26.187631 32249 sched.cpp:357] Successfully authenticated with master master@67.195.81.187:40696
I1029 08:25:26.187659 32249 sched.cpp:476] Sending registration request to master@67.195.81.187:40696
I1029 08:25:27.028445 32251 hierarchical_allocator_process.hpp:697] No resources available to allocate!
I1029 08:25:28.045682 32251 hierarchical_allocator_process.hpp:659] Performed allocation for 0 slaves in 1.017231941secs
I1029 08:25:28.045760 32249 sched.cpp:476] Sending registration request to master@67.195.81.187:40696
I1029 08:25:28.045900 32253 master.cpp:1499] Received re-registration request from framework 20141029-082526-3142697795-40696-32232-0000 (default) at scheduler-9ba6b803-40b4-48b9-bcef-45a329f6b2a4@67.195.81.187:40696
I1029 08:25:28.045989 32253 master.cpp:1321] Authorizing framework principal 'test-principal' to receive offers for role '*'
I1029 08:25:28.046455 32253 master.cpp:1499] Received re-registration request from framework 20141029-082526-3142697795-40696-32232-0000 (default) at scheduler-9ba6b803-40b4-48b9-bcef-45a329f6b2a4@67.195.81.187:40696
I1029 08:25:28.046529 32253 master.cpp:1321] Authorizing framework principal 'test-principal' to receive offers for role '*'
I1029 08:25:28.050155 32247 sched.cpp:233] New master detected at master@67.195.81.187:40696
I1029 08:25:28.050217 32247 sched.cpp:283] Authenticating with master master@67.195.81.187:40696
I1029 08:25:28.050405 32252 master.cpp:1552] Re-registering framework 20141029-082526-3142697795-40696-32232-0000 (default)  at scheduler-9ba6b803-40b4-48b9-bcef-45a329f6b2a4@67.195.81.187:40696
I1029 08:25:28.050509 32253 authenticatee.hpp:133] Creating new client SASL connection
I1029 08:25:28.050566 32252 master.cpp:1592] Allowing framework 20141029-082526-3142697795-40696-32232-0000 (default) at scheduler-9ba6b803-40b4-48b9-bcef-45a329f6b2a4@67.195.81.187:40696 to re-register with an already used id
I1029 08:25:28.051084 32257 sched.cpp:449] Framework re-registered with 20141029-082526-3142697795-40696-32232-0000
I1029 08:25:28.051151 32252 master.cpp:3853] Authenticating scheduler-9ba6b803-40b4-48b9-bcef-45a329f6b2a4@67.195.81.187:40696
I1029 08:25:28.051167 32257 sched.cpp:463] Scheduler::reregistered took 52801ns
I1029 08:25:28.051723 32261 authenticator.hpp:161] Creating new server SASL connection
I1029 08:25:28.052042 32249 authenticatee.hpp:224] Received SASL authentication mechanisms: CRAM-MD5
I1029 08:25:28.052077 32249 authenticatee.hpp:250] Attempting to authenticate with mechanism 'CRAM-MD5'
I1029 08:25:28.052170 32249 master.cpp:1534] Dropping re-registration request of framework 20141029-082526-3142697795-40696-32232-0000 (default) at scheduler-9ba6b803-40b4-48b9-bcef-45a329f6b2a4@67.195.81.187:40696 because new authentication attempt is in progress
I1029 08:25:28.052218 32257 authenticator.hpp:267] Received SASL authentication start
I1029 08:25:28.052325 32257 authenticator.hpp:389] Authentication requires more steps
I1029 08:25:28.052428 32257 authenticatee.hpp:270] Received SASL authentication step
I1029 08:25:28.052641 32246 authenticator.hpp:295] Received SASL authentication step
I1029 08:25:28.052685 32246 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'pomona.apache.org' server FQDN: 'pomona.apache.org' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I1029 08:25:28.052701 32246 auxprop.cpp:153] Looking up auxiliary property '*userPassword'
I1029 08:25:28.052739 32246 auxprop.cpp:153] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I1029 08:25:28.052767 32246 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'pomona.apache.org' server FQDN: 'pomona.apache.org' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I1029 08:25:28.052779 32246 auxprop.cpp:103] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I1029 08:25:28.052788 32246 auxprop.cpp:103] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I1029 08:25:28.052804 32246 authenticator.hpp:381] Authentication success
I1029 08:25:28.052947 32252 authenticatee.hpp:310] Authentication success
I1029 08:25:28.053020 32246 master.cpp:3893] Successfully authenticated principal 'test-principal' at scheduler-9ba6b803-40b4-48b9-bcef-45a329f6b2a4@67.195.81.187:40696
I1029 08:25:28.053462 32247 sched.cpp:357] Successfully authenticated with master master@67.195.81.187:40696
I1029 08:25:29.046855 32261 hierarchical_allocator_process.hpp:697] No resources available to allocate!
I1029 08:25:29.046880 32261 hierarchical_allocator_process.hpp:659] Performed allocation for 0 slaves in 35632ns
I1029 08:25:30.047458 32253 hierarchical_allocator_process.hpp:697] No resources available to allocate!
I1029 08:25:30.047487 32253 hierarchical_allocator_process.hpp:659] Performed allocation for 0 slaves in 43031ns
I1029 08:25:31.028373 32261 master.cpp:120] No whitelist given. Advertising offers for all slaves
I1029 08:25:31.048673 32249 hierarchical_allocator_process.hpp:697] No resources available to allocate!
I1029 08:25:31.048702 32249 hierarchical_allocator_process.hpp:659] Performed allocation for 0 slaves in 44769ns
I1029 08:25:32.049576 32259 hierarchical_allocator_process.hpp:697] No resources available to allocate!
I1029 08:25:32.049604 32259 hierarchical_allocator_process.hpp:659] Performed allocation for 0 slaves in 51919ns
I1029 08:25:33.050864 32249 hierarchical_allocator_process.hpp:697] No resources available to allocate!
I1029 08:25:33.050896 32249 hierarchical_allocator_process.hpp:659] Performed allocation for 0 slaves in 38019ns
I1029 08:25:34.051961 32251 hierarchical_allocator_process.hpp:697] No resources available to allocate!
I1029 08:25:34.051993 32251 hierarchical_allocator_process.hpp:659] Performed allocation for 0 slaves in 64619ns
I1029 08:25:35.052196 32249 hierarchical_allocator_process.hpp:697] No resources available to allocate!
I1029 08:25:35.052223 32249 hierarchical_allocator_process.hpp:659] Performed allocation for 0 slaves in 34475ns
I1029 08:25:36.029101 32259 master.cpp:120] No whitelist given. Advertising offers for all slaves
I1029 08:25:36.053067 32249 hierarchical_allocator_process.hpp:697] No resources available to allocate!
I1029 08:25:36.053095 32249 hierarchical_allocator_process.hpp:659] Performed allocation for 0 slaves in 38354ns
I1029 08:25:37.053506 32259 hierarchical_allocator_process.hpp:697] No resources available to allocate!
I1029 08:25:37.053536 32259 hierarchical_allocator_process.hpp:659] Performed allocation for 0 slaves in 38249ns
tests/master_authorization_tests.cpp:877: Failure
Failed to wait 10secs for frameworkReregisteredMessage
I1029 08:25:38.053241 32259 master.cpp:768] Framework 20141029-082526-3142697795-40696-32232-0000 (default) at scheduler-9ba6b803-40b4-48b9-bcef-45a329f6b2a4@67.195.81.187:40696 disconnected
I1029 08:25:38.053375 32259 master.cpp:1731] Disconnecting framework 20141029-082526-3142697795-40696-32232-0000 (default) at scheduler-9ba6b803-40b4-48b9-bcef-45a329f6b2a4@67.195.81.187:40696
I1029 08:25:38.053426 32259 master.cpp:1747] Deactivating framework 20141029-082526-3142697795-40696-32232-0000 (default) at scheduler-9ba6b803-40b4-48b9-bcef-45a329f6b2a4@67.195.81.187:40696
I1029 08:25:38.053932 32259 master.cpp:790] Giving framework 20141029-082526-3142697795-40696-32232-0000 (default) at scheduler-9ba6b803-40b4-48b9-bcef-45a329f6b2a4@67.195.81.187:40696 0ns to failover
I1029 08:25:38.054072 32257 hierarchical_allocator_process.hpp:405] Deactivated framework 20141029-082526-3142697795-40696-32232-0000
I1029 08:25:38.054208 32257 hierarchical_allocator_process.hpp:697] No resources available to allocate!
I1029 08:25:38.054236 32257 hierarchical_allocator_process.hpp:659] Performed allocation for 0 slaves in 38534ns
I1029 08:25:38.054508 32258 master.cpp:3665] Framework failover timeout, removing framework 20141029-082526-3142697795-40696-32232-0000 (default) at scheduler-9ba6b803-40b4-48b9-bcef-45a329f6b2a4@67.195.81.187:40696
I1029 08:25:38.054549 32258 master.cpp:4201] Removing framework 20141029-082526-3142697795-40696-32232-0000 (default) at scheduler-9ba6b803-40b4-48b9-bcef-45a329f6b2a4@67.195.81.187:40696
I1029 08:25:38.055179 32252 master.cpp:677] Master terminating
I1029 08:25:38.055181 32254 hierarchical_allocator_process.hpp:360] Removed framework 20141029-082526-3142697795-40696-32232-0000
../3rdparty/libprocess/include/process/gmock.hpp:345: Failure
Actual function call count doesn't match EXPECT_CALL(filter->mock, filter(testing::A<const MessageEvent&>()))...
    Expected args: message matcher (8-byte object <B8-BD 01-88 4A-2B 00-00>, 1-byte object <95>, 1-byte object <30>)
         Expected: to be called once
           Actual: never called - unsatisfied and active
[  FAILED  ] MasterAuthorizationTest.DuplicateReregistration (12042 ms)
{noformat}",Bug,Major,xujyan,2014-11-10T23:27:14.000+0000,5,Resolved,Complete,MasterAuthorizationTest.DuplicateReregistration is flaky,2014-11-10T23:27:14.000+0000,MESOS-2008,2.0,mesos,Twitter Mesos Q4 Sprint 3
klueska,2014-10-29T17:42:16.000+0000,xujyan,"{noformat:title=}
[ RUN      ] AllocatorTest/0.SlaveReregistersFirst
Using temporary directory '/tmp/AllocatorTest_0_SlaveReregistersFirst_YPe61d'
I1028 23:48:22.360447 31190 leveldb.cpp:176] Opened db in 2.192575ms
I1028 23:48:22.361253 31190 leveldb.cpp:183] Compacted db in 760753ns
I1028 23:48:22.361320 31190 leveldb.cpp:198] Created db iterator in 22188ns
I1028 23:48:22.361340 31190 leveldb.cpp:204] Seeked to beginning of db in 1950ns
I1028 23:48:22.361351 31190 leveldb.cpp:273] Iterated through 0 keys in the db in 345ns
I1028 23:48:22.361403 31190 replica.cpp:741] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned
I1028 23:48:22.362185 31217 recover.cpp:437] Starting replica recovery
I1028 23:48:22.362764 31219 recover.cpp:463] Replica is in EMPTY status
I1028 23:48:22.363955 31210 replica.cpp:638] Replica in EMPTY status received a broadcasted recover request
I1028 23:48:22.364320 31217 recover.cpp:188] Received a recover response from a replica in EMPTY status
I1028 23:48:22.364820 31211 recover.cpp:554] Updating replica status to STARTING
I1028 23:48:22.365365 31215 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 418991ns
I1028 23:48:22.365391 31215 replica.cpp:320] Persisted replica status to STARTING
I1028 23:48:22.365617 31217 recover.cpp:463] Replica is in STARTING status
I1028 23:48:22.366328 31206 master.cpp:312] Master 20141028-234822-3193029443-50043-31190 (pietas.apache.org) started on 67.195.81.190:50043
I1028 23:48:22.366377 31206 master.cpp:358] Master only allowing authenticated frameworks to register
I1028 23:48:22.366391 31206 master.cpp:363] Master only allowing authenticated slaves to register
I1028 23:48:22.366402 31206 credentials.hpp:36] Loading credentials for authentication from '/tmp/AllocatorTest_0_SlaveReregistersFirst_YPe61d/credentials'
I1028 23:48:22.366708 31206 master.cpp:392] Authorization enabled
I1028 23:48:22.366886 31209 replica.cpp:638] Replica in STARTING status received a broadcasted recover request
I1028 23:48:22.367311 31208 master.cpp:120] No whitelist given. Advertising offers for all slaves
I1028 23:48:22.367312 31207 recover.cpp:188] Received a recover response from a replica in STARTING status
I1028 23:48:22.367686 31211 hierarchical_allocator_process.hpp:299] Initializing hierarchical allocator process with master : master@67.195.81.190:50043
I1028 23:48:22.367863 31212 recover.cpp:554] Updating replica status to VOTING
I1028 23:48:22.368477 31218 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 375527ns
I1028 23:48:22.368505 31218 replica.cpp:320] Persisted replica status to VOTING
I1028 23:48:22.368517 31204 master.cpp:1242] The newly elected leader is master@67.195.81.190:50043 with id 20141028-234822-3193029443-50043-31190
I1028 23:48:22.368549 31204 master.cpp:1255] Elected as the leading master!
I1028 23:48:22.368567 31204 master.cpp:1073] Recovering from registrar
I1028 23:48:22.368621 31215 recover.cpp:568] Successfully joined the Paxos group
I1028 23:48:22.368716 31219 registrar.cpp:313] Recovering registrar
I1028 23:48:22.369000 31215 recover.cpp:452] Recover process terminated
I1028 23:48:22.369523 31208 log.cpp:656] Attempting to start the writer
I1028 23:48:22.370909 31205 replica.cpp:474] Replica received implicit promise request with proposal 1
I1028 23:48:22.371266 31205 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 325016ns
I1028 23:48:22.371290 31205 replica.cpp:342] Persisted promised to 1
I1028 23:48:22.371979 31218 coordinator.cpp:230] Coordinator attemping to fill missing position
I1028 23:48:22.373378 31210 replica.cpp:375] Replica received explicit promise request for position 0 with proposal 2
I1028 23:48:22.373746 31210 leveldb.cpp:343] Persisting action (8 bytes) to leveldb took 329018ns
I1028 23:48:22.373772 31210 replica.cpp:676] Persisted action at 0
I1028 23:48:22.374897 31214 replica.cpp:508] Replica received write request for position 0
I1028 23:48:22.374951 31214 leveldb.cpp:438] Reading position from leveldb took 26002ns
I1028 23:48:22.375272 31214 leveldb.cpp:343] Persisting action (14 bytes) to leveldb took 289094ns
I1028 23:48:22.375298 31214 replica.cpp:676] Persisted action at 0
I1028 23:48:22.375886 31204 replica.cpp:655] Replica received learned notice for position 0
I1028 23:48:22.376258 31204 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 346650ns
I1028 23:48:22.376277 31204 replica.cpp:676] Persisted action at 0
I1028 23:48:22.376298 31204 replica.cpp:661] Replica learned NOP action at position 0
I1028 23:48:22.376843 31215 log.cpp:672] Writer started with ending position 0
I1028 23:48:22.378056 31205 leveldb.cpp:438] Reading position from leveldb took 28265ns
I1028 23:48:22.380323 31217 registrar.cpp:346] Successfully fetched the registry (0B) in 11.55584ms
I1028 23:48:22.380466 31217 registrar.cpp:445] Applied 1 operations in 50632ns; attempting to update the 'registry'
I1028 23:48:22.382472 31217 log.cpp:680] Attempting to append 139 bytes to the log
I1028 23:48:22.382715 31210 coordinator.cpp:340] Coordinator attempting to write APPEND action at position 1
I1028 23:48:22.383463 31210 replica.cpp:508] Replica received write request for position 1
I1028 23:48:22.383857 31210 leveldb.cpp:343] Persisting action (158 bytes) to leveldb took 363758ns
I1028 23:48:22.383875 31210 replica.cpp:676] Persisted action at 1
I1028 23:48:22.384397 31218 replica.cpp:655] Replica received learned notice for position 1
I1028 23:48:22.384840 31218 leveldb.cpp:343] Persisting action (160 bytes) to leveldb took 420161ns
I1028 23:48:22.384862 31218 replica.cpp:676] Persisted action at 1
I1028 23:48:22.384882 31218 replica.cpp:661] Replica learned APPEND action at position 1
I1028 23:48:22.385684 31211 registrar.cpp:490] Successfully updated the 'registry' in 5.158144ms
I1028 23:48:22.385818 31211 registrar.cpp:376] Successfully recovered registrar
I1028 23:48:22.385912 31214 log.cpp:699] Attempting to truncate the log to 1
I1028 23:48:22.386101 31218 coordinator.cpp:340] Coordinator attempting to write TRUNCATE action at position 2
I1028 23:48:22.386124 31211 master.cpp:1100] Recovered 0 slaves from the Registry (101B) ; allowing 10mins for slaves to re-register
I1028 23:48:22.387398 31209 replica.cpp:508] Replica received write request for position 2
I1028 23:48:22.387758 31209 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 334969ns
I1028 23:48:22.387776 31209 replica.cpp:676] Persisted action at 2
I1028 23:48:22.388272 31204 replica.cpp:655] Replica received learned notice for position 2
I1028 23:48:22.388453 31204 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 159390ns
I1028 23:48:22.388501 31204 leveldb.cpp:401] Deleting ~1 keys from leveldb took 30409ns
I1028 23:48:22.388516 31204 replica.cpp:676] Persisted action at 2
I1028 23:48:22.388531 31204 replica.cpp:661] Replica learned TRUNCATE action at position 2
I1028 23:48:22.400737 31207 slave.cpp:169] Slave started on 34)@67.195.81.190:50043
I1028 23:48:22.400786 31207 credentials.hpp:84] Loading credential for authentication from '/tmp/AllocatorTest_0_SlaveReregistersFirst_QPPV21/credential'
I1028 23:48:22.400996 31207 slave.cpp:276] Slave using credential for: test-principal
I1028 23:48:22.401304 31207 slave.cpp:289] Slave resources: cpus(*):2; mem(*):1024; disk(*):3.70122e+06; ports(*):[31000-32000]
I1028 23:48:22.401413 31207 slave.cpp:318] Slave hostname: pietas.apache.org
I1028 23:48:22.401520 31207 slave.cpp:319] Slave checkpoint: false
W1028 23:48:22.401535 31207 slave.cpp:321] Disabling checkpointing is deprecated and the --checkpoint flag will be removed in a future release. Please avoid using this flag
I1028 23:48:22.402349 31207 state.cpp:33] Recovering state from '/tmp/AllocatorTest_0_SlaveReregistersFirst_QPPV21/meta'
I1028 23:48:22.402678 31207 status_update_manager.cpp:197] Recovering status update manager
I1028 23:48:22.403048 31211 slave.cpp:3456] Finished recovery
I1028 23:48:22.403815 31215 slave.cpp:602] New master detected at master@67.195.81.190:50043
I1028 23:48:22.403852 31215 slave.cpp:665] Authenticating with master master@67.195.81.190:50043
I1028 23:48:22.403875 31206 status_update_manager.cpp:171] Pausing sending status updates
I1028 23:48:22.403961 31215 slave.cpp:638] Detecting new master
I1028 23:48:22.404016 31211 authenticatee.hpp:133] Creating new client SASL connection
I1028 23:48:22.404230 31204 master.cpp:3853] Authenticating slave(34)@67.195.81.190:50043
I1028 23:48:22.404464 31205 authenticator.hpp:161] Creating new server SASL connection
I1028 23:48:22.404613 31211 authenticatee.hpp:224] Received SASL authentication mechanisms: CRAM-MD5
I1028 23:48:22.404649 31211 authenticatee.hpp:250] Attempting to authenticate with mechanism 'CRAM-MD5'
I1028 23:48:22.404734 31211 authenticator.hpp:267] Received SASL authentication start
I1028 23:48:22.404783 31211 authenticator.hpp:389] Authentication requires more steps
I1028 23:48:22.404898 31215 authenticatee.hpp:270] Received SASL authentication step
I1028 23:48:22.404999 31215 authenticator.hpp:295] Received SASL authentication step
I1028 23:48:22.405030 31215 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'pietas.apache.org' server FQDN: 'pietas.apache.org' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I1028 23:48:22.405047 31215 auxprop.cpp:153] Looking up auxiliary property '*userPassword'
I1028 23:48:22.405086 31215 auxprop.cpp:153] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I1028 23:48:22.405109 31215 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'pietas.apache.org' server FQDN: 'pietas.apache.org' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I1028 23:48:22.405122 31215 auxprop.cpp:103] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I1028 23:48:22.405129 31215 auxprop.cpp:103] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I1028 23:48:22.405146 31215 authenticator.hpp:381] Authentication success
I1028 23:48:22.405243 31213 authenticatee.hpp:310] Authentication success
I1028 23:48:22.405253 31214 master.cpp:3893] Successfully authenticated principal 'test-principal' at slave(34)@67.195.81.190:50043
I1028 23:48:22.405505 31213 slave.cpp:722] Successfully authenticated with master master@67.195.81.190:50043
I1028 23:48:22.405619 31213 slave.cpp:1050] Will retry registration in 17.050994ms if necessary
I1028 23:48:22.405819 31215 master.cpp:3032] Registering slave at slave(34)@67.195.81.190:50043 (pietas.apache.org) with id 20141028-234822-3193029443-50043-31190-S0
I1028 23:48:22.406262 31216 registrar.cpp:445] Applied 1 operations in 52647ns; attempting to update the 'registry'
I1028 23:48:22.406697 31190 sched.cpp:137] Version: 0.21.0
I1028 23:48:22.407083 31211 sched.cpp:233] New master detected at master@67.195.81.190:50043
I1028 23:48:22.407114 31211 sched.cpp:283] Authenticating with master master@67.195.81.190:50043
I1028 23:48:22.407290 31214 authenticatee.hpp:133] Creating new client SASL connection
I1028 23:48:22.407424 31214 master.cpp:3853] Authenticating scheduler-0aa33fc7-0d29-487c-80eb-f933681f9c95@67.195.81.190:50043
I1028 23:48:22.407659 31207 authenticator.hpp:161] Creating new server SASL connection
I1028 23:48:22.407757 31207 authenticatee.hpp:224] Received SASL authentication mechanisms: CRAM-MD5
I1028 23:48:22.407774 31207 authenticatee.hpp:250] Attempting to authenticate with mechanism 'CRAM-MD5'
I1028 23:48:22.407830 31207 authenticator.hpp:267] Received SASL authentication start
I1028 23:48:22.407868 31207 authenticator.hpp:389] Authentication requires more steps
I1028 23:48:22.407927 31207 authenticatee.hpp:270] Received SASL authentication step
I1028 23:48:22.408015 31212 authenticator.hpp:295] Received SASL authentication step
I1028 23:48:22.408037 31212 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'pietas.apache.org' server FQDN: 'pietas.apache.org' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I1028 23:48:22.408046 31212 auxprop.cpp:153] Looking up auxiliary property '*userPassword'
I1028 23:48:22.408072 31212 auxprop.cpp:153] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I1028 23:48:22.408092 31212 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'pietas.apache.org' server FQDN: 'pietas.apache.org' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I1028 23:48:22.408100 31212 auxprop.cpp:103] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I1028 23:48:22.408105 31212 auxprop.cpp:103] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I1028 23:48:22.408116 31212 authenticator.hpp:381] Authentication success
I1028 23:48:22.408192 31210 authenticatee.hpp:310] Authentication success
I1028 23:48:22.408210 31217 master.cpp:3893] Successfully authenticated principal 'test-principal' at scheduler-0aa33fc7-0d29-487c-80eb-f933681f9c95@67.195.81.190:50043
I1028 23:48:22.408419 31210 sched.cpp:357] Successfully authenticated with master master@67.195.81.190:50043
I1028 23:48:22.408460 31210 sched.cpp:476] Sending registration request to master@67.195.81.190:50043
I1028 23:48:22.408568 31217 master.cpp:1362] Received registration request for framework 'default' at scheduler-0aa33fc7-0d29-487c-80eb-f933681f9c95@67.195.81.190:50043
I1028 23:48:22.408617 31217 master.cpp:1321] Authorizing framework principal 'test-principal' to receive offers for role '*'
I1028 23:48:22.408937 31214 master.cpp:1426] Registering framework 20141028-234822-3193029443-50043-31190-0000 (default) at scheduler-0aa33fc7-0d29-487c-80eb-f933681f9c95@67.195.81.190:50043
I1028 23:48:22.409265 31213 sched.cpp:407] Framework registered with 20141028-234822-3193029443-50043-31190-0000
I1028 23:48:22.409267 31212 hierarchical_allocator_process.hpp:329] Added framework 20141028-234822-3193029443-50043-31190-0000
I1028 23:48:22.409312 31212 hierarchical_allocator_process.hpp:697] No resources available to allocate!
I1028 23:48:22.409324 31215 log.cpp:680] Attempting to append 316 bytes to the log
I1028 23:48:22.409333 31213 sched.cpp:421] Scheduler::registered took 38591ns
I1028 23:48:22.409327 31212 hierarchical_allocator_process.hpp:659] Performed allocation for 0 slaves in 24107ns
I1028 23:48:22.409518 31205 coordinator.cpp:340] Coordinator attempting to write APPEND action at position 3
I1028 23:48:22.410127 31206 replica.cpp:508] Replica received write request for position 3
I1028 23:48:22.410706 31206 leveldb.cpp:343] Persisting action (335 bytes) to leveldb took 554098ns
I1028 23:48:22.410725 31206 replica.cpp:676] Persisted action at 3
I1028 23:48:22.411151 31217 replica.cpp:655] Replica received learned notice for position 3
I1028 23:48:22.411499 31217 leveldb.cpp:343] Persisting action (337 bytes) to leveldb took 326572ns
I1028 23:48:22.411519 31217 replica.cpp:676] Persisted action at 3
I1028 23:48:22.411533 31217 replica.cpp:661] Replica learned APPEND action at position 3
I1028 23:48:22.412292 31219 registrar.cpp:490] Successfully updated the 'registry' in 5.972992ms
I1028 23:48:22.412518 31218 log.cpp:699] Attempting to truncate the log to 3
I1028 23:48:22.412621 31213 coordinator.cpp:340] Coordinator attempting to write TRUNCATE action at position 4
I1028 23:48:22.412734 31219 slave.cpp:2522] Received ping from slave-observer(38)@67.195.81.190:50043
I1028 23:48:22.412787 31206 master.cpp:3086] Registered slave 20141028-234822-3193029443-50043-31190-S0 at slave(34)@67.195.81.190:50043 (pietas.apache.org) with cpus(*):2; mem(*):1024; disk(*):3.70122e+06; ports(*):[31000-32000]
I1028 23:48:22.412858 31219 slave.cpp:756] Registered with master master@67.195.81.190:50043; given slave ID 20141028-234822-3193029443-50043-31190-S0
I1028 23:48:22.412994 31210 status_update_manager.cpp:178] Resuming sending status updates
I1028 23:48:22.413014 31211 hierarchical_allocator_process.hpp:442] Added slave 20141028-234822-3193029443-50043-31190-S0 (pietas.apache.org) with cpus(*):2; mem(*):1024; disk(*):3.70122e+06; ports(*):[31000-32000] (and cpus(*):2; mem(*):1024; disk(*):3.70122e+06; ports(*):[31000-32000] available)
I1028 23:48:22.413159 31211 hierarchical_allocator_process.hpp:734] Offering cpus(*):2; mem(*):1024; disk(*):3.70122e+06; ports(*):[31000-32000] on slave 20141028-234822-3193029443-50043-31190-S0 to framework 20141028-234822-3193029443-50043-31190-0000
I1028 23:48:22.413290 31208 replica.cpp:508] Replica received write request for position 4
I1028 23:48:22.413421 31211 hierarchical_allocator_process.hpp:679] Performed allocation for slave 20141028-234822-3193029443-50043-31190-S0 in 346658ns
I1028 23:48:22.413650 31208 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 336067ns
I1028 23:48:22.413668 31208 replica.cpp:676] Persisted action at 4
I1028 23:48:22.413797 31216 master.cpp:3795] Sending 1 offers to framework 20141028-234822-3193029443-50043-31190-0000 (default) at scheduler-0aa33fc7-0d29-487c-80eb-f933681f9c95@67.195.81.190:50043
I1028 23:48:22.414077 31212 replica.cpp:655] Replica received learned notice for position 4
I1028 23:48:22.414356 31212 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 260401ns
I1028 23:48:22.414403 31212 leveldb.cpp:401] Deleting ~2 keys from leveldb took 28541ns
I1028 23:48:22.414417 31212 replica.cpp:676] Persisted action at 4
I1028 23:48:22.414446 31212 replica.cpp:661] Replica learned TRUNCATE action at position 4
I1028 23:48:22.414422 31207 sched.cpp:544] Scheduler::resourceOffers took 310278ns
I1028 23:48:22.415086 31214 master.cpp:2321] Processing reply for offers: [ 20141028-234822-3193029443-50043-31190-O0 ] on slave 20141028-234822-3193029443-50043-31190-S0 at slave(34)@67.195.81.190:50043 (pietas.apache.org) for framework 20141028-234822-3193029443-50043-31190-0000 (default) at scheduler-0aa33fc7-0d29-487c-80eb-f933681f9c95@67.195.81.190:50043
W1028 23:48:22.415163 31214 master.cpp:1969] Executor default for task 0 uses less CPUs (None) than the minimum required (0.01). Please update your executor, as this will be mandatory in future releases.
W1028 23:48:22.415186 31214 master.cpp:1980] Executor default for task 0 uses less memory (None) than the minimum required (32MB). Please update your executor, as this will be mandatory in future releases.
I1028 23:48:22.415256 31214 master.cpp:2417] Authorizing framework principal 'test-principal' to launch task 0 as user 'jenkins'
I1028 23:48:22.416033 31219 master.hpp:877] Adding task 0 with resources cpus(*):1; mem(*):500 on slave 20141028-234822-3193029443-50043-31190-S0 (pietas.apache.org)
I1028 23:48:22.416084 31219 master.cpp:2480] Launching task 0 of framework 20141028-234822-3193029443-50043-31190-0000 (default) at scheduler-0aa33fc7-0d29-487c-80eb-f933681f9c95@67.195.81.190:50043 with resources cpus(*):1; mem(*):500 on slave 20141028-234822-3193029443-50043-31190-S0 at slave(34)@67.195.81.190:50043 (pietas.apache.org)
I1028 23:48:22.416317 31214 slave.cpp:1081] Got assigned task 0 for framework 20141028-234822-3193029443-50043-31190-0000
I1028 23:48:22.416679 31215 hierarchical_allocator_process.hpp:563] Recovered cpus(*):1; mem(*):524; disk(*):3.70122e+06; ports(*):[31000-32000] (total allocatable: cpus(*):1; mem(*):524; disk(*):3.70122e+06; ports(*):[31000-32000]) on slave 20141028-234822-3193029443-50043-31190-S0 from framework 20141028-234822-3193029443-50043-31190-0000
I1028 23:48:22.416721 31215 hierarchical_allocator_process.hpp:599] Framework 20141028-234822-3193029443-50043-31190-0000 filtered slave 20141028-234822-3193029443-50043-31190-S0 for 5secs
I1028 23:48:22.416724 31214 slave.cpp:1191] Launching task 0 for framework 20141028-234822-3193029443-50043-31190-0000
I1028 23:48:22.418534 31214 slave.cpp:3871] Launching executor default of framework 20141028-234822-3193029443-50043-31190-0000 in work directory '/tmp/AllocatorTest_0_SlaveReregistersFirst_QPPV21/slaves/20141028-234822-3193029443-50043-31190-S0/frameworks/20141028-234822-3193029443-50043-31190-0000/executors/default/runs/d593f433-3c16-4678-8f76-4038fe2841c4'
I1028 23:48:22.420557 31214 exec.cpp:132] Version: 0.21.0
I1028 23:48:22.420755 31213 exec.cpp:182] Executor started at: executor(22)@67.195.81.190:50043 with pid 31190
I1028 23:48:22.420903 31214 slave.cpp:1317] Queuing task '0' for executor default of framework '20141028-234822-3193029443-50043-31190-0000
I1028 23:48:22.420997 31214 slave.cpp:555] Successfully attached file '/tmp/AllocatorTest_0_SlaveReregistersFirst_QPPV21/slaves/20141028-234822-3193029443-50043-31190-S0/frameworks/20141028-234822-3193029443-50043-31190-0000/executors/default/runs/d593f433-3c16-4678-8f76-4038fe2841c4'
I1028 23:48:22.421058 31214 slave.cpp:1849] Got registration for executor 'default' of framework 20141028-234822-3193029443-50043-31190-0000 from executor(22)@67.195.81.190:50043
I1028 23:48:22.421295 31214 slave.cpp:1968] Flushing queued task 0 for executor 'default' of framework 20141028-234822-3193029443-50043-31190-0000
I1028 23:48:22.421391 31205 exec.cpp:206] Executor registered on slave 20141028-234822-3193029443-50043-31190-S0
I1028 23:48:22.421495 31214 slave.cpp:2802] Monitoring executor 'default' of framework '20141028-234822-3193029443-50043-31190-0000' in container 'd593f433-3c16-4678-8f76-4038fe2841c4'
I1028 23:48:22.422873 31205 exec.cpp:218] Executor::registered took 19148ns
I1028 23:48:22.422991 31205 exec.cpp:293] Executor asked to run task '0'
I1028 23:48:22.423085 31205 exec.cpp:302] Executor::launchTask took 76519ns
I1028 23:48:22.424541 31205 exec.cpp:525] Executor sending status update TASK_RUNNING (UUID: 10174aa0-0e5a-4f9d-a530-dee64e93f222) for task 0 of framework 20141028-234822-3193029443-50043-31190-0000
I1028 23:48:22.424724 31205 slave.cpp:2202] Handling status update TASK_RUNNING (UUID: 10174aa0-0e5a-4f9d-a530-dee64e93f222) for task 0 of framework 20141028-234822-3193029443-50043-31190-0000 from executor(22)@67.195.81.190:50043
I1028 23:48:22.424932 31213 status_update_manager.cpp:317] Received status update TASK_RUNNING (UUID: 10174aa0-0e5a-4f9d-a530-dee64e93f222) for task 0 of framework 20141028-234822-3193029443-50043-31190-0000
I1028 23:48:22.424963 31213 status_update_manager.cpp:494] Creating StatusUpdate stream for task 0 of framework 20141028-234822-3193029443-50043-31190-0000
I1028 23:48:22.425122 31213 status_update_manager.cpp:371] Forwarding update TASK_RUNNING (UUID: 10174aa0-0e5a-4f9d-a530-dee64e93f222) for task 0 of framework 20141028-234822-3193029443-50043-31190-0000 to the slave
I1028 23:48:22.425257 31205 slave.cpp:2442] Forwarding the update TASK_RUNNING (UUID: 10174aa0-0e5a-4f9d-a530-dee64e93f222) for task 0 of framework 20141028-234822-3193029443-50043-31190-0000 to master@67.195.81.190:50043
I1028 23:48:22.425398 31205 slave.cpp:2369] Status update manager successfully handled status update TASK_RUNNING (UUID: 10174aa0-0e5a-4f9d-a530-dee64e93f222) for task 0 of framework 20141028-234822-3193029443-50043-31190-0000
I1028 23:48:22.425420 31205 slave.cpp:2375] Sending acknowledgement for status update TASK_RUNNING (UUID: 10174aa0-0e5a-4f9d-a530-dee64e93f222) for task 0 of framework 20141028-234822-3193029443-50043-31190-0000 to executor(22)@67.195.81.190:50043
I1028 23:48:22.425583 31212 master.cpp:3410] Forwarding status update TASK_RUNNING (UUID: 10174aa0-0e5a-4f9d-a530-dee64e93f222) for task 0 of framework 20141028-234822-3193029443-50043-31190-0000
I1028 23:48:22.425621 31206 exec.cpp:339] Executor received status update acknowledgement 10174aa0-0e5a-4f9d-a530-dee64e93f222 for task 0 of framework 20141028-234822-3193029443-50043-31190-0000
I1028 23:48:22.425786 31212 master.cpp:3382] Status update TASK_RUNNING (UUID: 10174aa0-0e5a-4f9d-a530-dee64e93f222) for task 0 of framework 20141028-234822-3193029443-50043-31190-0000 from slave 20141028-234822-3193029443-50043-31190-S0 at slave(34)@67.195.81.190:50043 (pietas.apache.org)
I1028 23:48:22.425832 31212 master.cpp:4617] Updating the latest state of task 0 of framework 20141028-234822-3193029443-50043-31190-0000 to TASK_RUNNING
I1028 23:48:22.425885 31208 sched.cpp:635] Scheduler::statusUpdate took 49727ns
I1028 23:48:22.426082 31208 master.cpp:2882] Forwarding status update acknowledgement 10174aa0-0e5a-4f9d-a530-dee64e93f222 for task 0 of framework 20141028-234822-3193029443-50043-31190-0000 (default) at scheduler-0aa33fc7-0d29-487c-80eb-f933681f9c95@67.195.81.190:50043 to slave 20141028-234822-3193029443-50043-31190-S0 at slave(34)@67.195.81.190:50043 (pietas.apache.org)
I1028 23:48:22.426360 31206 status_update_manager.cpp:389] Received status update acknowledgement (UUID: 10174aa0-0e5a-4f9d-a530-dee64e93f222) for task 0 of framework 20141028-234822-3193029443-50043-31190-0000
I1028 23:48:22.426623 31206 slave.cpp:1789] Status update manager successfully handled status update acknowledgement (UUID: 10174aa0-0e5a-4f9d-a530-dee64e93f222) for task 0 of framework 20141028-234822-3193029443-50043-31190-0000
I1028 23:48:22.426893 31210 master.cpp:677] Master terminating
W1028 23:48:22.427028 31210 master.cpp:4662] Removing task 0 with resources cpus(*):1; mem(*):500 of framework 20141028-234822-3193029443-50043-31190-0000 on slave 20141028-234822-3193029443-50043-31190-S0 at slave(34)@67.195.81.190:50043 (pietas.apache.org) in non-terminal state TASK_RUNNING
I1028 23:48:22.427397 31209 hierarchical_allocator_process.hpp:563] Recovered cpus(*):1; mem(*):500 (total allocatable: cpus(*):2; mem(*):1024; disk(*):3.70122e+06; ports(*):[31000-32000]) on slave 20141028-234822-3193029443-50043-31190-S0 from framework 20141028-234822-3193029443-50043-31190-0000
I1028 23:48:22.427512 31210 master.cpp:4705] Removing executor 'default' with resources  of framework 20141028-234822-3193029443-50043-31190-0000 on slave 20141028-234822-3193029443-50043-31190-S0 at slave(34)@67.195.81.190:50043 (pietas.apache.org)
I1028 23:48:22.428129 31206 slave.cpp:2607] master@67.195.81.190:50043 exited
W1028 23:48:22.428153 31206 slave.cpp:2610] Master disconnected! Waiting for a new master to be elected
I1028 23:48:22.434645 31190 leveldb.cpp:176] Opened db in 2.551453ms
I1028 23:48:22.437157 31190 leveldb.cpp:183] Compacted db in 2.484612ms
I1028 23:48:22.437203 31190 leveldb.cpp:198] Created db iterator in 19171ns
I1028 23:48:22.437235 31190 leveldb.cpp:204] Seeked to beginning of db in 18300ns
I1028 23:48:22.437306 31190 leveldb.cpp:273] Iterated through 3 keys in the db in 59465ns
I1028 23:48:22.437347 31190 replica.cpp:741] Replica recovered with log positions 3 -> 4 with 0 holes and 0 unlearned
I1028 23:48:22.437827 31216 recover.cpp:437] Starting replica recovery
I1028 23:48:22.438127 31216 recover.cpp:463] Replica is in VOTING status
I1028 23:48:22.438443 31216 recover.cpp:452] Recover process terminated
I1028 23:48:22.439877 31212 master.cpp:312] Master 20141028-234822-3193029443-50043-31190 (pietas.apache.org) started on 67.195.81.190:50043
I1028 23:48:22.439916 31212 master.cpp:358] Master only allowing authenticated frameworks to register
I1028 23:48:22.439931 31212 master.cpp:363] Master only allowing authenticated slaves to register
I1028 23:48:22.439946 31212 credentials.hpp:36] Loading credentials for authentication from '/tmp/AllocatorTest_0_SlaveReregistersFirst_YPe61d/credentials'
I1028 23:48:22.440142 31212 master.cpp:392] Authorization enabled
I1028 23:48:22.440439 31218 master.cpp:120] No whitelist given. Advertising offers for all slaves
I1028 23:48:22.440901 31213 hierarchical_allocator_process.hpp:299] Initializing hierarchical allocator process with master : master@67.195.81.190:50043
I1028 23:48:22.441395 31206 master.cpp:1242] The newly elected leader is master@67.195.81.190:50043 with id 20141028-234822-3193029443-50043-31190
I1028 23:48:22.441421 31206 master.cpp:1255] Elected as the leading master!
I1028 23:48:22.441457 31206 master.cpp:1073] Recovering from registrar
I1028 23:48:22.441623 31205 registrar.cpp:313] Recovering registrar
I1028 23:48:22.442172 31219 log.cpp:656] Attempting to start the writer
I1028 23:48:22.443235 31219 replica.cpp:474] Replica received implicit promise request with proposal 2
I1028 23:48:22.443685 31219 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 427888ns
I1028 23:48:22.443703 31219 replica.cpp:342] Persisted promised to 2
I1028 23:48:22.444371 31213 coordinator.cpp:230] Coordinator attemping to fill missing position
I1028 23:48:22.444687 31209 log.cpp:672] Writer started with ending position 4
I1028 23:48:22.445754 31215 leveldb.cpp:438] Reading position from leveldb took 47909ns
I1028 23:48:22.445826 31215 leveldb.cpp:438] Reading position from leveldb took 30611ns
I1028 23:48:22.446941 31218 registrar.cpp:346] Successfully fetched the registry (277B) in 5.213184ms
I1028 23:48:22.447118 31218 registrar.cpp:445] Applied 1 operations in 42362ns; attempting to update the 'registry'
I1028 23:48:22.449329 31204 log.cpp:680] Attempting to append 316 bytes to the log
I1028 23:48:22.449477 31218 coordinator.cpp:340] Coordinator attempting to write APPEND action at position 5
I1028 23:48:22.450187 31215 replica.cpp:508] Replica received write request for position 5
I1028 23:48:22.450767 31215 leveldb.cpp:343] Persisting action (335 bytes) to leveldb took 554400ns
I1028 23:48:22.450788 31215 replica.cpp:676] Persisted action at 5
I1028 23:48:22.451561 31215 replica.cpp:655] Replica received learned notice for position 5
I1028 23:48:22.451979 31215 leveldb.cpp:343] Persisting action (337 bytes) to leveldb took 397219ns
I1028 23:48:22.452000 31215 replica.cpp:676] Persisted action at 5
I1028 23:48:22.452020 31215 replica.cpp:661] Replica learned APPEND action at position 5
I1028 23:48:22.452993 31213 registrar.cpp:490] Successfully updated the 'registry' in 5.816832ms
I1028 23:48:22.453136 31213 registrar.cpp:376] Successfully recovered registrar
I1028 23:48:22.453238 31208 log.cpp:699] Attempting to truncate the log to 5
I1028 23:48:22.453384 31214 coordinator.cpp:340] Coordinator attempting to write TRUNCATE action at position 6
I1028 23:48:22.453518 31215 master.cpp:1100] Recovered 1 slaves from the Registry (277B) ; allowing 10mins for slaves to re-register
I1028 23:48:22.454116 31207 replica.cpp:508] Replica received write request for position 6
I1028 23:48:22.454570 31207 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 427424ns
I1028 23:48:22.454589 31207 replica.cpp:676] Persisted action at 6
I1028 23:48:22.455095 31219 replica.cpp:655] Replica received learned notice for position 6
I1028 23:48:22.455399 31219 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 282466ns
I1028 23:48:22.455462 31219 leveldb.cpp:401] Deleting ~2 keys from leveldb took 43939ns
I1028 23:48:22.455478 31219 replica.cpp:676] Persisted action at 6
I1028 23:48:22.455494 31219 replica.cpp:661] Replica learned TRUNCATE action at position 6
I1028 23:48:22.465553 31213 status_update_manager.cpp:171] Pausing sending status updates
I1028 23:48:22.465566 31216 slave.cpp:602] New master detected at master@67.195.81.190:50043
I1028 23:48:22.465612 31216 slave.cpp:665] Authenticating with master master@67.195.81.190:50043
I1028 23:48:23.441506 31206 hierarchical_allocator_process.hpp:697] No resources available to allocate!
I1028 23:48:27.441004 31214 master.cpp:120] No whitelist given. Advertising offers for all slaves
I1028 23:48:30.101379 31206 hierarchical_allocator_process.hpp:659] Performed allocation for 0 slaves in 6.659877806secs
I1028 23:48:30.101568 31216 slave.cpp:638] Detecting new master
I1028 23:48:30.101632 31214 authenticatee.hpp:133] Creating new client SASL connection
I1028 23:48:30.102021 31218 master.cpp:3853] Authenticating slave(34)@67.195.81.190:50043
I1028 23:48:30.102329 31212 authenticator.hpp:161] Creating new server SASL connection
I1028 23:48:30.102505 31216 authenticatee.hpp:224] Received SASL authentication mechanisms: CRAM-MD5
I1028 23:48:30.102545 31216 authenticatee.hpp:250] Attempting to authenticate with mechanism 'CRAM-MD5'
I1028 23:48:30.102638 31216 authenticator.hpp:267] Received SASL authentication start
I1028 23:48:30.102709 31216 authenticator.hpp:389] Authentication requires more steps
I1028 23:48:30.102812 31216 authenticatee.hpp:270] Received SASL authentication step
I1028 23:48:30.102957 31204 authenticator.hpp:295] Received SASL authentication step
I1028 23:48:30.102982 31204 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'pietas.apache.org' server FQDN: 'pietas.apache.org' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I1028 23:48:30.102993 31204 auxprop.cpp:153] Looking up auxiliary property '*userPassword'
I1028 23:48:30.103032 31204 auxprop.cpp:153] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I1028 23:48:30.103049 31204 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'pietas.apache.org' server FQDN: 'pietas.apache.org' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I1028 23:48:30.103056 31204 auxprop.cpp:103] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I1028 23:48:30.103061 31204 auxprop.cpp:103] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I1028 23:48:30.103073 31204 authenticator.hpp:381] Authentication success
I1028 23:48:30.103149 31209 authenticatee.hpp:310] Authentication success
I1028 23:48:30.103153 31204 master.cpp:3893] Successfully authenticated principal 'test-principal' at slave(34)@67.195.81.190:50043
I1028 23:48:30.103371 31209 slave.cpp:722] Successfully authenticated with master master@67.195.81.190:50043
I1028 23:48:30.103773 31209 slave.cpp:1050] Will retry registration in 12.861518ms if necessary
I1028 23:48:30.104068 31219 master.cpp:3210] Re-registering slave 20141028-234822-3193029443-50043-31190-S0 at slave(34)@67.195.81.190:50043 (pietas.apache.org)
I1028 23:48:30.104760 31216 registrar.cpp:445] Applied 1 operations in 71655ns; attempting to update the 'registry'
I1028 23:48:30.107877 31205 log.cpp:680] Attempting to append 316 bytes to the log
I1028 23:48:30.108070 31219 coordinator.cpp:340] Coordinator attempting to write APPEND action at position 7
I1028 23:48:30.109110 31211 replica.cpp:508] Replica received write request for position 7
I1028 23:48:30.109434 31211 leveldb.cpp:343] Persisting action (335 bytes) to leveldb took 281545ns
I1028 23:48:30.109484 31211 replica.cpp:676] Persisted action at 7
I1028 23:48:30.110124 31219 replica.cpp:655] Replica received learned notice for position 7
I1028 23:48:30.110903 31219 leveldb.cpp:343] Persisting action (337 bytes) to leveldb took 750414ns
I1028 23:48:30.110927 31219 replica.cpp:676] Persisted action at 7
I1028 23:48:30.110950 31219 replica.cpp:661] Replica learned APPEND action at position 7
I1028 23:48:30.112160 31205 registrar.cpp:490] Successfully updated the 'registry' in 7.33824ms
I1028 23:48:30.112529 31217 log.cpp:699] Attempting to truncate the log to 7
I1028 23:48:30.112714 31207 coordinator.cpp:340] Coordinator attempting to write TRUNCATE action at position 8
I1028 23:48:30.112870 31210 master.hpp:877] Adding task 0 with resources cpus(*):1; mem(*):500 on slave 20141028-234822-3193029443-50043-31190-S0 (pietas.apache.org)
W1028 23:48:30.113136 31210 master.cpp:4394] Possibly orphaned task 0 of framework 20141028-234822-3193029443-50043-31190-0000 running on slave 20141028-234822-3193029443-50043-31190-S0 at slave(34)@67.195.81.190:50043 (pietas.apache.org)
I1028 23:48:30.113198 31219 slave.cpp:2522] Received ping from slave-observer(39)@67.195.81.190:50043
I1028 23:48:30.113340 31210 master.cpp:3278] Re-registered slave 20141028-234822-3193029443-50043-31190-S0 at slave(34)@67.195.81.190:50043 (pietas.apache.org) with cpus(*):2; mem(*):1024; disk(*):3.70122e+06; ports(*):[31000-32000]
I1028 23:48:30.113499 31219 slave.cpp:824] Re-registered with master master@67.195.81.190:50043
I1028 23:48:30.113636 31219 replica.cpp:508] Replica received write request for position 8
I1028 23:48:30.113652 31210 status_update_manager.cpp:178] Resuming sending status updates
I1028 23:48:30.113759 31212 hierarchical_allocator_process.hpp:442] Added slave 20141028-234822-3193029443-50043-31190-S0 (pietas.apache.org) with cpus(*):2; mem(*):1024; disk(*):3.70122e+06; ports(*):[31000-32000] (and cpus(*):1; mem(*):524; disk(*):3.70122e+06; ports(*):[31000-32000] available)
I1028 23:48:30.113904 31212 hierarchical_allocator_process.hpp:679] Performed allocation for slave 20141028-234822-3193029443-50043-31190-S0 in 74698ns
I1028 23:48:30.114116 31219 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 452165ns
I1028 23:48:30.114142 31219 replica.cpp:676] Persisted action at 8
I1028 23:48:30.114786 31213 replica.cpp:655] Replica received learned notice for position 8
I1028 23:48:30.115337 31213 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 525187ns
I1028 23:48:30.115399 31213 leveldb.cpp:401] Deleting ~2 keys from leveldb took 37689ns
I1028 23:48:30.115418 31213 replica.cpp:676] Persisted action at 8
I1028 23:48:30.115484 31213 replica.cpp:661] Replica learned TRUNCATE action at position 8
I1028 23:48:30.116603 31212 sched.cpp:227] Scheduler::disconnected took 16969ns
I1028 23:48:30.116624 31212 sched.cpp:233] New master detected at master@67.195.81.190:50043
I1028 23:48:30.116657 31212 sched.cpp:283] Authenticating with master master@67.195.81.190:50043
I1028 23:48:30.116870 31205 authenticatee.hpp:133] Creating new client SASL connection
I1028 23:48:30.117084 31207 master.cpp:3853] Authenticating scheduler-0aa33fc7-0d29-487c-80eb-f933681f9c95@67.195.81.190:50043
I1028 23:48:30.117279 31212 authenticator.hpp:161] Creating new server SASL connection
I1028 23:48:30.117410 31210 authenticatee.hpp:224] Received SASL authentication mechanisms: CRAM-MD5
I1028 23:48:30.117507 31210 authenticatee.hpp:250] Attempting to authenticate with mechanism 'CRAM-MD5'
I1028 23:48:30.117604 31214 authenticator.hpp:267] Received SASL authentication start
I1028 23:48:30.117652 31214 authenticator.hpp:389] Authentication requires more steps
I1028 23:48:30.117738 31210 authenticatee.hpp:270] Received SASL authentication step
I1028 23:48:30.117905 31208 authenticator.hpp:295] Received SASL authentication step
I1028 23:48:30.117935 31208 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'pietas.apache.org' server FQDN: 'pietas.apache.org' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I1028 23:48:30.117947 31208 auxprop.cpp:153] Looking up auxiliary property '*userPassword'
I1028 23:48:30.117979 31208 auxprop.cpp:153] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I1028 23:48:30.118001 31208 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'pietas.apache.org' server FQDN: 'pietas.apache.org' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I../../src/tests/allocator_tests.cpp:2405: Failure
1028 23:48:30.118013 31208 auxprop.cpp:103] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
Failed to wait 10secs for resourceOffers2
I1028 23:48:31.101976 31212 hierarchical_allocator_process.hpp:659] Performed allocation for 1 slaves in 124354ns
I1028 23:48:58.775811 31208 auxprop.cpp:103] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
W1028 23:48:35.117725 31214 sched.cpp:378] Authentication timed out
W1028 23:48:35.117784 31219 master.cpp:3911] Authentication timed out
I1028 23:48:45.114322 31213 slave.cpp:2522] Received ping from slave-observer(39)@67.195.81.190:50043
I1028 23:48:35.102212 31206 master.cpp:120] No whitelist given. Advertising offers for all slaves
I1028 23:48:58.775874 31208 authenticator.hpp:381] Authentication success
I1028 23:48:58.776267 31214 sched.cpp:338] Failed to authenticate with master master@67.195.81.190:50043: Authentication discarded
../../src/tests/allocator_tests.cpp:2396: Failure
Actual function call count doesn't match EXPECT_CALL(allocator2, frameworkAdded(_, _, _))...
         Expected: to be called once
           Actual: never called - unsatisfied and active
I1028 23:48:58.776526 31204 master.cpp:3893] Successfully authenticated principal 'test-principal' at scheduler-0aa33fc7-0d29-487c-80eb-f933681f9c95@67.195.81.190:50043
I1028 23:48:58.776626 31214 sched.cpp:283] Authenticating with master master@67.195.81.190:50043
I1028 23:48:58.776928 31204 authenticatee.hpp:133] Creating new client SASL connection
I1028 23:48:58.777194 31210 master.cpp:3853] Authenticating scheduler-0aa33fc7-0d29-487c-80eb-f933681f9c95@67.195.81.190:50043
W1028 23:48:58.777528 31210 master.cpp:3888] Failed to authenticate scheduler-0aa33fc7-0d29-487c-80eb-f933681f9c95@67.195.81.190:50043: Failed to communicate with authenticatee
../../src/tests/allocator_tests.cpp:2399: Failure
Actual function call count doesn't match EXPECT_CALL(sched, resourceOffers(&driver, _))...
         Expected: to be called once
           Actual: never called - unsatisfied and active
../../src/tests/allocator_tests.cpp:2394: Failure
Actual function call count doesn't match EXPECT_CALL(sched, registered(&driver, _, _))...
         Expected: to be called once
           Actual: never called - unsatisfied and active
I1028 23:48:58.778053 31205 slave.cpp:591] Re-detecting master
I1028 23:48:58.778084 31205 slave.cpp:638] Detecting new master
I1028 23:48:58.778115 31207 status_update_manager.cpp:171] Pausing sending status updates
F1028 23:48:58.778115 31205 logging.cpp:57] RAW: Pure virtual method called
I1028 23:48:58.778724 31210 master.cpp:677] Master terminating
W1028 23:48:58.778919 31210 master.cpp:4662] Removing task 0 with resources cpus(*):1; mem(*):500 of framework 20141028-234822-3193029443-50043-31190-0000 on slave 20141028-234822-3193029443-50043-31190-S0 at slave(34)@67.195.81.190:50043 (pietas.apache.org) in non-terminal state TASK_RUNNING
*** Aborted at 1414540138 (unix time) try ""date -d @1414540138"" if you are using GNU date ***
PC: @           0x91bc86 process::PID<>::PID()
*** SIGSEGV (@0x0) received by PID 31190 (TID 0x2b20a6d95700) from PID 0; stack trace: ***
    @     0x2b20a41ff340 (unknown)
    @     0x2b20a1f2a188  google::LogMessage::Fail()
    @     0x2b20a1f2f87c  google::RawLog__()
    @           0x91bc86 process::PID<>::PID()
    @           0x91bf24 process::Process<>::self()
    @     0x2b20a15d5c06  __cxa_pure_virtual
    @     0x2b20a1877752  mesos::internal::slave::Slave::detected()
    @     0x2b20a1671f24 process::dispatch<>()
    @     0x2b20a18b35f9  _ZZN7process8dispatchIN5mesos8internal5slave5SlaveERKNS_6FutureI6OptionINS1_10MasterInfoEEEES9_EEvRKNS_3PIDIT_EEMSD_FvT0_ET1_ENKUlPNS_11ProcessBaseEE_clESM_
    @     0x2b20a1663217 mesos::internal::master::allocator::Allocator::resourcesRecovered()
    @     0x2b20a1650d01 mesos::internal::master::Master::removeTask()
    @     0x2b20a162fb41 mesos::internal::master::Master::finalize()
    @     0x2b20a1eb69a1 process::ProcessBase::visit()
    @     0x2b20a1ec0464 process::TerminateEvent::visit()
    @           0x8e0812 process::ProcessBase::serve()
    @     0x2b20a18da89e  _ZNSt17_Function_handlerIFvPN7process11ProcessBaseEEZNS0_8dispatchIN5mesos8internal5slave5SlaveERKNS0_6FutureI6OptionINS5_10MasterInfoEEEESD_EEvRKNS0_3PIDIT_EEMSH_FvT0_ET1_EUlS2_E_E9_M_invokeERKSt9_Any_dataS2_
    @     0x2b20a1eb1ca0 process::ProcessManager::resume()
    @     0x2b20a1ea8365 process::schedule()
    @     0x2b20a41f7182 start_thread
    @     0x2b20a4507fbd (unknown)
make[3]: *** [check-local] Segmentation fault
{noformat}",Bug,Major,xujyan,2016-02-16T19:59:48.000+0000,5,Resolved,Complete,AllocatorTest/0.SlaveReregistersFirst is flaky,2016-02-16T19:59:48.000+0000,MESOS-2007,2.0,mesos,Mesosphere Sprint 28
chzhcn,2014-10-24T00:57:33.000+0000,jieyu,"Looks like the TX/RX network stats reported is the reverse of the actual network stats. The reason is because we simply get TX/RX data from veth on the host.

Since veth pair is a tunnel, the ingress of veth on host is the egress of eth0 in container (and vice versa). Therefore, we need to flip the data we got from veth.

{noformat}
[jyu@... ~]$ sudo ip netns exec 24926 /sbin/ip -s link show dev eth0
2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP mode DEFAULT qlen 1000
    link/ether f0:4d:a2:75:74:05 brd ff:ff:ff:ff:ff:ff
    RX: bytes  packets  errors  dropped overrun mcast   
    46030857691178 12561038581 0       0       0       0      
    TX: bytes  packets  errors  dropped carrier collsns 
    29792886058561 15036798198 0       0       0       0      
[jyu@... ~]$ ip -s link show dev mesos24926
7412: mesos24926: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP mode DEFAULT qlen 1000
    link/ether f0:4d:a2:75:74:05 brd ff:ff:ff:ff:ff:ff
    RX: bytes  packets  errors  dropped overrun mcast   
    29793066979551 15036894749 0       0       0       0      
    TX: bytes  packets  errors  dropped carrier collsns 
    46031126366116 12561113732 0       0       0       0
{noformat}",Bug,Major,jieyu,2014-10-26T04:54:21.000+0000,5,Resolved,Complete,Container network stats reported by the port mapping isolator is the reverse of the actual network stats.,2014-10-26T04:54:21.000+0000,MESOS-1989,1.0,mesos,Twitter Mesos Q4 Sprint 2
jieyu,2014-10-23T18:41:19.000+0000,jieyu,"As we introduce DiskInfo and reservation for Resource. We need to change the C++ Resources abstraction to properly deal with merge/split of resources with those additional fields.

Also, the existing C++ 'Resources' interfaces are poorly designed. Some of them are confusing and unintuitive. Some of them are overloaded with too many functionalities. For instance,

{noformat}
bool operator <= (const Resource& left, const Resource& right);
{noformat}

This interface in non-intuitive because A <= B doesn't imply !(B <= A).

{noformat}
Resource operator + (const Resource& left, const Resource& right);
{noformat}

This one is also non-intuitive because if 'left' is not compatible with 'right', the result is 'left' (why not right???). Similar for operator '-'.

{noformat}
Option<Resource> Resources::get(const Resource& r) const;
{noformat}

This one assume Resources is flattened, but it might not be.

As we start to introduce persistent disk resources (MESOS-1554), things will get more complicated. For example, one may want to get two types of 'disk()' functions: one returns the ephemeral disk bytes (with no disk info), one returns the total disk bytes (including ones that have disk info). We may wanna introduce a concept about Resource that indicates that a resource cannot be merged or split (e.g., atomic?).

Since we need to change this class anyway. I wanna take this chance to refactor it.",Improvement,Major,jieyu,2014-11-19T08:56:44.000+0000,5,Resolved,Complete,Refactor the C++ Resources abstraction for DiskInfo,2014-11-19T19:53:19.000+0000,MESOS-1974,8.0,mesos,Twitter Mesos Q4 Sprint 2
vinodkone,2014-10-23T17:18:24.000+0000,vinodkone,"As we move towards pure scheduler/executor clients, it is imperative that the scheduler driver doesn't do validation of tasks and generate TASK_LOST messages itself. All that logic should live in the master. Schedulers should reconcile dropped messages via reconciliation.
",Improvement,Major,vinodkone,2014-10-24T20:42:46.000+0000,5,Resolved,Complete,Move TASK_LOST generations due to invalid tasks from scheduler driver to master,2014-10-24T20:42:46.000+0000,MESOS-1972,3.0,mesos,Twitter Mesos Q4 Sprint 2
dhamon,2014-10-22T20:29:46.000+0000,dhamon,It is currently impossible to tell slave ids and offer ids apart when looking at logs. Adding some differentiator will make log reading a little simpler.,Bug,Minor,dhamon,2014-10-22T21:28:05.000+0000,5,Resolved,Complete,slave and offer ids are indistinguishable in the logs,2014-10-24T21:20:59.000+0000,MESOS-1970,1.0,mesos,Twitter Mesos Q4 Sprint 2
dhamon,2014-10-22T20:25:25.000+0000,dhamon,the {{support/post-reviews.py}} script doesn't differentiate between RBT versions although the calling conventions for passing revision ranges are different. ,Bug,Major,dhamon,2014-10-22T20:45:39.000+0000,5,Resolved,Complete,RBT only takes revision ranges as args for versions >= 0.6,2014-10-24T21:20:53.000+0000,MESOS-1969,1.0,mesos,Twitter Mesos Q4 Sprint 2
chzhcn,2014-10-22T18:10:00.000+0000,jieyu,"{noformat}
[ RUN      ] RoutingTest.INETSockets
../../../mesos/src/tests/routing_tests.cpp:238: Failure
infos: Input data out of range
ABORT: (../../../mesos/3rdparty/libprocess/3rdparty/stout/include/stout/try.hpp:92): Try::get() but state == ERROR: Input data out of range*** Aborted at 1414000937 (unix time) try ""date -d @1414000937"" if you are using GNU date ***
PC: @     0x7f2c2d509fc5 __GI_raise
*** SIGABRT (@0x1b49000040b1) received by PID 16561 (TID 0x7f2c31031720) from PID 16561; stack trace: ***
    @     0x7f2c2f0d4ca0 (unknown)
    @     0x7f2c2d509fc5 __GI_raise
    @     0x7f2c2d50ba70 __GI_abort
    @           0x4cf782 _Abort()
    @           0x4cf7bc _Abort()
    @           0x99459e RoutingTest_INETSockets_Test::TestBody()
    @           0xa1c363 testing::internal::HandleExceptionsInMethodIfSupported<>()
    @           0xa13617 testing::Test::Run()
    @           0xa136be testing::TestInfo::Run()
    @           0xa137c5 testing::TestCase::Run()
    @           0xa13a68 testing::internal::UnitTestImpl::RunAllTests()
    @           0xa13cf7 testing::UnitTest::Run()
    @           0x49bc4b main
    @     0x7f2c2d4f79f4 __libc_start_main
    @           0x4aad79 (unknown)
make[3]: *** [check-local] Aborted
{noformat}",Bug,Major,jieyu,2014-10-26T04:53:10.000+0000,5,Resolved,Complete,Test RoutingTest.INETSockets fails on some machine,2014-10-26T04:54:00.000+0000,MESOS-1967,2.0,mesos,Twitter Mesos Q4 Sprint 2
idownes,2014-10-22T04:00:54.000+0000,idownes,"Mesos release 0.21.0 will include the following major feature(s):

- Provide state reconciliation for frameworks. [(MESOS-1407)|https://issues.apache.org/jira/browse/MESOS-1407]

Possible features to include:
- Isolation of system directories (/tmp) for Mesos containers [(MESOS-1586)|https://issues.apache.org/jira/browse/MESOS-1586]
- Expose reason for TASK_KILLED [(1930)|https://issues.apache.org/jira/browse/MESOS-1930]

This ticket will be used to track blockers to this release.
",Task,Major,idownes,2014-11-18T19:53:27.000+0000,5,Resolved,Complete,0.21.0 release,2016-02-26T21:08:19.000+0000,MESOS-1964,5.0,mesos,Twitter Mesos Q4 Sprint 3
,2014-10-21T16:07:09.000+0000,bernd-mesos,"We should create a precise specification of what the Mesos source code is supposed to be implementing wrt. the life cycle of executors and tasks. And in addition, we should document why certain design decisions have been made one way or another, to provide guidance for future code changes.

With such a source code-independent specification, we could write unbiased regression and scale tests, which would be instrumental in maintaining high quality.

Furthermore, this would make the source code more amenable.

Why pick this particular area of the source code? Shouldn't more of Mesos have a thorough specification? Probably so. But we need to start somewhere and this area seems to be a good choice, given both its intricacy and its importance.
",Documentation,Major,bernd-mesos,,1,Open,New,Specification for Executor and Task life cycles in Slave,2014-11-11T09:37:48.000+0000,MESOS-1955,5.0,mesos,
dhamon,2014-10-17T17:30:12.000+0000,dhamon,"In the master process, we expose metrics for event queue sizes for various event types. We should do the same for the scheduler driver process.",Task,Minor,dhamon,2014-10-21T21:22:30.000+0000,5,Resolved,Complete,Add event queue size metrics to scheduler driver,2014-10-21T21:22:30.000+0000,MESOS-1943,2.0,mesos,Twitter Mesos Q4 Sprint 2
idownes,2014-10-16T22:52:34.000+0000,mohitsoni,"Currently, when cgroups are enabled, and executor is spawned, it's mounted under, for ex: /sys/fs/cgroup/cpu/mesos/<mesos-id>. This directory in current implementation is only writable by root user. This prevents process launched by executor to mount its child processes under this cgroup, because the cgroup directory is only writable by root.

To enable a executor spawned process to mount it's child processes under it's cgroup directory, the cgroup directory should be made writable by the user which spawns the executor.",Improvement,Minor,mohitsoni,2014-11-04T18:42:38.000+0000,5,Resolved,Complete,Make executor's user owner of executor's cgroup directory,2014-11-04T18:42:38.000+0000,MESOS-1941,3.0,mesos,Twitter Mesos Q4 Sprint 2
vinodkone,2014-10-10T18:43:47.000+0000,dhamon,To avoid so many duplicate framework re-registration attempts (and thus offer rescinds) we should add backoff to re-registration retries.,Task,Major,dhamon,2014-11-07T02:19:54.000+0000,5,Resolved,Complete,Add backoff to framework re-registration retries,2014-11-07T02:19:54.000+0000,MESOS-1903,3.0,mesos,Twitter Mesos Q4 Sprint 2
jieyu,2014-10-10T18:33:40.000+0000,jieyu,"The 'resources' field in Slave is uninitialized.

Also, seems that 'attributes' field in Slave is redundant as we store slave info. ",Bug,Major,jieyu,2014-10-10T21:00:15.000+0000,5,Resolved,Complete,Slave resources obtained from localhost:5051/state.json is not correct.,2014-10-10T21:00:24.000+0000,MESOS-1901,2.0,mesos,Twitter Q4 Sprint 1
idownes,2014-10-08T17:17:37.000+0000,idownes,"If groups == true and/or sessions == true then os::killtree() should continue to signal all processes in the process group and/or session, even if the leading pid has terminated.",Bug,Major,idownes,2014-10-31T20:07:33.000+0000,5,Resolved,Complete,os::killtree() incorrectly returns early if pid has terminated,2014-10-31T20:07:33.000+0000,MESOS-1875,2.0,mesos,Twitter Q4 Sprint 1
bmahler,2014-10-06T17:37:21.000+0000,vinodkone,"In reregisterSlave() we send 'SlaveReregisteredMessage' before we link the slave pid, which means a temporary socket will be created and used.

Subsequently, after linking, we send the UpdateFrameworkMessage, which creates and uses a persistent socket.

This might lead to out-of-order delivery, resulting in UpdateFrameworkMessage reaching the slave before the SlaveReregisteredMessage and getting dropped because the slave is not yet (re-)registered.",Bug,Major,vinodkone,2014-10-09T01:32:04.000+0000,5,Resolved,Complete,UpdateFramework message might reach the slave before Reregistered message and get dropped,2014-10-09T01:32:04.000+0000,MESOS-1869,1.0,mesos,Twitter Q4 Sprint 1
vinodkone,2014-10-03T23:58:26.000+0000,vinodkone,"The master might get a duplicate authenticate() request while a previous authentication attempt is in progress. Depending on what the AuthenticatorProcess is executing at the time, there are 2 possible race conditions which will cause scheduler/slave to continuously retry authentication but never succeed.

We have seen both the race conditions in a heavily loaded production cluster.

Race1:
----------
--> An authenticate() event was dispatched to AuthenticatorProcess (Master::authenticate() called Authenticator::authenticate())

--> A terminate() event was then injected into the front of the AuthenticatorProcess queue (duplicate Master::authenticate() did ~Authenticator) before the above authenticate() event was executed.

--> Due to the bug in libprocess, the future returned by Master::authenticate() was never transitioned to discarded (Master::_authenticate() was never called).

--> This caused all the subsequent authentication retries to be enqueued on the master waiting for Master::_authenticate() to be executed.

Fix: Transition the dispatched future to discarded if the libprocess is terminated (https://reviews.apache.org/r/25945/)

Race 2:
-----------
--> An authenticate() event was dispatched to AuthenticatorProcess (Master::authenticate() called Authenticator::authenticate())

--> AuthenticatorProcess::authenticate() executed and set promise.onDiscard(defer(self, Self::discarded)). NOTE: The internal promise of AuthenticatorProcess is discarded in AuthenticatorProcess::discarded()

--> A terminate() event was then injected into the front of the AuthenticatorProcess queue (duplicate Master::authenticate() did 
~Authenticator) before the above discarded() event was executed)

--> ~AuthenticatorProcess is destructed without ever discarding the internal promise (Master::_authenticate() was never called).

--> This caused all the subsequent authentication retries to be enqueued on the master waiting for Master::_authenticate() to be executed.

Fix: The fix here is to discard the internal promise when the AuthenticatorProcess is destructed.",Bug,Critical,vinodkone,2014-10-07T02:29:03.000+0000,5,Resolved,Complete,Race between ~Authenticator() and Authenticator::authenticate() can lead to schedulers/slaves to never get authenticated,2015-01-30T20:09:07.000+0000,MESOS-1866,2.0,mesos,Twitter Q4 Sprint 1
haosdent@gmail.com,2014-10-03T23:09:23.000+0000,stevenschlansker,"Some of the API endpoints, for example /master/tasks.json, will return bogus information if you query a non-leading master:

{code}
[steven@Anesthetize:~]% curl http://master1.mesos-vpcqa.otenv.com:5050/master/tasks.json | jq . | head -n 10
{
  ""tasks"": []
}
[steven@Anesthetize:~]% curl http://master2.mesos-vpcqa.otenv.com:5050/master/tasks.json | jq . | head -n 10
{
  ""tasks"": []
}
[steven@Anesthetize:~]% curl http://master3.mesos-vpcqa.otenv.com:5050/master/tasks.json | jq . | head -n 10
{
  ""tasks"": [
    {
      ""executor_id"": """",
      ""framework_id"": ""20140724-231003-419644938-5050-1707-0000"",
      ""id"": ""pp.guestcenterwebhealthmonitor.606cd6ee-4b50-11e4-825b-5212e05f35db"",
      ""name"": ""pp.guestcenterwebhealthmonitor.606cd6ee-4b50-11e4-825b-5212e05f35db"",
      ""resources"": {
        ""cpus"": 0.25,
        ""disk"": 0,
{code}

This is very hard for end-users to work around.  For example if I query ""which master is leading"" followed by ""leader: which tasks are running"" it is possible that the leader fails over in between, leaving me with an incorrect answer and no way to know that this happened.

In my opinion the API should return the correct response (by asking the current leader?) or an error (500 Not the leader?) but it's unacceptable to return a successful wrong answer.
",Bug,Major,stevenschlansker,2016-04-22T21:13:07.000+0000,5,Resolved,Complete,Redirect to the leader master when current master is not a leader,2016-05-01T04:27:09.000+0000,MESOS-1865,3.0,mesos,Mesosphere Sprint 33
vinodkone,2014-10-03T21:02:24.000+0000,vinodkone,"Both launchTasks() and declineOffers() scheduler driver calls end up in ""messages_launch_tasks"" metric on the master. It would be nice to split them for differentiating these two calls.",Improvement,Major,vinodkone,2014-10-06T19:03:44.000+0000,5,Resolved,Complete,Split launch tasks and decline offers metrics,2014-10-06T19:03:44.000+0000,MESOS-1863,1.0,mesos,Mesos Q3 Sprint 6
bmahler,2014-10-03T19:25:58.000+0000,bmahler,"As part of the change to hold on to terminal unacknowledged tasks in the master, we introduced a performance regression during the following patch:

https://github.com/apache/mesos/commit/0760b007ad65bc91e8cea377339978c78d36d247
{noformat}
commit 0760b007ad65bc91e8cea377339978c78d36d247
Author: Benjamin Mahler <bmahler@twitter.com>
Date:   Thu Sep 11 10:48:20 2014 -0700

    Minor cleanups to the Master code.

    Review: https://reviews.apache.org/r/25566
{noformat}

Rather than keeping a running count of allocated resources, we now compute resources on-demand. This was done in order to ignore terminal task's resources.

As a result of this change, the /stats.json and /metrics/snapshot endpoints on the master have slowed down substantially on large clusters.

{noformat}
$ time curl localhost:5050/health
real	0m0.004s
user	0m0.001s
sys	0m0.002s

$ time curl localhost:5050/stats.json > /dev/null
real	0m15.402s
user	0m0.001s
sys	0m0.003s

$ time curl localhost:5050/metrics/snapshot > /dev/null
real	0m6.059s
user	0m0.002s
sys	0m0.002s
{noformat}

{{perf top}} reveals some of the resource computation during a request to stats.json:
{noformat: perf top}
Events: 36K cycles
 10.53%  libc-2.5.so             [.] _int_free
  9.90%  libc-2.5.so             [.] malloc
  8.56%  libmesos-0.21.0.so  [.] std::_Rb_tree<process::ProcessBase*, process::ProcessBase*, std::_Identity<process::ProcessBase*>, std::less<process::ProcessBase*>, std::allocator<process::ProcessBase*> >::
  8.23%  libc-2.5.so             [.] _int_malloc
  5.80%  libstdc++.so.6.0.8      [.] std::_Rb_tree_increment(std::_Rb_tree_node_base*)
  5.33%  [kernel]                [k] _raw_spin_lock
  3.13%  libstdc++.so.6.0.8      [.] std::string::assign(std::string const&)
  2.95%  libmesos-0.21.0.so  [.] process::SocketManager::exited(process::ProcessBase*)
  2.43%  libmesos-0.21.0.so  [.] mesos::Resource::MergeFrom(mesos::Resource const&)
  1.88%  libmesos-0.21.0.so  [.] mesos::internal::master::Slave::used() const
  1.48%  libstdc++.so.6.0.8      [.] __gnu_cxx::__atomic_add(int volatile*, int)
  1.45%  [kernel]                [k] find_busiest_group
  1.41%  libc-2.5.so             [.] free
  1.38%  libmesos-0.21.0.so  [.] mesos::Value_Range::MergeFrom(mesos::Value_Range const&)
  1.13%  libmesos-0.21.0.so  [.] mesos::Value_Scalar::MergeFrom(mesos::Value_Scalar const&)
  1.12%  libmesos-0.21.0.so  [.] mesos::Resource::SharedDtor()
  1.07%  libstdc++.so.6.0.8      [.] __gnu_cxx::__exchange_and_add(int volatile*, int)
  0.94%  libmesos-0.21.0.so  [.] google::protobuf::UnknownFieldSet::MergeFrom(google::protobuf::UnknownFieldSet const&)
  0.92%  libstdc++.so.6.0.8      [.] operator new(unsigned long)
  0.88%  libmesos-0.21.0.so  [.] mesos::Value_Ranges::MergeFrom(mesos::Value_Ranges const&)
  0.75%  libmesos-0.21.0.so  [.] mesos::matches(mesos::Resource const&, mesos::Resource const&)
{noformat}",Bug,Blocker,bmahler,2014-10-07T01:39:00.000+0000,5,Resolved,Complete,Performance regression in the Master's http metrics.,2014-10-07T01:39:00.000+0000,MESOS-1862,3.0,mesos,Twitter Q4 Sprint 1
jieyu,2014-10-02T18:18:43.000+0000,jieyu,"https://github.com/apache/mesos/blob/master/src/slave/status_update_manager.hpp#L180

We should set cloexec for 'fd'.",Bug,Major,jieyu,2014-10-17T17:32:15.000+0000,5,Resolved,Complete,Leaked file descriptors in StatusUpdateStream.,2014-10-17T17:35:12.000+0000,MESOS-1858,1.0,mesos,Twitter Q4 Sprint 1
karya,2014-10-01T22:10:00.000+0000,jieyu,"LIBNL_CFLAGS uses a hard-coded path in the configure script, instead of detecting the location.",Task,Blocker,jieyu,2015-06-23T23:48:09.000+0000,5,Resolved,Complete,Support specifying libnl3 install location.,2015-06-23T23:48:09.000+0000,MESOS-1856,2.0,mesos,Mesosphere Sprint 13
jieyu,2014-10-01T20:24:54.000+0000,diptanuc@gmail.com,"The compilation of Mesos 0.20.1 fails on Ubuntu Trusty with the following error -

slave/containerizer/mesos/containerizer.cpp  -fPIC -DPIC -o slave/containerizer/mesos/.libs/libmesos_no_3rdparty_la-containerizer.o
In file included from ./linux/routing/filter/ip.hpp:36:0,
                 from ./slave/containerizer/isolators/network/port_mapping.hpp:42,
                 from slave/containerizer/mesos/containerizer.cpp:44:
./linux/routing/filter/filter.hpp:29:43: fatal error: linux/routing/filter/handle.hpp: No such file or directory
 #include ""linux/routing/filter/handle.hpp""
                                           ^",Bug,Major,diptanuc@gmail.com,2014-10-06T22:03:57.000+0000,5,Resolved,Complete,Mesos 0.20.1 doesn't compile,2014-10-13T16:49:46.000+0000,MESOS-1855,1.0,mesos,Twitter Q4 Sprint 1
idownes,2014-10-01T17:36:46.000+0000,idownes,"/proc/net reflects a new network namespace regardless and remount doesn't actually do what we expected anyway, i.e., it's not sufficient for a new pid namespace and a new mount is required.",Bug,Major,idownes,2014-10-27T17:39:10.000+0000,5,Resolved,Complete,Remove /proc and /sys remounts from port_mapping isolator,2014-11-05T18:59:31.000+0000,MESOS-1853,3.0,mesos,Twitter Q4 Sprint 1
vinodkone,2014-09-30T01:31:25.000+0000,vinodkone,"{code}
[ RUN      ] AllocatorTest/0.SlaveLost
Using temporary directory '/tmp/AllocatorTest_0_SlaveLost_Z2oazw'
I0929 16:58:29.484141  3486 leveldb.cpp:176] Opened db in 604109ns
I0929 16:58:29.484629  3486 leveldb.cpp:183] Compacted db in 172697ns
I0929 16:58:29.484912  3486 leveldb.cpp:198] Created db iterator in 6429ns
I0929 16:58:29.485133  3486 leveldb.cpp:204] Seeked to beginning of db in 1618ns
I0929 16:58:29.485337  3486 leveldb.cpp:273] Iterated through 0 keys in the db in 752ns
I0929 16:58:29.485595  3486 replica.cpp:741] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned
I0929 16:58:29.486017  3500 recover.cpp:425] Starting replica recovery
I0929 16:58:29.486304  3500 recover.cpp:451] Replica is in EMPTY status
I0929 16:58:29.486793  3500 replica.cpp:638] Replica in EMPTY status received a broadcasted recover request
I0929 16:58:29.487205  3500 recover.cpp:188] Received a recover response from a replica in EMPTY status
I0929 16:58:29.487540  3500 recover.cpp:542] Updating replica status to STARTING
I0929 16:58:29.487911  3500 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 36629ns
I0929 16:58:29.488173  3500 replica.cpp:320] Persisted replica status to STARTING
I0929 16:58:29.488438  3500 recover.cpp:451] Replica is in STARTING status
I0929 16:58:29.488891  3500 replica.cpp:638] Replica in STARTING status received a broadcasted recover request
I0929 16:58:29.489187  3500 recover.cpp:188] Received a recover response from a replica in STARTING status
I0929 16:58:29.489516  3500 recover.cpp:542] Updating replica status to VOTING
I0929 16:58:29.489887  3502 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 32099ns
I0929 16:58:29.490124  3502 replica.cpp:320] Persisted replica status to VOTING
I0929 16:58:29.490381  3500 recover.cpp:556] Successfully joined the Paxos group
I0929 16:58:29.490713  3500 recover.cpp:440] Recover process terminated
I0929 16:58:29.493401  3506 master.cpp:312] Master 20140929-165829-2759502016-55618-3486 (fedora-20) started on 192.168.122.164:55618
I0929 16:58:29.493700  3506 master.cpp:358] Master only allowing authenticated frameworks to register
I0929 16:58:29.493921  3506 master.cpp:363] Master only allowing authenticated slaves to register
I0929 16:58:29.494123  3506 credentials.hpp:36] Loading credentials for authentication from '/tmp/AllocatorTest_0_SlaveLost_Z2oazw/credentials'
I0929 16:58:29.494500  3506 master.cpp:392] Authorization enabled
I0929 16:58:29.495249  3506 master.cpp:120] No whitelist given. Advertising offers for all slaves
I0929 16:58:29.495728  3502 hierarchical_allocator_process.hpp:299] Initializing hierarchical allocator process with master : master@192.168.122.164:55618
I0929 16:58:29.496196  3506 master.cpp:1241] The newly elected leader is master@192.168.122.164:55618 with id 20140929-165829-2759502016-55618-3486
I0929 16:58:29.496469  3506 master.cpp:1254] Elected as the leading master!
I0929 16:58:29.496713  3506 master.cpp:1072] Recovering from registrar
I0929 16:58:29.497020  3506 registrar.cpp:312] Recovering registrar
I0929 16:58:29.497486  3506 log.cpp:656] Attempting to start the writer
I0929 16:58:29.498105  3506 replica.cpp:474] Replica received implicit promise request with proposal 1
I0929 16:58:29.498373  3506 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 27145ns
I0929 16:58:29.498605  3506 replica.cpp:342] Persisted promised to 1
I0929 16:58:29.500880  3500 coordinator.cpp:230] Coordinator attemping to fill missing position
I0929 16:58:29.501404  3500 replica.cpp:375] Replica received explicit promise request for position 0 with proposal 2
I0929 16:58:29.501687  3500 leveldb.cpp:343] Persisting action (8 bytes) to leveldb took 57971ns
I0929 16:58:29.501935  3500 replica.cpp:676] Persisted action at 0
I0929 16:58:29.504905  3507 replica.cpp:508] Replica received write request for position 0
I0929 16:58:29.505130  3507 leveldb.cpp:438] Reading position from leveldb took 18418ns
I0929 16:58:29.505377  3507 leveldb.cpp:343] Persisting action (14 bytes) to leveldb took 19998ns
I0929 16:58:29.505571  3507 replica.cpp:676] Persisted action at 0
I0929 16:58:29.505957  3507 replica.cpp:655] Replica received learned notice for position 0
I0929 16:58:29.506186  3507 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 21648ns
I0929 16:58:29.506433  3507 replica.cpp:676] Persisted action at 0
I0929 16:58:29.506767  3507 replica.cpp:661] Replica learned NOP action at position 0
I0929 16:58:29.507199  3507 log.cpp:672] Writer started with ending position 0
I0929 16:58:29.507730  3507 leveldb.cpp:438] Reading position from leveldb took 11532ns
I0929 16:58:29.508915  3507 registrar.cpp:345] Successfully fetched the registry (0B)
I0929 16:58:29.509230  3507 registrar.cpp:421] Attempting to update the 'registry'
I0929 16:58:29.510516  3500 log.cpp:680] Attempting to append 130 bytes to the log
I0929 16:58:29.510949  3500 coordinator.cpp:340] Coordinator attempting to write APPEND action at position 1
I0929 16:58:29.511363  3500 replica.cpp:508] Replica received write request for position 1
I0929 16:58:29.511697  3500 leveldb.cpp:343] Persisting action (149 bytes) to leveldb took 66530ns
I0929 16:58:29.512039  3500 replica.cpp:676] Persisted action at 1
I0929 16:58:29.512460  3500 replica.cpp:655] Replica received learned notice for position 1
I0929 16:58:29.512778  3500 leveldb.cpp:343] Persisting action (151 bytes) to leveldb took 24121ns
I0929 16:58:29.513013  3500 replica.cpp:676] Persisted action at 1
I0929 16:58:29.513239  3500 replica.cpp:661] Replica learned APPEND action at position 1
I0929 16:58:29.513674  3500 log.cpp:699] Attempting to truncate the log to 1
I0929 16:58:29.513954  3500 coordinator.cpp:340] Coordinator attempting to write TRUNCATE action at position 2
I0929 16:58:29.514385  3500 replica.cpp:508] Replica received write request for position 2
I0929 16:58:29.514680  3500 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 65014ns
I0929 16:58:29.514991  3500 replica.cpp:676] Persisted action at 2
I0929 16:58:29.516978  3501 replica.cpp:655] Replica received learned notice for position 2
I0929 16:58:29.517319  3501 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 24103ns
I0929 16:58:29.517546  3501 leveldb.cpp:401] Deleting ~1 keys from leveldb took 16533ns
I0929 16:58:29.517801  3501 replica.cpp:676] Persisted action at 2
I0929 16:58:29.518039  3501 replica.cpp:661] Replica learned TRUNCATE action at position 2
I0929 16:58:29.518539  3507 registrar.cpp:478] Successfully updated 'registry'
I0929 16:58:29.518885  3507 registrar.cpp:371] Successfully recovered registrar
I0929 16:58:29.519201  3507 master.cpp:1099] Recovered 0 slaves from the Registry (94B) ; allowing 10mins for slaves to re-register
I0929 16:58:29.533073  3505 slave.cpp:169] Slave started on 57)@192.168.122.164:55618
I0929 16:58:29.533500  3505 credentials.hpp:84] Loading credential for authentication from '/tmp/AllocatorTest_0_SlaveLost_xdXHfg/credential'
I0929 16:58:29.533834  3505 slave.cpp:276] Slave using credential for: test-principal
I0929 16:58:29.534168  3505 slave.cpp:289] Slave resources: cpus(*):2; mem(*):1024; disk(*):752; ports(*):[31000-32000]
I0929 16:58:29.534751  3505 slave.cpp:317] Slave hostname: fedora-20
I0929 16:58:29.534965  3505 slave.cpp:318] Slave checkpoint: false
I0929 16:58:29.535557  3505 state.cpp:33] Recovering state from '/tmp/AllocatorTest_0_SlaveLost_xdXHfg/meta'
I0929 16:58:29.535951  3505 status_update_manager.cpp:193] Recovering status update manager
I0929 16:58:29.536290  3505 slave.cpp:3271] Finished recovery
I0929 16:58:29.536782  3505 slave.cpp:598] New master detected at master@192.168.122.164:55618
I0929 16:58:29.537122  3505 slave.cpp:672] Authenticating with master master@192.168.122.164:55618
I0929 16:58:29.537492  3505 slave.cpp:645] Detecting new master
I0929 16:58:29.537294  3506 status_update_manager.cpp:167] New master detected at master@192.168.122.164:55618
I0929 16:58:29.537642  3507 authenticatee.hpp:128] Creating new client SASL connection
I0929 16:58:29.538769  3502 master.cpp:3737] Authenticating slave(57)@192.168.122.164:55618
I0929 16:58:29.539091  3502 authenticator.hpp:156] Creating new server SASL connection
I0929 16:58:29.539710  3503 authenticatee.hpp:219] Received SASL authentication mechanisms: CRAM-MD5
I0929 16:58:29.539943  3503 authenticatee.hpp:245] Attempting to authenticate with mechanism 'CRAM-MD5'
I0929 16:58:29.540206  3502 authenticator.hpp:262] Received SASL authentication start
I0929 16:58:29.540457  3502 authenticator.hpp:384] Authentication requires more steps
I0929 16:58:29.540757  3502 authenticatee.hpp:265] Received SASL authentication step
I0929 16:58:29.541121  3502 authenticator.hpp:290] Received SASL authentication step
I0929 16:58:29.541368  3502 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'fedora-20' server FQDN: 'fedora-20' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0929 16:58:29.541599  3502 auxprop.cpp:153] Looking up auxiliary property '*userPassword'
I0929 16:58:29.541874  3502 auxprop.cpp:153] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0929 16:58:29.542129  3502 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'fedora-20' server FQDN: 'fedora-20' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0929 16:58:29.542333  3502 auxprop.cpp:103] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0929 16:58:29.542553  3502 auxprop.cpp:103] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0929 16:58:29.542785  3502 authenticator.hpp:376] Authentication success
I0929 16:58:29.543047  3502 authenticatee.hpp:305] Authentication success
I0929 16:58:29.543381  3502 slave.cpp:729] Successfully authenticated with master master@192.168.122.164:55618
I0929 16:58:29.543707  3502 slave.cpp:992] Will retry registration in 11.795692ms if necessary
I0929 16:58:29.543179  3503 master.cpp:3777] Successfully authenticated principal 'test-principal' at slave(57)@192.168.122.164:55618
I0929 16:58:29.544255  3503 master.cpp:2930] Registering slave at slave(57)@192.168.122.164:55618 (fedora-20) with id 20140929-165829-2759502016-55618-3486-0
I0929 16:58:29.544587  3503 registrar.cpp:421] Attempting to update the 'registry'
I0929 16:58:29.545816  3500 log.cpp:680] Attempting to append 299 bytes to the log
I0929 16:58:29.546267  3500 coordinator.cpp:340] Coordinator attempting to write APPEND action at position 3
I0929 16:58:29.546749  3500 replica.cpp:508] Replica received write request for position 3
I0929 16:58:29.547030  3500 leveldb.cpp:343] Persisting action (318 bytes) to leveldb took 31759ns
I0929 16:58:29.547236  3500 replica.cpp:676] Persisted action at 3
I0929 16:58:29.548902  3506 replica.cpp:655] Replica received learned notice for position 3
I0929 16:58:29.549139  3506 leveldb.cpp:343] Persisting action (320 bytes) to leveldb took 25595ns
I0929 16:58:29.549343  3506 replica.cpp:676] Persisted action at 3
I0929 16:58:29.549607  3506 replica.cpp:661] Replica learned APPEND action at position 3
I0929 16:58:29.550081  3506 log.cpp:699] Attempting to truncate the log to 3
I0929 16:58:29.550497  3506 coordinator.cpp:340] Coordinator attempting to write TRUNCATE action at position 4
I0929 16:58:29.550943  3506 replica.cpp:508] Replica received write request for position 4
I0929 16:58:29.551198  3506 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 20852ns
I0929 16:58:29.551409  3506 replica.cpp:676] Persisted action at 4
I0929 16:58:29.551795  3506 replica.cpp:655] Replica received learned notice for position 4
I0929 16:58:29.552094  3506 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 22182ns
I0929 16:58:29.552320  3506 leveldb.cpp:401] Deleting ~2 keys from leveldb took 18503ns
I0929 16:58:29.552525  3506 replica.cpp:676] Persisted action at 4
I0929 16:58:29.552781  3506 replica.cpp:661] Replica learned TRUNCATE action at position 4
I0929 16:58:29.550289  3503 registrar.cpp:478] Successfully updated 'registry'
I0929 16:58:29.553553  3503 master.cpp:2970] Registered slave 20140929-165829-2759502016-55618-3486-0 at slave(57)@192.168.122.164:55618 (fedora-20)
I0929 16:58:29.553807  3503 master.cpp:4180] Adding slave 20140929-165829-2759502016-55618-3486-0 at slave(57)@192.168.122.164:55618 (fedora-20) with cpus(*):2; mem(*):1024; disk(*):752; ports(*):[31000-32000]
I0929 16:58:29.554152  3503 slave.cpp:763] Registered with master master@192.168.122.164:55618; given slave ID 20140929-165829-2759502016-55618-3486-0
I0929 16:58:29.554455  3503 slave.cpp:2345] Received ping from slave-observer(56)@192.168.122.164:55618
I0929 16:58:29.554707  3504 hierarchical_allocator_process.hpp:442] Added slave 20140929-165829-2759502016-55618-3486-0 (fedora-20) with cpus(*):2; mem(*):1024; disk(*):752; ports(*):[31000-32000] (and cpus(*):2; mem(*):1024; disk(*):752; ports(*):[31000-32000] available)
I0929 16:58:29.555064  3504 hierarchical_allocator_process.hpp:679] Performed allocation for slave 20140929-165829-2759502016-55618-3486-0 in 13111ns
I0929 16:58:29.558220  3486 sched.cpp:137] Version: 0.21.0
I0929 16:58:29.558821  3501 sched.cpp:233] New master detected at master@192.168.122.164:55618
I0929 16:58:29.559054  3501 sched.cpp:283] Authenticating with master master@192.168.122.164:55618
I0929 16:58:29.559360  3501 authenticatee.hpp:128] Creating new client SASL connection
I0929 16:58:29.560096  3501 master.cpp:3737] Authenticating scheduler-c8df3f3b-2552-476f-9daf-9aa2f012ad28@192.168.122.164:55618
I0929 16:58:29.560430  3501 authenticator.hpp:156] Creating new server SASL connection
I0929 16:58:29.561141  3501 authenticatee.hpp:219] Received SASL authentication mechanisms: CRAM-MD5
I0929 16:58:29.561465  3501 authenticatee.hpp:245] Attempting to authenticate with mechanism 'CRAM-MD5'
I0929 16:58:29.561743  3501 authenticator.hpp:262] Received SASL authentication start
I0929 16:58:29.562098  3501 authenticator.hpp:384] Authentication requires more steps
I0929 16:58:29.562353  3501 authenticatee.hpp:265] Received SASL authentication step
I0929 16:58:29.562721  3507 authenticator.hpp:290] Received SASL authentication step
I0929 16:58:29.563022  3507 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'fedora-20' server FQDN: 'fedora-20' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0929 16:58:29.563254  3507 auxprop.cpp:153] Looking up auxiliary property '*userPassword'
I0929 16:58:29.563484  3507 auxprop.cpp:153] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0929 16:58:29.563736  3507 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'fedora-20' server FQDN: 'fedora-20' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0929 16:58:29.563976  3507 auxprop.cpp:103] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0929 16:58:29.564188  3507 auxprop.cpp:103] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0929 16:58:29.564415  3507 authenticator.hpp:376] Authentication success
I0929 16:58:29.564673  3507 master.cpp:3777] Successfully authenticated principal 'test-principal' at scheduler-c8df3f3b-2552-476f-9daf-9aa2f012ad28@192.168.122.164:55618
I0929 16:58:29.568681  3501 authenticatee.hpp:305] Authentication success
I0929 16:58:29.569046  3501 sched.cpp:357] Successfully authenticated with master master@192.168.122.164:55618
I0929 16:58:29.569286  3501 sched.cpp:476] Sending registration request to master@192.168.122.164:55618
I0929 16:58:29.569581  3507 master.cpp:1360] Received registration request from scheduler-c8df3f3b-2552-476f-9daf-9aa2f012ad28@192.168.122.164:55618
I0929 16:58:29.569846  3507 master.cpp:1320] Authorizing framework principal 'test-principal' to receive offers for role '*'
I0929 16:58:29.570219  3507 master.cpp:1419] Registering framework 20140929-165829-2759502016-55618-3486-0000 at scheduler-c8df3f3b-2552-476f-9daf-9aa2f012ad28@192.168.122.164:55618
I0929 16:58:29.570543  3506 sched.cpp:407] Framework registered with 20140929-165829-2759502016-55618-3486-0000
I0929 16:58:29.570811  3506 sched.cpp:421] Scheduler::registered took 13811ns
I0929 16:58:29.571135  3502 hierarchical_allocator_process.hpp:329] Added framework 20140929-165829-2759502016-55618-3486-0000
I0929 16:58:29.571393  3502 hierarchical_allocator_process.hpp:734] Offering cpus(*):2; mem(*):1024; disk(*):752; ports(*):[31000-32000] on slave 20140929-165829-2759502016-55618-3486-0 to framework 20140929-165829-2759502016-55618-3486-0000
I0929 16:58:29.571723  3502 hierarchical_allocator_process.hpp:659] Performed allocation for 1 slaves in 368547ns
I0929 16:58:29.572125  3507 master.hpp:868] Adding offer 20140929-165829-2759502016-55618-3486-0 with resources cpus(*):2; mem(*):1024; disk(*):752; ports(*):[31000-32000] on slave 20140929-165829-2759502016-55618-3486-0 (fedora-20)
I0929 16:58:29.572374  3507 master.cpp:3679] Sending 1 offers to framework 20140929-165829-2759502016-55618-3486-0000
I0929 16:58:29.572841  3503 sched.cpp:544] Scheduler::resourceOffers took 114306ns
I0929 16:58:29.573197  3507 master.hpp:877] Removing offer 20140929-165829-2759502016-55618-3486-0 with resources cpus(*):2; mem(*):1024; disk(*):752; ports(*):[31000-32000] on slave 20140929-165829-2759502016-55618-3486-0 (fedora-20)
I0929 16:58:29.573457  3507 master.cpp:2274] Processing reply for offers: [ 20140929-165829-2759502016-55618-3486-0 ] on slave 20140929-165829-2759502016-55618-3486-0 at slave(57)@192.168.122.164:55618 (fedora-20) for framework 20140929-165829-2759502016-55618-3486-0000
W0929 16:58:29.573717  3507 master.cpp:1944] Executor default for task 0 uses less CPUs (None) than the minimum required (0.01). Please update your executor, as this will be mandatory in future releases.
W0929 16:58:29.573953  3507 master.cpp:1955] Executor default for task 0 uses less memory (None) than the minimum required (32MB). Please update your executor, as this will be mandatory in future releases.
I0929 16:58:29.574177  3507 master.cpp:2357] Authorizing framework principal 'test-principal' to launch task 0 as user 'jenkins'
I0929 16:58:29.574745  3507 master.hpp:845] Adding task 0 with resources cpus(*):2; mem(*):512 on slave 20140929-165829-2759502016-55618-3486-0 (fedora-20)
I0929 16:58:29.574992  3507 master.cpp:2423] Launching task 0 of framework 20140929-165829-2759502016-55618-3486-0000 with resources cpus(*):2; mem(*):512 on slave 20140929-165829-2759502016-55618-3486-0 at slave(57)@192.168.122.164:55618 (fedora-20)
I0929 16:58:29.575315  3503 slave.cpp:1023] Got assigned task 0 for framework 20140929-165829-2759502016-55618-3486-0000
I0929 16:58:29.575724  3503 slave.cpp:1133] Launching task 0 for framework 20140929-165829-2759502016-55618-3486-0000
I0929 16:58:29.578129  3503 exec.cpp:132] Version: 0.21.0
I0929 16:58:29.578505  3504 exec.cpp:182] Executor started at: executor(30)@192.168.122.164:55618 with pid 3486
I0929 16:58:29.578867  3503 slave.cpp:1246] Queuing task '0' for executor default of framework '20140929-165829-2759502016-55618-3486-0000
I0929 16:58:29.579144  3503 slave.cpp:554] Successfully attached file '/tmp/AllocatorTest_0_SlaveLost_xdXHfg/slaves/20140929-165829-2759502016-55618-3486-0/frameworks/20140929-165829-2759502016-55618-3486-0000/executors/default/runs/b0de9759-7054-4763-90f4-889ddc3a8524'
I0929 16:58:29.579401  3503 slave.cpp:1756] Got registration for executor 'default' of framework 20140929-165829-2759502016-55618-3486-0000 from executor(30)@192.168.122.164:55618
I0929 16:58:29.579879  3506 exec.cpp:206] Executor registered on slave 20140929-165829-2759502016-55618-3486-0
I0929 16:58:29.580921  3506 exec.cpp:218] Executor::registered took 17644ns
I0929 16:58:29.581188  3503 slave.cpp:1875] Flushing queued task 0 for executor 'default' of framework 20140929-165829-2759502016-55618-3486-0000
I0929 16:58:29.581526  3504 exec.cpp:293] Executor asked to run task '0'
I0929 16:58:29.581807  3504 exec.cpp:302] Executor::launchTask took 42649ns
I0929 16:58:29.583133  3504 exec.cpp:525] Executor sending status update TASK_RUNNING (UUID: 454bdb88-fd27-4201-b2c7-4ea03a6d00b3) for task 0 of framework 20140929-165829-2759502016-55618-3486-0000
I0929 16:58:29.586869  3503 slave.cpp:2611] Monitoring executor 'default' of framework '20140929-165829-2759502016-55618-3486-0000' in container 'b0de9759-7054-4763-90f4-889ddc3a8524'
I0929 16:58:29.587252  3503 slave.cpp:2109] Handling status update TASK_RUNNING (UUID: 454bdb88-fd27-4201-b2c7-4ea03a6d00b3) for task 0 of framework 20140929-165829-2759502016-55618-3486-0000 from executor(30)@192.168.122.164:55618
I0929 16:58:29.587723  3502 hierarchical_allocator_process.hpp:563] Recovered mem(*):512; disk(*):752; ports(*):[31000-32000] (total allocatable: mem(*):512; disk(*):752; ports(*):[31000-32000]) on slave 20140929-165829-2759502016-55618-3486-0 from framework 20140929-165829-2759502016-55618-3486-0000
I0929 16:58:29.588127  3502 hierarchical_allocator_process.hpp:599] Framework 20140929-165829-2759502016-55618-3486-0000 filtered slave 20140929-165829-2759502016-55618-3486-0 for 5secs
I0929 16:58:29.588433  3506 status_update_manager.cpp:320] Received status update TASK_RUNNING (UUID: 454bdb88-fd27-4201-b2c7-4ea03a6d00b3) for task 0 of framework 20140929-165829-2759502016-55618-3486-0000
I0929 16:58:29.588767  3506 status_update_manager.cpp:499] Creating StatusUpdate stream for task 0 of framework 20140929-165829-2759502016-55618-3486-0000
I0929 16:58:29.589054  3506 status_update_manager.cpp:373] Forwarding status update TASK_RUNNING (UUID: 454bdb88-fd27-4201-b2c7-4ea03a6d00b3) for task 0 of framework 20140929-165829-2759502016-55618-3486-0000 to master@192.168.122.164:55618
I0929 16:58:29.589400  3506 master.cpp:3301] Forwarding status update TASK_RUNNING (UUID: 454bdb88-fd27-4201-b2c7-4ea03a6d00b3) for task 0 of framework 20140929-165829-2759502016-55618-3486-0000
I0929 16:58:29.589702  3506 master.cpp:3273] Status update TASK_RUNNING (UUID: 454bdb88-fd27-4201-b2c7-4ea03a6d00b3) for task 0 of framework 20140929-165829-2759502016-55618-3486-0000 from slave 20140929-165829-2759502016-55618-3486-0 at slave(57)@192.168.122.164:55618 (fedora-20)
I0929 16:58:29.589923  3500 sched.cpp:635] Scheduler::statusUpdate took 36034ns
I0929 16:58:29.590337  3500 master.cpp:2777] Forwarding status update acknowledgement 454bdb88-fd27-4201-b2c7-4ea03a6d00b3 for task 0 of framework 20140929-165829-2759502016-55618-3486-0000 to slave 20140929-165829-2759502016-55618-3486-0 at slave(57)@192.168.122.164:55618 (fedora-20)
I0929 16:58:29.590643  3503 slave.cpp:477] Slave terminating
I0929 16:58:29.590893  3503 slave.cpp:1429] Asked to shut down framework 20140929-165829-2759502016-55618-3486-0000 by @0.0.0.0:0
I0929 16:58:29.591136  3503 slave.cpp:1454] Shutting down framework 20140929-165829-2759502016-55618-3486-0000
I0929 16:58:29.591367  3503 slave.cpp:2951] Shutting down executor 'default' of framework 20140929-165829-2759502016-55618-3486-0000
I0929 16:58:29.591701  3501 master.cpp:817] Slave 20140929-165829-2759502016-55618-3486-0 at slave(57)@192.168.122.164:55618 (fedora-20) disconnected
I0929 16:58:29.591917  3501 master.cpp:821] Removing disconnected slave 20140929-165829-2759502016-55618-3486-0 at slave(57)@192.168.122.164:55618 (fedora-20) because it is not checkpointing!
I0929 16:58:29.592149  3501 master.cpp:4301] Removing slave 20140929-165829-2759502016-55618-3486-0 at slave(57)@192.168.122.164:55618 (fedora-20)
I0929 16:58:29.593868  3505 hierarchical_allocator_process.hpp:467] Removed slave 20140929-165829-2759502016-55618-3486-0
I0929 16:58:29.594907  3486 containerizer.cpp:89] Using isolation: posix/cpu,posix/mem
I0929 16:58:29.595091  3501 master.cpp:4485] Removing task 0 with resources cpus(*):2; mem(*):512 of framework 20140929-165829-2759502016-55618-3486-0000 on slave 20140929-165829-2759502016-55618-3486-0 at slave(57)@192.168.122.164:55618 (fedora-20)
I0929 16:58:29.595960  3501 master.cpp:4514] Removing executor 'default' with resources  of framework 20140929-165829-2759502016-55618-3486-0000 on slave 20140929-165829-2759502016-55618-3486-0 at slave(57)@192.168.122.164:55618 (fedora-20)
tests/allocator_tests.cpp:1552: Failure
Mock function called more times than expected - taking default action specified at:
./tests/mesos.hpp:616:
    Function call: resourcesRecovered(@0x7f958007f590 20140929-165829-2759502016-55618-3486-0000, @0x7f958007f5b0 20140929-165829-2759502016-55618-3486-0, @0x7f958007f5d0 {}, @0x7f958007f5e8 16-byte object <01-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00>)
         Expected: to be called twice
I0929 16:58:29.596640  3506 registrar.cpp:421] Attempting to update the 'registry'
           Actual: called 3 times - over-saturated and active
I0929 16:58:29.598697  3506 log.cpp:680] Attempting to append 133 bytes to the log
I0929 16:58:29.598984  3500 coordinator.cpp:340] Coordinator attempting to write APPEND action at position 5
I0929 16:58:29.599422  3500 replica.cpp:508] Replica received write request for position 5
I0929 16:58:29.599712  3500 leveldb.cpp:343] Persisting action (152 bytes) to leveldb took 65914ns
I0929 16:58:29.599931  3500 replica.cpp:676] Persisted action at 5
I0929 16:58:29.600332  3500 replica.cpp:655] Replica received learned notice for position 5
I0929 16:58:29.600621  3500 leveldb.cpp:343] Persisting action (154 bytes) to leveldb took 24641ns
I0929 16:58:29.600858  3500 replica.cpp:676] Persisted action at 5
I0929 16:58:29.601060  3500 replica.cpp:661] Replica learned APPEND action at position 5
I0929 16:58:29.601588  3506 registrar.cpp:478] Successfully updated 'registry'
I0929 16:58:29.601765  3500 log.cpp:699] Attempting to truncate the log to 5
I0929 16:58:29.602308  3501 coordinator.cpp:340] Coordinator attempting to write TRUNCATE action at position 6
I0929 16:58:29.602736  3505 replica.cpp:508] Replica received write request for position 6
I0929 16:58:29.602967  3505 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 22681ns
I0929 16:58:29.603175  3505 replica.cpp:676] Persisted action at 6
I0929 16:58:29.603591  3501 replica.cpp:655] Replica received learned notice for position 6
I0929 16:58:29.603903  3501 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 23564ns
I0929 16:58:29.604161  3501 leveldb.cpp:401] Deleting ~2 keys from leveldb took 18683ns
I0929 16:58:29.604378  3501 replica.cpp:676] Persisted action at 6
I0929 16:58:29.604575  3501 replica.cpp:661] Replica learned TRUNCATE action at position 6
I0929 16:58:29.604970  3502 master.cpp:4393] Removed slave 20140929-165829-2759502016-55618-3486-0 (fedora-20)
I0929 16:58:29.605197  3502 master.cpp:3296] Sending status update TASK_LOST (UUID: cfc350bc-4ebf-4ea1-9fe4-27f53825c787) for task 0 of framework 20140929-165829-2759502016-55618-3486-0000 'Slave fedora-20 removed'
I0929 16:58:29.605445  3502 master.cpp:4411] Notifying framework 20140929-165829-2759502016-55618-3486-0000 of lost slave 20140929-165829-2759502016-55618-3486-0 (fedora-20) after recovering
I0929 16:58:29.605756  3502 sched.cpp:635] Scheduler::statusUpdate took 9369ns
I0929 16:58:29.605996  3502 sched.cpp:686] Lost slave 20140929-165829-2759502016-55618-3486-0
I0929 16:58:29.606210  3502 sched.cpp:697] Scheduler::slaveLost took 13761ns
I0929 16:58:29.607326  3501 slave.cpp:169] Slave started on 58)@192.168.122.164:55618
I0929 16:58:29.607640  3501 credentials.hpp:84] Loading credential for authentication from '/tmp/AllocatorTest_0_SlaveLost_NcoJ6Z/credential'
I0929 16:58:29.607975  3501 slave.cpp:276] Slave using credential for: test-principal
I0929 16:58:29.608253  3501 slave.cpp:289] Slave resources: cpus(*):3; mem(*):256; disk(*):1024; ports(*):[31000-32000]
I0929 16:58:29.608832  3501 slave.cpp:317] Slave hostname: fedora-20
I0929 16:58:29.608989  3501 slave.cpp:318] Slave checkpoint: false
I0929 16:58:29.609542  3501 state.cpp:33] Recovering state from '/tmp/AllocatorTest_0_SlaveLost_NcoJ6Z/meta'
I0929 16:58:29.609904  3500 status_update_manager.cpp:193] Recovering status update manager
I0929 16:58:29.610119  3500 containerizer.cpp:252] Recovering containerizer
I0929 16:58:29.610589  3507 slave.cpp:3271] Finished recovery
I0929 16:58:29.611037  3507 slave.cpp:598] New master detected at master@192.168.122.164:55618
I0929 16:58:29.611264  3507 slave.cpp:672] Authenticating with master master@192.168.122.164:55618
I0929 16:58:29.611529  3507 slave.cpp:645] Detecting new master
I0929 16:58:29.611385  3506 status_update_manager.cpp:167] New master detected at master@192.168.122.164:55618
I0929 16:58:29.611719  3503 authenticatee.hpp:128] Creating new client SASL connection
I0929 16:58:29.612570  3503 master.cpp:3737] Authenticating slave(58)@192.168.122.164:55618
I0929 16:58:29.612843  3503 authenticator.hpp:156] Creating new server SASL connection
I0929 16:58:29.613394  3503 authenticatee.hpp:219] Received SASL authentication mechanisms: CRAM-MD5
I0929 16:58:29.613706  3503 authenticatee.hpp:245] Attempting to authenticate with mechanism 'CRAM-MD5'
I0929 16:58:29.614083  3503 authenticator.hpp:262] Received SASL authentication start
I0929 16:58:29.614326  3503 authenticator.hpp:384] Authentication requires more steps
I0929 16:58:29.614552  3503 authenticatee.hpp:265] Received SASL authentication step
I0929 16:58:29.614828  3503 authenticator.hpp:290] Received SASL authentication step
I0929 16:58:29.615067  3503 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'fedora-20' server FQDN: 'fedora-20' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0929 16:58:29.615314  3503 auxprop.cpp:153] Looking up auxiliary property '*userPassword'
I0929 16:58:29.615562  3503 auxprop.cpp:153] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0929 16:58:29.615766  3503 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'fedora-20' server FQDN: 'fedora-20' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0929 16:58:29.616060  3503 auxprop.cpp:103] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0929 16:58:29.616387  3503 auxprop.cpp:103] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0929 16:58:29.616631  3503 authenticator.hpp:376] Authentication success
I0929 16:58:29.616929  3503 authenticatee.hpp:305] Authentication success
I0929 16:58:29.617081  3501 master.cpp:3777] Successfully authenticated principal 'test-principal' at slave(58)@192.168.122.164:55618
I0929 16:58:29.620779  3500 slave.cpp:729] Successfully authenticated with master master@192.168.122.164:55618
I0929 16:58:29.621150  3500 slave.cpp:992] Will retry registration in 15.66596ms if necessary
I0929 16:58:29.621526  3501 master.cpp:2930] Registering slave at slave(58)@192.168.122.164:55618 (fedora-20) with id 20140929-165829-2759502016-55618-3486-1
I0929 16:58:29.621976  3501 registrar.cpp:421] Attempting to update the 'registry'
I0929 16:58:29.623364  3506 log.cpp:680] Attempting to append 299 bytes to the log
I0929 16:58:29.623780  3506 coordinator.cpp:340] Coordinator attempting to write APPEND action at position 7
I0929 16:58:29.624407  3506 replica.cpp:508] Replica received write request for position 7
I0929 16:58:29.624712  3506 leveldb.cpp:343] Persisting action (318 bytes) to leveldb took 64462ns
I0929 16:58:29.624984  3506 replica.cpp:676] Persisted action at 7
I0929 16:58:29.625460  3506 replica.cpp:655] Replica received learned notice for position 7
I0929 16:58:29.625838  3506 leveldb.cpp:343] Persisting action (320 bytes) to leveldb took 30316ns
I0929 16:58:29.626093  3506 replica.cpp:676] Persisted action at 7
I0929 16:58:29.626382  3506 replica.cpp:661] Replica learned APPEND action at position 7
I0929 16:58:29.626832  3506 log.cpp:699] Attempting to truncate the log to 7
I0929 16:58:29.627231  3506 coordinator.cpp:340] Coordinator attempting to write TRUNCATE action at position 8
I0929 16:58:29.627789  3506 replica.cpp:508] Replica received write request for position 8
I0929 16:58:29.628073  3506 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 26181ns
I0929 16:58:29.628347  3506 replica.cpp:676] Persisted action at 8
I0929 16:58:29.628829  3506 replica.cpp:655] Replica received learned notice for position 8
I0929 16:58:29.629323  3506 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 28559ns
I0929 16:58:29.629581  3506 leveldb.cpp:401] Deleting ~2 keys from leveldb took 22253ns
I0929 16:58:29.629897  3506 replica.cpp:676] Persisted action at 8
I0929 16:58:29.630159  3506 replica.cpp:661] Replica learned TRUNCATE action at position 8
I0929 16:58:29.630910  3501 registrar.cpp:478] Successfully updated 'registry'
I0929 16:58:29.631356  3501 master.cpp:2970] Registered slave 20140929-165829-2759502016-55618-3486-1 at slave(58)@192.168.122.164:55618 (fedora-20)
I0929 16:58:29.631624  3501 master.cpp:4180] Adding slave 20140929-165829-2759502016-55618-3486-1 at slave(58)@192.168.122.164:55618 (fedora-20) with cpus(*):3; mem(*):256; disk(*):1024; ports(*):[31000-32000]
I0929 16:58:29.632066  3501 slave.cpp:763] Registered with master master@192.168.122.164:55618; given slave ID 20140929-165829-2759502016-55618-3486-1
I0929 16:58:29.632493  3501 slave.cpp:2345] Received ping from slave-observer(57)@192.168.122.164:55618
I0929 16:58:29.632298  3506 hierarchical_allocator_process.hpp:442] Added slave 20140929-165829-2759502016-55618-3486-1 (fedora-20) with cpus(*):3; mem(*):256; disk(*):1024; ports(*):[31000-32000] (and cpus(*):3; mem(*):256; disk(*):1024; ports(*):[31000-32000] available)
I0929 16:58:29.633102  3506 hierarchical_allocator_process.hpp:734] Offering cpus(*):3; mem(*):256; disk(*):1024; ports(*):[31000-32000] on slave 20140929-165829-2759502016-55618-3486-1 to framework 20140929-165829-2759502016-55618-3486-0000
I0929 16:58:29.633496  3507 master.hpp:868] Adding offer 20140929-165829-2759502016-55618-3486-1 with resources cpus(*):3; mem(*):256; disk(*):1024; ports(*):[31000-32000] on slave 20140929-165829-2759502016-55618-3486-1 (fedora-20)
I0929 16:58:29.633833  3507 master.cpp:3679] Sending 1 offers to framework 20140929-165829-2759502016-55618-3486-0000
I0929 16:58:29.634218  3507 sched.cpp:544] Scheduler::resourceOffers took 32550ns
I0929 16:58:29.634784  3507 sched.cpp:745] Stopping framework '20140929-165829-2759502016-55618-3486-0000'
I0929 16:58:29.634558  3486 master.cpp:676] Master terminating
I0929 16:58:29.635319  3486 master.hpp:877] Removing offer 20140929-165829-2759502016-55618-3486-1 with resources cpus(*):3; mem(*):256; disk(*):1024; ports(*):[31000-32000] on slave 20140929-165829-2759502016-55618-3486-1 (fedora-20)
I0929 16:58:29.635725  3506 hierarchical_allocator_process.hpp:679] Performed allocation for slave 20140929-165829-2759502016-55618-3486-1 in 2.656855ms
I0929 16:58:29.644737  3503 slave.cpp:2430] master@192.168.122.164:55618 exited
W0929 16:58:29.645407  3503 slave.cpp:2433] Master disconnected! Waiting for a new master to be elected
I0929 16:58:29.656318  3486 slave.cpp:477] Slave terminating
tests/allocator_tests.cpp:1532: Failure
Actual function call count doesn't match EXPECT_CALL(this->allocator, resourcesRecovered(_, _, _, _))...
         Expected: to be called once
           Actual: never called - unsatisfied and active
[  FAILED  ] AllocatorTest/0.SlaveLost, where TypeParam = mesos::internal::master::allocator::HierarchicalAllocatorProcess<mesos::internal::master::allocator::DRFSorter, mesos::internal::master::allocator::DRFSorter> (179 ms)
{code}",Bug,Major,vinodkone,2014-09-30T04:33:56.000+0000,5,Resolved,Complete,AllocatorTest/0.SlaveLost is flaky,2014-09-30T04:33:56.000+0000,MESOS-1844,1.0,mesos,Mesos Q3 Sprint 6
dhamon,2014-09-25T18:36:44.000+0000,wfarner,The master exports a monotonically-increasing counter of tasks transitioned to TASK_LOST.  This loses fidelity of the source of the lost task.  A first step in exposing the source of lost tasks might be to just differentiate between TASK_LOST transitions initiated by the master vs the slave (and maybe bad input from the scheduler).,Story,Minor,wfarner,2015-02-10T23:23:06.000+0000,5,Resolved,Complete,Expose master stats differentiating between master-generated and slave-generated LOST tasks,2015-02-10T23:23:06.000+0000,MESOS-1830,5.0,mesos,Twitter Q4 Sprint 1
vinodkone,2014-09-19T00:17:19.000+0000,nnielsen,"We have run into a problem that cause tasks which completes, when a framework is disconnected and has a fail-over time, to remain in a running state even though the tasks actually finishes. This hogs the cluster and gives users a inconsistent view of the cluster state. Going to the slave, the task is finished. Going to the master, the task is still in a non-terminal state. When the scheduler reattaches or the failover timeout expires, the tasks finishes correctly. The current workflow of this scheduler has a long fail-over timeout, but may on the other hand never reattach.

Here is a test framework we have been able to reproduce the issue with: https://gist.github.com/nqn/9b9b1de9123a6e836f54
It launches many short-lived tasks (1 second sleep) and when killing the framework instance, the master reports the tasks as running even after several minutes: http://cl.ly/image/2R3719461e0t/Screen%20Shot%202014-09-10%20at%203.19.39%20PM.png

When clicking on one of the slaves where, for example, task 49 runs; the slave knows that it completed: http://cl.ly/image/2P410L3m1O1N/Screen%20Shot%202014-09-10%20at%203.21.29%20PM.png

Here is the log of a mesos-local instance where I reproduced it: https://gist.github.com/nqn/f7ee20601199d70787c0 (Here task 10 to 19 are stuck in running state).
There is a lot of output, so here is a filtered log for task 10: https://gist.github.com/nqn/a53e5ea05c5e41cd5a7d

The problem turn out to be an issue with the ack-cycle of status updates:
If the framework disconnects (with a failover timeout set), the status update manage on the slaves will keep trying to send the front of status update stream to the master (which in turn forwards it to the framework). If the first status update after the disconnect is terminal, things work out fine; the master pick the terminal state up, removes the task and release the resources.
If, on the other hand, one non-terminal status is in the stream. The master will never know that the task finished (or failed) before the framework reconnects.

During a discussion on the dev mailing list (http://mail-archives.apache.org/mod_mbox/mesos-dev/201409.mbox/%3cCADKthhAVR5mrq1s9HXw1BB_XFALXWWxjutp7MV4y3wP-Bh=aWg@mail.gmail.com%3e) we enumerated a couple of options to solve this problem.

First off, having two ack-cycles: one between masters and slaves and one between masters and frameworks, would be ideal. We would be able to replay the statuses in order while keeping the master state current. However, this requires us to persist the master state in a replicated storage.

As a first pass, we can make sure that the tasks caught in a running state doesn't hog the cluster when completed and the framework being disconnected.

Here is a proof-of-concept to work out of: https://github.com/nqn/mesos/tree/niklas/status-update-disconnect/

A new (optional) field have been added to the internal status update message:
https://github.com/nqn/mesos/blob/niklas/status-update-disconnect/src/messages/messages.proto#L68

Which makes it possible for the status update manager to set the field, if the latest status was terminal: https://github.com/nqn/mesos/blob/niklas/status-update-disconnect/src/slave/status_update_manager.cpp#L501

I added a test which should high-light the issue as well:
https://github.com/nqn/mesos/blob/niklas/status-update-disconnect/src/tests/fault_tolerance_tests.cpp#L2478

I would love some input on the approach before moving on.
There are rough edges in the PoC which (of course) should be addressed before bringing it for up review.",Bug,Major,nnielsen,2014-10-21T22:49:44.000+0000,5,Resolved,Complete,Completed tasks remains in TASK_RUNNING when framework is disconnected,2014-10-21T22:49:44.000+0000,MESOS-1817,2.0,mesos,Twitter Q4 Sprint 1
bernd-mesos,2014-09-18T17:33:45.000+0000,dhamon,"We have a committer's guide, but the process by which one becomes a committer is unclear. We should set some guidelines and a process by which we can grow contributors into committers.",Documentation,Major,dhamon,2015-07-30T09:02:25.000+0000,5,Resolved,Complete,Create a guide to becoming a committer,2015-07-30T09:02:25.000+0000,MESOS-1815,3.0,mesos,Mesosphere Sprint 15
vinodkone,2014-09-18T16:42:28.000+0000,vinodkone,"{code}
[ RUN      ] ExamplesTest.JavaFramework
Using temporary directory '/tmp/ExamplesTest_JavaFramework_2PcFCh'
Enabling authentication for the framework
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0917 23:14:35.199069 31510 process.cpp:1771] libprocess is initialized on 127.0.1.1:34609 for 8 cpus
I0917 23:14:35.199794 31510 logging.cpp:177] Logging to STDERR
I0917 23:14:35.225342 31510 leveldb.cpp:176] Opened db in 22.197149ms
I0917 23:14:35.231133 31510 leveldb.cpp:183] Compacted db in 5.601897ms
I0917 23:14:35.231498 31510 leveldb.cpp:198] Created db iterator in 215441ns
I0917 23:14:35.231608 31510 leveldb.cpp:204] Seeked to beginning of db in 11488ns
I0917 23:14:35.231722 31510 leveldb.cpp:273] Iterated through 0 keys in the db in 14016ns
I0917 23:14:35.231917 31510 replica.cpp:741] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned
I0917 23:14:35.233129 31526 recover.cpp:425] Starting replica recovery
I0917 23:14:35.233614 31526 recover.cpp:451] Replica is in EMPTY status
I0917 23:14:35.234994 31526 replica.cpp:638] Replica in EMPTY status received a broadcasted recover request
I0917 23:14:35.240116 31519 recover.cpp:188] Received a recover response from a replica in EMPTY status
I0917 23:14:35.240782 31519 recover.cpp:542] Updating replica status to STARTING
I0917 23:14:35.242846 31524 master.cpp:286] Master 20140917-231435-16842879-34609-31503 (saucy) started on 127.0.1.1:34609
I0917 23:14:35.243191 31524 master.cpp:332] Master only allowing authenticated frameworks to register
I0917 23:14:35.243288 31524 master.cpp:339] Master allowing unauthenticated slaves to register
I0917 23:14:35.243399 31524 credentials.hpp:36] Loading credentials for authentication from '/tmp/ExamplesTest_JavaFramework_2PcFCh/credentials'
W0917 23:14:35.243588 31524 credentials.hpp:51] Permissions on credentials file '/tmp/ExamplesTest_JavaFramework_2PcFCh/credentials' are too open. It is recommended that your credentials file is NOT accessible by others.
I0917 23:14:35.243846 31524 master.cpp:366] Authorization enabled
I0917 23:14:35.244882 31520 hierarchical_allocator_process.hpp:299] Initializing hierarchical allocator process with master : master@127.0.1.1:34609
I0917 23:14:35.245224 31520 master.cpp:120] No whitelist given. Advertising offers for all slaves
I0917 23:14:35.246934 31524 master.cpp:1211] The newly elected leader is master@127.0.1.1:34609 with id 20140917-231435-16842879-34609-31503
I0917 23:14:35.247234 31524 master.cpp:1224] Elected as the leading master!
I0917 23:14:35.247336 31524 master.cpp:1042] Recovering from registrar
I0917 23:14:35.247542 31526 registrar.cpp:313] Recovering registrar
I0917 23:14:35.250555 31510 containerizer.cpp:89] Using isolation: posix/cpu,posix/mem
I0917 23:14:35.252326 31510 containerizer.cpp:89] Using isolation: posix/cpu,posix/mem
I0917 23:14:35.252821 31520 slave.cpp:169] Slave started on 1)@127.0.1.1:34609
I0917 23:14:35.253552 31520 slave.cpp:289] Slave resources: cpus(*):1; mem(*):1001; disk(*):24988; ports(*):[31000-32000]
I0917 23:14:35.253906 31520 slave.cpp:317] Slave hostname: saucy
I0917 23:14:35.254004 31520 slave.cpp:318] Slave checkpoint: true
I0917 23:14:35.254818 31520 state.cpp:33] Recovering state from '/tmp/mesos-w8snRW/0/meta'
I0917 23:14:35.255106 31519 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 13.99622ms
I0917 23:14:35.255235 31519 replica.cpp:320] Persisted replica status to STARTING
I0917 23:14:35.255419 31519 recover.cpp:451] Replica is in STARTING status
I0917 23:14:35.255834 31519 replica.cpp:638] Replica in STARTING status received a broadcasted recover request
I0917 23:14:35.256000 31519 recover.cpp:188] Received a recover response from a replica in STARTING status
I0917 23:14:35.256217 31519 recover.cpp:542] Updating replica status to VOTING
I0917 23:14:35.256641 31520 status_update_manager.cpp:193] Recovering status update manager
I0917 23:14:35.257064 31520 containerizer.cpp:252] Recovering containerizer
I0917 23:14:35.257725 31520 slave.cpp:3220] Finished recovery
I0917 23:14:35.258463 31520 slave.cpp:600] New master detected at master@127.0.1.1:34609
I0917 23:14:35.258769 31524 status_update_manager.cpp:167] New master detected at master@127.0.1.1:34609
I0917 23:14:35.258885 31520 slave.cpp:636] No credentials provided. Attempting to register without authentication
I0917 23:14:35.259024 31520 slave.cpp:647] Detecting new master
I0917 23:14:35.259863 31520 slave.cpp:169] Slave started on 2)@127.0.1.1:34609
I0917 23:14:35.260288 31520 slave.cpp:289] Slave resources: cpus(*):1; mem(*):1001; disk(*):24988; ports(*):[31000-32000]
I0917 23:14:35.260493 31520 slave.cpp:317] Slave hostname: saucy
I0917 23:14:35.260588 31520 slave.cpp:318] Slave checkpoint: true
I0917 23:14:35.265127 31510 containerizer.cpp:89] Using isolation: posix/cpu,posix/mem
I0917 23:14:35.265877 31519 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 9.536278ms
I0917 23:14:35.265983 31519 replica.cpp:320] Persisted replica status to VOTING
I0917 23:14:35.266324 31519 recover.cpp:556] Successfully joined the Paxos group
I0917 23:14:35.266511 31519 recover.cpp:440] Recover process terminated
I0917 23:14:35.266978 31519 log.cpp:656] Attempting to start the writer
I0917 23:14:35.268165 31523 replica.cpp:474] Replica received implicit promise request with proposal 1
I0917 23:14:35.269850 31525 slave.cpp:169] Slave started on 3)@127.0.1.1:34609
I0917 23:14:35.270365 31525 slave.cpp:289] Slave resources: cpus(*):1; mem(*):1001; disk(*):24988; ports(*):[31000-32000]
I0917 23:14:35.270658 31525 slave.cpp:317] Slave hostname: saucy
I0917 23:14:35.270781 31525 slave.cpp:318] Slave checkpoint: true
I0917 23:14:35.271332 31525 state.cpp:33] Recovering state from '/tmp/mesos-w8snRW/2/meta'
I0917 23:14:35.271580 31522 status_update_manager.cpp:193] Recovering status update manager
I0917 23:14:35.271838 31522 containerizer.cpp:252] Recovering containerizer
I0917 23:14:35.272238 31525 slave.cpp:3220] Finished recovery
I0917 23:14:35.273002 31525 slave.cpp:600] New master detected at master@127.0.1.1:34609
I0917 23:14:35.273252 31521 status_update_manager.cpp:167] New master detected at master@127.0.1.1:34609
I0917 23:14:35.273360 31525 slave.cpp:636] No credentials provided. Attempting to register without authentication
I0917 23:14:35.273507 31525 slave.cpp:647] Detecting new master
I0917 23:14:35.275413 31525 state.cpp:33] Recovering state from '/tmp/mesos-w8snRW/1/meta'
I0917 23:14:35.278506 31523 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 10.232514ms
I0917 23:14:35.278712 31523 replica.cpp:342] Persisted promised to 1
I0917 23:14:35.279585 31523 coordinator.cpp:230] Coordinator attemping to fill missing position
I0917 23:14:35.280400 31523 replica.cpp:375] Replica received explicit promise request for position 0 with proposal 2
I0917 23:14:35.280900 31526 status_update_manager.cpp:193] Recovering status update manager
I0917 23:14:35.281282 31519 containerizer.cpp:252] Recovering containerizer
I0917 23:14:35.281615 31520 slave.cpp:3220] Finished recovery
I0917 23:14:35.281891 31510 sched.cpp:137] Version: 0.21.0
I0917 23:14:35.282306 31526 sched.cpp:233] New master detected at master@127.0.1.1:34609
I0917 23:14:35.282464 31526 sched.cpp:283] Authenticating with master master@127.0.1.1:34609
I0917 23:14:35.282891 31526 authenticatee.hpp:104] Initializing client SASL
I0917 23:14:35.284816 31526 authenticatee.hpp:128] Creating new client SASL connection
I0917 23:14:35.285428 31519 master.cpp:873] Dropping 'mesos.internal.AuthenticateMessage' message since not recovered yet
I0917 23:14:35.288007 31521 slave.cpp:600] New master detected at master@127.0.1.1:34609
I0917 23:14:35.288399 31521 slave.cpp:636] No credentials provided. Attempting to register without authentication
I0917 23:14:35.288535 31521 slave.cpp:647] Detecting new master
I0917 23:14:35.288501 31519 status_update_manager.cpp:167] New master detected at master@127.0.1.1:34609
I0917 23:14:35.289625 31523 leveldb.cpp:343] Persisting action (8 bytes) to leveldb took 8.997343ms
I0917 23:14:35.289784 31523 replica.cpp:676] Persisted action at 0
I0917 23:14:35.292667 31521 replica.cpp:508] Replica received write request for position 0
I0917 23:14:35.293112 31521 leveldb.cpp:438] Reading position from leveldb took 325638ns
I0917 23:14:35.301774 31521 leveldb.cpp:343] Persisting action (14 bytes) to leveldb took 8.576338ms
I0917 23:14:35.301916 31521 replica.cpp:676] Persisted action at 0
I0917 23:14:35.302289 31521 replica.cpp:655] Replica received learned notice for position 0
I0917 23:14:35.310542 31521 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 8.087789ms
I0917 23:14:35.310675 31521 replica.cpp:676] Persisted action at 0
I0917 23:14:35.310946 31521 replica.cpp:661] Replica learned NOP action at position 0
I0917 23:14:35.311254 31521 log.cpp:672] Writer started with ending position 0
I0917 23:14:35.311957 31521 leveldb.cpp:438] Reading position from leveldb took 35110ns
I0917 23:14:35.320283 31521 registrar.cpp:346] Successfully fetched the registry (0B)
I0917 23:14:35.320513 31521 registrar.cpp:422] Attempting to update the 'registry'
I0917 23:14:35.322226 31525 log.cpp:680] Attempting to append 118 bytes to the log
I0917 23:14:35.322549 31525 coordinator.cpp:340] Coordinator attempting to write APPEND action at position 1
I0917 23:14:35.322931 31525 replica.cpp:508] Replica received write request for position 1
I0917 23:14:35.330169 31525 leveldb.cpp:343] Persisting action (135 bytes) to leveldb took 7.133053ms
I0917 23:14:35.330340 31525 replica.cpp:676] Persisted action at 1
I0917 23:14:35.330890 31525 replica.cpp:655] Replica received learned notice for position 1
I0917 23:14:35.339218 31525 leveldb.cpp:343] Persisting action (137 bytes) to leveldb took 8.192024ms
I0917 23:14:35.339380 31525 replica.cpp:676] Persisted action at 1
I0917 23:14:35.339715 31525 replica.cpp:661] Replica learned APPEND action at position 1
I0917 23:14:35.340615 31525 registrar.cpp:479] Successfully updated 'registry'
I0917 23:14:35.340802 31525 registrar.cpp:372] Successfully recovered registrar
I0917 23:14:35.341104 31525 log.cpp:699] Attempting to truncate the log to 1
I0917 23:14:35.341351 31525 master.cpp:1069] Recovered 0 slaves from the Registry (82B) ; allowing 10mins for slaves to re-register
I0917 23:14:35.341527 31525 coordinator.cpp:340] Coordinator attempting to write TRUNCATE action at position 2
I0917 23:14:35.341964 31525 replica.cpp:508] Replica received write request for position 2
I0917 23:14:35.352336 31525 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 10.213086ms
I0917 23:14:35.352494 31525 replica.cpp:676] Persisted action at 2
I0917 23:14:35.356258 31523 replica.cpp:655] Replica received learned notice for position 2
I0917 23:14:35.364992 31523 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 8.606522ms
I0917 23:14:35.365166 31523 leveldb.cpp:401] Deleting ~1 keys from leveldb took 48378ns
I0917 23:14:35.365404 31523 replica.cpp:676] Persisted action at 2
I0917 23:14:35.365537 31523 replica.cpp:661] Replica learned TRUNCATE action at position 2
I0917 23:14:35.568366 31523 slave.cpp:994] Will retry registration in 423.208575ms if necessary
I0917 23:14:35.568840 31522 master.cpp:2870] Registering slave at slave(3)@127.0.1.1:34609 (saucy) with id 20140917-231435-16842879-34609-31503-0
I0917 23:14:35.569422 31522 registrar.cpp:422] Attempting to update the 'registry'
I0917 23:14:35.572013 31522 log.cpp:680] Attempting to append 289 bytes to the log
I0917 23:14:35.572273 31519 coordinator.cpp:340] Coordinator attempting to write APPEND action at position 3
I0917 23:14:35.572816 31519 replica.cpp:508] Replica received write request for position 3
I0917 23:14:35.579784 31519 leveldb.cpp:343] Persisting action (308 bytes) to leveldb took 6.809365ms
I0917 23:14:35.579907 31519 replica.cpp:676] Persisted action at 3
I0917 23:14:35.580512 31519 replica.cpp:655] Replica received learned notice for position 3
I0917 23:14:35.588748 31519 leveldb.cpp:343] Persisting action (310 bytes) to leveldb took 8.112519ms
I0917 23:14:35.588888 31519 replica.cpp:676] Persisted action at 3
I0917 23:14:35.588985 31519 replica.cpp:661] Replica learned APPEND action at position 3
I0917 23:14:35.589754 31519 registrar.cpp:479] Successfully updated 'registry'
I0917 23:14:35.590070 31519 master.cpp:2910] Registered slave 20140917-231435-16842879-34609-31503-0 at slave(3)@127.0.1.1:34609 (saucy)
I0917 23:14:35.590255 31519 master.cpp:4118] Adding slave 20140917-231435-16842879-34609-31503-0 at slave(3)@127.0.1.1:34609 (saucy) with cpus(*):1; mem(*):1001; disk(*):24988; ports(*):[31000-32000]
I0917 23:14:35.590831 31519 slave.cpp:765] Registered with master master@127.0.1.1:34609; given slave ID 20140917-231435-16842879-34609-31503-0
I0917 23:14:35.589913 31523 log.cpp:699] Attempting to truncate the log to 3
I0917 23:14:35.591414 31523 coordinator.cpp:340] Coordinator attempting to write TRUNCATE action at position 4
I0917 23:14:35.591815 31523 replica.cpp:508] Replica received write request for position 4
I0917 23:14:35.591117 31521 hierarchical_allocator_process.hpp:442] Added slave 20140917-231435-16842879-34609-31503-0 (saucy) with cpus(*):1; mem(*):1001; disk(*):24988; ports(*):[31000-32000] (and cpus(*):1; mem(*):1001; disk(*):24988; ports(*):[31000-32000] available)
I0917 23:14:35.592293 31521 hierarchical_allocator_process.hpp:679] Performed allocation for slave 20140917-231435-16842879-34609-31503-0 in 64364ns
I0917 23:14:35.592953 31519 slave.cpp:778] Checkpointing SlaveInfo to '/tmp/mesos-w8snRW/2/meta/slaves/20140917-231435-16842879-34609-31503-0/slave.info'
I0917 23:14:35.593475 31519 slave.cpp:2347] Received ping from slave-observer(1)@127.0.1.1:34609
I0917 23:14:35.601356 31523 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 9.420461ms
I0917 23:14:35.601539 31523 replica.cpp:676] Persisted action at 4
I0917 23:14:35.602325 31523 replica.cpp:655] Replica received learned notice for position 4
I0917 23:14:35.610779 31523 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 8.34398ms
I0917 23:14:35.611114 31523 leveldb.cpp:401] Deleting ~2 keys from leveldb took 66521ns
I0917 23:14:35.611554 31523 replica.cpp:676] Persisted action at 4
I0917 23:14:35.611690 31523 replica.cpp:661] Replica learned TRUNCATE action at position 4
I0917 23:14:36.033941 31523 slave.cpp:994] Will retry registration in 322.705631ms if necessary
I0917 23:14:36.034276 31521 master.cpp:2870] Registering slave at slave(1)@127.0.1.1:34609 (saucy) with id 20140917-231435-16842879-34609-31503-1
I0917 23:14:36.034536 31521 registrar.cpp:422] Attempting to update the 'registry'
I0917 23:14:36.035889 31521 log.cpp:680] Attempting to append 454 bytes to the log
I0917 23:14:36.036099 31524 coordinator.cpp:340] Coordinator attempting to write APPEND action at position 5
I0917 23:14:36.036416 31524 replica.cpp:508] Replica received write request for position 5
I0917 23:14:36.046672 31524 leveldb.cpp:343] Persisting action (473 bytes) to leveldb took 10.160627ms
I0917 23:14:36.047035 31524 replica.cpp:676] Persisted action at 5
I0917 23:14:36.047613 31524 replica.cpp:655] Replica received learned notice for position 5
I0917 23:14:36.053006 31524 leveldb.cpp:343] Persisting action (475 bytes) to leveldb took 5.180742ms
I0917 23:14:36.053246 31524 replica.cpp:676] Persisted action at 5
I0917 23:14:36.053678 31524 replica.cpp:661] Replica learned APPEND action at position 5
I0917 23:14:36.060384 31524 registrar.cpp:479] Successfully updated 'registry'
I0917 23:14:36.061328 31524 master.cpp:2910] Registered slave 20140917-231435-16842879-34609-31503-1 at slave(1)@127.0.1.1:34609 (saucy)
I0917 23:14:36.061537 31524 master.cpp:4118] Adding slave 20140917-231435-16842879-34609-31503-1 at slave(1)@127.0.1.1:34609 (saucy) with cpus(*):1; mem(*):1001; disk(*):24988; ports(*):[31000-32000]
I0917 23:14:36.061982 31524 slave.cpp:765] Registered with master master@127.0.1.1:34609; given slave ID 20140917-231435-16842879-34609-31503-1
I0917 23:14:36.062891 31524 slave.cpp:778] Checkpointing SlaveInfo to '/tmp/mesos-w8snRW/0/meta/slaves/20140917-231435-16842879-34609-31503-1/slave.info'
I0917 23:14:36.061050 31525 log.cpp:699] Attempting to truncate the log to 5
I0917 23:14:36.063244 31525 coordinator.cpp:340] Coordinator attempting to write TRUNCATE action at position 6
I0917 23:14:36.063746 31525 replica.cpp:508] Replica received write request for position 6
I0917 23:14:36.062386 31520 hierarchical_allocator_process.hpp:442] Added slave 20140917-231435-16842879-34609-31503-1 (saucy) with cpus(*):1; mem(*):1001; disk(*):24988; ports(*):[31000-32000] (and cpus(*):1; mem(*):1001; disk(*):24988; ports(*):[31000-32000] available)
I0917 23:14:36.064352 31520 hierarchical_allocator_process.hpp:679] Performed allocation for slave 20140917-231435-16842879-34609-31503-1 in 35730ns
I0917 23:14:36.065166 31524 slave.cpp:2347] Received ping from slave-observer(2)@127.0.1.1:34609
I0917 23:14:36.070137 31525 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 6.242192ms
I0917 23:14:36.070355 31525 replica.cpp:676] Persisted action at 6
I0917 23:14:36.071005 31525 replica.cpp:655] Replica received learned notice for position 6
I0917 23:14:36.076560 31525 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 5.368532ms
I0917 23:14:36.077137 31525 leveldb.cpp:401] Deleting ~2 keys from leveldb took 371245ns
I0917 23:14:36.077241 31525 replica.cpp:676] Persisted action at 6
I0917 23:14:36.077345 31525 replica.cpp:661] Replica learned TRUNCATE action at position 6
I0917 23:14:36.141270 31522 slave.cpp:994] Will retry registration in 1.857205901secs if necessary
I0917 23:14:36.141644 31522 master.cpp:2870] Registering slave at slave(2)@127.0.1.1:34609 (saucy) with id 20140917-231435-16842879-34609-31503-2
I0917 23:14:36.141930 31522 registrar.cpp:422] Attempting to update the 'registry'
I0917 23:14:36.143316 31521 log.cpp:680] Attempting to append 619 bytes to the log
I0917 23:14:36.143646 31521 coordinator.cpp:340] Coordinator attempting to write APPEND action at position 7
I0917 23:14:36.143954 31521 replica.cpp:508] Replica received write request for position 7
I0917 23:14:36.148875 31521 leveldb.cpp:343] Persisting action (638 bytes) to leveldb took 4.787834ms
I0917 23:14:36.149085 31521 replica.cpp:676] Persisted action at 7
I0917 23:14:36.149673 31521 replica.cpp:655] Replica received learned notice for position 7
I0917 23:14:36.155232 31521 leveldb.cpp:343] Persisting action (640 bytes) to leveldb took 5.472209ms
I0917 23:14:36.155522 31521 replica.cpp:676] Persisted action at 7
I0917 23:14:36.155936 31521 replica.cpp:661] Replica learned APPEND action at position 7
I0917 23:14:36.156481 31521 registrar.cpp:479] Successfully updated 'registry'
I0917 23:14:36.156663 31526 log.cpp:699] Attempting to truncate the log to 7
I0917 23:14:36.156813 31526 coordinator.cpp:340] Coordinator attempting to write TRUNCATE action at position 8
I0917 23:14:36.157155 31526 replica.cpp:508] Replica received write request for position 8
I0917 23:14:36.157510 31520 master.cpp:2910] Registered slave 20140917-231435-16842879-34609-31503-2 at slave(2)@127.0.1.1:34609 (saucy)
I0917 23:14:36.157645 31520 master.cpp:4118] Adding slave 20140917-231435-16842879-34609-31503-2 at slave(2)@127.0.1.1:34609 (saucy) with cpus(*):1; mem(*):1001; disk(*):24988; ports(*):[31000-32000]
I0917 23:14:36.157928 31520 slave.cpp:765] Registered with master master@127.0.1.1:34609; given slave ID 20140917-231435-16842879-34609-31503-2
I0917 23:14:36.158304 31520 slave.cpp:778] Checkpointing SlaveInfo to '/tmp/mesos-w8snRW/1/meta/slaves/20140917-231435-16842879-34609-31503-2/slave.info'
I0917 23:14:36.158685 31520 hierarchical_allocator_process.hpp:442] Added slave 20140917-231435-16842879-34609-31503-2 (saucy) with cpus(*):1; mem(*):1001; disk(*):24988; ports(*):[31000-32000] (and cpus(*):1; mem(*):1001; disk(*):24988; ports(*):[31000-32000] available)
I0917 23:14:36.158821 31520 hierarchical_allocator_process.hpp:679] Performed allocation for slave 20140917-231435-16842879-34609-31503-2 in 23287ns
I0917 23:14:36.158965 31520 slave.cpp:2347] Received ping from slave-observer(3)@127.0.1.1:34609
I0917 23:14:36.167183 31526 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 9.894561ms
I0917 23:14:36.167323 31526 replica.cpp:676] Persisted action at 8
I0917 23:14:36.167765 31526 replica.cpp:655] Replica received learned notice for position 8
I0917 23:14:36.177480 31526 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 9.585411ms
I0917 23:14:36.177675 31526 leveldb.cpp:401] Deleting ~2 keys from leveldb took 37564ns
I0917 23:14:36.177973 31526 replica.cpp:676] Persisted action at 8
I0917 23:14:36.178089 31526 replica.cpp:661] Replica learned TRUNCATE action at position 8
I0917 23:14:36.245735 31526 hierarchical_allocator_process.hpp:659] Performed allocation for 3 slaves in 96108ns
I0917 23:14:37.246182 31519 hierarchical_allocator_process.hpp:659] Performed allocation for 3 slaves in 83121ns
I0917 23:14:38.246640 31519 hierarchical_allocator_process.hpp:659] Performed allocation for 3 slaves in 126479ns
I0917 23:14:39.247378 31526 hierarchical_allocator_process.hpp:659] Performed allocation for 3 slaves in 81524ns
I0917 23:14:39.895262 31488 exec.cpp:86] Committing suicide by killing the process group
I0917 23:14:39.900475 31494 exec.cpp:86] Committing suicide by killing the process group
I0917 23:14:39.904479 31482 exec.cpp:86] Committing suicide by killing the process group
I0917 23:14:40.246654 31520 master.cpp:120] No whitelist given. Advertising offers for all slaves
I0917 23:14:40.247879 31524 hierarchical_allocator_process.hpp:659] Performed allocation for 3 slaves in 84970ns
W0917 23:14:40.283375 31522 sched.cpp:378] Authentication timed out
I0917 23:14:40.283819 31522 sched.cpp:338] Failed to authenticate with master master@127.0.1.1:34609: Authentication discarded
I0917 23:14:40.283980 31522 sched.cpp:283] Authenticating with master master@127.0.1.1:34609
I0917 23:14:40.284317 31522 authenticatee.hpp:128] Creating new client SASL connection
I0917 23:14:40.284855 31522 master.cpp:3669] Authenticating scheduler-4c79cf12-ea1b-49e5-bae1-d34f91605227@127.0.1.1:34609
I0917 23:14:40.285302 31522 authenticator.hpp:94] Initializing server SASL
I0917 23:14:40.285907 31522 auxprop.cpp:45] Initialized in-memory auxiliary property plugin
I0917 23:14:40.286022 31522 authenticator.hpp:156] Creating new server SASL connection
I0917 23:14:40.286351 31525 authenticatee.hpp:219] Received SASL authentication mechanisms: CRAM-MD5
I0917 23:14:40.286486 31525 authenticatee.hpp:245] Attempting to authenticate with mechanism 'CRAM-MD5'
I0917 23:14:40.286731 31522 authenticator.hpp:262] Received SASL authentication start
I0917 23:14:40.286923 31522 authenticator.hpp:384] Authentication requires more steps
I0917 23:14:40.287080 31524 authenticatee.hpp:265] Received SASL authentication step
I0917 23:14:40.287230 31522 authenticator.hpp:290] Received SASL authentication step
I0917 23:14:40.287385 31522 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'saucy' server FQDN: 'saucy' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0917 23:14:40.287508 31522 auxprop.cpp:153] Looking up auxiliary property '*userPassword'
I0917 23:14:40.287664 31522 auxprop.cpp:153] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0917 23:14:40.287777 31522 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'saucy' server FQDN: 'saucy' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0917 23:14:40.287912 31522 auxprop.cpp:103] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0917 23:14:40.288020 31522 auxprop.cpp:103] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0917 23:14:40.288214 31522 authenticator.hpp:376] Authentication success
I0917 23:14:40.288396 31526 authenticatee.hpp:305] Authentication success
I0917 23:14:40.291566 31526 sched.cpp:357] Successfully authenticated with master master@127.0.1.1:34609
I0917 23:14:40.291960 31521 master.cpp:3709] Successfully authenticated principal 'test-principal' at scheduler-4c79cf12-ea1b-49e5-bae1-d34f91605227@127.0.1.1:34609
I0917 23:14:40.292358 31526 sched.cpp:476] Sending registration request to master@127.0.1.1:34609
I0917 23:14:40.292563 31525 master.cpp:1330] Received registration request from scheduler-4c79cf12-ea1b-49e5-bae1-d34f91605227@127.0.1.1:34609
I0917 23:14:40.292762 31525 master.cpp:1290] Authorizing framework principal 'test-principal' to receive offers for role '*'
I0917 23:14:40.293253 31525 master.cpp:1389] Registering framework 20140917-231435-16842879-34609-31503-0000 at scheduler-4c79cf12-ea1b-49e5-bae1-d34f91605227@127.0.1.1:34609
I0917 23:14:40.293767 31525 hierarchical_allocator_process.hpp:329] Added framework 20140917-231435-16842879-34609-31503-0000
I0917 23:14:40.294075 31525 hierarchical_allocator_process.hpp:734] Offering cpus(*):1; mem(*):1001; disk(*):24988; ports(*):[31000-32000] on slave 20140917-231435-16842879-34609-31503-0 to framework 20140917-231435-16842879-34609-31503-0000
I0917 23:14:40.294450 31525 hierarchical_allocator_process.hpp:734] Offering cpus(*):1; mem(*):1001; disk(*):24988; ports(*):[31000-32000] on slave 20140917-231435-16842879-34609-31503-1 to framework 20140917-231435-16842879-34609-31503-0000
I0917 23:14:40.294659 31525 hierarchical_allocator_process.hpp:734] Offering cpus(*):1; mem(*):1001; disk(*):24988; ports(*):[31000-32000] on slave 20140917-231435-16842879-34609-31503-2 to framework 20140917-231435-16842879-34609-31503-0000
I0917 23:14:40.295035 31520 master.hpp:862] Adding offer 20140917-231435-16842879-34609-31503-0 with resources cpus(*):1; mem(*):1001; disk(*):24988; ports(*):[31000-32000] on slave 20140917-231435-16842879-34609-31503-0 (saucy)
I0917 23:14:40.295318 31520 master.hpp:862] Adding offer 20140917-231435-16842879-34609-31503-1 with resources cpus(*):1; mem(*):1001; disk(*):24988; ports(*):[31000-32000] on slave 20140917-231435-16842879-34609-31503-2 (saucy)
I0917 23:14:40.295614 31520 master.hpp:862] Adding offer 20140917-231435-16842879-34609-31503-2 with resources cpus(*):1; mem(*):1001; disk(*):24988; ports(*):[31000-32000] on slave 20140917-231435-16842879-34609-31503-1 (saucy)
I0917 23:14:40.295778 31520 master.cpp:3616] Sending 3 offers to framework 20140917-231435-16842879-34609-31503-0000
I0917 23:14:40.295984 31525 hierarchical_allocator_process.hpp:659] Performed allocation for 3 slaves in 1.975722ms
I0917 23:14:40.296185 31526 sched.cpp:407] Framework registered with 20140917-231435-16842879-34609-31503-0000
Registered! ID = 20140917-231435-16842879-34609-31503-0000
I0917 23:14:40.317849 31526 sched.cpp:421] Scheduler::registered took 21.576128ms
Launching task 0
Launching task 1
Launching task 2
I0917 23:14:40.365347 31526 sched.cpp:544] Scheduler::resourceOffers took 47.086567ms
I0917 23:14:40.365958 31523 master.hpp:871] Removing offer 20140917-231435-16842879-34609-31503-0 with resources cpus(*):1; mem(*):1001; disk(*):24988; ports(*):[31000-32000] on slave 20140917-231435-16842879-34609-31503-0 (saucy)
I0917 23:14:40.366112 31523 master.cpp:2228] Processing reply for offers: [ 20140917-231435-16842879-34609-31503-0 ] on slave 20140917-231435-16842879-34609-31503-0 at slave(3)@127.0.1.1:34609 (saucy) for framework 20140917-231435-16842879-34609-31503-0000
W0917 23:14:40.366549 31523 master.cpp:1898] Executor default for task 0 uses less CPUs (None) than the minimum required (0.01). Please update your executor, as this will be mandatory in future releases.
W0917 23:14:40.366693 31523 master.cpp:1909] Executor default for task 0 uses less memory (None) than the minimum required (32MB). Please update your executor, as this will be mandatory in future releases.
I0917 23:14:40.366950 31523 master.cpp:2311] Authorizing framework principal 'test-principal' to launch task 0 as user 'jenkins'
I0917 23:14:40.367545 31523 master.hpp:839] Adding task 0 with resources cpus(*):1; mem(*):128 on slave 20140917-231435-16842879-34609-31503-0 (saucy)
I0917 23:14:40.367696 31523 master.cpp:2377] Launching task 0 of framework 20140917-231435-16842879-34609-31503-0000 with resources cpus(*):1; mem(*):128 on slave 20140917-231435-16842879-34609-31503-0 at slave(3)@127.0.1.1:34609 (saucy)
I0917 23:14:40.367955 31524 slave.cpp:1025] Got assigned task 0 for framework 20140917-231435-16842879-34609-31503-0000
I0917 23:14:40.368505 31524 slave.cpp:1135] Launching task 0 for framework 20140917-231435-16842879-34609-31503-0000
I0917 23:14:40.370308 31522 hierarchical_allocator_process.hpp:563] Recovered mem(*):873; disk(*):24988; ports(*):[31000-32000] (total allocatable: mem(*):873; disk(*):24988; ports(*):[31000-32000]) on slave 20140917-231435-16842879-34609-31503-0 from framework 20140917-231435-16842879-34609-31503-0000
I0917 23:14:40.370450 31522 hierarchical_allocator_process.hpp:599] Framework 20140917-231435-16842879-34609-31503-0000 filtered slave 20140917-231435-16842879-34609-31503-0 for 1secs
I0917 23:14:40.371031 31520 containerizer.cpp:394] Starting container 'a70a0cfd-ee78-43a0-b0b7-26bcd205f142' for executor 'default' of framework '20140917-231435-16842879-34609-31503-0000'
I0917 23:14:40.373828 31524 slave.cpp:1248] Queuing task '0' for executor default of framework '20140917-231435-16842879-34609-31503-0000
I0917 23:14:40.374595 31524 slave.cpp:554] Successfully attached file '/tmp/mesos-w8snRW/2/slaves/20140917-231435-16842879-34609-31503-0/frameworks/20140917-231435-16842879-34609-31503-0000/executors/default/runs/a70a0cfd-ee78-43a0-b0b7-26bcd205f142'
I0917 23:14:40.374486 31520 launcher.cpp:137] Forked child with pid '31534' for container 'a70a0cfd-ee78-43a0-b0b7-26bcd205f142'
I0917 23:14:40.375830 31520 containerizer.cpp:510] Fetching URIs for container 'a70a0cfd-ee78-43a0-b0b7-26bcd205f142' using command '/var/jenkins/workspace/mesos-ubuntu-13.10-clang/src/mesos-fetcher'
I0917 23:14:40.377596 31523 master.hpp:871] Removing offer 20140917-231435-16842879-34609-31503-1 with resources cpus(*):1; mem(*):1001; disk(*):24988; ports(*):[31000-32000] on slave 20140917-231435-16842879-34609-31503-2 (saucy)
I0917 23:14:40.377781 31523 master.cpp:2228] Processing reply for offers: [ 20140917-231435-16842879-34609-31503-1 ] on slave 20140917-231435-16842879-34609-31503-2 at slave(2)@127.0.1.1:34609 (saucy) for framework 20140917-231435-16842879-34609-31503-0000
W0917 23:14:40.378062 31523 master.cpp:1898] Executor default for task 1 uses less CPUs (None) than the minimum required (0.01). Please update your executor, as this will be mandatory in future releases.
W0917 23:14:40.378233 31523 master.cpp:1909] Executor default for task 1 uses less memory (None) than the minimum required (32MB). Please update your executor, as this will be mandatory in future releases.
I0917 23:14:40.378429 31523 master.cpp:2311] Authorizing framework principal 'test-principal' to launch task 1 as user 'jenkins'
I0917 23:14:40.379017 31523 master.hpp:839] Adding task 1 with resources cpus(*):1; mem(*):128 on slave 20140917-231435-16842879-34609-31503-2 (saucy)
I0917 23:14:40.379166 31523 master.cpp:2377] Launching task 1 of framework 20140917-231435-16842879-34609-31503-0000 with resources cpus(*):1; mem(*):128 on slave 20140917-231435-16842879-34609-31503-2 at slave(2)@127.0.1.1:34609 (saucy)
I0917 23:14:40.379525 31525 slave.cpp:1025] Got assigned task 1 for framework 20140917-231435-16842879-34609-31503-0000
I0917 23:14:40.379914 31525 slave.cpp:1135] Launching task 1 for framework 20140917-231435-16842879-34609-31503-0000
I0917 23:14:40.381691 31519 containerizer.cpp:394] Starting container '1c6cf8b1-972b-4b73-8467-bc4503dd9332' for executor 'default' of framework '20140917-231435-16842879-34609-31503-0000'
I0917 23:14:40.383585 31525 slave.cpp:1248] Queuing task '1' for executor default of framework '20140917-231435-16842879-34609-31503-0000
I0917 23:14:40.384318 31522 hierarchical_allocator_process.hpp:563] Recovered mem(*):873; disk(*):24988; ports(*):[31000-32000] (total allocatable: mem(*):873; disk(*):24988; ports(*):[31000-32000]) on slave 20140917-231435-16842879-34609-31503-2 from framework 20140917-231435-16842879-34609-31503-0000
I0917 23:14:40.384598 31522 hierarchical_allocator_process.hpp:599] Framework 20140917-231435-16842879-34609-31503-0000 filtered slave 20140917-231435-16842879-34609-31503-2 for 1secs
I0917 23:14:40.384814 31525 slave.cpp:554] Successfully attached file '/tmp/mesos-w8snRW/1/slaves/20140917-231435-16842879-34609-31503-2/frameworks/20140917-231435-16842879-34609-31503-0000/executors/default/runs/1c6cf8b1-972b-4b73-8467-bc4503dd9332'
I0917 23:14:40.385326 31526 master.hpp:871] Removing offer 20140917-231435-16842879-34609-31503-2 with resources cpus(*):1; mem(*):1001; disk(*):24988; ports(*):[31000-32000] on slave 20140917-231435-16842879-34609-31503-1 (saucy)
I0917 23:14:40.385509 31526 master.cpp:2228] Processing reply for offers: [ 20140917-231435-16842879-34609-31503-2 ] on slave 20140917-231435-16842879-34609-31503-1 at slave(1)@127.0.1.1:34609 (saucy) for framework 20140917-231435-16842879-34609-31503-0000
W0917 23:14:40.385666 31526 master.cpp:1898] Executor default for task 2 uses less CPUs (None) than the minimum required (0.01). Please update your executor, as this will be mandatory in future releases.
W0917 23:14:40.385856 31526 master.cpp:1909] Executor default for task 2 uses less memory (None) than the minimum required (32MB). Please update your executor, as this will be mandatory in future releases.
I0917 23:14:40.386008 31526 master.cpp:2311] Authorizing framework principal 'test-principal' to launch task 2 as user 'jenkins'
I0917 23:14:40.386518 31526 master.hpp:839] Adding task 2 with resources cpus(*):1; mem(*):128 on slave 20140917-231435-16842879-34609-31503-1 (saucy)
I0917 23:14:40.386818 31519 launcher.cpp:137] Forked child with pid '31536' for container '1c6cf8b1-972b-4b73-8467-bc4503dd9332'
I0917 23:14:40.393805 31519 containerizer.cpp:510] Fetching URIs for container '1c6cf8b1-972b-4b73-8467-bc4503dd9332' using command '/var/jenkins/workspace/mesos-ubuntu-13.10-clang/src/mesos-fetcher'
I0917 23:14:40.395980 31526 master.cpp:2377] Launching task 2 of framework 20140917-231435-16842879-34609-31503-0000 with resources cpus(*):1; mem(*):128 on slave 20140917-231435-16842879-34609-31503-1 at slave(1)@127.0.1.1:34609 (saucy)
I0917 23:14:40.396574 31521 slave.cpp:1025] Got assigned task 2 for framework 20140917-231435-16842879-34609-31503-0000
I0917 23:14:40.396946 31521 slave.cpp:1135] Launching task 2 for framework 20140917-231435-16842879-34609-31503-0000
I0917 23:14:40.398582 31521 slave.cpp:1248] Queuing task '2' for executor default of framework '20140917-231435-16842879-34609-31503-0000
I0917 23:14:40.400519 31521 slave.cpp:554] Successfully attached file '/tmp/mesos-w8snRW/0/slaves/20140917-231435-16842879-34609-31503-1/frameworks/20140917-231435-16842879-34609-31503-0000/executors/default/runs/6add4792-3bc4-4ac9-8225-969f09279561'
I0917 23:14:40.400287 31525 containerizer.cpp:394] Starting container '6add4792-3bc4-4ac9-8225-969f09279561' for executor 'default' of framework '20140917-231435-16842879-34609-31503-0000'
I0917 23:14:40.401595 31522 hierarchical_allocator_process.hpp:563] Recovered mem(*):873; disk(*):24988; ports(*):[31000-32000] (total allocatable: mem(*):873; disk(*):24988; ports(*):[31000-32000]) on slave 20140917-231435-16842879-34609-31503-1 from framework 20140917-231435-16842879-34609-31503-0000
I0917 23:14:40.403118 31522 hierarchical_allocator_process.hpp:599] Framework 20140917-231435-16842879-34609-31503-0000 filtered slave 20140917-231435-16842879-34609-31503-1 for 1secs
I0917 23:14:40.431401 31525 launcher.cpp:137] Forked child with pid '31551' for container '6add4792-3bc4-4ac9-8225-969f09279561'
I0917 23:14:40.436882 31525 containerizer.cpp:510] Fetching URIs for container '6add4792-3bc4-4ac9-8225-969f09279561' using command '/var/jenkins/workspace/mesos-ubuntu-13.10-clang/src/mesos-fetcher'
I0917 23:14:41.248602 31523 hierarchical_allocator_process.hpp:816] Filtered mem(*):873; disk(*):24988; ports(*):[31000-32000] on slave 20140917-231435-16842879-34609-31503-2 for framework 20140917-231435-16842879-34609-31503-0000
I0917 23:14:41.249043 31523 hierarchical_allocator_process.hpp:816] Filtered mem(*):873; disk(*):24988; ports(*):[31000-32000] on slave 20140917-231435-16842879-34609-31503-0 for framework 20140917-231435-16842879-34609-31503-0000
I0917 23:14:41.249259 31523 hierarchical_allocator_process.hpp:816] Filtered mem(*):873; disk(*):24988; ports(*):[31000-32000] on slave 20140917-231435-16842879-34609-31503-1 for framework 20140917-231435-16842879-34609-31503-0000
I0917 23:14:41.249377 31523 hierarchical_allocator_process.hpp:659] Performed allocation for 3 slaves in 1.040603ms
I0917 23:14:41.384321 31526 slave.cpp:2560] Monitoring executor 'default' of framework '20140917-231435-16842879-34609-31503-0000' in container 'a70a0cfd-ee78-43a0-b0b7-26bcd205f142'
I0917 23:14:41.395431 31526 slave.cpp:2560] Monitoring executor 'default' of framework '20140917-231435-16842879-34609-31503-0000' in container '1c6cf8b1-972b-4b73-8467-bc4503dd9332'
I0917 23:14:41.413548 31522 slave.cpp:2560] Monitoring executor 'default' of framework '20140917-231435-16842879-34609-31503-0000' in container '6add4792-3bc4-4ac9-8225-969f09279561'
WARNING: Logging before InitGoogleLogging() is written to STDERR
WARNING: Logging before InitGoogleLogging() is written to STDERR
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0917 23:14:41.935613 31641 process.cpp:1771] libprocess is initialized on 127.0.1.1:40557 for 8 cpus
I0917 23:14:41.938254 31641 logging.cpp:177] Logging to STDERR
I0917 23:14:41.934036 31643 process.cpp:1771] libprocess is initialized on 127.0.1.1:33558 for 8 cpus
I0917 23:14:41.939977 31643 logging.cpp:177] Logging to STDERR
I0917 23:14:41.937566 31644 process.cpp:1771] libprocess is initialized on 127.0.1.1:51898 for 8 cpus
II0917 23:14:41.941807 31644 logging.cpp:177] Logging to STDERR
II0917 23:14:41.943460 31641 exec.cpp:132] Version: 0.21.0
0917 23:14:41.941642 31643 exec.cpp:132] Version: 0.21.0
0917 23:14:41.943018 31644 exec.cpp:132] Version: 0.21.0
II0917 23:14:41.950268 31683 exec.cpp:182] Executor started at: executor(1)@127.0.1.1:40557 with pid 31636
II0917 23:14:41.951695 31522 slave.cpp:1758] Got registration for executor 'default' of framework 20140917-231435-16842879-34609-31503-0000 from executor(1)@127.0.1.1:40557
I0917 23:14:41.952024 31522 slave.cpp:1877] Flushing queued task 0 for executor 'default' of framework 20140917-231435-16842879-34609-31503-0000
I0917 23:14:41.951012 31674 exec.cpp:182] Executor started at: executor(1)@127.0.1.1:33558 with pid 31639
I0917 23:14:41.954056 31526 slave.cpp:1758] Got registration for executor 'default' of framework 20140917-231435-16842879-34609-31503-0000 from executor(1)@127.0.1.1:33558
I0917 23:14:41.954582 31526 slave.cpp:1877] Flushing queued task 1 for executor 'default' of framework 20140917-231435-16842879-34609-31503-0000
I0917 23:14:41.950121 31692 exec.cpp:182] Executor started at: executor(1)@127.0.1.1:51898 with pid 31642
I0917 23:14:41.955857 31521 slave.cpp:1758] Got registration for executor 'default' of framework 20140917-231435-16842879-34609-31503-0000 from executor(1)@127.0.1.1:51898
I0917 23:14:41.957639 31521 slave.cpp:1877] Flushing queued task 2 for executor 'default' of framework 20140917-231435-16842879-34609-31503-0000
0917 23:14:41.955232 31676 exec.cpp:206] Executor registered on slave 20140917-231435-16842879-34609-31503-2
0917 23:14:41.953346 31685 exec.cpp:206] Executor registered on slave 20140917-231435-16842879-34609-31503-0
I0917 23:14:41.968797 31694 exec.cpp:206] Executor registered on slave 20140917-231435-16842879-34609-31503-1
Registered executor on saucyRegistered executor on saucyRegistered executor on saucy

II
I0917 23:14:42.169991 31694 exec.cpp:218] Executor::registered took 199.821555ms
I0917 23:14:42.170224 31676 exec.cpp:218] Executor::registered took 208.770192ms
I0917 23:14:42.170574 31685 exec.cpp:218] Executor::registered took 205.42106ms
I0917 23:14:42.171221 31676 exec.cpp:293] Executor asked to run task '1'
0917 23:14:42.170928 31694 exec.cpp:293] Executor asked to run task '2'
0917 23:14:42.171532 31685 exec.cpp:293] Executor asked to run task '0'
III0917 23:14:42.194025 31685 exec.cpp:302] Executor::launchTask took 20.907244ms
0917 23:14:42.194463 31694 exec.cpp:302] Executor::launchTask took 22.463085ms
0917 23:14:42.193789 31676 exec.cpp:302] Executor::launchTask took 22.090512ms
I0917 23:14:42.249930 31520 hierarchical_allocator_process.hpp:734] Offering mem(*):873; disk(*):24988; ports(*):[31000-32000] on slave 20140917-231435-16842879-34609-31503-0 to framework 20140917-231435-16842879-34609-31503-0000
I0917 23:14:42.250046 31520 hierarchical_allocator_process.hpp:734] Offering mem(*):873; disk(*):24988; ports(*):[31000-32000] on slave 20140917-231435-16842879-34609-31503-1 to framework 20140917-231435-16842879-34609-31503-0000
I0917 23:14:42.250120 31520 hierarchical_allocator_process.hpp:734] Offering mem(*):873; disk(*):24988; ports(*):[31000-32000] on slave 20140917-231435-16842879-34609-31503-2 to framework 20140917-231435-16842879-34609-31503-0000
I0917 23:14:42.250221 31520 hierarchical_allocator_process.hpp:659] Performed allocation for 3 slaves in 427470ns
I0917 23:14:42.250301 31520 master.hpp:862] Adding offer 20140917-231435-16842879-34609-31503-3 with resources mem(*):873; disk(*):24988; ports(*):[31000-32000] on slave 20140917-231435-16842879-34609-31503-0 (saucy)
I0917 23:14:42.250375 31520 master.hpp:862] Adding offer 20140917-231435-16842879-34609-31503-4 with resources mem(*):873; disk(*):24988; ports(*):[31000-32000] on slave 20140917-231435-16842879-34609-31503-2 (saucy)
I0917 23:14:42.250427 31520 master.hpp:862] Adding offer 20140917-231435-16842879-34609-31503-5 with resources mem(*):873; disk(*):24988; ports(*):[31000-32000] on slave 20140917-231435-16842879-34609-31503-1 (saucy)
I0917 23:14:42.250454 31520 master.cpp:3616] Sending 3 offers to framework 20140917-231435-16842879-34609-31503-0000
Launching task 3
Launching task 4
I0917 23:14:42.256711 31520 sched.cpp:544] Scheduler::resourceOffers took 6.128709ms
I0917 23:14:42.257067 31519 master.hpp:871] Removing offer 20140917-231435-16842879-34609-31503-3 with resources mem(*):873; disk(*):24988; ports(*):[31000-32000] on slave 20140917-231435-16842879-34609-31503-0 (saucy)
I0917 23:14:42.259182 31519 master.cpp:2228] Processing reply for offers: [ 20140917-231435-16842879-34609-31503-3 ] on slave 20140917-231435-16842879-34609-31503-0 at slave(3)@127.0.1.1:34609 (saucy) for framework 20140917-231435-16842879-34609-31503-0000
II0917 23:14:42.263245 31519 master.cpp:3234] Sending status update TASK_LOST (UUID: 6c082b40-ec70-4c8a-a1b8-5580374e3340) for task 3 of framework 20140917-231435-16842879-34609-31503-0000 'Task 3 attempted to use cpus(*):1; mem(*):128 which is greater than offered mem(*):873; disk(*):24988; ports(*):[31000-32000]'
I0917 23:14:42.263723 31523 hierarchical_allocator_process.hpp:563] Recovered mem(*):873; disk(*):24988; ports(*):[31000-32000] (total allocatable: mem(*):873; disk(*):24988; ports(*):[31000-32000]) on slave 20140917-231435-16842879-34609-31503-0 from framework 20140917-231435-16842879-34609-31503-0000
I0917 23:14:42.264802 31523 hierarchical_allocator_process.hpp:599] Framework 20140917-231435-16842879-34609-31503-0000 filtered slave 20140917-231435-16842879-34609-31503-0 for 1secs
I0917 23:14:42.265305 31522 master.hpp:871] Removing offer 20140917-231435-16842879-34609-31503-4 with resources mem(*):873; disk(*):24988; ports(*):[31000-32000] on slave 20140917-231435-16842879-34609-31503-2 (saucy)
II0917 23:14:42.266269 31522 master.cpp:2228] Processing reply for offers: [ 20140917-231435-16842879-34609-31503-4 ] on slave 20140917-231435-16842879-34609-31503-2 at slave(2)@127.0.1.1:34609 (saucy) for framework 20140917-231435-16842879-34609-31503-0000
I0917 23:14:42.266079 31689 exec.cpp:525] Executor sending status update TASK_RUNNING (UUID: d4ac4252-ea9d-4c1d-b8c9-ea2d9704e6ad) for task 2 of framework 20140917-231435-16842879-34609-31503-0000
0917 23:14:42.267781 31672 exec.cpp:525] Executor sending status update TASK_RUNNING (UUID: 3a405ad5-f8f8-4a7d-9f3d-ba0d02ab4cdb) for task 1 of framework 20140917-231435-16842879-34609-31503-0000
0917 23:14:42.262806 31678 exec.cpp:525] Executor sending status update TASK_RUNNING (UUID: 774aa76c-87bd-4d39-a5e2-d880206fedb3) for task 0 of framework 20140917-231435-16842879-34609-31503-0000
I0917 23:14:42.276619 31521 slave.cpp:2111] Handling status update TASK_RUNNING (UUID: 3a405ad5-f8f8-4a7d-9f3d-ba0d02ab4cdb) for task 1 of framework 20140917-231435-16842879-34609-31503-0000 from executor(1)@127.0.1.1:33558
I0917 23:14:42.277762 31521 status_update_manager.cpp:320] Received status update TASK_RUNNING (UUID: 3a405ad5-f8f8-4a7d-9f3d-ba0d02ab4cdb) for task 1 of framework 20140917-231435-16842879-34609-31503-0000
I0917 23:14:42.277570 31525 slave.cpp:2111] Handling status update TASK_RUNNING (UUID: 774aa76c-87bd-4d39-a5e2-d880206fedb3) for task 0 of framework 20140917-231435-16842879-34609-31503-0000 from executor(1)@127.0.1.1:40557
I0917 23:14:42.277437 31526 slave.cpp:2111] Handling status update TASK_RUNNING (UUID: d4ac4252-ea9d-4c1d-b8c9-ea2d9704e6ad) for task 2 of framework 20140917-231435-16842879-34609-31503-0000 from executor(1)@127.0.1.1:51898
I0917 23:14:42.280566 31526 status_update_manager.cpp:320] Received status update TASK_RUNNING (UUID: d4ac4252-ea9d-4c1d-b8c9-ea2d9704e6ad) for task 2 of framework 20140917-231435-16842879-34609-31503-0000
I0917 23:14:42.280719 31526 status_update_manager.cpp:499] Creating StatusUpdate stream for task 2 of framework 20140917-231435-16842879-34609-31503-0000
I0917 23:14:42.280987 31526 status_update_manager.cpp:373] Forwarding status update TASK_RUNNING (UUID: d4ac4252-ea9d-4c1d-b8c9-ea2d9704e6ad) for task 2 of framework 20140917-231435-16842879-34609-31503-0000 to master@127.0.1.1:34609
I0917 23:14:42.281231 31526 slave.cpp:2268] Status update manager successfully handled status update TASK_RUNNING (UUID: d4ac4252-ea9d-4c1d-b8c9-ea2d9704e6ad) for task 2 of framework 20140917-231435-16842879-34609-31503-0000
I0917 23:14:42.281352 31526 slave.cpp:2274] Sending acknowledgement for status update TASK_RUNNING (UUID: d4ac4252-ea9d-4c1d-b8c9-ea2d9704e6ad) for task 2 of framework 20140917-231435-16842879-34609-31503-0000 to executor(1)@127.0.1.1:51898
I0917 23:14:42.280252 31525 status_update_manager.cpp:320] Received status update TASK_RUNNING (UUID: 774aa76c-87bd-4d39-a5e2-d880206fedb3) for task 0 of framework 20140917-231435-16842879-34609-31503-0000
I0917 23:14:42.281714 31525 status_update_manager.cpp:499] Creating StatusUpdate stream for task 0 of framework 20140917-231435-16842879-34609-31503-0000
I0917 23:14:42.281883 31525 status_update_manager.cpp:373] Forwarding status update TASK_RUNNING (UUID: 774aa76c-87bd-4d39-a5e2-d880206fedb3) for task 0 of framework 20140917-231435-16842879-34609-31503-0000 to master@127.0.1.1:34609
I0917 23:14:42.282068 31525 slave.cpp:2268] Status update manager successfully handled status update TASK_RUNNING (UUID: 774aa76c-87bd-4d39-a5e2-d880206fedb3) for task 0 of framework 20140917-231435-16842879-34609-31503-0000
I0917 23:14:42.282349 31525 slave.cpp:2274] Sending acknowledgement for status update TASK_RUNNING (UUID: 774aa76c-87bd-4d39-a5e2-d880206fedb3) for task 0 of framework 20140917-231435-16842879-34609-31503-0000 to executor(1)@127.0.1.1:40557
I0917 23:14:42.282624 31521 status_update_manager.cpp:499] Creating StatusUpdate stream for task 1 of framework 20140917-231435-16842879-34609-31503-0000
I0917 23:14:42.282802 31521 status_update_manager.cpp:373] Forwarding status update TASK_RUNNING (UUID: 3a405ad5-f8f8-4a7d-9f3d-ba0d02ab4cdb) for task 1 of framework 20140917-231435-16842879-34609-31503-0000 to master@127.0.1.1:34609
I0917 23:14:42.282940 31522 master.cpp:3234] Sending status update TASK_LOST (UUID: 24d8174b-ab65-47d0-8e4b-9ad1b5c05030) for task 4 of framework 20140917-231435-16842879-34609-31503-0000 'Task 4 attempted to use cpus(*):1; mem(*):128 which is greater than offered mem(*):873; disk(*):24988; ports(*):[31000-32000]'
I0917 23:14:42.283248 31524 slave.cpp:2268] Status update manager successfully handled status update TASK_RUNNING (UUID: 3a405ad5-f8f8-4a7d-9f3d-ba0d02ab4cdb) for task 1 of framework 20140917-231435-16842879-34609-31503-0000
II0917 23:14:42.283634 31524 slave.cpp:2274] Sending acknowledgement for status update TASK_RUNNING (UUID: 3a405ad5-f8f8-4a7d-9f3d-ba0d02ab4cdb) for task 1 of framework 20140917-231435-16842879-34609-31503-0000 to executor(1)@127.0.1.1:33558
I0917 23:14:42.284031 31522 master.cpp:3239] Forwarding status update TASK_RUNNING (UUID: d4ac4252-ea9d-4c1d-b8c9-ea2d9704e6ad) for task 2 of framework 20140917-231435-16842879-34609-31503-0000
0917 23:14:42.283514 31684 exec.cpp:339] Executor received status update acknowledgement 774aa76c-87bd-4d39-a5e2-d880206fedb3 for task 0 of framework 20140917-231435-16842879-34609-31503-0000
I0917 23:14:42.285044 31522 master.cpp:3205] Status update TASK_RUNNING (UUID: d4ac4252-ea9d-4c1d-b8c9-ea2d9704e6ad) for task 2 of framework 20140917-231435-16842879-34609-31503-0000 from slave 20140917-231435-16842879-34609-31503-1 at slave(1)@127.0.1.1:34609 (saucy)
I0917 23:14:42.284976 31525 hierarchical_allocator_process.hpp:563] Recovered mem(*):873; disk(*):24988; ports(*):[31000-32000] (total allocatable: mem(*):873; disk(*):24988; ports(*):[31000-32000]) on slave 20140917-231435-16842879-34609-31503-2 from framework 20140917-231435-16842879-34609-31503-0000
I0917 23:14:42.285399 31525 hierarchical_allocator_process.hpp:599] Framework 20140917-231435-16842879-34609-31503-0000 filtered slave 20140917-231435-16842879-34609-31503-2 for 1secs
I0917 23:14:42.285539 31522 master.cpp:3239] Forwarding status update TASK_RUNNING (UUID: 774aa76c-87bd-4d39-a5e2-d880206fedb3) for task 0 of framework 20140917-231435-16842879-34609-31503-0000
I0917 23:14:42.286206 31522 master.cpp:3205] Status update TASK_RUNNING (UUID: 774aa76c-87bd-4d39-a5e2-d880206fedb3) for task 0 of framework 20140917-231435-16842879-34609-31503-0000 from slave 20140917-231435-16842879-34609-31503-0 at slave(3)@127.0.1.1:34609 (saucy)
I0917 23:14:42.286468 31522 master.cpp:3239] Forwarding status update TASK_RUNNING (UUID: 3a405ad5-f8f8-4a7d-9f3d-ba0d02ab4cdb) for task 1 of framework 20140917-231435-16842879-34609-31503-0000
I0917 23:14:42.286607 31522 master.cpp:3205] Status update TASK_RUNNING (UUID: 3a405ad5-f8f8-4a7d-9f3d-ba0d02ab4cdb) for task 1 of framework 20140917-231435-16842879-34609-31503-0000 from slave 20140917-231435-16842879-34609-31503-2 at slave(2)@127.0.1.1:34609 (saucy)
I0917 23:14:42.286854 31524 master.hpp:871] Removing offer 20140917-231435-16842879-34609-31503-5 with resources mem(*):873; disk(*):24988; ports(*):[31000-32000] on slave 20140917-231435-16842879-34609-31503-1 (saucy)
II0917 23:14:42.287259 31524 master.cpp:2228] Processing reply for offers: [ 20140917-231435-16842879-34609-31503-5 ] on slave 20140917-231435-16842879-34609-31503-1 at slave(1)@127.0.1.1:34609 (saucy) for framework 20140917-231435-16842879-34609-31503-0000
0917 23:14:42.287132 31671 exec.cpp:339] Executor received status update acknowledgement 3a405ad5-f8f8-4a7d-9f3d-ba0d02ab4cdb for task 1 of framework 20140917-231435-16842879-34609-31503-0000
I0917 23:14:42.287842 31524 hierarchical_allocator_process.hpp:563] Recovered mem(*):873; disk(*):24988; ports(*):[31000-32000] (total allocatable: mem(*):873; disk(*):24988; ports(*):[31000-32000]) on slave 20140917-231435-16842879-34609-31503-1 from framework 20140917-231435-16842879-34609-31503-0000
II0917 23:14:42.291944 31524 hierarchical_allocator_process.hpp:599] Framework 20140917-231435-16842879-34609-31503-0000 filtered slave 20140917-231435-16842879-34609-31503-1 for 1secs
0917 23:14:42.288897 31688 exec.cpp:339] Executor received status update acknowledgement d4ac4252-ea9d-4c1d-b8c9-ea2d9704e6ad for task 2 of framework 20140917-231435-16842879-34609-31503-0000
Status update: task 3 is in state TASK_LOST
I0917 23:14:42.314281 31520 sched.cpp:635] Scheduler::statusUpdate took 21.943967ms
Status update: task 4 is in state TASK_LOST
I0917 23:14:42.314991 31520 sched.cpp:635] Scheduler::statusUpdate took 513983ns
Status update: task 2 is in state TASK_RUNNING
I0917 23:14:42.316504 31520 sched.cpp:635] Scheduler::statusUpdate took 1.370298ms
I0917 23:14:42.316826 31526 master.cpp:2720] Forwarding status update acknowledgement d4ac4252-ea9d-4c1d-b8c9-ea2d9704e6ad for task 2 of framework 20140917-231435-16842879-34609-31503-0000 to slave 20140917-231435-16842879-34609-31503-1 at slave(1)@127.0.1.1:34609 (saucy)
I0917 23:14:42.318004 31526 status_update_manager.cpp:398] Received status update acknowledgement (UUID: d4ac4252-ea9d-4c1d-b8c9-ea2d9704e6ad) for task 2 of framework 20140917-231435-16842879-34609-31503-0000
I0917 23:14:42.318320 31526 slave.cpp:1698] Status update manager successfully handled status update acknowledgement (UUID: d4ac4252-ea9d-4c1d-b8c9-ea2d9704e6ad) for task 2 of framework 20140917-231435-16842879-34609-31503-0000
Status update: task 0 is in state TASK_RUNNING
I0917 23:14:42.319246 31520 sched.cpp:635] Scheduler::statusUpdate took 546461ns
I0917 23:14:42.319432 31521 master.cpp:2720] Forwarding status update acknowledgement 774aa76c-87bd-4d39-a5e2-d880206fedb3 for task 0 of framework 20140917-231435-16842879-34609-31503-0000 to slave 20140917-231435-16842879-34609-31503-0 at slave(3)@127.0.1.1:34609 (saucy)
I0917 23:14:42.319612 31521 status_update_manager.cpp:398] Received status update acknowledgement (UUID: 774aa76c-87bd-4d39-a5e2-d880206fedb3) for task 0 of framework 20140917-231435-16842879-34609-31503-0000
I0917 23:14:42.319789 31521 slave.cpp:1698] Status update manager successfully handled status update acknowledgement (UUID: 774aa76c-87bd-4d39-a5e2-d880206fedb3) for task 0 of framework 20140917-231435-16842879-34609-31503-0000
Status update: task 1 is in state TASK_RUNNING
I0917 23:14:42.325856 31520 sched.cpp:635] Scheduler::statusUpdate took 5.910398ms
I0917 23:14:42.326161 31519 master.cpp:2720] Forwarding status update acknowledgement 3a405ad5-f8f8-4a7d-9f3d-ba0d02ab4cdb for task 1 of framework 20140917-231435-16842879-34609-31503-0000 to slave 20140917-231435-16842879-34609-31503-2 at slave(2)@127.0.1.1:34609 (saucy)
I0917 23:14:42.326392 31522 status_update_manager.cpp:398] Received status update acknowledgement (UUID: 3a405ad5-f8f8-4a7d-9f3d-ba0d02ab4cdb) for task 1 of framework 20140917-231435-16842879-34609-31503-0000
I0917 23:14:42.326689 31522 slave.cpp:1698] Status update manager successfully handled status update acknowledgement (UUID: 3a405ad5-f8f8-4a7d-9f3d-ba0d02ab4cdb) for task 1 of framework 20140917-231435-16842879-34609-31503-0000
I0917 23:14:42.385427 31524 monitor.cpp:140] Failed to collect resource usage for container 'a70a0cfd-ee78-43a0-b0b7-26bcd205f142' for executor 'default' of framework '20140917-231435-16842879-34609-31503-0000': Unknown container: a70a0cfd-ee78-43a0-b0b7-26bcd205f142
I0917 23:14:42.396478 31524 monitor.cpp:140] Failed to collect resource usage for container '1c6cf8b1-972b-4b73-8467-bc4503dd9332' for executor 'default' of framework '20140917-231435-16842879-34609-31503-0000': Unknown container: 1c6cf8b1-972b-4b73-8467-bc4503dd9332
Running task value: ""1""
Running task value: ""0""

Running task value: ""2""


I0917 23:14:42.747750 31669 exec.cpp:525] Executor sending status update TASK_FINISHED (UUID: c3e603da-200e-4ac6-8ec4-ab09bd839be7) for task 1 of framework 20140917-231435-16842879-34609-31503-0000
I0917 23:14:42.748468 31525 slave.cpp:2111] Handling status update TASK_FINISHED (UUID: c3e603da-200e-4ac6-8ec4-ab09bd839be7) for task 1 of framework 20140917-231435-16842879-34609-31503-0000 from executor(1)@127.0.1.1:33558
I0917 23:14:42.748540 31525 slave.cpp:3938] Terminating task 1
I0917 23:14:42.748910 31525 status_update_manager.cpp:320] Received status update TASK_FINISHED (UUID: c3e603da-200e-4ac6-8ec4-ab09bd839be7) for task 1 of framework 20140917-231435-16842879-34609-31503-0000
I0917 23:14:42.748955 31525 status_update_manager.cpp:373] Forwarding status update TASK_FINISHED (UUID: c3e603da-200e-4ac6-8ec4-ab09bd839be7) for task 1 of framework 20140917-231435-16842879-34609-31503-0000 to master@127.0.1.1:34609
I0917 23:14:42.749049 31525 master.cpp:3239] Forwarding status update TASK_FINISHED (UUID: c3e603da-200e-4ac6-8ec4-ab09bd839be7) for task 1 of framework 20140917-231435-16842879-34609-31503-0000
I0917 23:14:42.749084 31525 master.cpp:3205] Status update TASK_FINISHED (UUID: c3e603da-200e-4ac6-8ec4-ab09bd839be7) for task 1 of framework 20140917-231435-16842879-34609-31503-0000 from slave 20140917-231435-16842879-34609-31503-2 at slave(2)@127.0.1.1:34609 (saucy)
I0917 23:14:42.749125 31525 master.cpp:4385] Removing task 1 with resources cpus(*):1; mem(*):128 of framework 20140917-231435-16842879-34609-31503-0000 on slave 20140917-231435-16842879-34609-31503-2 at slave(2)@127.0.1.1:34609 (saucy)
I0917 23:14:42.749271 31525 slave.cpp:2268] Status update manager successfully handled status update TASK_FINISHED (UUID: c3e603da-200e-4ac6-8ec4-ab09bd839be7) for task 1 of framework 20140917-231435-16842879-34609-31503-0000
I0917 23:14:42.749285 31525 slave.cpp:2274] Sending acknowledgement for status update TASK_FINISHED (UUID: c3e603da-200e-4ac6-8ec4-ab09bd839be7) for task 1 of framework 20140917-231435-16842879-34609-31503-0000 to executor(1)@127.0.1.1:33558
I0917 23:14:42.749987 31521 hierarchical_allocator_process.hpp:563] Recovered cpus(*):1; mem(*):128 (total allocatable: mem(*):1001; disk(*):24988; ports(*):[31000-32000]; cpus(*):1) on slave 20140917-231435-16842879-34609-31503-2 from framework 20140917-231435-16842879-34609-31503-0000
Status update: task 1 is in state TASK_FINISHED
Finished tasks: 1
I0917 23:14:42.751500 31525 sched.cpp:635] Scheduler::statusUpdate took 2.067671ms
I0917 23:14:42.751701 31521 master.cpp:2720] Forwarding status update acknowledgement c3e603da-200e-4ac6-8ec4-ab09bd839be7 for task 1 of framework 20140917-231435-16842879-34609-31503-0000 to slave 20140917-231435-16842879-34609-31503-2 at slave(2)@127.0.1.1:34609 (saucy)
I0917 23:14:42.751891 31521 status_update_manager.cpp:398] Received status update acknowledgement (UUID: c3e603da-200e-4ac6-8ec4-ab09bd839be7) for task 1 of framework 20140917-231435-16842879-34609-31503-0000
I0917 23:14:42.752101 31521 status_update_manager.cpp:530] Cleaning up status update stream for task 1 of framework 20140917-231435-16842879-34609-31503-0000
I0917 23:14:42.752358 31521 slave.cpp:1698] Status update manager successfully handled status update acknowledgement (UUID: c3e603da-200e-4ac6-8ec4-ab09bd839be7) for task 1 of framework 20140917-231435-16842879-34609-31503-0000
I0917 23:14:42.752497 31521 slave.cpp:3977] Completing task 1
III0917 23:14:42.753000 31669 exec.cpp:339] Executor received status update acknowledgement c3e603da-200e-4ac6-8ec4-ab09bd839be7 for task 1 of framework 20140917-231435-16842879-34609-31503-0000
0917 23:14:42.753515 31680 exec.cpp:525] Executor sending status update TASK_FINISHED (UUID: 7870de4c-5721-4dbb-b50c-0077513d6e78) for task 0 of framework 20140917-231435-16842879-34609-31503-0000
I0917 23:14:42.755370 31519 slave.cpp:2111] Handling status update TASK_FINISHED (UUID: 7870de4c-5721-4dbb-b50c-0077513d6e78) for task 0 of framework 20140917-231435-16842879-34609-31503-0000 from executor(1)@127.0.1.1:40557
I0917 23:14:42.755519 31519 slave.cpp:3938] Terminating task 0
I0917 23:14:42.755897 31519 status_update_manager.cpp:320] Received status update TASK_FINISHED (UUID: 7870de4c-5721-4dbb-b50c-0077513d6e78) for task 0 of framework 20140917-231435-16842879-34609-31503-0000
I0917 23:14:42.756033 31519 status_update_manager.cpp:373] Forwarding status update TASK_FINISHED (UUID: 7870de4c-5721-4dbb-b50c-0077513d6e78) for task 0 of framework 20140917-231435-16842879-34609-31503-0000 to master@127.0.1.1:34609
I0917 23:14:42.756324 31519 master.cpp:3239] Forwarding status update TASK_FINISHED (UUID: 7870de4c-5721-4dbb-b50c-0077513d6e78) for task 0 of framework 20140917-231435-16842879-34609-31503-0000
I0917 23:14:42.756497 31519 master.cpp:3205] Status update TASK_FINISHED (UUID: 7870de4c-5721-4dbb-b50c-0077513d6e78) for task 0 of framework 20140917-231435-16842879-34609-31503-0000 from slave 20140917-231435-16842879-34609-31503-0 at slave(3)@127.0.1.1:34609 (saucy)
0917 23:14:42.754186 31693 exec.cpp:525] Executor sending status update TASK_FINISHED (UUID: de91c600-abe2-4081-ad39-25fbd637b938) for task 2 of framework 20140917-231435-16842879-34609-31503-0000
Status update: task 0 is in state TASK_FINISHEDI0917 23:14:42.764149 31519 master.cpp:4385] Removing task 0 with resources cpus(*):1; mem(*):128 of framework 20140917-231435-16842879-34609-31503-0000 on slave 20140917-231435-16842879-34609-31503-0 at slave(3)@127.0.1.1:34609 (saucy)
I0917 23:14:42.756443 31523 slave.cpp:2268] Status update manager successfully handled status update TASK_FINISHED (UUID: 7870de4c-5721-4dbb-b50c-0077513d6e78) for task 0 of framework 20140917-231435-16842879-34609-31503-0000
I0917 23:14:42.766275 31523 slave.cpp:2274] Sending acknowledgement for status update TASK_FINISHED (UUID: 7870de4c-5721-4dbb-b50c-0077513d6e78) for task 0 of framework 20140917-231435-16842879-34609-31503-0000 to executor(1)@127.0.1.1:40557
I0917 23:14:42.766705 31523 hierarchical_allocator_process.hpp:563] Recovered cpus(*):1; mem(*):128 (total allocatable: mem(*):1001; disk(*):24988; ports(*):[31000-32000]; cpus(*):1) on slave 20140917-231435-16842879-34609-31503-0 from framework 20140917-231435-16842879-34609-31503-0000
I0917 23:14:42.767081 31519 slave.cpp:2111] Handling status update TASK_FINISHED (UUID: de91c600-abe2-4081-ad39-25fbd637b938) for task 2 of framework 20140917-231435-16842879-34609-31503-0000 from executor(1)@127.0.1.1:51898
I0917 23:14:42.767195 31519 slave.cpp:3938] Terminating task 2
I0917 23:14:42.767546 31519 status_update_manager.cpp:320] Received status update TASK_FINISHED (UUID: de91c600-abe2-4081-ad39-25fbd637b938) for task 2 of framework 20140917-231435-16842879-34609-31503-0000

Finished tasks: 2
I0917 23:14:42.768301 31519 status_update_manager.cpp:373] Forwarding status update TASK_FINISHED (UUID: de91c600-abe2-4081-ad39-25fbd637b938) for task 2 of framework 20140917-231435-16842879-34609-31503-0000 to master@127.0.1.1:34609
I0917 23:14:42.768503 31519 master.cpp:3239] Forwarding status update TASK_FINISHED (UUID: de91c600-abe2-4081-ad39-25fbd637b938) for task 2 of framework 20140917-231435-16842879-34609-31503-0000
I0917 23:14:42.768614 31523 slave.cpp:2268] Status update manager successfully handled status update TASK_FINISHED (UUID: de91c600-abe2-4081-ad39-25fbd637b938) for task 2 of framework 20140917-231435-16842879-34609-31503-0000
I0917 23:14:42.768723 31523 slave.cpp:2274] Sending acknowledgement for status update TASK_FINISHED (UUID: de91c600-abe2-4081-ad39-25fbd637b938) for task 2 of framework 20140917-231435-16842879-34609-31503-0000 to executor(1)@127.0.1.1:51898
I0917 23:14:42.768264 31521 sched.cpp:635] Scheduler::statusUpdate took 7.95946ms
I0917 23:14:42.769163 31519 master.cpp:3205] Status update TASK_FINISHED (UUID: de91c600-abe2-4081-ad39-25fbd637b938) for task 2 of framework 20140917-231435-16842879-34609-31503-0000 from slave 20140917-231435-16842879-34609-31503-1 at slave(1)@127.0.1.1:34609 (saucy)
Status update: task 2 is in state TASK_FINISHEDII0917 23:14:42.769947 31691 exec.cpp:339] Executor received status update acknowledgement de91c600-abe2-4081-ad39-25fbd637b938 for task 2 of framework 20140917-231435-16842879-34609-31503-0000
0917 23:14:42.770182 31682 exec.cpp:339] Executor received status update acknowledgement 7870de4c-5721-4dbb-b50c-0077513d6e78 for task 0 of framework 20140917-231435-16842879-34609-31503-0000

Finished tasks: 3
I0917 23:14:42.776676 31519 master.cpp:4385] Removing task 2 with resources cpus(*):1; mem(*):128 of framework 20140917-231435-16842879-34609-31503-0000 on slave 20140917-231435-16842879-34609-31503-1 at slave(1)@127.0.1.1:34609 (saucy)
I0917 23:14:42.776577 31523 sched.cpp:635] Scheduler::statusUpdate took 7.246343ms
I0917 23:14:42.777061 31519 master.cpp:2720] Forwarding status update acknowledgement 7870de4c-5721-4dbb-b50c-0077513d6e78 for task 0 of framework 20140917-231435-16842879-34609-31503-0000 to slave 20140917-231435-16842879-34609-31503-0 at slave(3)@127.0.1.1:34609 (saucy)
I0917 23:14:42.777197 31523 hierarchical_allocator_process.hpp:563] Recovered cpus(*):1; mem(*):128 (total allocatable: mem(*):1001; disk(*):24988; ports(*):[31000-32000]; cpus(*):1) on slave 20140917-231435-16842879-34609-31503-1 from framework 20140917-231435-16842879-34609-31503-0000
I0917 23:14:42.777384 31519 master.cpp:2720] Forwarding status update acknowledgement de91c600-abe2-4081-ad39-25fbd637b938 for task 2 of framework 20140917-231435-16842879-34609-31503-0000 to slave 20140917-231435-16842879-34609-31503-1 at slave(1)@127.0.1.1:34609 (saucy)
I0917 23:14:42.777580 31519 status_update_manager.cpp:398] Received status update acknowledgement (UUID: de91c600-abe2-4081-ad39-25fbd637b938) for task 2 of framework 20140917-231435-16842879-34609-31503-0000
I0917 23:14:42.777721 31519 status_update_manager.cpp:530] Cleaning up status update stream for task 2 of framework 20140917-231435-16842879-34609-31503-0000
I0917 23:14:42.777523 31523 status_update_manager.cpp:398] Received status update acknowledgement (UUID: 7870de4c-5721-4dbb-b50c-0077513d6e78) for task 0 of framework 20140917-231435-16842879-34609-31503-0000
I0917 23:14:42.777992 31523 status_update_manager.cpp:530] Cleaning up status update stream for task 0 of framework 20140917-231435-16842879-34609-31503-0000
I0917 23:14:42.778156 31519 slave.cpp:1698] Status update manager successfully handled status update acknowledgement (UUID: de91c600-abe2-4081-ad39-25fbd637b938) for task 2 of framework 20140917-231435-16842879-34609-31503-0000
I0917 23:14:42.778313 31519 slave.cpp:3977] Completing task 2
I0917 23:14:42.778291 31521 slave.cpp:1698] Status update manager successfully handled status update acknowledgement (UUID: 7870de4c-5721-4dbb-b50c-0077513d6e78) for task 0 of framework 20140917-231435-16842879-34609-31503-0000
I0917 23:14:42.779542 31521 slave.cpp:3977] Completing task 0
I0917 23:14:43.250752 31520 hierarchical_allocator_process.hpp:734] Offering mem(*):1001; disk(*):24988; ports(*):[31000-32000]; cpus(*):1 on slave 20140917-231435-16842879-34609-31503-0 to framework 20140917-231435-16842879-34609-31503-0000
I0917 23:14:43.251107 31520 hierarchical_allocator_process.hpp:734] Offering mem(*):1001; disk(*):24988; ports(*):[31000-32000]; cpus(*):1 on slave 20140917-231435-16842879-34609-31503-1 to framework 20140917-231435-16842879-34609-31503-0000
I0917 23:14:43.251287 31520 hierarchical_allocator_process.hpp:734] Offering mem(*):1001; disk(*):24988; ports(*):[31000-32000]; cpus(*):1 on slave 20140917-231435-16842879-34609-31503-2 to framework 20140917-231435-16842879-34609-31503-0000
I0917 23:14:43.251521 31520 hierarchical_allocator_process.hpp:659] Performed allocation for 3 slaves in 867376ns
I0917 23:14:43.251665 31523 master.hpp:862] Adding offer 20140917-231435-16842879-34609-31503-6 with resources mem(*):1001; disk(*):24988; ports(*):[31000-32000]; cpus(*):1 on slave 20140917-231435-16842879-34609-31503-0 (saucy)
I0917 23:14:43.251854 31523 master.hpp:862] Adding offer 20140917-231435-16842879-34609-31503-7 with resources mem(*):1001; disk(*):24988; ports(*):[31000-32000]; cpus(*):1 on slave 20140917-231435-16842879-34609-31503-2 (saucy)
I0917 23:14:43.252030 31523 master.hpp:862] Adding offer 20140917-231435-16842879-34609-31503-8 with resources mem(*):1001; disk(*):24988; ports(*):[31000-32000]; cpus(*):1 on slave 20140917-231435-16842879-34609-31503-1 (saucy)
I0917 23:14:43.252184 31523 master.cpp:3616] Sending 3 offers to framework 20140917-231435-16842879-34609-31503-0000
I0917 23:14:43.253651 31523 sched.cpp:544] Scheduler::resourceOffers took 1.199343ms
I0917 23:14:43.254057 31524 master.hpp:871] Removing offer 20140917-231435-16842879-34609-31503-6 with resources mem(*):1001; disk(*):24988; ports(*):[31000-32000]; cpus(*):1 on slave 20140917-231435-16842879-34609-31503-0 (saucy)
I0917 23:14:43.254179 31524 master.cpp:2228] Processing reply for offers: [ 20140917-231435-16842879-34609-31503-6 ] on slave 20140917-231435-16842879-34609-31503-0 at slave(3)@127.0.1.1:34609 (saucy) for framework 20140917-231435-16842879-34609-31503-0000
I0917 23:14:43.254555 31524 hierarchical_allocator_process.hpp:563] Recovered mem(*):1001; disk(*):24988; ports(*):[31000-32000]; cpus(*):1 (total allocatable: mem(*):1001; disk(*):24988; ports(*):[31000-32000]; cpus(*):1) on slave 20140917-231435-16842879-34609-31503-0 from framework 20140917-231435-16842879-34609-31503-0000
I0917 23:14:43.254693 31524 hierarchical_allocator_process.hpp:599] Framework 20140917-231435-16842879-34609-31503-0000 filtered slave 20140917-231435-16842879-34609-31503-0 for 1secs
I0917 23:14:43.254902 31521 master.hpp:871] Removing offer 20140917-231435-16842879-34609-31503-7 with resources mem(*):1001; disk(*):24988; ports(*):[31000-32000]; cpus(*):1 on slave 20140917-231435-16842879-34609-31503-2 (saucy)
I0917 23:14:43.255035 31521 master.cpp:2228] Processing reply for offers: [ 20140917-231435-16842879-34609-31503-7 ] on slave 20140917-231435-16842879-34609-31503-2 at slave(2)@127.0.1.1:34609 (saucy) for framework 20140917-231435-16842879-34609-31503-0000
I0917 23:14:43.255257 31521 hierarchical_allocator_process.hpp:563] Recovered mem(*):1001; disk(*):24988; ports(*):[31000-32000]; cpus(*):1 (total allocatable: mem(*):1001; disk(*):24988; ports(*):[31000-32000]; cpus(*):1) on slave 20140917-231435-16842879-34609-31503-2 from framework 20140917-231435-16842879-34609-31503-0000
I0917 23:14:43.255414 31521 hierarchical_allocator_process.hpp:599] Framework 20140917-231435-16842879-34609-31503-0000 filtered slave 20140917-231435-16842879-34609-31503-2 for 1secs
I0917 23:14:43.255630 31522 master.hpp:871] Removing offer 20140917-231435-16842879-34609-31503-8 with resources mem(*):1001; disk(*):24988; ports(*):[31000-32000]; cpus(*):1 on slave 20140917-231435-16842879-34609-31503-1 (saucy)
I0917 23:14:43.255754 31522 master.cpp:2228] Processing reply for offers: [ 20140917-231435-16842879-34609-31503-8 ] on slave 20140917-231435-16842879-34609-31503-1 at slave(1)@127.0.1.1:34609 (saucy) for framework 20140917-231435-16842879-34609-31503-0000
I0917 23:14:43.255944 31522 hierarchical_allocator_process.hpp:563] Recovered mem(*):1001; disk(*):24988; ports(*):[31000-32000]; cpus(*):1 (total allocatable: mem(*):1001; disk(*):24988; ports(*):[31000-32000]; cpus(*):1) on slave 20140917-231435-16842879-34609-31503-1 from framework 20140917-231435-16842879-34609-31503-0000
I0917 23:14:43.256085 31522 hierarchical_allocator_process.hpp:599] Framework 20140917-231435-16842879-34609-31503-0000 filtered slave 20140917-231435-16842879-34609-31503-1 for 1secs
I0917 23:14:43.386253 31522 monitor.cpp:140] Failed to collect resource usage for container 'a70a0cfd-ee78-43a0-b0b7-26bcd205f142' for executor 'default' of framework '20140917-231435-16842879-34609-31503-0000': Unknown container: a70a0cfd-ee78-43a0-b0b7-26bcd205f142
I0917 23:14:43.397584 31521 monitor.cpp:140] Failed to collect resource usage for container '1c6cf8b1-972b-4b73-8467-bc4503dd9332' for executor 'default' of framework '20140917-231435-16842879-34609-31503-0000': Unknown container: 1c6cf8b1-972b-4b73-8467-bc4503dd9332
I0917 23:14:44.252192 31525 hierarchical_allocator_process.hpp:816] Filtered mem(*):1001; disk(*):24988; ports(*):[31000-32000]; cpus(*):1 on slave 20140917-231435-16842879-34609-31503-0 for framework 20140917-231435-16842879-34609-31503-0000
I0917 23:14:44.252557 31525 hierarchical_allocator_process.hpp:816] Filtered mem(*):1001; disk(*):24988; ports(*):[31000-32000]; cpus(*):1 on slave 20140917-231435-16842879-34609-31503-2 for framework 20140917-231435-16842879-34609-31503-0000
I0917 23:14:44.252712 31525 hierarchical_allocator_process.hpp:816] Filtered mem(*):1001; disk(*):24988; ports(*):[31000-32000]; cpus(*):1 on slave 20140917-231435-16842879-34609-31503-1 for framework 20140917-231435-16842879-34609-31503-0000
I0917 23:14:44.252879 31525 hierarchical_allocator_process.hpp:659] Performed allocation for 3 slaves in 791479ns
I0917 23:14:44.386862 31523 monitor.cpp:140] Failed to collect resource usage for container 'a70a0cfd-ee78-43a0-b0b7-26bcd205f142' for executor 'default' of framework '20140917-231435-16842879-34609-31503-0000': Unknown container: a70a0cfd-ee78-43a0-b0b7-26bcd205f142
I0917 23:14:44.398048 31524 monitor.cpp:140] Failed to collect resource usage for container '1c6cf8b1-972b-4b73-8467-bc4503dd9332' for executor 'default' of framework '20140917-231435-16842879-34609-31503-0000': Unknown container: 1c6cf8b1-972b-4b73-8467-bc4503dd9332
I0917 23:14:45.247548 31520 master.cpp:120] No whitelist given. Advertising offers for all slaves
I0917 23:14:45.253939 31521 hierarchical_allocator_process.hpp:734] Offering mem(*):1001; disk(*):24988; ports(*):[31000-32000]; cpus(*):1 on slave 20140917-231435-16842879-34609-31503-0 to framework 20140917-231435-16842879-34609-31503-0000
I0917 23:14:45.254165 31521 hierarchical_allocator_process.hpp:734] Offering mem(*):1001; disk(*):24988; ports(*):[31000-32000]; cpus(*):1 on slave 20140917-231435-16842879-34609-31503-2 to framework 20140917-231435-16842879-34609-31503-0000
I0917 23:14:45.254356 31521 hierarchical_allocator_process.hpp:734] Offering mem(*):1001; disk(*):24988; ports(*):[31000-32000]; cpus(*):1 on slave 20140917-231435-16842879-34609-31503-1 to framework 20140917-231435-16842879-34609-31503-0000
I0917 23:14:45.254585 31521 hierarchical_allocator_process.hpp:659] Performed allocation for 3 slaves in 762921ns
I0917 23:14:45.254741 31523 master.hpp:862] Adding offer 20140917-231435-16842879-34609-31503-9 with resources mem(*):1001; disk(*):24988; ports(*):[31000-32000]; cpus(*):1 on slave 20140917-231435-16842879-34609-31503-1 (saucy)
I0917 23:14:45.254945 31523 master.hpp:862] Adding offer 20140917-231435-16842879-34609-31503-10 with resources mem(*):1001; disk(*):24988; ports(*):[31000-32000]; cpus(*):1 on slave 20140917-231435-16842879-34609-31503-0 (saucy)
I0917 23:14:45.255112 31523 master.hpp:862] Adding offer 20140917-231435-16842879-34609-31503-11 with resources mem(*):1001; disk(*):24988; ports(*):[31000-32000]; cpus(*):1 on slave 20140917-231435-16842879-34609-31503-2 (saucy)
I0917 23:14:45.255259 31523 master.cpp:3616] Sending 3 offers to framework 20140917-231435-16842879-34609-31503-0000
I0917 23:14:45.256947 31523 sched.cpp:544] Scheduler::resourceOffers took 1.430971ms
I0917 23:14:45.257366 31524 master.hpp:871] Removing offer 20140917-231435-16842879-34609-31503-9 with resources mem(*):1001; disk(*):24988; ports(*):[31000-32000]; cpus(*):1 on slave 20140917-231435-16842879-34609-31503-1 (saucy)
I0917 23:14:45.257498 31524 master.cpp:2228] Processing reply for offers: [ 20140917-231435-16842879-34609-31503-9 ] on slave 20140917-231435-16842879-34609-31503-1 at slave(1)@127.0.1.1:34609 (saucy) for framework 20140917-231435-16842879-34609-31503-0000
I0917 23:14:45.257766 31524 hierarchical_allocator_process.hpp:563] Recovered mem(*):1001; disk(*):24988; ports(*):[31000-32000]; cpus(*):1 (total allocatable: mem(*):1001; disk(*):24988; ports(*):[31000-32000]; cpus(*):1) on slave 20140917-231435-16842879-34609-31503-1 from framework 20140917-231435-16842879-34609-31503-0000
I0917 23:14:45.257901 31524 hierarchical_allocator_process.hpp:599] Framework 20140917-231435-16842879-34609-31503-0000 filtered slave 20140917-231435-16842879-34609-31503-1 for 1secs
I0917 23:14:45.258277 31522 master.hpp:871] Removing offer 20140917-231435-16842879-34609-31503-10 with resources mem(*):1001; disk(*):24988; ports(*):[31000-32000]; cpus(*):1 on slave 20140917-231435-16842879-34609-31503-0 (saucy)
I0917 23:14:45.258407 31522 master.cpp:2228] Processing reply for offers: [ 20140917-231435-16842879-34609-31503-10 ] on slave 20140917-231435-16842879-34609-31503-0 at slave(3)@127.0.1.1:34609 (saucy) for framework 20140917-231435-16842879-34609-31503-0000
I0917 23:14:45.258625 31522 hierarchical_allocator_process.hpp:563] Recovered mem(*):1001; disk(*):24988; ports(*):[31000-32000]; cpus(*):1 (total allocatable: mem(*):1001; disk(*):24988; ports(*):[31000-32000]; cpus(*):1) on slave 20140917-231435-16842879-34609-31503-0 from framework 20140917-231435-16842879-34609-31503-0000
I0917 23:14:45.258771 31522 hierarchical_allocator_process.hpp:599] Framework 20140917-231435-16842879-34609-31503-0000 filtered slave 20140917-231435-16842879-34609-31503-0 for 1secs
I0917 23:14:45.258975 31522 master.hpp:871] Removing offer 20140917-231435-16842879-34609-31503-11 with resources mem(*):1001; disk(*):24988; ports(*):[31000-32000]; cpus(*):1 on slave 20140917-231435-16842879-34609-31503-2 (saucy)
I0917 23:14:45.259114 31522 master.cpp:2228] Processing reply for offers: [ 20140917-231435-16842879-34609-31503-11 ] on slave 20140917-231435-16842879-34609-31503-2 at slave(2)@127.0.1.1:34609 (saucy) for framework 20140917-231435-16842879-34609-31503-0000
I0917 23:14:45.259313 31522 hierarchical_allocator_process.hpp:563] Recovered mem(*):1001; disk(*):24988; ports(*):[31000-32000]; cpus(*):1 (total allocatable: mem(*):1001; disk(*):24988; ports(*):[31000-32000]; cpus(*):1) on slave 20140917-231435-16842879-34609-31503-2 from framework 20140917-231435-16842879-34609-31503-0000
I0917 23:14:45.259464 31522 hierarchical_allocator_process.hpp:599] Framework 20140917-231435-16842879-34609-31503-0000 filtered slave 20140917-231435-16842879-34609-31503-2 for 1secs
I0917 23:14:45.388000 31524 monitor.cpp:140] Failed to collect resource usage for container 'a70a0cfd-ee78-43a0-b0b7-26bcd205f142' for executor 'default' of framework '20140917-231435-16842879-34609-31503-0000': Unknown container: a70a0cfd-ee78-43a0-b0b7-26bcd205f142
I0917 23:14:45.399271 31521 monitor.cpp:140] Failed to collect resource usage for container '1c6cf8b1-972b-4b73-8467-bc4503dd9332' for executor 'default' of framework '20140917-231435-16842879-34609-31503-0000': Unknown container: 1c6cf8b1-972b-4b73-8467-bc4503dd9332
I0917 23:14:46.254964 31523 hierarchical_allocator_process.hpp:816] Filtered mem(*):1001; disk(*):24988; ports(*):[31000-32000]; cpus(*):1 on slave 20140917-231435-16842879-34609-31503-2 for framework 20140917-231435-16842879-34609-31503-0000
I0917 23:14:46.255506 31523 hierarchical_allocator_process.hpp:816] Filtered mem(*):1001; disk(*):24988; ports(*):[31000-32000]; cpus(*):1 on slave 20140917-231435-16842879-34609-31503-1 for framework 20140917-231435-16842879-34609-31503-0000
I0917 23:14:46.255667 31523 hierarchical_allocator_process.hpp:816] Filtered mem(*):1001; disk(*):24988; ports(*):[31000-32000]; cpus(*):1 on slave 20140917-231435-16842879-34609-31503-0 for framework 20140917-231435-16842879-34609-31503-0000
I0917 23:14:46.255867 31523 hierarchical_allocator_process.hpp:659] Performed allocation for 3 slaves in 1.022627ms
I0917 23:14:46.388937 31525 monitor.cpp:140] Failed to collect resource usage for container 'a70a0cfd-ee78-43a0-b0b7-26bcd205f142' for executor 'default' of framework '20140917-231435-16842879-34609-31503-0000': Unknown container: a70a0cfd-ee78-43a0-b0b7-26bcd205f142
I0917 23:14:46.400216 31520 monitor.cpp:140] Failed to collect resource usage for container '1c6cf8b1-972b-4b73-8467-bc4503dd9332' for executor 'default' of framework '20140917-231435-16842879-34609-31503-0000': Unknown container: 1c6cf8b1-972b-4b73-8467-bc4503dd9332
I0917 23:14:47.256769 31523 hierarchical_allocator_process.hpp:734] Offering mem(*):1001; disk(*):24988; ports(*):[31000-32000]; cpus(*):1 on slave 20140917-231435-16842879-34609-31503-0 to framework 20140917-231435-16842879-34609-31503-0000
I0917 23:14:47.257114 31523 hierarchical_allocator_process.hpp:734] Offering mem(*):1001; disk(*):24988; ports(*):[31000-32000]; cpus(*):1 on slave 20140917-231435-16842879-34609-31503-1 to framework 20140917-231435-16842879-34609-31503-0000
I0917 23:14:47.257302 31523 hierarchical_allocator_process.hpp:734] Offering mem(*):1001; disk(*):24988; ports(*):[31000-32000]; cpus(*):1 on slave 20140917-231435-16842879-34609-31503-2 to framework 20140917-231435-16842879-34609-31503-0000
I0917 23:14:47.257544 31523 hierarchical_allocator_process.hpp:659] Performed allocation for 3 slaves in 848269ns
I0917 23:14:47.257709 31521 master.hpp:862] Adding offer 20140917-231435-16842879-34609-31503-12 with resources mem(*):1001; disk(*):24988; ports(*):[31000-32000]; cpus(*):1 on slave 20140917-231435-16842879-34609-31503-0 (saucy)
I0917 23:14:47.257920 31521 master.hpp:862] Adding offer 20140917-231435-16842879-34609-31503-13 with resources mem(*):1001; disk(*):24988; ports(*):[31000-32000]; cpus(*):1 on slave 20140917-231435-16842879-34609-31503-2 (saucy)
I0917 23:14:47.258103 31521 master.hpp:862] Adding offer 20140917-231435-16842879-34609-31503-14 with resources mem(*):1001; disk(*):24988; ports(*):[31000-32000]; cpus(*):1 on slave 20140917-231435-16842879-34609-31503-1 (saucy)
I0917 23:14:47.258236 31521 master.cpp:3616] Sending 3 offers to framework 20140917-231435-16842879-34609-31503-0000
I0917 23:14:47.260226 31521 sched.cpp:544] Scheduler::resourceOffers took 1.743624ms
I0917 23:14:47.260470 31525 master.hpp:871] Removing offer 20140917-231435-16842879-34609-31503-12 with resources mem(*):1001; disk(*):24988; ports(*):[31000-32000]; cpus(*):1 on slave 20140917-231435-16842879-34609-31503-0 (saucy)
I0917 23:14:47.260594 31525 master.cpp:2228] Processing reply for offers: [ 20140917-231435-16842879-34609-31503-12 ] on slave 20140917-231435-16842879-34609-31503-0 at slave(3)@127.0.1.1:34609 (saucy) for framework 20140917-231435-16842879-34609-31503-0000
I0917 23:14:47.260891 31525 hierarchical_allocator_process.hpp:563] Recovered mem(*):1001; disk(*):24988; ports(*):[31000-32000]; cpus(*):1 (total allocatable: mem(*):1001; disk(*):24988; ports(*):[31000-32000]; cpus(*):1) on slave 20140917-231435-16842879-34609-31503-0 from framework 20140917-231435-16842879-34609-31503-0000
I0917 23:14:47.261092 31525 hierarchical_allocator_process.hpp:599] Framework 20140917-231435-16842879-34609-31503-0000 filtered slave 20140917-231435-16842879-34609-31503-0 for 1secs
I0917 23:14:47.261380 31519 master.hpp:871] Removing offer 20140917-231435-16842879-34609-31503-13 with resources mem(*):1001; disk(*):24988; ports(*):[31000-32000]; cpus(*):1 on slave 20140917-231435-16842879-34609-31503-2 (saucy)
I0917 23:14:47.261514 31519 master.cpp:2228] Processing reply for offers: [ 20140917-231435-16842879-34609-31503-13 ] on slave 20140917-231435-16842879-34609-31503-2 at slave(2)@127.0.1.1:34609 (saucy) for framework 20140917-231435-16842879-34609-31503-0000
I0917 23:14:47.261744 31519 hierarchical_allocator_process.hpp:563] Recovered mem(*):1001; disk(*):24988; ports(*):[31000-32000]; cpus(*):1 (total allocatable: mem(*):1001; disk(*):24988; ports(*):[31000-32000]; cpus(*):1) on slave 20140917-231435-16842879-34609-31503-2 from framework 20140917-231435-16842879-34609-31503-0000
I0917 23:14:47.261873 31519 hierarchical_allocator_process.hpp:599] Framework 20140917-231435-16842879-34609-31503-0000 filtered slave 20140917-231435-16842879-34609-31503-2 for 1secs
I0917 23:14:47.262086 31520 master.hpp:871] Removing offer 20140917-231435-16842879-34609-31503-14 with resources mem(*):1001; disk(*):24988; ports(*):[31000-32000]; cpus(*):1 on slave 20140917-231435-16842879-34609-31503-1 (saucy)
I0917 23:14:47.262212 31520 master.cpp:2228] Processing reply for offers: [ 20140917-231435-16842879-34609-31503-14 ] on slave 20140917-231435-16842879-34609-31503-1 at slave(1)@127.0.1.1:34609 (saucy) for framework 20140917-231435-16842879-34609-31503-0000
I0917 23:14:47.262408 31520 hierarchical_allocator_process.hpp:563] Recovered mem(*):1001; disk(*):24988; ports(*):[31000-32000]; cpus(*):1 (total allocatable: mem(*):1001; disk(*):24988; ports(*):[31000-32000]; cpus(*):1) on slave 20140917-231435-16842879-34609-31503-1 from framework 20140917-231435-16842879-34609-31503-0000
I0917 23:14:47.262526 31520 hierarchical_allocator_process.hpp:599] Framework 20140917-231435-16842879-34609-31503-0000 filtered slave 20140917-231435-16842879-34609-31503-1 for 1secs
I0917 23:14:47.390128 31520 monitor.cpp:140] Failed to collect resource usage for container 'a70a0cfd-ee78-43a0-b0b7-26bcd205f142' for executor 'default' of framework '20140917-231435-16842879-34609-31503-0000': Unknown container: a70a0cfd-ee78-43a0-b0b7-26bcd205f142
I0917 23:14:47.401535 31520 monitor.cpp:140] Failed to collect resource usage for container '1c6cf8b1-972b-4b73-8467-bc4503dd9332' for executor 'default' of framework '20140917-231435-16842879-34609-31503-0000': Unknown container: 1c6cf8b1-972b-4b73-8467-bc4503dd9332
I0917 23:14:48.258159 31520 hierarchical_allocator_process.hpp:816] Filtered mem(*):1001; disk(*):24988; ports(*):[31000-32000]; cpus(*):1 on slave 20140917-231435-16842879-34609-31503-0 for framework 20140917-231435-16842879-34609-31503-0000
I0917 23:14:48.258453 31520 hierarchical_allocator_process.hpp:816] Filtered mem(*):1001; disk(*):24988; ports(*):[31000-32000]; cpus(*):1 on slave 20140917-231435-16842879-34609-31503-2 for framework 20140917-231435-16842879-34609-31503-0000
I0917 23:14:48.258616 31520 hierarchical_allocator_process.hpp:816] Filtered mem(*):1001; disk(*):24988; ports(*):[31000-32000]; cpus(*):1 on slave 20140917-231435-16842879-34609-31503-1 for framework 20140917-231435-16842879-34609-31503-0000
I0917 23:14:48.258735 31520 hierarchical_allocator_process.hpp:659] Performed allocation for 3 slaves in 726125ns
I0917 23:14:48.390913 31519 monitor.cpp:140] Failed to collect resource usage for container 'a70a0cfd-ee78-43a0-b0b7-26bcd205f142' for executor 'default' of framework '20140917-231435-16842879-34609-31503-0000': Unknown container: a70a0cfd-ee78-43a0-b0b7-26bcd205f142
I0917 23:14:48.402195 31523 monitor.cpp:140] Failed to collect resource usage for container '1c6cf8b1-972b-4b73-8467-bc4503dd9332' for executor 'default' of framework '20140917-231435-16842879-34609-31503-0000': Unknown container: 1c6cf8b1-972b-4b73-8467-bc4503dd9332
I0917 23:14:49.260015 31519 hierarchical_allocator_process.hpp:734] Offering mem(*):1001; disk(*):24988; ports(*):[31000-32000]; cpus(*):1 on slave 20140917-231435-16842879-34609-31503-2 to framework 20140917-231435-16842879-34609-31503-0000
I0917 23:14:49.260422 31519 hierarchical_allocator_process.hpp:734] Offering mem(*):1001; disk(*):24988; ports(*):[31000-32000]; cpus(*):1 on slave 20140917-231435-16842879-34609-31503-1 to framework 20140917-231435-16842879-34609-31503-0000
I0917 23:14:49.260701 31519 hierarchical_allocator_process.hpp:734] Offering mem(*):1001; disk(*):24988; ports(*):[31000-32000]; cpus(*):1 on slave 20140917-231435-16842879-34609-31503-0 to framework 20140917-231435-16842879-34609-31503-0000
I0917 23:14:49.260953 31519 hierarchical_allocator_process.hpp:659] Performed allocation for 3 slaves in 1.015379ms
I0917 23:14:49.261144 31520 master.hpp:862] Adding offer 20140917-231435-16842879-34609-31503-15 with resources mem(*):1001; disk(*):24988; ports(*):[31000-32000]; cpus(*):1 on slave 20140917-231435-16842879-34609-31503-2 (saucy)
I0917 23:14:49.261371 31520 master.hpp:862] Adding offer 20140917-231435-16842879-34609-31503-16 with resources mem(*):1001; disk(*):24988; ports(*):[31000-32000]; cpus(*):1 on slave 20140917-231435-16842879-34609-31503-0 (saucy)
I0917 23:14:49.261576 31520 master.hpp:862] Adding offer 20140917-231435-16842879-34609-31503-17 with resources mem(*):1001; disk(*):24988; ports(*):[31000-32000]; cpus(*):1 on slave 20140917-231435-16842879-34609-31503-1 (saucy)
I0917 23:14:49.261729 31520 master.cpp:3616] Sending 3 offers to framework 20140917-231435-16842879-34609-31503-0000
I0917 23:14:49.263228 31520 sched.cpp:544] Scheduler::resourceOffers took 1.214892ms
I0917 23:14:49.263566 31524 master.hpp:871] Removing offer 20140917-231435-16842879-34609-31503-15 with resources mem(*):1001; disk(*):24988; ports(*):[31000-32000]; cpus(*):1 on slave 20140917-231435-16842879-34609-31503-2 (saucy)
I0917 23:14:49.263701 31524 master.cpp:2228] Processing reply for offers: [ 20140917-231435-16842879-34609-31503-15 ] on slave 20140917-231435-16842879-34609-31503-2 at slave(2)@127.0.1.1:34609 (saucy) for framework 20140917-231435-16842879-34609-31503-0000
I0917 23:14:49.264742 31524 hierarchical_allocator_process.hpp:563] Recovered mem(*):1001; disk(*):24988; ports(*):[31000-32000]; cpus(*):1 (total allocatable: mem(*):1001; disk(*):24988; ports(*):[31000-32000]; cpus(*):1) on slave 20140917-231435-16842879-34609-31503-2 from framework 20140917-231435-16842879-34609-31503-0000
I0917 23:14:49.264775 31524 hierarchical_allocator_process.hpp:599] Framework 20140917-231435-16842879-34609-31503-0000 filtered slave 20140917-231435-16842879-34609-31503-2 for 1secs

{code}",Bug,Major,vinodkone,2014-09-19T01:24:49.000+0000,5,Resolved,Complete,Task attempted to use more offers than requested in example jave and python frameworks,2014-09-19T01:24:49.000+0000,MESOS-1814,2.0,mesos,Mesos Q3 Sprint 5
vinodkone,2014-09-18T03:42:47.000+0000,vinodkone,"Most of the example frameworks launch a bunch of tasks and exit if *all* of them reach FINISHED state. But if there is a bug in the code resulting in TASK_LOST, the framework waits forever. Instead the framework should abort if an un-expected task state is encountered.",Improvement,Major,vinodkone,2014-09-19T01:24:06.000+0000,5,Resolved,Complete,Fail fast in example frameworks if task goes into unexpected state,2014-09-19T01:24:06.000+0000,MESOS-1813,1.0,mesos,Mesos Q3 Sprint 5
vinodkone,2014-09-18T01:20:45.000+0000,vinodkone,"Currently the master code treats a deactivated and disconnected slave similarly, by setting 'disconnected' variable in the slave struct. This causes us to disconnect() a slave in cases where we really only want to deactivate() the slave (e.g., authentication).

It would be nice to differentiate these semantics by adding a new variable ""active"" in the Slave struct.

We might want to do the same with the Framework struct for consistency.",Improvement,Major,vinodkone,2014-09-25T20:55:57.000+0000,5,Resolved,Complete,Reconcile disconnected/deactivated semantics in the master code,2014-09-25T20:55:57.000+0000,MESOS-1811,3.0,mesos,Mesos Q3 Sprint 5
chzhcn,2014-09-17T22:59:19.000+0000,dhamon,"As we expose the bandwidth, so we should expose the RTT as a measure of latency each container is experiencing.

We can use {{ss}} to get the per-socket statistics and filter and aggregate accordingly to get a measure of RTT.",Task,Major,dhamon,2014-10-02T22:17:09.000+0000,5,Resolved,Complete,Expose RTT in container stats,2015-06-18T18:13:00.000+0000,MESOS-1808,3.0,mesos,Mesos Q3 Sprint 6
,2014-09-17T18:31:35.000+0000,vinodkone,"Currently master allows executors to be launched with either only cpus or only memory but we shouldn't allow that.
This is because executor is an actual unix process that is launched by the slave. If an executor doesn't specify cpus, what should the cpu limits be for that executor when there are no tasks running on it? If no cpu limits are set then it might starve other executors/tasks on the slave violating isolation guarantees. Same goes with memory. Moreover, the current containerizer/isolator code will throw failures when using such an executor, e.g., when the last task on the executor finishes and Containerizer::update() is called with 0 cpus or 0 mem.

According to a source code [TODO | https://github.com/apache/mesos/blob/0226620747e1769434a1a83da547bfc3470a9549/src/master/validation.cpp#L400] this should also include checking whether requested resources are greater than  MIN_CPUS/MIN_BYTES.",Improvement,Major,vinodkone,,10020,Accepted,In Progress,Disallow executors with cpu only or memory only resources,2016-03-31T03:11:34.000+0000,MESOS-1807,3.0,mesos,Twitter Q4 Sprint 1
vinodkone,2014-09-16T20:04:42.000+0000,bmahler,"When a slave re-registers with the master, it currently sends the latest task state for all tasks that are not both terminal and acknowledged.

However, reconciliation assumes that we always have the latest unacknowledged state of the task represented in the master.

As a result, out-of-order updates are possible, e.g.

(1) Slave has task T in TASK_FINISHED, with unacknowledged updates: [TASK_RUNNING, TASK_FINISHED].
(2) Master fails over.
(3) New master re-registers the slave with T in TASK_FINISHED.
(4) Reconciliation request arrives, master sends TASK_FINISHED.
(5) Slave sends TASK_RUNNING to master, master sends TASK_RUNNING.

I think the fix here is to preserve the task state invariants in the master, namely, that the master has the latest unacknowledged state of the task. This means when the slave re-registers, it should instead send the latest acknowledged state of each task.",Bug,Major,bmahler,2014-10-21T22:49:27.000+0000,5,Resolved,Complete,Reconciliation can send out-of-order updates.,2014-10-21T22:49:27.000+0000,MESOS-1799,3.0,mesos,Mesos Q3 Sprint 6
jklucar,2014-09-13T18:06:32.000+0000,vinodkone,"Mesos fetcher always chown()s the extracted executor URIs as the executor user but sometimes this is not desirable, e.g., ""setuid"" bit gets lost during chown() if slave/fetcher is running as root. 

It would be nice to give frameworks the ability to skip the chown.",Improvement,Major,vinodkone,,10006,Reviewable,New,"Add ""chown"" option to CommandInfo.URI",2016-04-14T16:05:18.000+0000,MESOS-1790,2.0,mesos,
vinodkone,2014-09-10T19:50:17.000+0000,vinodkone,"Currently, there is no easy way for frameworks to update their FrameworkInfo., resulting in issues like MESOS-703 and MESOS-1218.

This ticket captures the design for doing FrameworkInfo update without having to roll masters/slaves/tasks/executors.",Documentation,Major,vinodkone,2014-09-15T21:33:18.000+0000,5,Resolved,Complete,Design the semantics for updating FrameworkInfo,2015-06-12T19:17:11.000+0000,MESOS-1784,3.0,mesos,Mesos Q3 Sprint 5
vinodkone,2014-09-09T23:15:48.000+0000,xujyan,"{noformat:title=}
[ RUN      ] AllocatorTest/0.FrameworkExited
Using temporary directory '/tmp/AllocatorTest_0_FrameworkExited_B6WZng'
I0909 08:02:35.116555 18112 leveldb.cpp:176] Opened db in 31.64686ms
I0909 08:02:35.126065 18112 leveldb.cpp:183] Compacted db in 9.449823ms
I0909 08:02:35.126118 18112 leveldb.cpp:198] Created db iterator in 5858ns
I0909 08:02:35.126137 18112 leveldb.cpp:204] Seeked to beginning of db in 1136ns
I0909 08:02:35.126150 18112 leveldb.cpp:273] Iterated through 0 keys in the db in 560ns
I0909 08:02:35.126178 18112 replica.cpp:741] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned
I0909 08:02:35.126502 18133 recover.cpp:425] Starting replica recovery
I0909 08:02:35.126601 18133 recover.cpp:451] Replica is in EMPTY status
I0909 08:02:35.127012 18133 replica.cpp:638] Replica in EMPTY status received a broadcasted recover request
I0909 08:02:35.127094 18133 recover.cpp:188] Received a recover response from a replica in EMPTY status
I0909 08:02:35.127223 18133 recover.cpp:542] Updating replica status to STARTING
I0909 08:02:35.226631 18133 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 99.308134ms
I0909 08:02:35.226690 18133 replica.cpp:320] Persisted replica status to STARTING
I0909 08:02:35.226812 18131 recover.cpp:451] Replica is in STARTING status
I0909 08:02:35.227246 18131 replica.cpp:638] Replica in STARTING status received a broadcasted recover request
I0909 08:02:35.227308 18131 recover.cpp:188] Received a recover response from a replica in STARTING status
I0909 08:02:35.227409 18131 recover.cpp:542] Updating replica status to VOTING
I0909 08:02:35.228540 18129 master.cpp:286] Master 20140909-080235-16842879-44005-18112 (precise) started on 127.0.1.1:44005
I0909 08:02:35.228593 18129 master.cpp:332] Master only allowing authenticated frameworks to register
I0909 08:02:35.228607 18129 master.cpp:337] Master only allowing authenticated slaves to register
I0909 08:02:35.228620 18129 credentials.hpp:36] Loading credentials for authentication from '/tmp/AllocatorTest_0_FrameworkExited_B6WZng/credentials'
I0909 08:02:35.228754 18129 master.cpp:366] Authorization enabled
I0909 08:02:35.229560 18129 master.cpp:120] No whitelist given. Advertising offers for all slaves
I0909 08:02:35.229933 18129 hierarchical_allocator_process.hpp:299] Initializing hierarchical allocator process with master : master@127.0.1.1:44005
I0909 08:02:35.230057 18127 master.cpp:1212] The newly elected leader is master@127.0.1.1:44005 with id 20140909-080235-16842879-44005-18112
I0909 08:02:35.230129 18127 master.cpp:1225] Elected as the leading master!
I0909 08:02:35.230144 18127 master.cpp:1043] Recovering from registrar
I0909 08:02:35.230257 18127 registrar.cpp:313] Recovering registrar
I0909 08:02:35.232461 18131 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 4.999384ms
I0909 08:02:35.232489 18131 replica.cpp:320] Persisted replica status to VOTING
I0909 08:02:35.232544 18131 recover.cpp:556] Successfully joined the Paxos group
I0909 08:02:35.232611 18131 recover.cpp:440] Recover process terminated
I0909 08:02:35.232727 18131 log.cpp:656] Attempting to start the writer
I0909 08:02:35.233012 18131 replica.cpp:474] Replica received implicit promise request with proposal 1
I0909 08:02:35.238785 18131 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 5.749504ms
I0909 08:02:35.238818 18131 replica.cpp:342] Persisted promised to 1
I0909 08:02:35.244056 18131 coordinator.cpp:230] Coordinator attemping to fill missing position
I0909 08:02:35.244580 18131 replica.cpp:375] Replica received explicit promise request for position 0 with proposal 2
I0909 08:02:35.250143 18131 leveldb.cpp:343] Persisting action (8 bytes) to leveldb took 5.382351ms
I0909 08:02:35.250319 18131 replica.cpp:676] Persisted action at 0
I0909 08:02:35.250901 18131 replica.cpp:508] Replica received write request for position 0
I0909 08:02:35.251137 18131 leveldb.cpp:438] Reading position from leveldb took 18689ns
I0909 08:02:35.256597 18131 leveldb.cpp:343] Persisting action (14 bytes) to leveldb took 5.274169ms
I0909 08:02:35.256764 18131 replica.cpp:676] Persisted action at 0
I0909 08:02:35.263712 18126 replica.cpp:655] Replica received learned notice for position 0
I0909 08:02:35.269613 18126 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 5.417225ms
I0909 08:02:35.351641 18126 replica.cpp:676] Persisted action at 0
I0909 08:02:35.351655 18126 replica.cpp:661] Replica learned NOP action at position 0
I0909 08:02:35.351889 18126 log.cpp:672] Writer started with ending position 0
I0909 08:02:35.352165 18126 leveldb.cpp:438] Reading position from leveldb took 25215ns
I0909 08:02:35.353163 18126 registrar.cpp:346] Successfully fetched the registry (0B)
I0909 08:02:35.353185 18126 registrar.cpp:422] Attempting to update the 'registry'
I0909 08:02:35.354152 18126 log.cpp:680] Attempting to append 120 bytes to the log
I0909 08:02:35.354195 18126 coordinator.cpp:340] Coordinator attempting to write APPEND action at position 1
I0909 08:02:35.354416 18126 replica.cpp:508] Replica received write request for position 1
I0909 08:02:35.351579 18127 hierarchical_allocator_process.hpp:697] No resources available to allocate!
I0909 08:02:35.354558 18127 hierarchical_allocator_process.hpp:659] Performed allocation for 0 slaves in 2.984795ms
I0909 08:02:35.360254 18126 leveldb.cpp:343] Persisting action (137 bytes) to leveldb took 5.811986ms
I0909 08:02:35.360285 18126 replica.cpp:676] Persisted action at 1
I0909 08:02:35.364126 18132 replica.cpp:655] Replica received learned notice for position 1
I0909 08:02:35.369856 18132 leveldb.cpp:343] Persisting action (139 bytes) to leveldb took 5.702756ms
I0909 08:02:35.369899 18132 replica.cpp:676] Persisted action at 1
I0909 08:02:35.369910 18132 replica.cpp:661] Replica learned APPEND action at position 1
I0909 08:02:35.370209 18132 registrar.cpp:479] Successfully updated 'registry'
I0909 08:02:35.370311 18132 registrar.cpp:372] Successfully recovered registrar
I0909 08:02:35.370477 18132 log.cpp:699] Attempting to truncate the log to 1
I0909 08:02:35.370553 18132 master.cpp:1070] Recovered 0 slaves from the Registry (84B) ; allowing 10mins for slaves to re-register
I0909 08:02:35.370594 18132 coordinator.cpp:340] Coordinator attempting to write TRUNCATE action at position 2
I0909 08:02:35.371201 18127 replica.cpp:508] Replica received write request for position 2
I0909 08:02:35.376760 18127 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 5.264501ms
I0909 08:02:35.377105 18127 replica.cpp:676] Persisted action at 2
I0909 08:02:35.377770 18127 replica.cpp:655] Replica received learned notice for position 2
I0909 08:02:35.383363 18127 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 5.272769ms
I0909 08:02:35.383818 18127 leveldb.cpp:401] Deleting ~1 keys from leveldb took 28148ns
I0909 08:02:35.384137 18127 replica.cpp:676] Persisted action at 2
I0909 08:02:35.384399 18127 replica.cpp:661] Replica learned TRUNCATE action at position 2
I0909 08:02:35.396512 18127 slave.cpp:167] Slave started on 64)@127.0.1.1:44005
I0909 08:02:35.654770 18131 hierarchical_allocator_process.hpp:697] No resources available to allocate!
I0909 08:02:35.654847 18131 hierarchical_allocator_process.hpp:659] Performed allocation for 0 slaves in 104933ns
I0909 08:02:35.654974 18127 credentials.hpp:84] Loading credential for authentication from '/tmp/AllocatorTest_0_FrameworkExited_xV9Mk4/credential'
I0909 08:02:35.655097 18127 slave.cpp:274] Slave using credential for: test-principal
I0909 08:02:35.655203 18127 slave.cpp:287] Slave resources: cpus(*):3; mem(*):1024; disk(*):25116; ports(*):[31000-32000]
I0909 08:02:35.655274 18127 slave.cpp:315] Slave hostname: precise
I0909 08:02:35.655285 18127 slave.cpp:316] Slave checkpoint: false
I0909 08:02:35.655804 18127 state.cpp:33] Recovering state from '/tmp/AllocatorTest_0_FrameworkExited_xV9Mk4/meta'
I0909 08:02:35.655913 18127 status_update_manager.cpp:193] Recovering status update manager
I0909 08:02:35.656005 18127 slave.cpp:3202] Finished recovery
I0909 08:02:35.656251 18127 slave.cpp:598] New master detected at master@127.0.1.1:44005
I0909 08:02:35.656285 18127 slave.cpp:672] Authenticating with master master@127.0.1.1:44005
I0909 08:02:35.656325 18127 slave.cpp:645] Detecting new master
I0909 08:02:35.656358 18127 status_update_manager.cpp:167] New master detected at master@127.0.1.1:44005
I0909 08:02:35.656389 18127 authenticatee.hpp:128] Creating new client SASL connection
I0909 08:02:35.656563 18127 master.cpp:3653] Authenticating slave(64)@127.0.1.1:44005
I0909 08:02:35.656651 18127 authenticator.hpp:156] Creating new server SASL connection
I0909 08:02:35.656770 18127 authenticatee.hpp:219] Received SASL authentication mechanisms: CRAM-MD5
I0909 08:02:35.656796 18127 authenticatee.hpp:245] Attempting to authenticate with mechanism 'CRAM-MD5'
I0909 08:02:35.656822 18127 authenticator.hpp:262] Received SASL authentication start
I0909 08:02:35.656858 18127 authenticator.hpp:384] Authentication requires more steps
I0909 08:02:35.656883 18127 authenticatee.hpp:265] Received SASL authentication step
I0909 08:02:35.656924 18127 authenticator.hpp:290] Received SASL authentication step
I0909 08:02:35.656960 18127 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'precise' server FQDN: 'precise' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0909 08:02:35.656971 18127 auxprop.cpp:153] Looking up auxiliary property '*userPassword'
I0909 08:02:35.656982 18127 auxprop.cpp:153] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0909 08:02:35.656997 18127 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'precise' server FQDN: 'precise' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0909 08:02:35.657004 18127 auxprop.cpp:103] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0909 08:02:35.657008 18127 auxprop.cpp:103] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0909 08:02:35.657019 18127 authenticator.hpp:376] Authentication success
I0909 08:02:35.657047 18127 authenticatee.hpp:305] Authentication success
I0909 08:02:35.657073 18127 master.cpp:3693] Successfully authenticated principal 'test-principal' at slave(64)@127.0.1.1:44005
I0909 08:02:35.657145 18127 slave.cpp:729] Successfully authenticated with master master@127.0.1.1:44005
I0909 08:02:35.657183 18127 slave.cpp:980] Will retry registration in 19.238717ms if necessary
I0909 08:02:35.657276 18128 master.cpp:2843] Registering slave at slave(64)@127.0.1.1:44005 (precise) with id 20140909-080235-16842879-44005-18112-0
I0909 08:02:35.657389 18128 registrar.cpp:422] Attempting to update the 'registry'
I0909 08:02:35.658382 18130 log.cpp:680] Attempting to append 295 bytes to the log
I0909 08:02:35.658432 18130 coordinator.cpp:340] Coordinator attempting to write APPEND action at position 3
I0909 08:02:35.658635 18130 replica.cpp:508] Replica received write request for position 3
I0909 08:02:35.660959 18112 sched.cpp:137] Version: 0.21.0
I0909 08:02:35.661093 18126 sched.cpp:233] New master detected at master@127.0.1.1:44005
I0909 08:02:35.661111 18126 sched.cpp:283] Authenticating with master master@127.0.1.1:44005
I0909 08:02:35.661175 18126 authenticatee.hpp:128] Creating new client SASL connection
I0909 08:02:35.661306 18126 master.cpp:3653] Authenticating scheduler-fd929918-7057-4fef-923a-ed9d6fd355be@127.0.1.1:44005
I0909 08:02:35.661376 18126 authenticator.hpp:156] Creating new server SASL connection
I0909 08:02:35.661466 18126 authenticatee.hpp:219] Received SASL authentication mechanisms: CRAM-MD5
I0909 08:02:35.661483 18126 authenticatee.hpp:245] Attempting to authenticate with mechanism 'CRAM-MD5'
I0909 08:02:35.661504 18126 authenticator.hpp:262] Received SASL authentication start
I0909 08:02:35.661530 18126 authenticator.hpp:384] Authentication requires more steps
I0909 08:02:35.661552 18126 authenticatee.hpp:265] Received SASL authentication step
I0909 08:02:35.661579 18126 authenticator.hpp:290] Received SASL authentication step
I0909 08:02:35.661592 18126 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'precise' server FQDN: 'precise' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0909 08:02:35.661598 18126 auxprop.cpp:153] Looking up auxiliary property '*userPassword'
I0909 08:02:35.661607 18126 auxprop.cpp:153] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0909 08:02:35.661613 18126 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'precise' server FQDN: 'precise' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0909 08:02:35.661619 18126 auxprop.cpp:103] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0909 08:02:35.661623 18126 auxprop.cpp:103] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0909 08:02:35.661633 18126 authenticator.hpp:376] Authentication success
I0909 08:02:35.661653 18126 authenticatee.hpp:305] Authentication success
I0909 08:02:35.661672 18126 master.cpp:3693] Successfully authenticated principal 'test-principal' at scheduler-fd929918-7057-4fef-923a-ed9d6fd355be@127.0.1.1:44005
I0909 08:02:35.661730 18126 sched.cpp:357] Successfully authenticated with master master@127.0.1.1:44005
I0909 08:02:35.661741 18126 sched.cpp:476] Sending registration request to master@127.0.1.1:44005
I0909 08:02:35.661782 18126 master.cpp:1331] Received registration request from scheduler-fd929918-7057-4fef-923a-ed9d6fd355be@127.0.1.1:44005
I0909 08:02:35.661798 18126 master.cpp:1291] Authorizing framework principal 'test-principal' to receive offers for role '*'
I0909 08:02:35.661917 18126 master.cpp:1390] Registering framework 20140909-080235-16842879-44005-18112-0000 at scheduler-fd929918-7057-4fef-923a-ed9d6fd355be@127.0.1.1:44005
I0909 08:02:35.662017 18126 sched.cpp:407] Framework registered with 20140909-080235-16842879-44005-18112-0000
I0909 08:02:35.662039 18126 sched.cpp:421] Scheduler::registered took 9070ns
I0909 08:02:35.662119 18126 hierarchical_allocator_process.hpp:329] Added framework 20140909-080235-16842879-44005-18112-0000
I0909 08:02:35.662130 18126 hierarchical_allocator_process.hpp:697] No resources available to allocate!
I0909 08:02:35.662135 18126 hierarchical_allocator_process.hpp:659] Performed allocation for 0 slaves in 5558ns
I0909 08:02:35.672230 18130 leveldb.cpp:343] Persisting action (314 bytes) to leveldb took 13.567526ms
I0909 08:02:35.672268 18130 replica.cpp:676] Persisted action at 3
I0909 08:02:35.672483 18130 replica.cpp:655] Replica received learned notice for position 3
I0909 08:02:35.677322 18132 slave.cpp:980] Will retry registration in 14.890338ms if necessary
I0909 08:02:35.677399 18132 master.cpp:2831] Ignoring register slave message from slave(64)@127.0.1.1:44005 (precise) as admission is already in progress
I0909 08:02:35.680881 18130 leveldb.cpp:343] Persisting action (316 bytes) to leveldb took 8.376798ms
I0909 08:02:35.680908 18130 replica.cpp:676] Persisted action at 3
I0909 08:02:35.680917 18130 replica.cpp:661] Replica learned APPEND action at position 3
I0909 08:02:35.681252 18130 registrar.cpp:479] Successfully updated 'registry'
I0909 08:02:35.681330 18130 log.cpp:699] Attempting to truncate the log to 3
I0909 08:02:35.681385 18130 master.cpp:2883] Registered slave 20140909-080235-16842879-44005-18112-0 at slave(64)@127.0.1.1:44005 (precise)
I0909 08:02:35.681399 18130 master.cpp:4126] Adding slave 20140909-080235-16842879-44005-18112-0 at slave(64)@127.0.1.1:44005 (precise) with cpus(*):3; mem(*):1024; disk(*):25116; ports(*):[31000-32000]
I0909 08:02:35.681504 18130 coordinator.cpp:340] Coordinator attempting to write TRUNCATE action at position 4
I0909 08:02:35.681570 18130 slave.cpp:763] Registered with master master@127.0.1.1:44005; given slave ID 20140909-080235-16842879-44005-18112-0
I0909 08:02:35.681689 18130 slave.cpp:2329] Received ping from slave-observer(50)@127.0.1.1:44005
I0909 08:02:35.681753 18130 hierarchical_allocator_process.hpp:442] Added slave 20140909-080235-16842879-44005-18112-0 (precise) with cpus(*):3; mem(*):1024; disk(*):25116; ports(*):[31000-32000] (and cpus(*):3; mem(*):1024; disk(*):25116; ports(*):[31000-32000] available)
I0909 08:02:35.681808 18130 hierarchical_allocator_process.hpp:734] Offering cpus(*):3; mem(*):1024; disk(*):25116; ports(*):[31000-32000] on slave 20140909-080235-16842879-44005-18112-0 to framework 20140909-080235-16842879-44005-18112-0000
I0909 08:02:35.681892 18130 hierarchical_allocator_process.hpp:679] Performed allocation for slave 20140909-080235-16842879-44005-18112-0 in 109580ns
I0909 08:02:35.681968 18130 master.hpp:861] Adding offer 20140909-080235-16842879-44005-18112-0 with resources cpus(*):3; mem(*):1024; disk(*):25116; ports(*):[31000-32000] on slave 20140909-080235-16842879-44005-18112-0 (precise)
I0909 08:02:35.682014 18130 master.cpp:3600] Sending 1 offers to framework 20140909-080235-16842879-44005-18112-0000
I0909 08:02:35.682443 18130 sched.cpp:544] Scheduler::resourceOffers took 254258ns
I0909 08:02:35.682633 18130 master.hpp:871] Removing offer 20140909-080235-16842879-44005-18112-0 with resources cpus(*):3; mem(*):1024; disk(*):25116; ports(*):[31000-32000] on slave 20140909-080235-16842879-44005-18112-0 (precise)
I0909 08:02:35.682684 18130 master.cpp:2201] Processing reply for offers: [ 20140909-080235-16842879-44005-18112-0 ] on slave 20140909-080235-16842879-44005-18112-0 at slave(64)@127.0.1.1:44005 (precise) for framework 20140909-080235-16842879-44005-18112-0000
I0909 08:02:35.682708 18130 master.cpp:2284] Authorizing framework principal 'test-principal' to launch task 0 as user 'jenkins'
I0909 08:02:35.682971 18130 replica.cpp:508] Replica received write request for position 4
I0909 08:02:35.683132 18132 master.hpp:833] Adding task 0 with resources cpus(*):2; mem(*):512 on slave 20140909-080235-16842879-44005-18112-0 (precise)
I0909 08:02:35.683159 18132 master.cpp:2350] Launching task 0 of framework 20140909-080235-16842879-44005-18112-0000 with resources cpus(*):2; mem(*):512 on slave 20140909-080235-16842879-44005-18112-0 at slave(64)@127.0.1.1:44005 (precise)
I0909 08:02:35.683363 18132 slave.cpp:1011] Got assigned task 0 for framework 20140909-080235-16842879-44005-18112-0000
I0909 08:02:35.683580 18132 slave.cpp:1121] Launching task 0 for framework 20140909-080235-16842879-44005-18112-0000
I0909 08:02:35.684833 18133 hierarchical_allocator_process.hpp:563] Recovered cpus(*):1; mem(*):512; disk(*):25116; ports(*):[31000-32000] (total allocatable: cpus(*):1; mem(*):512; disk(*):25116; ports(*):[31000-32000]) on slave 20140909-080235-16842879-44005-18112-0 from framework 20140909-080235-16842879-44005-18112-0000
I0909 08:02:35.684864 18133 hierarchical_allocator_process.hpp:599] Framework 20140909-080235-16842879-44005-18112-0000 filtered slave 20140909-080235-16842879-44005-18112-0 for 5secs
I0909 08:02:35.686401 18132 exec.cpp:132] Version: 0.21.0
I0909 08:02:35.686848 18128 exec.cpp:182] Executor started at: executor(8)@127.0.1.1:44005 with pid 18112
I0909 08:02:35.687095 18132 slave.cpp:1231] Queuing task '0' for executor executor-1 of framework '20140909-080235-16842879-44005-18112-0000
I0909 08:02:35.687302 18132 slave.cpp:552] Successfully attached file '/tmp/AllocatorTest_0_FrameworkExited_xV9Mk4/slaves/20140909-080235-16842879-44005-18112-0/frameworks/20140909-080235-16842879-44005-18112-0000/executors/executor-1/runs/c4458e43-94ee-4b5e-bd74-5d39a09deff6'
I0909 08:02:35.687568 18132 slave.cpp:1741] Got registration for executor 'executor-1' of framework 20140909-080235-16842879-44005-18112-0000
I0909 08:02:35.687893 18127 exec.cpp:206] Executor registered on slave 20140909-080235-16842879-44005-18112-0
I0909 08:02:35.688789 18127 exec.cpp:218] Executor::registered took 15015ns
I0909 08:02:35.688977 18132 slave.cpp:1859] Flushing queued task 0 for executor 'executor-1' of framework 20140909-080235-16842879-44005-18112-0000
I0909 08:02:35.689260 18133 exec.cpp:293] Executor asked to run task '0'
I0909 08:02:35.689441 18133 exec.cpp:302] Executor::launchTask took 24599ns
I0909 08:02:35.691651 18112 sched.cpp:137] Version: 0.21.0
I0909 08:02:35.691946 18131 sched.cpp:233] New master detected at master@127.0.1.1:44005
I0909 08:02:35.692126 18131 sched.cpp:283] Authenticating with master master@127.0.1.1:44005
I0909 08:02:35.692399 18131 authenticatee.hpp:128] Creating new client SASL connection
I0909 08:02:35.692791 18131 master.cpp:3653] Authenticating scheduler-6e711fc6-aad6-48bd-9ce7-2316b45c5482@127.0.1.1:44005
I0909 08:02:35.693068 18131 authenticator.hpp:156] Creating new server SASL connection
I0909 08:02:35.693351 18131 authenticatee.hpp:219] Received SASL authentication mechanisms: CRAM-MD5
I0909 08:02:35.693532 18131 authenticatee.hpp:245] Attempting to authenticate with mechanism 'CRAM-MD5'
I0909 08:02:35.693739 18131 authenticator.hpp:262] Received SASL authentication start
I0909 08:02:35.693979 18131 authenticator.hpp:384] Authentication requires more steps
I0909 08:02:35.694202 18131 authenticatee.hpp:265] Received SASL authentication step
I0909 08:02:35.694449 18131 authenticator.hpp:290] Received SASL authentication step
I0909 08:02:35.694633 18131 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'precise' server FQDN: 'precise' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0909 08:02:35.694792 18131 auxprop.cpp:153] Looking up auxiliary property '*userPassword'
I0909 08:02:35.694980 18131 auxprop.cpp:153] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0909 08:02:35.695158 18131 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'precise' server FQDN: 'precise' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0909 08:02:35.695369 18131 auxprop.cpp:103] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0909 08:02:35.695724 18131 auxprop.cpp:103] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0909 08:02:35.695907 18131 authenticator.hpp:376] Authentication success
I0909 08:02:35.696117 18128 authenticatee.hpp:305] Authentication success
I0909 08:02:35.698509 18130 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 15.520863ms
I0909 08:02:35.698698 18130 replica.cpp:676] Persisted action at 4
I0909 08:02:35.698940 18128 sched.cpp:357] Successfully authenticated with master master@127.0.1.1:44005
I0909 08:02:35.699095 18126 master.cpp:3693] Successfully authenticated principal 'test-principal' at scheduler-6e711fc6-aad6-48bd-9ce7-2316b45c5482@127.0.1.1:44005
I0909 08:02:35.699354 18130 replica.cpp:655] Replica received learned notice for position 4
I0909 08:02:35.699973 18128 sched.cpp:476] Sending registration request to master@127.0.1.1:44005
I0909 08:02:35.700265 18128 master.cpp:1331] Received registration request from scheduler-6e711fc6-aad6-48bd-9ce7-2316b45c5482@127.0.1.1:44005
I0909 08:02:35.700515 18128 master.cpp:1291] Authorizing framework principal 'test-principal' to receive offers for role '*'
I0909 08:02:35.700809 18128 master.cpp:1390] Registering framework 20140909-080235-16842879-44005-18112-0001 at scheduler-6e711fc6-aad6-48bd-9ce7-2316b45c5482@127.0.1.1:44005
I0909 08:02:35.701037 18133 sched.cpp:407] Framework registered with 20140909-080235-16842879-44005-18112-0001
I0909 08:02:35.701211 18133 sched.cpp:421] Scheduler::registered took 11991ns
I0909 08:02:35.701488 18131 hierarchical_allocator_process.hpp:329] Added framework 20140909-080235-16842879-44005-18112-0001
I0909 08:02:35.701728 18131 hierarchical_allocator_process.hpp:734] Offering cpus(*):1; mem(*):512; disk(*):25116; ports(*):[31000-32000] on slave 20140909-080235-16842879-44005-18112-0 to framework 20140909-080235-16842879-44005-18112-0001
I0909 08:02:35.701992 18131 hierarchical_allocator_process.hpp:659] Performed allocation for 1 slaves in 297969ns
I0909 08:02:35.702229 18128 master.hpp:861] Adding offer 20140909-080235-16842879-44005-18112-1 with resources cpus(*):1; mem(*):512; disk(*):25116; ports(*):[31000-32000] on slave 20140909-080235-16842879-44005-18112-0 (precise)
I0909 08:02:35.702481 18128 master.cpp:3600] Sending 1 offers to framework 20140909-080235-16842879-44005-18112-0001
I0909 08:02:35.702901 18129 sched.cpp:544] Scheduler::resourceOffers took 127949ns
I0909 08:02:35.703305 18128 master.hpp:871] Removing offer 20140909-080235-16842879-44005-18112-1 with resources cpus(*):1; mem(*):512; disk(*):25116; ports(*):[31000-32000] on slave 20140909-080235-16842879-44005-18112-0 (precise)
I0909 08:02:35.703629 18128 master.cpp:2201] Processing reply for offers: [ 20140909-080235-16842879-44005-18112-1 ] on slave 20140909-080235-16842879-44005-18112-0 at slave(64)@127.0.1.1:44005 (precise) for framework 20140909-080235-16842879-44005-18112-0001
I0909 08:02:35.703908 18128 master.cpp:2284] Authorizing framework principal 'test-principal' to launch task 0 as user 'jenkins'
I0909 08:02:35.703789 18132 slave.cpp:2542] Monitoring executor 'executor-1' of framework '20140909-080235-16842879-44005-18112-0000' in container 'c4458e43-94ee-4b5e-bd74-5d39a09deff6'
I0909 08:02:35.704763 18128 master.hpp:833] Adding task 0 with resources cpus(*):1; mem(*):256 on slave 20140909-080235-16842879-44005-18112-0 (precise)
I0909 08:02:35.704951 18128 master.cpp:2350] Launching task 0 of framework 20140909-080235-16842879-44005-18112-0001 with resources cpus(*):1; mem(*):256 on slave 20140909-080235-16842879-44005-18112-0 at slave(64)@127.0.1.1:44005 (precise)
I0909 08:02:35.705255 18129 slave.cpp:1011] Got assigned task 0 for framework 20140909-080235-16842879-44005-18112-0001
I0909 08:02:35.705582 18129 slave.cpp:1121] Launching task 0 for framework 20140909-080235-16842879-44005-18112-0001
I0909 08:02:35.707756 18129 exec.cpp:132] Version: 0.21.0
I0909 08:02:35.708035 18130 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 8.127072ms
I0909 08:02:35.708281 18130 leveldb.cpp:401] Deleting ~2 keys from leveldb took 28817ns
I0909 08:02:35.708459 18130 replica.cpp:676] Persisted action at 4
I0909 08:02:35.708632 18130 replica.cpp:661] Replica learned TRUNCATE action at position 4
I0909 08:02:35.708869 18133 exec.cpp:182] Executor started at: executor(9)@127.0.1.1:44005 with pid 18112
I0909 08:02:35.709120 18127 hierarchical_allocator_process.hpp:659] Performed allocation for 1 slaves in 35083ns
I0909 08:02:35.709511 18129 slave.cpp:1231] Queuing task '0' for executor executor-2 of framework '20140909-080235-16842879-44005-18112-0001
I0909 08:02:35.709707 18129 slave.cpp:552] Successfully attached file '/tmp/AllocatorTest_0_FrameworkExited_xV9Mk4/slaves/20140909-080235-16842879-44005-18112-0/frameworks/20140909-080235-16842879-44005-18112-0001/executors/executor-2/runs/7654870b-fd36-40b2-aac7-37b1bcfa821e'
I0909 08:02:35.709913 18129 slave.cpp:1741] Got registration for executor 'executor-2' of framework 20140909-080235-16842879-44005-18112-0001
I0909 08:02:35.710188 18129 slave.cpp:1859] Flushing queued task 0 for executor 'executor-2' of framework 20140909-080235-16842879-44005-18112-0001
I0909 08:02:35.710516 18129 slave.cpp:2542] Monitoring executor 'executor-2' of framework '20140909-080235-16842879-44005-18112-0001' in container '7654870b-fd36-40b2-aac7-37b1bcfa821e'
I0909 08:02:35.710321 18130 exec.cpp:206] Executor registered on slave 20140909-080235-16842879-44005-18112-0
I0909 08:02:35.711678 18130 exec.cpp:218] Executor::registered took 14355ns
I0909 08:02:35.711987 18130 exec.cpp:293] Executor asked to run task '0'
I0909 08:02:35.715551 18130 exec.cpp:302] Executor::launchTask took 3.40476ms
I0909 08:02:35.716006 18131 sched.cpp:745] Stopping framework '20140909-080235-16842879-44005-18112-0000'
I0909 08:02:35.716292 18128 master.cpp:1640] Asked to unregister framework 20140909-080235-16842879-44005-18112-0000
I0909 08:02:35.716490 18127 hierarchical_allocator_process.hpp:563] Recovered mem(*):256; disk(*):25116; ports(*):[31000-32000] (total allocatable: mem(*):256; disk(*):25116; ports(*):[31000-32000]) on slave 20140909-080235-16842879-44005-18112-0 from framework 20140909-080235-16842879-44005-18112-0001
I0909 08:02:35.716792 18127 hierarchical_allocator_process.hpp:599] Framework 20140909-080235-16842879-44005-18112-0001 filtered slave 20140909-080235-16842879-44005-18112-0 for 5secs
I0909 08:02:35.717018 18128 master.cpp:3976] Removing framework 20140909-080235-16842879-44005-18112-0000
I0909 08:02:35.717269 18128 master.hpp:851] Removing task 0 with resources cpus(*):2; mem(*):512 on slave 20140909-080235-16842879-44005-18112-0 (precise)
W0909 08:02:35.717607 18128 master.cpp:4419] Removing task 0 of framework 20140909-080235-16842879-44005-18112-0000 and slave 20140909-080235-16842879-44005-18112-0 in non-terminal state TASK_STAGING
I0909 08:02:35.717470 18131 hierarchical_allocator_process.hpp:405] Deactivated framework 20140909-080235-16842879-44005-18112-0000
I0909 08:02:35.718065 18131 hierarchical_allocator_process.hpp:563] Recovered cpus(*):2; mem(*):512 (total allocatable: mem(*):768; disk(*):25116; ports(*):[31000-32000]; cpus(*):2) on slave 20140909-080235-16842879-44005-18112-0 from framework 20140909-080235-16842879-44005-18112-0000
I0909 08:02:35.717438 18132 slave.cpp:1414] Asked to shut down framework 20140909-080235-16842879-44005-18112-0000 by master@127.0.1.1:44005
I0909 08:02:35.718444 18132 slave.cpp:1439] Shutting down framework 20140909-080235-16842879-44005-18112-0000
I0909 08:02:35.718621 18132 slave.cpp:2882] Shutting down executor 'executor-1' of framework 20140909-080235-16842879-44005-18112-0000
I0909 08:02:35.718843 18133 exec.cpp:379] Executor asked to shutdown
I0909 08:02:35.719022 18133 exec.cpp:394] Executor::shutdown took 13745ns
I0909 08:02:35.722009 18128 hierarchical_allocator_process.hpp:360] Removed framework 20140909-080235-16842879-44005-18112-0000
I0909 08:02:35.830785 18131 hierarchical_allocator_process.hpp:734] Offering mem(*):768; disk(*):25116; ports(*):[31000-32000]; cpus(*):2 on slave 20140909-080235-16842879-44005-18112-0 to framework 20140909-080235-16842879-44005-18112-0001
I0909 08:02:35.830940 18131 hierarchical_allocator_process.hpp:659] Performed allocation for 1 slaves in 218030ns
I0909 08:02:35.831056 18127 master.hpp:861] Adding offer 20140909-080235-16842879-44005-18112-2 with resources mem(*):768; disk(*):25116; ports(*):[31000-32000]; cpus(*):2 on slave 20140909-080235-16842879-44005-18112-0 (precise)
I0909 08:02:35.831115 18127 master.cpp:3600] Sending 1 offers to framework 20140909-080235-16842879-44005-18112-0001
I0909 08:02:35.831248 18127 sched.cpp:544] Scheduler::resourceOffers took 18178ns
I0909 08:02:35.831387 18112 master.cpp:650] Master terminating
I0909 08:02:35.831441 18112 master.hpp:851] Removing task 0 with resources cpus(*):1; mem(*):256 on slave 20140909-080235-16842879-44005-18112-0 (precise)
W0909 08:02:35.831488 18112 master.cpp:4419] Removing task 0 of framework 20140909-080235-16842879-44005-18112-0001 and slave 20140909-080235-16842879-44005-18112-0 in non-terminal state TASK_STAGING
I0909 08:02:35.831573 18112 master.hpp:871] Removing offer 20140909-080235-16842879-44005-18112-2 with resources mem(*):768; disk(*):25116; ports(*):[31000-32000]; cpus(*):2 on slave 20140909-080235-16842879-44005-18112-0 (precise)
I0909 08:02:35.832608 18112 slave.cpp:475] Slave terminating
I0909 08:02:35.832630 18112 slave.cpp:1414] Asked to shut down framework 20140909-080235-16842879-44005-18112-0000 by @0.0.0.0:0
W0909 08:02:35.832643 18112 slave.cpp:1435] Ignoring shutdown framework 20140909-080235-16842879-44005-18112-0000 because it is terminating
I0909 08:02:35.832648 18112 slave.cpp:1414] Asked to shut down framework 20140909-080235-16842879-44005-18112-0001 by @0.0.0.0:0
I0909 08:02:35.832654 18112 slave.cpp:1439] Shutting down framework 20140909-080235-16842879-44005-18112-0001
I0909 08:02:35.832664 18112 slave.cpp:2882] Shutting down executor 'executor-2' of framework 20140909-080235-16842879-44005-18112-0001
tests/allocator_tests.cpp:1444: Failure
Actual function call count doesn't match EXPECT_CALL(this->allocator, resourcesRecovered(_, _, _, _))...
         Expected: to be called once
           Actual: never called - unsatisfied and active
[  FAILED  ] AllocatorTest/0.FrameworkExited, where TypeParam = mesos::internal::master::allocator::HierarchicalAllocatorProcess<mesos::internal::master::allocator::DRFSorter, mesos::internal::master::allocator::DRFSorter> (756 ms)
{noformat}",Bug,Major,xujyan,2014-09-29T21:10:48.000+0000,5,Resolved,Complete,AllocatorTest/0.FrameworkExited is flaky,2014-09-29T21:10:48.000+0000,MESOS-1782,1.0,mesos,Mesos Q3 Sprint 6
ijimenez,2014-09-09T15:19:34.000+0000,alex-mesos,"Currently we can provide the default value for a flag, but cannot check if the flag is set to a reasonable value and, e.g., issue a warning. Passing an optional lambda checker to {{FlagBase::add()}} can be a possible solution.",Improvement,Minor,alexr,2016-02-10T16:48:18.000+0000,5,Resolved,Complete,Provide an option to validate flag value in stout/flags. ,2016-02-10T16:48:19.000+0000,MESOS-1778,3.0,mesos,
dhamon,2014-09-05T23:01:41.000+0000,dhamon,"* add unique_ptr to the configure check
* document use of unique_ptr in style guide
** use when possible, use std::move when necessary
* move raw pointers to Owned to establish ownership
* deprecate Owned in favour of unique_ptr
",Improvement,Major,dhamon,2014-10-24T16:56:59.000+0000,5,Resolved,Complete,introduce unique_ptr,2014-10-27T17:19:30.000+0000,MESOS-1771,1.0,mesos,Twitter Mesos Q4 Sprint 2
vinodkone,2014-09-05T17:24:31.000+0000,vinodkone,"{code}
[ RUN      ] MasterAuthorizationTest.DuplicateRegistration
Using temporary directory '/tmp/MasterAuthorizationTest_DuplicateRegistration_pVJg7m'
I0905 15:53:16.398993 25769 leveldb.cpp:176] Opened db in 2.601036ms
I0905 15:53:16.399566 25769 leveldb.cpp:183] Compacted db in 546216ns
I0905 15:53:16.399590 25769 leveldb.cpp:198] Created db iterator in 2787ns
I0905 15:53:16.399605 25769 leveldb.cpp:204] Seeked to beginning of db in 500ns
I0905 15:53:16.399617 25769 leveldb.cpp:273] Iterated through 0 keys in the db in 185ns
I0905 15:53:16.399633 25769 replica.cpp:741] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned
I0905 15:53:16.399817 25786 recover.cpp:425] Starting replica recovery
I0905 15:53:16.399952 25793 recover.cpp:451] Replica is in EMPTY status
I0905 15:53:16.400683 25795 replica.cpp:638] Replica in EMPTY status received a broadcasted recover request
I0905 15:53:16.400795 25787 recover.cpp:188] Received a recover response from a replica in EMPTY status
I0905 15:53:16.401005 25783 recover.cpp:542] Updating replica status to STARTING
I0905 15:53:16.401470 25786 master.cpp:286] Master 20140905-155316-3125920579-49188-25769 (penates.apache.org) started on 67.195.81.186:49188
I0905 15:53:16.401521 25786 master.cpp:332] Master only allowing authenticated frameworks to register
I0905 15:53:16.401533 25786 master.cpp:337] Master only allowing authenticated slaves to register
I0905 15:53:16.401543 25786 credentials.hpp:36] Loading credentials for authentication from '/tmp/MasterAuthorizationTest_DuplicateRegistration_pVJg7m/credentials'
I0905 15:53:16.401558 25793 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 474683ns
I0905 15:53:16.401582 25793 replica.cpp:320] Persisted replica status to STARTING
I0905 15:53:16.401667 25793 recover.cpp:451] Replica is in STARTING status
I0905 15:53:16.401669 25786 master.cpp:366] Authorization enabled
I0905 15:53:16.401898 25795 master.cpp:120] No whitelist given. Advertising offers for all slaves
I0905 15:53:16.401936 25796 hierarchical_allocator_process.hpp:299] Initializing hierarchical allocator process with master : master@67.195.81.186:49188
I0905 15:53:16.402160 25784 replica.cpp:638] Replica in STARTING status received a broadcasted recover request
I0905 15:53:16.402333 25790 master.cpp:1205] The newly elected leader is master@67.195.81.186:49188 with id 20140905-155316-3125920579-49188-25769
I0905 15:53:16.402359 25790 master.cpp:1218] Elected as the leading master!
I0905 15:53:16.402371 25790 master.cpp:1036] Recovering from registrar
I0905 15:53:16.402472 25798 registrar.cpp:313] Recovering registrar
I0905 15:53:16.402529 25791 recover.cpp:188] Received a recover response from a replica in STARTING status
I0905 15:53:16.402782 25788 recover.cpp:542] Updating replica status to VOTING
I0905 15:53:16.403002 25795 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 116403ns
I0905 15:53:16.403020 25795 replica.cpp:320] Persisted replica status to VOTING
I0905 15:53:16.403081 25791 recover.cpp:556] Successfully joined the Paxos group
I0905 15:53:16.403197 25791 recover.cpp:440] Recover process terminated
I0905 15:53:16.403388 25796 log.cpp:656] Attempting to start the writer
I0905 15:53:16.403993 25784 replica.cpp:474] Replica received implicit promise request with proposal 1
I0905 15:53:16.404147 25784 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 132156ns
I0905 15:53:16.404167 25784 replica.cpp:342] Persisted promised to 1
I0905 15:53:16.404542 25795 coordinator.cpp:230] Coordinator attemping to fill missing position
I0905 15:53:16.405498 25787 replica.cpp:375] Replica received explicit promise request for position 0 with proposal 2
I0905 15:53:16.405868 25787 leveldb.cpp:343] Persisting action (8 bytes) to leveldb took 347231ns
I0905 15:53:16.405886 25787 replica.cpp:676] Persisted action at 0
I0905 15:53:16.406553 25788 replica.cpp:508] Replica received write request for position 0
I0905 15:53:16.406582 25788 leveldb.cpp:438] Reading position from leveldb took 11402ns
I0905 15:53:16.529067 25788 leveldb.cpp:343] Persisting action (14 bytes) to leveldb took 535803ns
I0905 15:53:16.529088 25788 replica.cpp:676] Persisted action at 0
I0905 15:53:16.529355 25784 replica.cpp:655] Replica received learned notice for position 0
I0905 15:53:16.529784 25784 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 406036ns
I0905 15:53:16.529806 25784 replica.cpp:676] Persisted action at 0
I0905 15:53:16.529817 25784 replica.cpp:661] Replica learned NOP action at position 0
I0905 15:53:16.530108 25783 log.cpp:672] Writer started with ending position 0
I0905 15:53:16.530597 25792 leveldb.cpp:438] Reading position from leveldb took 14594ns
I0905 15:53:16.532060 25787 registrar.cpp:346] Successfully fetched the registry (0B)
I0905 15:53:16.532091 25787 registrar.cpp:422] Attempting to update the 'registry'
I0905 15:53:16.533537 25785 log.cpp:680] Attempting to append 140 bytes to the log
I0905 15:53:16.533596 25785 coordinator.cpp:340] Coordinator attempting to write APPEND action at position 1
I0905 15:53:16.533998 25798 replica.cpp:508] Replica received write request for position 1
I0905 15:53:16.534397 25798 leveldb.cpp:343] Persisting action (159 bytes) to leveldb took 372452ns
I0905 15:53:16.534416 25798 replica.cpp:676] Persisted action at 1
I0905 15:53:16.534808 25793 replica.cpp:655] Replica received learned notice for position 1
I0905 15:53:16.534996 25793 leveldb.cpp:343] Persisting action (161 bytes) to leveldb took 164609ns
I0905 15:53:16.535014 25793 replica.cpp:676] Persisted action at 1
I0905 15:53:16.535025 25793 replica.cpp:661] Replica learned APPEND action at position 1
I0905 15:53:16.535368 25784 registrar.cpp:479] Successfully updated 'registry'
I0905 15:53:16.535419 25784 registrar.cpp:372] Successfully recovered registrar
I0905 15:53:16.535452 25785 log.cpp:699] Attempting to truncate the log to 1
I0905 15:53:16.535555 25791 coordinator.cpp:340] Coordinator attempting to write TRUNCATE action at position 2
I0905 15:53:16.535553 25792 master.cpp:1063] Recovered 0 slaves from the Registry (102B) ; allowing 10mins for slaves to re-register
I0905 15:53:16.536038 25784 replica.cpp:508] Replica received write request for position 2
I0905 15:53:16.536166 25784 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 101619ns
I0905 15:53:16.536185 25784 replica.cpp:676] Persisted action at 2
I0905 15:53:16.536497 25791 replica.cpp:655] Replica received learned notice for position 2
I0905 15:53:16.536633 25791 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 109281ns
I0905 15:53:16.536664 25791 leveldb.cpp:401] Deleting ~1 keys from leveldb took 14164ns
I0905 15:53:16.536677 25791 replica.cpp:676] Persisted action at 2
I0905 15:53:16.536689 25791 replica.cpp:661] Replica learned TRUNCATE action at position 2
I0905 15:53:16.548408 25769 sched.cpp:137] Version: 0.21.0
I0905 15:53:16.548627 25792 sched.cpp:233] New master detected at master@67.195.81.186:49188
I0905 15:53:16.548653 25792 sched.cpp:283] Authenticating with master master@67.195.81.186:49188
I0905 15:53:16.548857 25797 authenticatee.hpp:128] Creating new client SASL connection
I0905 15:53:16.548950 25797 master.cpp:3637] Authenticating scheduler-33430370-6af5-4c7b-bbd8-f6a43269ecf5@67.195.81.186:49188
I0905 15:53:16.549041 25797 authenticator.hpp:156] Creating new server SASL connection
I0905 15:53:16.549120 25797 authenticatee.hpp:219] Received SASL authentication mechanisms: CRAM-MD5
I0905 15:53:16.549141 25797 authenticatee.hpp:245] Attempting to authenticate with mechanism 'CRAM-MD5'
I0905 15:53:16.549180 25797 authenticator.hpp:262] Received SASL authentication start
I0905 15:53:16.549229 25797 authenticator.hpp:384] Authentication requires more steps
I0905 15:53:16.549268 25797 authenticatee.hpp:265] Received SASL authentication step
I0905 15:53:16.549351 25787 authenticator.hpp:290] Received SASL authentication step
I0905 15:53:16.549378 25787 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'penates.apache.org' server FQDN: 'penates.apache.org' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false
I0905 15:53:16.549391 25787 auxprop.cpp:153] Looking up auxiliary property '*userPassword'
I0905 15:53:16.549403 25787 auxprop.cpp:153] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0905 15:53:16.549415 25787 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'penates.apache.org' server FQDN: 'penates.apache.org' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true
I0905 15:53:16.549424 25787 auxprop.cpp:103] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0905 15:53:16.549432 25787 auxprop.cpp:103] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0905 15:53:16.549448 25787 authenticator.hpp:376] Authentication success
I0905 15:53:16.549489 25787 authenticatee.hpp:305] Authentication success
I0905 15:53:16.549525 25787 master.cpp:3677] Successfully authenticated principal 'test-principal' at scheduler-33430370-6af5-4c7b-bbd8-f6a43269ecf5@67.195.81.186:49188
I0905 15:53:16.549669 25783 sched.cpp:357] Successfully authenticated with master master@67.195.81.186:49188
I0905 15:53:16.549690 25783 sched.cpp:476] Sending registration request to master@67.195.81.186:49188
I0905 15:53:16.549751 25787 master.cpp:1324] Received registration request from scheduler-33430370-6af5-4c7b-bbd8-f6a43269ecf5@67.195.81.186:49188
I0905 15:53:16.549782 25787 master.cpp:1284] Authorizing framework principal 'test-principal' to receive offers for role '*'
I0905 15:53:16.551250 25791 sched.cpp:233] New master detected at master@67.195.81.186:49188
I0905 15:53:16.551273 25791 sched.cpp:283] Authenticating with master master@67.195.81.186:49188
I0905 15:53:16.551357 25788 authenticatee.hpp:128] Creating new client SASL connection
I0905 15:53:16.551456 25791 master.cpp:3637] Authenticating scheduler-33430370-6af5-4c7b-bbd8-f6a43269ecf5@67.195.81.186:49188
I0905 15:53:16.551553 25788 authenticator.hpp:156] Creating new server SASL connection
I0905 15:53:16.551673 25786 authenticatee.hpp:219] Received SASL authentication mechanisms: CRAM-MD5
I0905 15:53:16.551697 25786 authenticatee.hpp:245] Attempting to authenticate with mechanism 'CRAM-MD5'
I0905 15:53:16.551755 25792 authenticator.hpp:262] Received SASL authentication start
I0905 15:53:16.551808 25792 authenticator.hpp:384] Authentication requires more steps
I0905 15:53:16.551856 25792 authenticatee.hpp:265] Received SASL authentication step
I0905 15:53:16.551920 25786 authenticator.hpp:290] Received SASL authentication step
I0905 15:53:16.551949 25786 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'penates.apache.org' server FQDN: 'penates.apache.org' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false
I0905 15:53:16.551966 25786 auxprop.cpp:153] Looking up auxiliary property '*userPassword'
I0905 15:53:16.551985 25786 auxprop.cpp:153] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0905 15:53:16.551997 25786 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'penates.apache.org' server FQDN: 'penates.apache.org' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true
I0905 15:53:16.552006 25786 auxprop.cpp:103] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0905 15:53:16.552014 25786 auxprop.cpp:103] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0905 15:53:16.552031 25786 authenticator.hpp:376] Authentication success
I0905 15:53:16.552081 25792 authenticatee.hpp:305] Authentication success
I0905 15:53:16.552100 25786 master.cpp:3677] Successfully authenticated principal 'test-principal' at scheduler-33430370-6af5-4c7b-bbd8-f6a43269ecf5@67.195.81.186:49188
I0905 15:53:16.552249 25792 sched.cpp:357] Successfully authenticated with master master@67.195.81.186:49188
I0905 15:53:17.402861 25793 hierarchical_allocator_process.hpp:697] No resources available to allocate!
I0905 15:53:18.874348 25792 sched.cpp:476] Sending registration request to master@67.195.81.186:49188
I0905 15:53:18.874364 25793 hierarchical_allocator_process.hpp:659] Performed allocation for 0 slaves in 1.471501003secs
I0905 15:53:18.874420 25792 sched.cpp:476] Sending registration request to master@67.195.81.186:49188
I0905 15:53:18.874451 25793 master.cpp:1324] Received registration request from scheduler-33430370-6af5-4c7b-bbd8-f6a43269ecf5@67.195.81.186:49188
I0905 15:53:18.874480 25793 master.cpp:1284] Authorizing framework principal 'test-principal' to receive offers for role '*'
I0905 15:53:18.874565 25793 master.cpp:1324] Received registration request from scheduler-33430370-6af5-4c7b-bbd8-f6a43269ecf5@67.195.81.186:49188
I0905 15:53:18.874588 25793 master.cpp:1284] Authorizing framework principal 'test-principal' to receive offers for role '*'
: Failure
Mock function called more times than expected - returning default value.
    Function call: authorize(@0x2b9ed7fe9350 40-byte object <90-BA B4-D4 9E-2B 00-00 00-00 00-00 00-00 00-00 A0-FA 06-F4 9E-2B 00-00 80-17 09-F4 9E-2B 00-00 00-00 00-00 03-00 00-00>)
    The mock function has no default action set, and its return type has no default value set.
*** Aborted at 1409932398 (unix time) try ""date -d @1409932398"" if you are using GNU date ***
PC: @     0x2b9ed6233f79 (unknown)
*** SIGABRT (@0x95c000064a9) received by PID 25769 (TID 0x2b9ed7fea700) from PID 25769; stack trace: ***
    @     0x2b9ed5fef340 (unknown)
    @     0x2b9ed6233f79 (unknown)
    @     0x2b9ed6237388 (unknown)
    @           0x93a5ec testing::internal::GoogleTestFailureReporter::ReportFailure()
    @           0x7296c5 testing::internal::FunctionMockerBase<>::UntypedPerformDefaultAction()
    @           0x933094 testing::internal::UntypedFunctionMockerBase::UntypedInvokeWith()
    @           0x71fbde mesos::internal::tests::MockAuthorizer::authorize()
    @     0x2b9ed4038caf mesos::internal::master::Master::validate()
    @     0x2b9ed4039763 mesos::internal::master::Master::registerFramework()
    @     0x2b9ed40a0c0f ProtobufProcess<>::handler1<>()
    @     0x2b9ed4050c57 std::_Function_handler<>::_M_invoke()
    @     0x2b9ed407d202 ProtobufProcess<>::visit()
    @     0x2b9ed402af1a mesos::internal::master::Master::_visit()
    @     0x2b9ed4037eb8 mesos::internal::master::Master::visit()
    @     0x2b9ed44cb792 process::ProcessManager::resume()
    @     0x2b9ed44cba9c process::schedule()
    @     0x2b9ed5fe7182 start_thread
    @     0x2b9ed62f830d (unknown)
{code}",Bug,Major,vinodkone,2014-09-11T00:45:56.000+0000,5,Resolved,Complete,MasterAuthorizationTest.DuplicateRegistration test is flaky,2014-09-17T00:38:10.000+0000,MESOS-1766,2.0,mesos,Mesos Q3 Sprint 5
idownes,2014-09-05T16:54:39.000+0000,wangcong,"There is some known kernel issue when we freeze the whole cgroup upon OOM. Mesos probably can just use PID namespace so that we will only need to kill the ""init"" of the pid namespace, instead of freezing all the processes and killing them one by one. But I am not quite sure if this would break the existing code.",Story,Major,wangcong,2014-10-28T19:24:26.000+0000,5,Resolved,Complete,Use PID namespace to avoid freezing cgroup,2015-10-27T05:41:48.000+0000,MESOS-1765,5.0,mesos,Mesos Q3 Sprint 5
vinodkone,2014-09-03T23:12:42.000+0000,vinodkone,"Observed this on Apache CI: https://builds.apache.org/job/Mesos-Trunk-Ubuntu-Build-Out-Of-Src-Disable-Java-Disable-Python-Disable-Webui/2355/changes

{code}
[ RUN] MasterAuthorizationTest.FrameworkRemovedBeforeReregistration
Using temporary directory '/tmp/MasterAuthorizationTest_FrameworkRemovedBeforeReregistration_0tw16Z'
I0903 22:04:33.520237 25565 leveldb.cpp:176] Opened db in 49.073821ms
I0903 22:04:33.538331 25565 leveldb.cpp:183] Compacted db in 18.065051ms
I0903 22:04:33.538363 25565 leveldb.cpp:198] Created db iterator in 4826ns
I0903 22:04:33.538377 25565 leveldb.cpp:204] Seeked to beginning of db in 682ns
I0903 22:04:33.538385 25565 leveldb.cpp:273] Iterated through 0 keys in the db in 312ns
I0903 22:04:33.538399 25565 replica.cpp:741] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned
I0903 22:04:33.538624 25593 recover.cpp:425] Starting replica recovery
I0903 22:04:33.538707 25598 recover.cpp:451] Replica is in EMPTY status
I0903 22:04:33.540909 25590 master.cpp:286] Master 20140903-220433-453759884-44122-25565 (hemera.apache.org) started on 140.211.11.27:44122
I0903 22:04:33.540932 25590 master.cpp:332] Master only allowing authenticated frameworks to register
I0903 22:04:33.540936 25590 master.cpp:337] Master only allowing authenticated slaves to register
I0903 22:04:33.540941 25590 credentials.hpp:36] Loading credentials for authentication from '/tmp/MasterAuthorizationTest_FrameworkRemovedBeforeReregistration_0tw16Z/credentials'
I0903 22:04:33.541337 25590 master.cpp:366] Authorization enabled
I0903 22:04:33.541508 25597 replica.cpp:638] Replica in EMPTY status received a broadcasted recover request
I0903 22:04:33.542343 25582 hierarchical_allocator_process.hpp:299] Initializing hierarchical allocator process with master : master@140.211.11.27:44122
I0903 22:04:33.542445 25592 master.cpp:120] No whitelist given. Advertising offers for all slaves
I0903 22:04:33.543175 25602 recover.cpp:188] Received a recover response from a replica in EMPTY status
I0903 22:04:33.543637 25587 recover.cpp:542] Updating replica status to STARTING
I0903 22:04:33.544256 25579 master.cpp:1205] The newly elected leader is master@140.211.11.27:44122 with id 20140903-220433-453759884-44122-25565
I0903 22:04:33.544275 25579 master.cpp:1218] Elected as the leading master!
I0903 22:04:33.544282 25579 master.cpp:1036] Recovering from registrar
I0903 22:04:33.544401 25579 registrar.cpp:313] Recovering registrar
I0903 22:04:33.558487 25593 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 14.678563ms
I0903 22:04:33.558531 25593 replica.cpp:320] Persisted replica status to STARTING
I0903 22:04:33.558653 25593 recover.cpp:451] Replica is in STARTING status
I0903 22:04:33.559867 25588 replica.cpp:638] Replica in STARTING status received a broadcasted recover request
I0903 22:04:33.560057 25602 recover.cpp:188] Received a recover response from a replica in STARTING status
I0903 22:04:33.561280 25584 recover.cpp:542] Updating replica status to VOTING
I0903 22:04:33.576900 25581 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 14.712427ms
I0903 22:04:33.576942 25581 replica.cpp:320] Persisted replica status to VOTING
I0903 22:04:33.577018 25581 recover.cpp:556] Successfully joined the Paxos group
I0903 22:04:33.577108 25581 recover.cpp:440] Recover process terminated
I0903 22:04:33.577401 25581 log.cpp:656] Attempting to start the writer
I0903 22:04:33.578559 25589 replica.cpp:474] Replica received implicit promise request with proposal 1
I0903 22:04:33.594611 25589 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 16.029152ms
I0903 22:04:33.594640 25589 replica.cpp:342] Persisted promised to 1
I0903 22:04:33.595391 25584 coordinator.cpp:230] Coordinator attemping to fill missing position
I0903 22:04:33.597512 25588 replica.cpp:375] Replica received explicit promise request for position 0 with proposal 2
I0903 22:04:33.613037 25588 leveldb.cpp:343] Persisting action (8 bytes) to leveldb took 15.502568ms
I0903 22:04:33.613065 25588 replica.cpp:676] Persisted action at 0
I0903 22:04:33.615435 25585 replica.cpp:508] Replica received write request for position 0
I0903 22:04:33.615463 25585 leveldb.cpp:438] Reading position from leveldb took 10743ns
I0903 22:04:33.630801 25585 leveldb.cpp:343] Persisting action (14 bytes) to leveldb took 15.320225ms
I0903 22:04:33.630852 25585 replica.cpp:676] Persisted action at 0
I0903 22:04:33.631126 25585 replica.cpp:655] Replica received learned notice for position 0
I0903 22:04:33.647801 25585 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 16.652951ms
I0903 22:04:33.647830 25585 replica.cpp:676] Persisted action at 0
I0903 22:04:33.647842 25585 replica.cpp:661] Replica learned NOP action at position 0
I0903 22:04:33.648548 25583 log.cpp:672] Writer started with ending position 0
I0903 22:04:33.649235 25583 leveldb.cpp:438] Reading position from leveldb took 25209ns
I0903 22:04:33.650897 25591 registrar.cpp:346] Successfully fetched the registry (0B)
I0903 22:04:33.650930 25591 registrar.cpp:422] Attempting to update the 'registry'
I0903 22:04:33.652861 25601 log.cpp:680] Attempting to append 138 bytes to the log
I0903 22:04:33.653097 25586 coordinator.cpp:340] Coordinator attempting to write APPEND action at position 1
I0903 22:04:33.655225 25590 replica.cpp:508] Replica received write request for position 1
I0903 22:04:33.669618 25590 leveldb.cpp:343] Persisting action (157 bytes) to leveldb took 14.337486ms
I0903 22:04:33.669663 25590 replica.cpp:676] Persisted action at 1
I0903 22:04:33.670045 25584 replica.cpp:655] Replica received learned notice for position 1
I0903 22:04:34.414243 25584 leveldb.cpp:343] Persisting action (159 bytes) to leveldb took 15.401247ms
I0903 22:04:34.414300 25584 replica.cpp:676] Persisted action at 1
I0903 22:04:34.414316 25584 replica.cpp:661] Replica learned APPEND action at position 1
I0903 22:04:34.414937 25589 registrar.cpp:479] Successfully updated 'registry'
I0903 22:04:34.415069 25585 log.cpp:699] Attempting to truncate the log to 1
I0903 22:04:34.415194 25589 registrar.cpp:372] Successfully recovered registrar
I0903 22:04:34.415284 25589 master.cpp:1063] Recovered 0 slaves from the Registry (100B) ; allowing 10mins for slaves to re-register
I0903 22:04:34.415362 25587 coordinator.cpp:340] Coordinator attempting to write TRUNCATE action at position 2
I0903 22:04:34.418926 25597 replica.cpp:508] Replica received write request for position 2
I0903 22:04:34.434321 25597 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 15.368147ms
I0903 22:04:34.434352 25597 replica.cpp:676] Persisted action at 2
I0903 22:04:34.435022 25582 replica.cpp:655] Replica received learned notice for position 2
I0903 22:04:34.450331 25582 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 15.284486ms
I0903 22:04:34.450387 25582 leveldb.cpp:401] Deleting ~1 keys from leveldb took 25774ns
I0903 22:04:34.450402 25582 replica.cpp:676] Persisted action at 2
I0903 22:04:34.450412 25582 replica.cpp:661] Replica learned TRUNCATE action at position 2
I0903 22:04:34.460691 25565 sched.cpp:137] Version: 0.21.0
I0903 22:04:34.460927 25582 sched.cpp:233] New master detected at master@140.211.11.27:44122
I0903 22:04:34.460948 25582 sched.cpp:283] Authenticating with master master@140.211.11.27:44122
I0903 22:04:34.461359 25582 authenticatee.hpp:128] Creating new client SASL connection
I0903 22:04:34.461647 25582 master.cpp:3637] Authenticating scheduler-04e0b571-7e0c-4ef3-bb14-c6bbfd8ac9a4@140.211.11.27:44122
I0903 22:04:34.461801 25598 authenticator.hpp:156] Creating new server SASL connection
I0903 22:04:34.462172 25598 authenticatee.hpp:219] Received SASL authentication mechanisms: CRAM-MD5
I0903 22:04:34.462185 25598 authenticatee.hpp:245] Attempting to authenticate with mechanism 'CRAM-MD5'
I0903 22:04:34.462257 25598 authenticator.hpp:262] Received SASL authentication start
I0903 22:04:34.462323 25598 authenticator.hpp:384] Authentication requires more steps
I0903 22:04:34.462345 25598 authenticatee.hpp:265] Received SASL authentication step
I0903 22:04:34.462417 25598 authenticator.hpp:290] Received SASL authentication step
I0903 22:04:34.462522 25598 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'hemera.apache.org' server FQDN: 'hemera.apache.org' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false
I0903 22:04:34.462529 25598 auxprop.cpp:153] Looking up auxiliary property '*userPassword'
I0903 22:04:34.462538 25598 auxprop.cpp:153] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0903 22:04:34.462543 25598 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'hemera.apache.org' server FQDN: 'hemera.apache.org' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true
I0903 22:04:34.462548 25598 auxprop.cpp:103] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0903 22:04:34.462550 25598 auxprop.cpp:103] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0903 22:04:34.462558 25598 authenticator.hpp:376] Authentication success
I0903 22:04:34.462635 25598 master.cpp:3677] Successfully authenticated principal 'test-principal' at scheduler-04e0b571-7e0c-4ef3-bb14-c6bbfd8ac9a4@140.211.11.27:44122
I0903 22:04:34.462687 25590 authenticatee.hpp:305] Authentication success
I0903 22:04:34.463219 25588 sched.cpp:357] Successfully authenticated with master master@140.211.11.27:44122
I0903 22:04:34.463243 25588 sched.cpp:476] Sending registration request to master@140.211.11.27:44122
I0903 22:04:34.463307 25588 master.cpp:1324] Received registration request from scheduler-04e0b571-7e0c-4ef3-bb14-c6bbfd8ac9a4@140.211.11.27:44122
I0903 22:04:34.463330 25588 master.cpp:1284] Authorizing framework principal 'test-principal' to receive offers for role '*'
I0903 22:04:34.463412 25588 master.cpp:1383] Registering framework 20140903-220433-453759884-44122-25565-0000 at scheduler-04e0b571-7e0c-4ef3-bb14-c6bbfd8ac9a4@140.211.11.27:44122
I0903 22:04:34.463577 25598 sched.cpp:407] Framework registered with 20140903-220433-453759884-44122-25565-0000
I0903 22:04:34.463728 25587 hierarchical_allocator_process.hpp:329] Added framework 20140903-220433-453759884-44122-25565-0000
I0903 22:04:34.463739 25587 hierarchical_allocator_process.hpp:697] No resources available to allocate!
I0903 22:04:34.463743 25587 hierarchical_allocator_process.hpp:659] Performed allocation for 0 slaves in 5016ns
I0903 22:04:34.463755 25598 sched.cpp:421] Scheduler::registered took 165035ns
I0903 22:04:34.465558 25583 sched.cpp:227] Scheduler::disconnected took 6254ns
I0903 22:04:34.465566 25583 sched.cpp:233] New master detected at master@140.211.11.27:44122
I0903 22:04:34.465575 25583 sched.cpp:283] Authenticating with master master@140.211.11.27:44122
I0903 22:04:34.465642 25583 authenticatee.hpp:128] Creating new client SASL connection
I0903 22:04:34.465790 25583 master.cpp:1680] Deactivating framework 20140903-220433-453759884-44122-25565-0000
I0903 22:04:34.465850 25583 master.cpp:3637] Authenticating scheduler-04e0b571-7e0c-4ef3-bb14-c6bbfd8ac9a4@140.211.11.27:44122
I0903 22:04:34.465879 25601 hierarchical_allocator_process.hpp:405] Deactivated framework 20140903-220433-453759884-44122-25565-0000
I0903 22:04:34.466047 25600 authenticator.hpp:156] Creating new server SASL connection
I0903 22:04:34.466315 25600 authenticatee.hpp:219] Received SASL authentication mechanisms: CRAM-MD5
I0903 22:04:34.466326 25600 authenticatee.hpp:245] Attempting to authenticate with mechanism 'CRAM-MD5'
I0903 22:04:34.466346 25600 authenticator.hpp:262] Received SASL authentication start
I0903 22:04:34.466418 25600 authenticator.hpp:384] Authentication requires more steps
I0903 22:04:34.466436 25600 authenticatee.hpp:265] Received SASL authentication step
I0903 22:04:34.466475 25600 authenticator.hpp:290] Received SASL authentication step
I0903 22:04:34.466486 25600 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'hemera.apache.org' server FQDN: 'hemera.apache.org' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false
I0903 22:04:34.466491 25600 auxprop.cpp:153] Looking up auxiliary property '*userPassword'
I0903 22:04:34.466496 25600 auxprop.cpp:153] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0903 22:04:34.466502 25600 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'hemera.apache.org' server FQDN: 'hemera.apache.org' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true
I0903 22:04:34.466506 25600 auxprop.cpp:103] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0903 22:04:34.466509 25600 auxprop.cpp:103] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0903 22:04:34.466516 25600 authenticator.hpp:376] Authentication success
I0903 22:04:34.466596 25588 master.cpp:3677] Successfully authenticated principal 'test-principal' at scheduler-04e0b571-7e0c-4ef3-bb14-c6bbfd8ac9a4@140.211.11.27:44122
I0903 22:04:34.466629 25597 authenticatee.hpp:305] Authentication success
I0903 22:04:34.467062 25594 sched.cpp:357] Successfully authenticated with master master@140.211.11.27:44122
I0903 22:04:34.467077 25594 sched.cpp:476] Sending registration request to master@140.211.11.27:44122
I0903 22:04:34.467190 25588 master.cpp:1448] Received re-registration request from framework 20140903-220433-453759884-44122-25565-0000 at scheduler-04e0b571-7e0c-4ef3-bb14-c6bbfd8ac9a4@140.211.11.27:44122
I0903 22:04:36.368134 25588 master.cpp:1284] Authorizing framework principal 'test-principal' to receive offers for role '*'
I0903 22:04:34.542999 25594 hierarchical_allocator_process.hpp:697] No resources available to allocate!
I0903 22:04:35.463639 25582 sched.cpp:476] Sending registration request to master@140.211.11.27:44122
I0903 22:04:36.368185 25594 hierarchical_allocator_process.hpp:659] Performed allocation for 0 slaves in 1.825177748secs
I0903 22:04:36.368302 25588 master.cpp:1448] Received re-registration request from framework 20140903-220433-453759884-44122-25565-0000 at scheduler-04e0b571-7e0c-4ef3-bb14-c6bbfd8ac9a4@140.211.11.27:44122
I0903 22:04:36.368330 25588 master.cpp:1284] Authorizing framework principal 'test-principal' to receive offers for role '*'
I0903 22:04:36.368388 25582 sched.cpp:476] Sending registration request to master@140.211.11.27:44122
: Failure
Mock function called more times than expected - returning default value.
    Function call: authorize(@0x2ba11964c1b0 40-byte object <D0-ED 39-16 A1-2B 00-00 00-00 00-00 00-00 00-00 00-6C 01-3C A1-2B 00-00 30-20 00-3C A1-2B 00-00 00-00 00-00 03-00 00-00>)
    The mock function has no default action set, and its return type has no default value set.
*** Aborted at 1409781876 (unix time) try ""date -d @1409781876"" if you are using GNU date ***
I0903 22:04:36.368913 25598 sched.cpp:745] Stopping framework '20140903-220433-453759884-44122-25565-0000'
PC: @     0x2ba117a990d5 (unknown)
*** SIGABRT (@0x3ea000063dd) received by PID 25565 (TID 0x2ba11964d700) from PID 25565; stack trace: ***
    @     0x2ba117854cb0 (unknown)
    @     0x2ba117a990d5 (unknown)
    @     0x2ba117a9c83b (unknown)
    @           0x9cba9d testing::internal::GoogleTestFailureReporter::ReportFailure()
    @           0x790091 testing::internal::FunctionMockerBase<>::PerformDefaultAction()
    @           0x790166 testing::internal::FunctionMockerBase<>::UntypedPerformDefaultAction()
    @           0x9c3daa testing::internal::UntypedFunctionMockerBase::UntypedInvokeWith()
    @           0x787279 mesos::internal::tests::MockAuthorizer::authorize()
    @     0x2ba1157c133d mesos::internal::master::Master::validate()
    @     0x2ba1157c2b7a mesos::internal::master::Master::reregisterFramework()
    @     0x2ba1157e0038 ProtobufProcess<>::handler2<>()
    @     0x2ba1157dde89 std::tr1::_Function_handler<>::_M_invoke()
    @     0x2ba1157b15f7 mesos::internal::master::Master::_visit()
    @     0x2ba1157bfa3e mesos::internal::master::Master::visit()
    @     0x2ba115caf5e7 process::ProcessManager::resume()
    @     0x2ba115cb027c process::schedule()
    @     0x2ba11784ce9a start_thread
    @     0x2ba117b5731d (unknown)
{code}",Bug,Major,vinodkone,2014-09-11T00:45:12.000+0000,5,Resolved,Complete,MasterAuthorizationTest.FrameworkRemovedBeforeReregistration is flaky,2014-09-17T00:37:29.000+0000,MESOS-1760,1.0,mesos,Mesos Q3 Sprint 5
vinodkone,2014-09-03T23:08:43.000+0000,bmahler,"In the past we've seen numerous issues around the freezer. Lately, on the 2.6.44 kernel, we've seen issues where we're unable to freeze the cgroup:

(1) An oom occurs.
(2) No indication of oom in the kernel logs.
(3) The slave is unable to freeze the cgroup.
(4) The task is marked as lost.

{noformat}
I0903 16:46:24.956040 25469 mem.cpp:575] Memory limit exceeded: Requested: 15488MB Maximum Used: 15488MB

MEMORY STATISTICS:
cache 7958691840
rss 8281653248
mapped_file 9474048
pgpgin 4487861
pgpgout 522933
pgfault 2533780
pgmajfault 11
inactive_anon 0
active_anon 8281653248
inactive_file 7631708160
active_file 326852608
unevictable 0
hierarchical_memory_limit 16240345088
total_cache 7958691840
total_rss 8281653248
total_mapped_file 9474048
total_pgpgin 4487861
total_pgpgout 522933
total_pgfault 2533780
total_pgmajfault 11
total_inactive_anon 0
total_active_anon 8281653248
total_inactive_file 7631728640
total_active_file 326852608
total_unevictable 0
I0903 16:46:24.956848 25469 containerizer.cpp:1041] Container bbb9732a-d600-4c1b-b326-846338c608c3 has reached its limit for resource mem(*):1.62403e+10 and will be terminated
I0903 16:46:24.957427 25469 containerizer.cpp:909] Destroying container 'bbb9732a-d600-4c1b-b326-846338c608c3'
I0903 16:46:24.958664 25481 cgroups.cpp:2192] Freezing cgroup /sys/fs/cgroup/freezer/mesos/bbb9732a-d600-4c1b-b326-846338c608c3
I0903 16:46:34.959529 25488 cgroups.cpp:2209] Thawing cgroup /sys/fs/cgroup/freezer/mesos/bbb9732a-d600-4c1b-b326-846338c608c3
I0903 16:46:34.962070 25482 cgroups.cpp:1404] Successfullly thawed cgroup /sys/fs/cgroup/freezer/mesos/bbb9732a-d600-4c1b-b326-846338c608c3 after 1.710848ms
I0903 16:46:34.962658 25479 cgroups.cpp:2192] Freezing cgroup /sys/fs/cgroup/freezer/mesos/bbb9732a-d600-4c1b-b326-846338c608c3
I0903 16:46:44.963349 25488 cgroups.cpp:2209] Thawing cgroup /sys/fs/cgroup/freezer/mesos/bbb9732a-d600-4c1b-b326-846338c608c3
I0903 16:46:44.965631 25472 cgroups.cpp:1404] Successfullly thawed cgroup /sys/fs/cgroup/freezer/mesos/bbb9732a-d600-4c1b-b326-846338c608c3 after 1.588224ms
I0903 16:46:44.966356 25472 cgroups.cpp:2192] Freezing cgroup /sys/fs/cgroup/freezer/mesos/bbb9732a-d600-4c1b-b326-846338c608c3
I0903 16:46:54.967254 25488 cgroups.cpp:2209] Thawing cgroup /sys/fs/cgroup/freezer/mesos/bbb9732a-d600-4c1b-b326-846338c608c3
I0903 16:46:56.008447 25475 cgroups.cpp:1404] Successfullly thawed cgroup /sys/fs/cgroup/freezer/mesos/bbb9732a-d600-4c1b-b326-846338c608c3 after 2.15296ms
I0903 16:46:56.009071 25466 cgroups.cpp:2192] Freezing cgroup /sys/fs/cgroup/freezer/mesos/bbb9732a-d600-4c1b-b326-846338c608c3
I0903 16:47:06.010329 25488 cgroups.cpp:2209] Thawing cgroup /sys/fs/cgroup/freezer/mesos/bbb9732a-d600-4c1b-b326-846338c608c3
I0903 16:47:06.012538 25467 cgroups.cpp:1404] Successfullly thawed cgroup /sys/fs/cgroup/freezer/mesos/bbb9732a-d600-4c1b-b326-846338c608c3 after 1.643008ms
I0903 16:47:06.013216 25467 cgroups.cpp:2192] Freezing cgroup /sys/fs/cgroup/freezer/mesos/bbb9732a-d600-4c1b-b326-846338c608c3
I0903 16:47:12.516348 25480 slave.cpp:3030] Current usage 9.57%. Max allowed age: 5.630238827780799days
I0903 16:47:16.015192 25488 cgroups.cpp:2209] Thawing cgroup /sys/fs/cgroup/freezer/mesos/bbb9732a-d600-4c1b-b326-846338c608c3
I0903 16:47:16.017043 25486 cgroups.cpp:1404] Successfullly thawed cgroup /sys/fs/cgroup/freezer/mesos/bbb9732a-d600-4c1b-b326-846338c608c3 after 1.511168ms
I0903 16:47:16.017555 25480 cgroups.cpp:2192] Freezing cgroup /sys/fs/cgroup/freezer/mesos/bbb9732a-d600-4c1b-b326-846338c608c3
I0903 16:47:19.862746 25483 http.cpp:245] HTTP request for '/slave(1)/stats.json'
E0903 16:47:24.960055 25472 slave.cpp:2557] Termination of executor 'E' of framework '201104070004-0000002563-0000' failed: Failed to destroy container: discarded future
I0903 16:47:24.962054 25472 slave.cpp:2087] Handling status update TASK_LOST (UUID: c0c1633b-7221-40dc-90a2-660ef639f747) for task T of framework 201104070004-0000002563-0000 from @0.0.0.0:0
I0903 16:47:24.963470 25469 mem.cpp:293] Updated 'memory.soft_limit_in_bytes' to 128MB for container bbb9732a-d600-4c1b-b326-846338c608c3
I0903 16:47:24.963541 25471 cpushare.cpp:338] Updated 'cpu.shares' to 256 (cpus 0.25) for container bbb9732a-d600-4c1b-b326-846338c608c3
I0903 16:47:24.964756 25471 cpushare.cpp:359] Updated 'cpu.cfs_period_us' to 100ms and 'cpu.cfs_quota_us' to 25ms (cpus 0.25) for container bbb9732a-d600-4c1b-b326-846338c608c3
I0903 16:47:43.406610 25476 status_update_manager.cpp:320] Received status update TASK_LOST (UUID: c0c1633b-7221-40dc-90a2-660ef639f747) for task T of framework 201104070004-0000002563-0000
I0903 16:47:43.406991 25476 status_update_manager.hpp:342] Checkpointing UPDATE for status update TASK_LOST (UUID: c0c1633b-7221-40dc-90a2-660ef639f747) for task T of framework 201104070004-0000002563-0000
I0903 16:47:43.410475 25476 status_update_manager.cpp:373] Forwarding status update TASK_LOST (UUID: c0c1633b-7221-40dc-90a2-660ef639f747) for task T of framework 201104070004-0000002563-0000 to master@<scrubbed_ip>:5050
I0903 16:47:43.439923 25480 status_update_manager.cpp:398] Received status update acknowledgement (UUID: c0c1633b-7221-40dc-90a2-660ef639f747) for task T of framework 201104070004-0000002563-0000
I0903 16:47:43.440115 25480 status_update_manager.hpp:342] Checkpointing ACK for status update TASK_LOST (UUID: c0c1633b-7221-40dc-90a2-660ef639f747) for task T of framework 201104070004-0000002563-0000
I0903 16:47:43.443595 25480 slave.cpp:2709] Cleaning up executor 'E' of framework 201104070004-0000002563-0000
{noformat}

We should consider avoiding the freezer entirely in favor of a kill(2) loop. We don't have to wait for pid namespaces to remove the freezer dependency.

At the very least, when the freezer fails, we should proceed with a kill(2) loop to ensure that we destroy the cgroup.",Bug,Major,bmahler,2014-09-09T19:18:58.000+0000,5,Resolved,Complete,Freezer failure leads to lost task during container destruction.,2015-10-27T05:41:24.000+0000,MESOS-1758,2.0,mesos,Mesos Q3 Sprint 5
dhamon,2014-09-02T17:54:18.000+0000,dhamon,"Add variadic templates to the C++11 configure check. Once there, we can start using them in the code-base.",Improvement,Minor,dhamon,2014-09-17T20:47:43.000+0000,5,Resolved,Complete,Allow variadic templates,2014-09-17T20:59:49.000+0000,MESOS-1752,1.0,mesos,Q3 Sprint 4
,2014-09-02T16:26:38.000+0000,alex-mesos,"Request for ""stats.json"" to master from a test case doesn't work after calling frameworks' {{driver.stop()}}. However, it works for ""state.json"". I think the problem is related to {{stats()}} continuation {{_stats()}}. The following test illustrates the issue:
{code:title=TestCase.cpp|borderStyle=solid}
TEST_F(MasterTest, RequestAfterDriverStop)
{
  Try<PID<Master> > master = StartMaster();
  ASSERT_SOME(master);

  Try<PID<Slave> > slave = StartSlave();
  ASSERT_SOME(slave);

  MockScheduler sched;
  MesosSchedulerDriver driver(
      &sched, DEFAULT_FRAMEWORK_INFO, master.get(), DEFAULT_CREDENTIAL);

  driver.start();
  
  Future<process::http::Response> response_before =
      process::http::get(master.get(), ""stats.json"");
  AWAIT_READY(response_before);

  driver.stop();

  Future<process::http::Response> response_after =
      process::http::get(master.get(), ""stats.json"");
  AWAIT_READY(response_after);

  driver.join();

  Shutdown();  // Must shutdown before 'containerizer' gets deallocated.
}
{code}",Bug,Minor,alexr,,10020,Accepted,In Progress,"Request for ""stats.json"" cannot be fulfilled after stopping the framework ",2015-10-14T10:00:40.000+0000,MESOS-1751,5.0,mesos,Mesos Q3 Sprint 6
jieyu,2014-08-29T18:07:34.000+0000,jieyu,"{noformat}
[ RUN      ] SlaveRecoveryTest/0.ShutdownSlave
Using temporary directory '/tmp/SlaveRecoveryTest_0_ShutdownSlave_3O5epS'
I0828 21:21:46.206990 27625 leveldb.cpp:176] Opened db in 24.461837ms
I0828 21:21:46.213706 27625 leveldb.cpp:183] Compacted db in 6.021499ms
I0828 21:21:46.214047 27625 leveldb.cpp:198] Created db iterator in 5566ns
I0828 21:21:46.214313 27625 leveldb.cpp:204] Seeked to beginning of db in 1433ns
I0828 21:21:46.214515 27625 leveldb.cpp:273] Iterated through 0 keys in the db in 723ns
I0828 21:21:46.214826 27625 replica.cpp:741] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned
I0828 21:21:46.215409 27642 recover.cpp:425] Starting replica recovery
I0828 21:21:46.215718 27642 recover.cpp:451] Replica is in EMPTY status
I0828 21:21:46.216264 27642 replica.cpp:638] Replica in EMPTY status received a broadcasted recover request
I0828 21:21:46.216557 27642 recover.cpp:188] Received a recover response from a replica in EMPTY status
I0828 21:21:46.216917 27642 recover.cpp:542] Updating replica status to STARTING
I0828 21:21:46.221271 27645 master.cpp:286] Master 20140828-212146-16842879-45613-27625 (saucy) started on 127.0.1.1:45613
I0828 21:21:46.221812 27645 master.cpp:332] Master only allowing authenticated frameworks to register
I0828 21:21:46.222038 27645 master.cpp:337] Master only allowing authenticated slaves to register
I0828 21:21:46.222250 27645 credentials.hpp:36] Loading credentials for authentication from '/tmp/SlaveRecoveryTest_0_ShutdownSlave_3O5epS/credentials'
I0828 21:21:46.222585 27645 master.cpp:366] Authorization enabled
I0828 21:21:46.222885 27642 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 5.596969ms
I0828 21:21:46.223085 27642 replica.cpp:320] Persisted replica status to STARTING
I0828 21:21:46.223424 27642 recover.cpp:451] Replica is in STARTING status
I0828 21:21:46.223933 27642 replica.cpp:638] Replica in STARTING status received a broadcasted recover request
I0828 21:21:46.224984 27642 recover.cpp:188] Received a recover response from a replica in STARTING status
I0828 21:21:46.225385 27642 recover.cpp:542] Updating replica status to VOTING
I0828 21:21:46.224750 27646 master.cpp:1205] The newly elected leader is master@127.0.1.1:45613 with id 20140828-212146-16842879-45613-27625
I0828 21:21:46.226132 27646 master.cpp:1218] Elected as the leading master!
I0828 21:21:46.226349 27646 master.cpp:1036] Recovering from registrar
I0828 21:21:46.226637 27646 registrar.cpp:313] Recovering registrar
I0828 21:21:46.224473 27641 master.cpp:120] No whitelist given. Advertising offers for all slaves
I0828 21:21:46.224431 27645 hierarchical_allocator_process.hpp:299] Initializing hierarchical allocator process with master : master@127.0.1.1:45613
I0828 21:21:46.240932 27642 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 15.182422ms
I0828 21:21:46.241453 27642 replica.cpp:320] Persisted replica status to VOTING
I0828 21:21:46.241926 27643 recover.cpp:556] Successfully joined the Paxos group
I0828 21:21:46.242228 27642 recover.cpp:440] Recover process terminated
I0828 21:21:46.242501 27645 log.cpp:656] Attempting to start the writer
I0828 21:21:46.243247 27645 replica.cpp:474] Replica received implicit promise request with proposal 1
I0828 21:21:46.253456 27645 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 9.95472ms
I0828 21:21:46.253955 27645 replica.cpp:342] Persisted promised to 1
I0828 21:21:46.254518 27645 coordinator.cpp:230] Coordinator attemping to fill missing position
I0828 21:21:46.255234 27641 replica.cpp:375] Replica received explicit promise request for position 0 with proposal 2
I0828 21:21:46.263128 27641 leveldb.cpp:343] Persisting action (8 bytes) to leveldb took 7.484042ms
I0828 21:21:46.263536 27641 replica.cpp:676] Persisted action at 0
I0828 21:21:46.263806 27641 replica.cpp:508] Replica received write request for position 0
I0828 21:21:46.263834 27641 leveldb.cpp:438] Reading position from leveldb took 14063ns
I0828 21:21:46.276149 27641 leveldb.cpp:343] Persisting action (14 bytes) to leveldb took 12.295476ms
I0828 21:21:46.276178 27641 replica.cpp:676] Persisted action at 0
I0828 21:21:46.276319 27641 replica.cpp:655] Replica received learned notice for position 0
I0828 21:21:46.285523 27641 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 9.185244ms
I0828 21:21:46.285552 27641 replica.cpp:676] Persisted action at 0
I0828 21:21:46.285560 27641 replica.cpp:661] Replica learned NOP action at position 0
I0828 21:21:46.289685 27642 log.cpp:672] Writer started with ending position 0
I0828 21:21:46.290166 27642 leveldb.cpp:438] Reading position from leveldb took 14463ns
I0828 21:21:46.297260 27642 registrar.cpp:346] Successfully fetched the registry (0B)
I0828 21:21:46.297622 27642 registrar.cpp:422] Attempting to update the 'registry'
I0828 21:21:46.298893 27645 log.cpp:680] Attempting to append 118 bytes to the log
I0828 21:21:46.299190 27645 coordinator.cpp:340] Coordinator attempting to write APPEND action at position 1
I0828 21:21:46.299643 27645 replica.cpp:508] Replica received write request for position 1
I0828 21:21:46.310351 27645 leveldb.cpp:343] Persisting action (135 bytes) to leveldb took 10.349409ms
I0828 21:21:46.310577 27645 replica.cpp:676] Persisted action at 1
I0828 21:21:46.311039 27645 replica.cpp:655] Replica received learned notice for position 1
I0828 21:21:46.322127 27645 leveldb.cpp:343] Persisting action (137 bytes) to leveldb took 10.858061ms
I0828 21:21:46.322614 27645 replica.cpp:676] Persisted action at 1
I0828 21:21:46.322875 27645 replica.cpp:661] Replica learned APPEND action at position 1
I0828 21:21:46.323480 27645 registrar.cpp:479] Successfully updated 'registry'
I0828 21:21:46.323874 27645 registrar.cpp:372] Successfully recovered registrar
I0828 21:21:46.323649 27639 log.cpp:699] Attempting to truncate the log to 1
I0828 21:21:46.324465 27644 coordinator.cpp:340] Coordinator attempting to write TRUNCATE action at position 2
I0828 21:21:46.324988 27644 replica.cpp:508] Replica received write request for position 2
I0828 21:21:46.325335 27643 master.cpp:1063] Recovered 0 slaves from the Registry (82B) ; allowing 10mins for slaves to re-register
I0828 21:21:46.335847 27644 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 10.651398ms
I0828 21:21:46.336320 27644 replica.cpp:676] Persisted action at 2
I0828 21:21:46.336896 27644 replica.cpp:655] Replica received learned notice for position 2
I0828 21:21:46.345854 27644 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 8.540555ms
I0828 21:21:46.346261 27644 leveldb.cpp:401] Deleting ~1 keys from leveldb took 30183ns
I0828 21:21:46.346282 27644 replica.cpp:676] Persisted action at 2
I0828 21:21:46.346315 27644 replica.cpp:661] Replica learned TRUNCATE action at position 2
I0828 21:21:46.356840 27625 containerizer.cpp:89] Using isolation: posix/cpu,posix/mem
I0828 21:21:46.361413 27644 slave.cpp:167] Slave started on 48)@127.0.1.1:45613
I0828 21:21:46.361753 27644 credentials.hpp:84] Loading credential for authentication from '/tmp/SlaveRecoveryTest_0_ShutdownSlave_umhraW/credential'
I0828 21:21:46.362046 27644 slave.cpp:274] Slave using credential for: test-principal
I0828 21:21:46.362810 27644 slave.cpp:287] Slave resources: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I0828 21:21:46.363088 27644 slave.cpp:315] Slave hostname: saucy
I0828 21:21:46.363301 27644 slave.cpp:316] Slave checkpoint: true
I0828 21:21:46.363986 27644 state.cpp:33] Recovering state from '/tmp/SlaveRecoveryTest_0_ShutdownSlave_umhraW/meta'
I0828 21:21:46.364308 27644 status_update_manager.cpp:193] Recovering status update manager
I0828 21:21:46.364600 27644 containerizer.cpp:252] Recovering containerizer
I0828 21:21:46.365325 27646 slave.cpp:3204] Finished recovery
I0828 21:21:46.365839 27646 slave.cpp:598] New master detected at master@127.0.1.1:45613
I0828 21:21:46.366041 27646 slave.cpp:672] Authenticating with master master@127.0.1.1:45613
I0828 21:21:46.366317 27646 slave.cpp:645] Detecting new master
I0828 21:21:46.366569 27646 status_update_manager.cpp:167] New master detected at master@127.0.1.1:45613
I0828 21:21:46.366827 27646 authenticatee.hpp:128] Creating new client SASL connection
I0828 21:21:46.367204 27646 master.cpp:3637] Authenticating slave(48)@127.0.1.1:45613
I0828 21:21:46.367553 27646 authenticator.hpp:156] Creating new server SASL connection
I0828 21:21:46.367857 27646 authenticatee.hpp:219] Received SASL authentication mechanisms: CRAM-MD5
I0828 21:21:46.368031 27646 authenticatee.hpp:245] Attempting to authenticate with mechanism 'CRAM-MD5'
I0828 21:21:46.368228 27646 authenticator.hpp:262] Received SASL authentication start
I0828 21:21:46.368444 27646 authenticator.hpp:384] Authentication requires more steps
I0828 21:21:46.368648 27646 authenticatee.hpp:265] Received SASL authentication step
I0828 21:21:46.368924 27646 authenticator.hpp:290] Received SASL authentication step
I0828 21:21:46.369120 27646 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'saucy' server FQDN: 'saucy' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0828 21:21:46.369350 27646 auxprop.cpp:153] Looking up auxiliary property '*userPassword'
I0828 21:21:46.369544 27646 auxprop.cpp:153] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0828 21:21:46.369730 27646 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'saucy' server FQDN: 'saucy' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0828 21:21:46.369958 27646 auxprop.cpp:103] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0828 21:21:46.370131 27646 auxprop.cpp:103] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0828 21:21:46.370311 27646 authenticator.hpp:376] Authentication success
I0828 21:21:46.370518 27646 authenticatee.hpp:305] Authentication success
I0828 21:21:46.370637 27642 master.cpp:3677] Successfully authenticated principal 'test-principal' at slave(48)@127.0.1.1:45613
I0828 21:21:46.371772 27641 slave.cpp:729] Successfully authenticated with master master@127.0.1.1:45613
I0828 21:21:46.371984 27641 slave.cpp:980] Will retry registration in 15.311045ms if necessary
I0828 21:21:46.372643 27641 master.cpp:2836] Registering slave at slave(48)@127.0.1.1:45613 (saucy) with id 20140828-212146-16842879-45613-27625-0
I0828 21:21:46.373016 27641 registrar.cpp:422] Attempting to update the 'registry'
I0828 21:21:46.374539 27641 log.cpp:680] Attempting to append 289 bytes to the log
I0828 21:21:46.374876 27641 coordinator.cpp:340] Coordinator attempting to write APPEND action at position 3
I0828 21:21:46.375296 27641 replica.cpp:508] Replica received write request for position 3
I0828 21:21:46.376046 27625 sched.cpp:137] Version: 0.21.0
I0828 21:21:46.376374 27646 sched.cpp:233] New master detected at master@127.0.1.1:45613
I0828 21:21:46.376595 27646 sched.cpp:283] Authenticating with master master@127.0.1.1:45613
I0828 21:21:46.376857 27646 authenticatee.hpp:128] Creating new client SASL connection
I0828 21:21:46.377234 27646 master.cpp:3637] Authenticating scheduler-cb5a0264-23cc-45d0-bc4c-a92fa5308158@127.0.1.1:45613
I0828 21:21:46.377496 27646 authenticator.hpp:156] Creating new server SASL connection
I0828 21:21:46.377771 27646 authenticatee.hpp:219] Received SASL authentication mechanisms: CRAM-MD5
I0828 21:21:46.377961 27646 authenticatee.hpp:245] Attempting to authenticate with mechanism 'CRAM-MD5'
I0828 21:21:46.378170 27646 authenticator.hpp:262] Received SASL authentication start
I0828 21:21:46.378360 27646 authenticator.hpp:384] Authentication requires more steps
I0828 21:21:46.378588 27639 authenticatee.hpp:265] Received SASL authentication step
I0828 21:21:46.378789 27646 authenticator.hpp:290] Received SASL authentication step
I0828 21:21:46.378942 27646 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'saucy' server FQDN: 'saucy' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0828 21:21:46.379091 27646 auxprop.cpp:153] Looking up auxiliary property '*userPassword'
I0828 21:21:46.379298 27646 auxprop.cpp:153] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0828 21:21:46.379539 27646 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'saucy' server FQDN: 'saucy' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0828 21:21:46.379720 27646 auxprop.cpp:103] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0828 21:21:46.379935 27646 auxprop.cpp:103] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0828 21:21:46.380089 27646 authenticator.hpp:376] Authentication success
I0828 21:21:46.380306 27642 authenticatee.hpp:305] Authentication success
I0828 21:21:46.382625 27642 sched.cpp:357] Successfully authenticated with master master@127.0.1.1:45613
I0828 21:21:46.383031 27642 sched.cpp:476] Sending registration request to master@127.0.1.1:45613
I0828 21:21:46.382928 27640 master.cpp:3677] Successfully authenticated principal 'test-principal' at scheduler-cb5a0264-23cc-45d0-bc4c-a92fa5308158@127.0.1.1:45613
I0828 21:21:46.383651 27640 master.cpp:1324] Received registration request from scheduler-cb5a0264-23cc-45d0-bc4c-a92fa5308158@127.0.1.1:45613
I0828 21:21:46.383846 27640 master.cpp:1284] Authorizing framework principal 'test-principal' to receive offers for role '*'
I0828 21:21:46.384184 27640 master.cpp:1383] Registering framework 20140828-212146-16842879-45613-27625-0000 at scheduler-cb5a0264-23cc-45d0-bc4c-a92fa5308158@127.0.1.1:45613
I0828 21:21:46.384464 27640 sched.cpp:407] Framework registered with 20140828-212146-16842879-45613-27625-0000
I0828 21:21:46.384764 27640 sched.cpp:421] Scheduler::registered took 18266ns
I0828 21:21:46.384600 27644 hierarchical_allocator_process.hpp:329] Added framework 20140828-212146-16842879-45613-27625-0000
I0828 21:21:46.385171 27644 hierarchical_allocator_process.hpp:691] No resources available to allocate!
I0828 21:21:46.385330 27644 hierarchical_allocator_process.hpp:653] Performed allocation for 0 slaves in 160171ns
I0828 21:21:46.386292 27641 leveldb.cpp:343] Persisting action (308 bytes) to leveldb took 10.815384ms
I0828 21:21:46.386492 27641 replica.cpp:676] Persisted action at 3
I0828 21:21:46.386844 27641 replica.cpp:655] Replica received learned notice for position 3
I0828 21:21:46.387980 27643 slave.cpp:980] Will retry registration in 19.851524ms if necessary
I0828 21:21:46.388140 27639 master.cpp:2824] Ignoring register slave message from slave(48)@127.0.1.1:45613 (saucy) as admission is already in progress
I0828 21:21:46.396355 27641 leveldb.cpp:343] Persisting action (310 bytes) to leveldb took 9.275034ms
I0828 21:21:46.396641 27641 replica.cpp:676] Persisted action at 3
I0828 21:21:46.396837 27641 replica.cpp:661] Replica learned APPEND action at position 3
I0828 21:21:46.397405 27641 registrar.cpp:479] Successfully updated 'registry'
I0828 21:21:46.397528 27645 log.cpp:699] Attempting to truncate the log to 3
I0828 21:21:46.397878 27645 coordinator.cpp:340] Coordinator attempting to write TRUNCATE action at position 4
I0828 21:21:46.398239 27645 replica.cpp:508] Replica received write request for position 4
I0828 21:21:46.398597 27641 master.cpp:2876] Registered slave 20140828-212146-16842879-45613-27625-0 at slave(48)@127.0.1.1:45613 (saucy)
I0828 21:21:46.398870 27641 master.cpp:4110] Adding slave 20140828-212146-16842879-45613-27625-0 at slave(48)@127.0.1.1:45613 (saucy) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I0828 21:21:46.399178 27639 slave.cpp:763] Registered with master master@127.0.1.1:45613; given slave ID 20140828-212146-16842879-45613-27625-0
I0828 21:21:46.399521 27639 slave.cpp:776] Checkpointing SlaveInfo to '/tmp/SlaveRecoveryTest_0_ShutdownSlave_umhraW/meta/slaves/20140828-212146-16842879-45613-27625-0/slave.info'
I0828 21:21:46.399961 27641 hierarchical_allocator_process.hpp:442] Added slave 20140828-212146-16842879-45613-27625-0 (saucy) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (and cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] available)
I0828 21:21:46.400316 27641 hierarchical_allocator_process.hpp:728] Offering cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave 20140828-212146-16842879-45613-27625-0 to framework 20140828-212146-16842879-45613-27625-0000
I0828 21:21:46.400158 27644 slave.cpp:2333] Received ping from slave-observer(45)@127.0.1.1:45613
I0828 21:21:46.400872 27639 master.hpp:857] Adding offer 20140828-212146-16842879-45613-27625-0 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave 20140828-212146-16842879-45613-27625-0 (saucy)
I0828 21:21:46.401105 27639 master.cpp:3584] Sending 1 offers to framework 20140828-212146-16842879-45613-27625-0000
I0828 21:21:46.401448 27639 sched.cpp:544] Scheduler::resourceOffers took 19056ns
I0828 21:21:46.401700 27641 hierarchical_allocator_process.hpp:673] Performed allocation for slave 20140828-212146-16842879-45613-27625-0 in 1.430159ms
I0828 21:21:46.403659 27644 master.hpp:867] Removing offer 20140828-212146-16842879-45613-27625-0 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave 20140828-212146-16842879-45613-27625-0 (saucy)
I0828 21:21:46.403903 27644 master.cpp:2194] Processing reply for offers: [ 20140828-212146-16842879-45613-27625-0 ] on slave 20140828-212146-16842879-45613-27625-0 at slave(48)@127.0.1.1:45613 (saucy) for framework 20140828-212146-16842879-45613-27625-0000
I0828 21:21:46.404116 27644 master.cpp:2277] Authorizing framework principal 'test-principal' to launch task cf5afc1b-c007-435b-8c36-be8aa3659d3a as user 'jenkins'
I0828 21:21:46.404578 27644 master.hpp:829] Adding task cf5afc1b-c007-435b-8c36-be8aa3659d3a with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave 20140828-212146-16842879-45613-27625-0 (saucy)
I0828 21:21:46.404824 27644 master.cpp:2343] Launching task cf5afc1b-c007-435b-8c36-be8aa3659d3a of framework 20140828-212146-16842879-45613-27625-0000 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave 20140828-212146-16842879-45613-27625-0 at slave(48)@127.0.1.1:45613 (saucy)
I0828 21:21:46.405206 27644 slave.cpp:1011] Got assigned task cf5afc1b-c007-435b-8c36-be8aa3659d3a for framework 20140828-212146-16842879-45613-27625-0000
I0828 21:21:46.405462 27644 slave.cpp:3542] Checkpointing FrameworkInfo to '/tmp/SlaveRecoveryTest_0_ShutdownSlave_umhraW/meta/slaves/20140828-212146-16842879-45613-27625-0/frameworks/20140828-212146-16842879-45613-27625-0000/framework.info'
I0828 21:21:46.405840 27644 slave.cpp:3549] Checkpointing framework pid 'scheduler-cb5a0264-23cc-45d0-bc4c-a92fa5308158@127.0.1.1:45613' to '/tmp/SlaveRecoveryTest_0_ShutdownSlave_umhraW/meta/slaves/20140828-212146-16842879-45613-27625-0/frameworks/20140828-212146-16842879-45613-27625-0000/framework.pid'
I0828 21:21:46.406122 27645 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 7.684731ms
I0828 21:21:46.406288 27645 replica.cpp:676] Persisted action at 4
I0828 21:21:46.406618 27645 replica.cpp:655] Replica received learned notice for position 4
I0828 21:21:46.407562 27644 slave.cpp:1121] Launching task cf5afc1b-c007-435b-8c36-be8aa3659d3a for framework 20140828-212146-16842879-45613-27625-0000
I0828 21:21:46.409296 27644 slave.cpp:3858] Checkpointing ExecutorInfo to '/tmp/SlaveRecoveryTest_0_ShutdownSlave_umhraW/meta/slaves/20140828-212146-16842879-45613-27625-0/frameworks/20140828-212146-16842879-45613-27625-0000/executors/cf5afc1b-c007-435b-8c36-be8aa3659d3a/executor.info'
I0828 21:21:46.410117 27641 containerizer.cpp:394] Starting container '1faf33b5-7b06-4384-8b17-cdf0db68e400' for executor 'cf5afc1b-c007-435b-8c36-be8aa3659d3a' of framework '20140828-212146-16842879-45613-27625-0000'
I0828 21:21:46.413606 27641 launcher.cpp:137] Forked child with pid '28309' for container '1faf33b5-7b06-4384-8b17-cdf0db68e400'
I0828 21:21:46.414163 27641 containerizer.cpp:678] Checkpointing executor's forked pid 28309 to '/tmp/SlaveRecoveryTest_0_ShutdownSlave_umhraW/meta/slaves/20140828-212146-16842879-45613-27625-0/frameworks/20140828-212146-16842879-45613-27625-0000/executors/cf5afc1b-c007-435b-8c36-be8aa3659d3a/runs/1faf33b5-7b06-4384-8b17-cdf0db68e400/pids/forked.pid'
I0828 21:21:46.415959 27641 containerizer.cpp:510] Fetching URIs for container '1faf33b5-7b06-4384-8b17-cdf0db68e400' using command '/var/jenkins/workspace/mesos-ubuntu-13.10-gcc/src/mesos-fetcher'
I0828 21:21:46.416506 27644 slave.cpp:3973] Checkpointing TaskInfo to '/tmp/SlaveRecoveryTest_0_ShutdownSlave_umhraW/meta/slaves/20140828-212146-16842879-45613-27625-0/frameworks/20140828-212146-16842879-45613-27625-0000/executors/cf5afc1b-c007-435b-8c36-be8aa3659d3a/runs/1faf33b5-7b06-4384-8b17-cdf0db68e400/tasks/cf5afc1b-c007-435b-8c36-be8aa3659d3a/task.info'
I0828 21:21:46.419023 27644 slave.cpp:1231] Queuing task 'cf5afc1b-c007-435b-8c36-be8aa3659d3a' for executor cf5afc1b-c007-435b-8c36-be8aa3659d3a of framework '20140828-212146-16842879-45613-27625-0000
I0828 21:21:46.419585 27644 slave.cpp:552] Successfully attached file '/tmp/SlaveRecoveryTest_0_ShutdownSlave_umhraW/slaves/20140828-212146-16842879-45613-27625-0/frameworks/20140828-212146-16842879-45613-27625-0000/executors/cf5afc1b-c007-435b-8c36-be8aa3659d3a/runs/1faf33b5-7b06-4384-8b17-cdf0db68e400'
I0828 21:21:46.420091 27645 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 13.201889ms
I0828 21:21:46.420384 27645 leveldb.cpp:401] Deleting ~2 keys from leveldb took 103379ns
I0828 21:21:46.420572 27645 replica.cpp:676] Persisted action at 4
I0828 21:21:46.420763 27645 replica.cpp:661] Replica learned TRUNCATE action at position 4
I0828 21:21:47.172636 27640 slave.cpp:2544] Monitoring executor 'cf5afc1b-c007-435b-8c36-be8aa3659d3a' of framework '20140828-212146-16842879-45613-27625-0000' in container '1faf33b5-7b06-4384-8b17-cdf0db68e400'
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0828 21:21:47.204005 28344 process.cpp:1771] libprocess is initialized on 127.0.1.1:58796 for 8 cpus
I0828 21:21:47.205031 28344 logging.cpp:177] Logging to STDERR
I0828 21:21:47.207900 28344 exec.cpp:132] Version: 0.21.0
I0828 21:21:47.208281 28358 exec.cpp:182] Executor started at: executor(1)@127.0.1.1:58796 with pid 28344
I0828 21:21:47.209727 27640 slave.cpp:1742] Got registration for executor 'cf5afc1b-c007-435b-8c36-be8aa3659d3a' of framework 20140828-212146-16842879-45613-27625-0000
I0828 21:21:47.209794 27640 slave.cpp:1827] Checkpointing executor pid 'executor(1)@127.0.1.1:58796' to '/tmp/SlaveRecoveryTest_0_ShutdownSlave_umhraW/meta/slaves/20140828-212146-16842879-45613-27625-0/frameworks/20140828-212146-16842879-45613-27625-0000/executors/cf5afc1b-c007-435b-8c36-be8aa3659d3a/runs/1faf33b5-7b06-4384-8b17-cdf0db68e400/pids/libprocess.pid'
I0828 21:21:47.210197 27640 slave.cpp:1861] Flushing queued task cf5afc1b-c007-435b-8c36-be8aa3659d3a for executor 'cf5afc1b-c007-435b-8c36-be8aa3659d3a' of framework 20140828-212146-16842879-45613-27625-0000
I0828 21:21:47.211431 28358 exec.cpp:206] Executor registered on slave 20140828-212146-16842879-45613-27625-0
Registered executor on saucy
I0828 21:21:47.212602 28358 exec.cpp:218] Executor::registered took 280603ns
I0828 21:21:47.212889 28358 exec.cpp:293] Executor asked to run task 'cf5afc1b-c007-435b-8c36-be8aa3659d3a'
Starting task cf5afc1b-c007-435b-8c36-be8aa3659d3a
I0828 21:21:47.213672 28358 exec.cpp:302] Executor::launchTask took 551047ns
sh -c 'sleep 1000'
Forked command at 28367
I0828 21:21:47.216408 28361 exec.cpp:525] Executor sending status update TASK_RUNNING (UUID: c0d5bd02-475c-4183-bcfd-8e1a43763e7e) for task cf5afc1b-c007-435b-8c36-be8aa3659d3a of framework 20140828-212146-16842879-45613-27625-0000
I0828 21:21:47.216979 27643 slave.cpp:2096] Handling status update TASK_RUNNING (UUID: c0d5bd02-475c-4183-bcfd-8e1a43763e7e) for task cf5afc1b-c007-435b-8c36-be8aa3659d3a of framework 20140828-212146-16842879-45613-27625-0000 from executor(1)@127.0.1.1:58796
I0828 21:21:47.217106 27643 status_update_manager.cpp:320] Received status update TASK_RUNNING (UUID: c0d5bd02-475c-4183-bcfd-8e1a43763e7e) for task cf5afc1b-c007-435b-8c36-be8aa3659d3a of framework 20140828-212146-16842879-45613-27625-0000
I0828 21:21:47.217121 27643 status_update_manager.cpp:499] Creating StatusUpdate stream for task cf5afc1b-c007-435b-8c36-be8aa3659d3a of framework 20140828-212146-16842879-45613-27625-0000
I0828 21:21:47.217356 27643 status_update_manager.hpp:342] Checkpointing UPDATE for status update TASK_RUNNING (UUID: c0d5bd02-475c-4183-bcfd-8e1a43763e7e) for task cf5afc1b-c007-435b-8c36-be8aa3659d3a of framework 20140828-212146-16842879-45613-27625-0000
I0828 21:21:47.228610 27639 hierarchical_allocator_process.hpp:653] Performed allocation for 1 slaves in 31882ns
2014-08-28 21:21:48,334:27625(0x2ad7b36ba700):ZOO_WARN@zookeeper_interest@1557: Exceeded deadline by 642ms
2014-08-28 21:21:48,335:27625(0x2ad7b36ba700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:33940] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
I0828 21:21:48.336568 27639 hierarchical_allocator_process.hpp:653] Performed allocation for 1 slaves in 27650ns
I0828 21:21:48.345005 27643 status_update_manager.cpp:373] Forwarding status update TASK_RUNNING (UUID: c0d5bd02-475c-4183-bcfd-8e1a43763e7e) for task cf5afc1b-c007-435b-8c36-be8aa3659d3a of framework 20140828-212146-16842879-45613-27625-0000 to master@127.0.1.1:45613
I0828 21:21:48.345942 27644 master.cpp:3205] Forwarding status update TASK_RUNNING (UUID: c0d5bd02-475c-4183-bcfd-8e1a43763e7e) for task cf5afc1b-c007-435b-8c36-be8aa3659d3a of framework 20140828-212146-16842879-45613-27625-0000
I0828 21:21:48.346225 27644 master.cpp:3171] Status update TASK_RUNNING (UUID: c0d5bd02-475c-4183-bcfd-8e1a43763e7e) for task cf5afc1b-c007-435b-8c36-be8aa3659d3a of framework 20140828-212146-16842879-45613-27625-0000 from slave 20140828-212146-16842879-45613-27625-0 at slave(48)@127.0.1.1:45613 (saucy)
I0828 21:21:48.346660 27642 sched.cpp:635] Scheduler::statusUpdate took 86992ns
I0828 21:21:48.347705 27642 master.cpp:2686] Forwarding status update acknowledgement c0d5bd02-475c-4183-bcfd-8e1a43763e7e for task cf5afc1b-c007-435b-8c36-be8aa3659d3a of framework 20140828-212146-16842879-45613-27625-0000 to slave 20140828-212146-16842879-45613-27625-0 at slave(48)@127.0.1.1:45613 (saucy)
I0828 21:21:48.348397 28360 exec.cpp:379] Executor asked to shutdown
I0828 21:21:48.348461 28360 exec.cpp:394] Executor::shutdown took 5608ns
I0828 21:21:48.348575 28360 exec.cpp:78] Scheduling shutdown of the executor
Shutting down
Sending SIGTERM to process tree at pid 28367
I0828 21:21:48.352062 27642 hierarchical_allocator_process.hpp:653] Performed allocation for 1 slaves in 38217ns
I0828 21:21:48.353659 27646 slave.cpp:2254] Status update manager successfully handled status update TASK_RUNNING (UUID: c0d5bd02-475c-4183-bcfd-8e1a43763e7e) for task cf5afc1b-c007-435b-8c36-be8aa3659d3a of framework 20140828-212146-16842879-45613-27625-0000
I0828 21:21:48.353884 27646 slave.cpp:2260] Sending acknowledgement for status update TASK_RUNNING (UUID: c0d5bd02-475c-4183-bcfd-8e1a43763e7e) for task cf5afc1b-c007-435b-8c36-be8aa3659d3a of framework 20140828-212146-16842879-45613-27625-0000 to executor(1)@127.0.1.1:58796
I0828 21:21:48.354588 27643 status_update_manager.cpp:398] Received status update acknowledgement (UUID: c0d5bd02-475c-4183-bcfd-8e1a43763e7e) for task cf5afc1b-c007-435b-8c36-be8aa3659d3a of framework 20140828-212146-16842879-45613-27625-0000
II0828 21:21:48.354921 27643 status_update_manager.hpp:342] Checkpointing ACK for status update TASK_RUNNING (UUID: c0d5bd02-475c-4183-bcfd-8e1a43763e7e) for task cf5afc1b-c007-435b-8c36-be8aa3659d3a of framework 20140828-212146-16842879-45613-27625-0000
0828 21:21:48.355509 28365 exec.cpp:332] Ignoring status update acknowledgement c0d5bd02-475c-4183-bcfd-8e1a43763e7e for task cf5afc1b-c007-435b-8c36-be8aa3659d3a of framework 20140828-212146-16842879-45613-27625-0000 because the driver is aborted!
I0828 21:21:48.373334 27643 slave.cpp:1682] Status update manager successfully handled status update acknowledgement (UUID: c0d5bd02-475c-4183-bcfd-8e1a43763e7e) for task cf5afc1b-c007-435b-8c36-be8aa3659d3a of framework 20140828-212146-16842879-45613-27625-0000
Killing the following process trees:
[ 
-+- 28367 sh -c sleep 1000 
 \--- 28368 sleep 1000 
]
I0828 21:21:50.095928 27642 hierarchical_allocator_process.hpp:653] Performed allocation for 1 slaves in 28759ns
I0828 21:21:50.136400 27640 master.cpp:120] No whitelist given. Advertising offers for all slaves
I0828 21:21:50.138304 27645 hierarchical_allocator_process.hpp:653] Performed allocation for 1 slaves in 29426ns
I0828 21:21:50.177014 27644 hierarchical_allocator_process.hpp:653] Performed allocation for 1 slaves in 28454ns
I0828 21:21:50.218552 27640 hierarchical_allocator_process.hpp:653] Performed allocation for 1 slaves in 27554ns
I0828 21:21:50.257881 27641 hierarchical_allocator_process.hpp:653] Performed allocation for 1 slaves in 27998ns
I0828 21:21:50.298316 27643 hierarchical_allocator_process.hpp:653] Performed allocation for 1 slaves in 27308ns
I0828 21:21:50.338696 27644 master.cpp:120] No whitelist given. Advertising offers for all slaves
I0828 21:21:50.338999 27645 hierarchical_allocator_process.hpp:653] Performed allocation for 1 slaves in 27084ns
I0828 21:21:50.379153 27646 hierarchical_allocator_process.hpp:653] Performed allocation for 1 slaves in 27156ns
I0828 21:21:50.419615 27644 hierarchical_allocator_process.hpp:653] Performed allocation for 1 slaves in 24254ns
I0828 21:21:50.449911 27639 hierarchical_allocator_process.hpp:653] Performed allocation for 1 slaves in 20276ns
I0828 21:21:50.490430 27640 hierarchical_allocator_process.hpp:653] Performed allocation for 1 slaves in 28566ns
I0828 21:21:50.530827 27642 master.cpp:120] No whitelist given. Advertising offers for all slaves
I0828 21:21:50.531673 27639 hierarchical_allocator_process.hpp:653] Performed allocation for 1 slaves in 28146ns
I0828 21:21:50.571229 27641 hierarchical_allocator_process.hpp:653] Performed allocation for 1 slaves in 26113ns
I0828 21:21:50.572326 27645 slave.cpp:2333] Received ping from slave-observer(45)@127.0.1.1:45613
I0828 21:21:50.611567 27642 hierarchical_allocator_process.hpp:653] Performed allocation for 1 slaves in 25396ns
I0828 21:21:50.651984 27641 hierarchical_allocator_process.hpp:653] Performed allocation for 1 slaves in 26641ns
ICommand terminated with signal Terminated (pid: 28367)
I0828 21:21:52.145009 28363 exec.cpp:525] Executor sending status update TASK_KILLED (UUID: b9a0bfa5-658e-4e40-9aff-c6c3023c06fb) for task cf5afc1b-c007-435b-8c36-be8aa3659d3a of framework 20140828-212146-16842879-45613-27625-0000
0828 21:21:50.692332 27642 hierarchical_allocator_process.hpp:653] Performed allocation for 1 slaves in 25882ns
2014-08-28 21:21:52,150:27625(0x2ad7b36ba700):ZOO_WARN@zookeeper_interest@1557: Exceeded deadline by 483ms
2014-08-28 21:21:52,155:27625(0x2ad7b36ba700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:33940] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
I0828 21:21:52.154714 27642 slave.cpp:2096] Handling status update TASK_KILLED (UUID: b9a0bfa5-658e-4e40-9aff-c6c3023c06fb) for task cf5afc1b-c007-435b-8c36-be8aa3659d3a of framework 20140828-212146-16842879-45613-27625-0000 from executor(1)@127.0.1.1:58796
I0828 21:21:52.156853 27642 slave.cpp:3906] Terminating task cf5afc1b-c007-435b-8c36-be8aa3659d3a
I0828 21:21:52.214007 27640 status_update_manager.cpp:320] Received status update TASK_KILLED (UUID: b9a0bfa5-658e-4e40-9aff-c6c3023c06fb) for task cf5afc1b-c007-435b-8c36-be8aa3659d3a of framework 20140828-212146-16842879-45613-27625-0000
I0828 21:21:52.214354 27640 status_update_manager.hpp:342] Checkpointing UPDATE for status update TASK_KILLED (UUID: b9a0bfa5-658e-4e40-9aff-c6c3023c06fb) for task cf5afc1b-c007-435b-8c36-be8aa3659d3a of framework 20140828-212146-16842879-45613-27625-0000
I0828 21:21:52.236850 27640 status_update_manager.cpp:373] Forwarding status update TASK_KILLED (UUID: b9a0bfa5-658e-4e40-9aff-c6c3023c06fb) for task cf5afc1b-c007-435b-8c36-be8aa3659d3a of framework 20140828-212146-16842879-45613-27625-0000 to master@127.0.1.1:45613
I0828 21:21:52.238040 27640 master.cpp:3205] Forwarding status update TASK_KILLED (UUID: b9a0bfa5-658e-4e40-9aff-c6c3023c06fb) for task cf5afc1b-c007-435b-8c36-be8aa3659d3a of framework 20140828-212146-16842879-45613-27625-0000
I0828 21:21:52.238370 27645 slave.cpp:2254] Status update manager successfully handled status update TASK_KILLED (UUID: b9a0bfa5-658e-4e40-9aff-c6c3023c06fb) for task cf5afc1b-c007-435b-8c36-be8aa3659d3a of framework 20140828-212146-16842879-45613-27625-0000
I0828 21:21:52.239017 27645 slave.cpp:2260] Sending acknowledgement for status update TASK_KILLED (UUID: b9a0bfa5-658e-4e40-9aff-c6c3023c06fb) for task cf5afc1b-c007-435b-8c36-be8aa3659d3a of framework 20140828-212146-16842879-45613-27625-0000 to executor(1)@127.0.1.1:58796
I0828 21:21:52.239811 28363 exec.cpp:332] Ignoring status update acknowledgement b9a0bfa5-658e-4e40-9aff-c6c3023c06fb for task cf5afc1b-c007-435b-8c36-be8aa3659d3a of framework 20140828-212146-16842879-45613-27625-0000 because the driver is aborted!
I0828 21:21:52.241734 27640 master.cpp:3171] Status update TASK_KILLED (UUID: b9a0bfa5-658e-4e40-9aff-c6c3023c06fb) for task cf5afc1b-c007-435b-8c36-be8aa3659d3a of framework 20140828-212146-16842879-45613-27625-0000 from slave 20140828-212146-16842879-45613-27625-0 at slave(48)@127.0.1.1:45613 (saucy)
I0828 21:21:52.242620 27640 master.hpp:847] Removing task cf5afc1b-c007-435b-8c36-be8aa3659d3a with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave 20140828-212146-16842879-45613-27625-0 (saucy)
I0828 21:21:52.242277 27645 sched.cpp:635] Scheduler::statusUpdate took 17112ns
I0828 21:21:52.243813 27640 master.cpp:2686] Forwarding status update acknowledgement b9a0bfa5-658e-4e40-9aff-c6c3023c06fb for task cf5afc1b-c007-435b-8c36-be8aa3659d3a of framework 20140828-212146-16842879-45613-27625-0000 to slave 20140828-212146-16842879-45613-27625-0 at slave(48)@127.0.1.1:45613 (saucy)
I0828 21:21:52.244935 27640 status_update_manager.cpp:398] Received status update acknowledgement (UUID: b9a0bfa5-658e-4e40-9aff-c6c3023c06fb) for task cf5afc1b-c007-435b-8c36-be8aa3659d3a of framework 20140828-212146-16842879-45613-27625-0000
I0828 21:21:52.245440 27640 status_update_manager.hpp:342] Checkpointing ACK for status update TASK_KILLED (UUID: b9a0bfa5-658e-4e40-9aff-c6c3023c06fb) for task cf5afc1b-c007-435b-8c36-be8aa3659d3a of framework 20140828-212146-16842879-45613-27625-0000
I0828 21:21:52.244264 27645 hierarchical_allocator_process.hpp:563] Recovered cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (total allocatable: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]) on slave 20140828-212146-16842879-45613-27625-0 from framework 20140828-212146-16842879-45613-27625-0000
I0828 21:21:52.246511 27645 hierarchical_allocator_process.hpp:594] Framework 20140828-212146-16842879-45613-27625-0000 filtered slave 20140828-212146-16842879-45613-27625-0 for 5secs
I0828 21:21:52.267685 27640 status_update_manager.cpp:530] Cleaning up status update stream for task cf5afc1b-c007-435b-8c36-be8aa3659d3a of framework 20140828-212146-16842879-45613-27625-0000
I0828 21:21:52.268086 27644 slave.cpp:1682] Status update manager successfully handled status update acknowledgement (UUID: b9a0bfa5-658e-4e40-9aff-c6c3023c06fb) for task cf5afc1b-c007-435b-8c36-be8aa3659d3a of framework 20140828-212146-16842879-45613-27625-0000
I0828 21:21:52.268123 27644 slave.cpp:3948] Completing task cf5afc1b-c007-435b-8c36-be8aa3659d3a
I0828 21:21:52.274886 27642 master.cpp:120] No whitelist given. Advertising offers for all slaves
I0828 21:21:52.275082 27642 hierarchical_allocator_process.hpp:810] Filtered cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave 20140828-212146-16842879-45613-27625-0 for framework 20140828-212146-16842879-45613-27625-0000
I0828 21:21:52.275148 27642 hierarchical_allocator_process.hpp:653] Performed allocation for 1 slaves in 137935ns
I0828 21:21:52.335613 27640 hierarchical_allocator_process.hpp:810] Filtered cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave 20140828-212146-16842879-45613-27625-0 for framework 20140828-212146-16842879-45613-27625-0000
I0828 21:21:52.625263 27640 hierarchical_allocator_process.hpp:653] Performed allocation for 1 slaves in 289.725737ms
I0828 21:21:53.916736 27643 hierarchical_allocator_process.hpp:810] Filtered cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave 20140828-212146-16842879-45613-27625-0 for framework 20140828-212146-16842879-45613-27625-0000
I0828 21:21:53.916975 27643 hierarchical_allocator_process.hpp:653] Performed allocation for 1 slaves in 412406ns
I0828 21:21:53.957610 27639 hierarchical_allocator_process.hpp:810] Filtered cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave 20140828-212146-16842879-45613-27625-0 for framework 20140828-212146-16842879-45613-27625-0000
I0828 21:21:53.961427 27639 hierarchical_allocator_process.hpp:653] Performed allocation for 1 slaves in 3.958428ms
I0828 21:21:53.961259 27643 containerizer.cpp:998] Executor for container '1faf33b5-7b06-4384-8b17-cdf0db68e400' has exited
I0828 21:21:53.963449 27643 containerizer.cpp:882] Destroying container '1faf33b5-7b06-4384-8b17-cdf0db68e400'
W0828 21:21:54.005010 27640 containerizer.cpp:824] Skipping resource statistic for container 1faf33b5-7b06-4384-8b17-cdf0db68e400 because: Failed to get usage: No process found at 28309
W0828 21:21:54.005357 27640 containerizer.cpp:824] Skipping resource statistic for container 1faf33b5-7b06-4384-8b17-cdf0db68e400 because: Failed to get usage: No process found at 28309
I0828 21:21:54.013495 27645 slave.cpp:2602] Executor 'cf5afc1b-c007-435b-8c36-be8aa3659d3a' of framework 20140828-212146-16842879-45613-27625-0000 exited with status 0
I0828 21:21:54.013799 27645 slave.cpp:2738] Cleaning up executor 'cf5afc1b-c007-435b-8c36-be8aa3659d3a' of framework 20140828-212146-16842879-45613-27625-0000
I0828 21:21:54.014480 27645 slave.cpp:2813] Cleaning up framework 20140828-212146-16842879-45613-27625-0000
I0828 21:21:54.014824 27645 gc.cpp:56] Scheduling '/tmp/SlaveRecoveryTest_0_ShutdownSlave_umhraW/slaves/20140828-212146-16842879-45613-27625-0/frameworks/20140828-212146-16842879-45613-27625-0000/executors/cf5afc1b-c007-435b-8c36-be8aa3659d3a/runs/1faf33b5-7b06-4384-8b17-cdf0db68e400' for gc 1.0000093467509weeks in the future
I0828 21:21:54.015060 27645 gc.cpp:56] Scheduling '/tmp/SlaveRecoveryTest_0_ShutdownSlave_umhraW/slaves/20140828-212146-16842879-45613-27625-0/frameworks/20140828-212146-16842879-45613-27625-0000/executors/cf5afc1b-c007-435b-8c36-be8aa3659d3a' for gc 1.0000093467509weeks in the future
I0828 21:21:54.015277 27645 gc.cpp:56] Scheduling '/tmp/SlaveRecoveryTest_0_ShutdownSlave_umhraW/meta/slaves/20140828-212146-16842879-45613-27625-0/frameworks/20140828-212146-16842879-45613-27625-0000/executors/cf5afc1b-c007-435b-8c36-be8aa3659d3a/runs/1faf33b5-7b06-4384-8b17-cdf0db68e400' for gc 1.0000093467509weeks in the future
I0828 21:21:54.015537 27645 gc.cpp:56] Scheduling '/tmp/SlaveRecoveryTest_0_ShutdownSlave_umhraW/meta/slaves/20140828-212146-16842879-45613-27625-0/frameworks/20140828-212146-16842879-45613-27625-0000/executors/cf5afc1b-c007-435b-8c36-be8aa3659d3a' for gc 1.0000093467509weeks in the future
I0828 21:21:54.015779 27645 gc.cpp:56] Scheduling '/tmp/SlaveRecoveryTest_0_ShutdownSlave_umhraW/slaves/20140828-212146-16842879-45613-27625-0/frameworks/20140828-212146-16842879-45613-27625-0000' for gc 1.0000093467509weeks in the future
I0828 21:21:54.015985 27645 gc.cpp:56] Scheduling '/tmp/SlaveRecoveryTest_0_ShutdownSlave_umhraW/meta/slaves/20140828-212146-16842879-45613-27625-0/frameworks/20140828-212146-16842879-45613-27625-0000' for gc 1.0000093467509weeks in the future
I0828 21:21:54.016201 27645 status_update_manager.cpp:282] Closing status update streams for framework 20140828-212146-16842879-45613-27625-0000
2014-08-28 21:21:55,491:27625(0x2ad7b36ba700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:33940] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2014-08-28 21:21:58,826:27625(0x2ad7b36ba700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:33940] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2014-08-28 21:22:02,162:27625(0x2ad7b36ba700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:33940] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
tests/slave_recovery_tests.cpp:1747: Failure
Failed to wait 10secs for offers2
I0828 21:22:04.043253 27641 master.cpp:734] Framework 20140828-212146-16842879-45613-27625-0000 disconnected
I0828 21:22:04.043427 27641 master.cpp:1680] Deactivating framework 20140828-212146-16842879-45613-27625-0000
I0828 21:22:04.043665 27641 master.cpp:756] Giving framework 20140828-212146-16842879-45613-27625-0000 0ns to failover
I0828 21:22:04.044415 27644 master.cpp:3464] Framework failover timeout, removing framework 20140828-212146-16842879-45613-27625-0000
I0828 21:22:04.044675 27644 master.cpp:3960] Removing framework 20140828-212146-16842879-45613-27625-0000
I0828 21:22:04.045117 27644 slave.cpp:1415] Asked to shut down framework 20140828-212146-16842879-45613-27625-0000 by master@127.0.1.1:45613
W0828 21:22:04.045548 27644 slave.cpp:1430] Cannot shut down unknown framework 20140828-212146-16842879-45613-27625-0000
I0828 21:22:04.043968 27639 hierarchical_allocator_process.hpp:405] Deactivated framework 20140828-212146-16842879-45613-27625-0000
I0828 21:22:04.046103 27639 hierarchical_allocator_process.hpp:360] Removed framework 20140828-212146-16842879-45613-27625-0000
tests/slave_recovery_tests.cpp:1698: Failure
Actual function call count doesn't match EXPECT_CALL(sched, resourceOffers(_, _))...
         Expected: to be called twice
           Actual: called once - unsatisfied and active
I0828 21:22:04.051811 27644 master.cpp:643] Master terminating
I0828 21:22:04.056529 27640 slave.cpp:2365] master@127.0.1.1:45613 exited
W0828 21:22:04.056694 27640 slave.cpp:2368] Master disconnected! Waiting for a new master to be elected
I0828 21:22:04.079474 27625 slave.cpp:475] Slave terminating
[  FAILED  ] SlaveRecoveryTest/0.ShutdownSlave, where TypeParam = mesos::internal::slave::MesosContainerizer (17900 ms)
{noformat}",Bug,Major,jieyu,2014-09-02T17:40:19.000+0000,5,Resolved,Complete,SlaveRecoveryTest.ShutdownSlave is flaky,2014-09-02T17:40:19.000+0000,MESOS-1749,2.0,mesos,Q3 Sprint 4
,2014-08-29T17:53:52.000+0000,xujyan,"{noformat:title=}
tests/master_tests.cpp:1795: Failure
Failed to wait 10secs for slaveRegisteredMessage
{noformat}

Should have placed the FUTURE_MESSAGE that attempts to capture this messages before the slave starts...",Bug,Major,xujyan,2014-11-05T19:49:58.000+0000,5,Resolved,Complete,MasterZooKeeperTest.LostZooKeeperCluster is flaky,2014-11-05T19:49:58.000+0000,MESOS-1748,1.0,mesos,Q3 Sprint 4
,2014-08-27T19:37:04.000+0000,preillyme,"Make it so that either via a slave restart or a out of process ""reconfigure"" ping, the attributes and resources of a slave can be updated to be a superset of what they used to be.",Epic,Major,preillyme,,10020,Accepted,In Progress,Allow slave reconfiguration on restart,2016-04-27T09:29:10.000+0000,MESOS-1739,3.0,mesos,Mesos Q3 Sprint 5
nekto0n,2014-08-21T16:37:02.000+0000,nekto0n,"When you attempt to start slave or master and there's another one already running there, it is nice to report what are the actual parameters to {{bind}} call that failed.",Improvement,Trivial,nekto0n,2014-09-10T21:59:51.000+0000,5,Resolved,Complete,Libprocess: report bind parameters on failure,2014-09-17T00:54:16.000+0000,MESOS-1728,1.0,mesos,Q3 Sprint 4
jieyu,2014-08-21T11:56:10.000+0000,lloesche,"I followed the ""Getting started"" documentation and did:
{noformat}
$ git clone http://git-wip-us.apache.org/repos/asf/mesos.git; cd mesos
$ ./bootstrap
$ mkdir build; cd build
$ ../configure
{noformat}
which aborts with
{noformat}
....
....
checking whether we are using the GNU C compiler... (cached) yes
checking whether gcc accepts -g... (cached) yes
checking for gcc option to accept ISO C89... (cached) none needed
checking dependency style of gcc... (cached) gcc3
../configure: line 18439: syntax error near unexpected token `PROTOBUFPREFIX,'
../configure: line 18439: `  PKG_CHECK_MODULES(PROTOBUFPREFIX,'
{noformat}",Bug,Major,lloesche,2014-08-27T22:20:26.000+0000,5,Resolved,Complete,"Configure fails with ../configure: line 18439: syntax error near unexpected token `PROTOBUFPREFIX,'",2014-10-19T08:39:03.000+0000,MESOS-1727,2.0,mesos,Q3 Sprint 4
klaus1982,2014-08-18T23:50:34.000+0000,bmahler,"Currently we give a small amount of resources to the command executor, in addition to resources used by the command task:

https://github.com/apache/mesos/blob/0.20.0-rc1/src/slave/slave.cpp#L2448
{code: title=}
ExecutorInfo Slave::getExecutorInfo(
    const FrameworkID& frameworkId,
    const TaskInfo& task)
{
  ...
    // Add an allowance for the command executor. This does lead to a
    // small overcommit of resources.
    executor.mutable_resources()->MergeFrom(
        Resources::parse(
          ""cpus:"" + stringify(DEFAULT_EXECUTOR_CPUS) + "";"" +
          ""mem:"" + stringify(DEFAULT_EXECUTOR_MEM.megabytes())).get());
  ...
}
{code}

This leads to an overcommit of the slave. Ideally, for command tasks we can ""transfer"" all of the task resources to the executor at the slave / isolation level.",Bug,Major,bmahler,,10006,Reviewable,New,Command executor can overcommit the slave.,2016-02-12T13:42:20.000+0000,MESOS-1718,3.0,mesos,Twitter Mesos Q4 Sprint 3
,2014-08-18T23:17:23.000+0000,bmahler,"The slave does not show pending tasks in the /state.json endpoint.

This is a bit tricky to add since we rely on knowing the executor directory.",Bug,Major,bmahler,,1,Open,New,The slave does not show pending tasks in the JSON endpoints.,2014-09-15T18:18:18.000+0000,MESOS-1717,1.0,mesos,Q3 Sprint 4
bmahler,2014-08-18T23:13:38.000+0000,bmahler,"In what looks like an oversight, the pending tasks and executors in the slave (Framework::pending) are not sent in the re-registration message.

For tasks, this can lead to spurious TASK_LOST notifications being generated by the master when it falsely thinks the tasks are not present on the slave.",Bug,Major,bmahler,2014-09-10T18:27:13.000+0000,5,Resolved,Complete,The slave does not send pending tasks during re-registration.,2015-04-09T23:58:29.000+0000,MESOS-1715,3.0,mesos,Q3 Sprint 4
cmaloney,2014-08-18T19:43:00.000+0000,vinodkone,"For various reasons, we don't want to mix mesos/libprocess/stout changes into a single commit. Typically, it is up to the reviewee/reviewer to catch this. 

It wold be nice to automate this via the pre-commit hook .",Bug,Major,vinodkone,2014-10-24T18:15:47.000+0000,5,Resolved,Complete,Automate disallowing of commits mixing mesos/libprocess/stout,2014-10-24T18:15:57.000+0000,MESOS-1712,2.0,mesos,Mesosphere Q4 Sprint 1 10/31
vinodkone,2014-08-15T15:15:16.000+0000,tstclair,"It's a pretty rare event, but happened more then once.  

[ RUN      ] SubprocessTest.Status
*** Aborted at 1408023909 (unix time) try ""date -d @1408023909"" if you are using GNU date ***
PC: @       0x35700094b1 (unknown)
*** SIGTERM (@0x3e8000041d8) received by PID 16872 (TID 0x7fa9ea426780) from PID 16856; stack trace: ***
    @       0x3570435cb0 (unknown)
    @       0x35700094b1 (unknown)
    @       0x3570009d9f (unknown)
    @       0x357000e726 (unknown)
    @       0x3570015185 (unknown)
    @           0x5ead42 process::childMain()
    @           0x5ece8d std::_Function_handler<>::_M_invoke()
    @           0x5eac9c process::defaultClone()
    @           0x5ebbd4 process::subprocess()
    @           0x55a229 process::subprocess()
    @           0x55a846 process::subprocess()
    @           0x54224c SubprocessTest_Status_Test::TestBody()
    @     0x7fa9ea460323 (unknown)
    @     0x7fa9ea455b67 (unknown)
    @     0x7fa9ea455c0e (unknown)
    @     0x7fa9ea455d15 (unknown)
    @     0x7fa9ea4593a8 (unknown)
    @     0x7fa9ea459647 (unknown)
    @           0x422466 main
    @       0x3570421d65 (unknown)
    @           0x4260bd (unknown)
[       OK ] SubprocessTest.Status (153 ms)",Bug,Minor,tstclair,2014-08-27T23:18:38.000+0000,5,Resolved,Complete,SubprocessTest.Status sometimes flakes out,2014-09-16T05:47:16.000+0000,MESOS-1705,2.0,mesos,Q3 Sprint 3
jieyu,2014-08-14T23:46:13.000+0000,jaybuff,"Aurora uses the mesos replicated log.  

If you don't run ""mesos-log initialize"" before starting aurora you'll get INFO messages in your aurora log:

{code}
I0814 15:18:38.346638 25141 replica.cpp:633] Replica in EMPTY status received a broadcasted recover request 
I0814 15:18:38.346796 25132 recover.cpp:220] Received a recover response from a replica in EMPTY status
{code}

It is has been deemed too dangerous to automatically run mesos-log initialize for the user (see AURORA-243). 

It would be helpful if that error message was made more friendly and at the ERROR level.  The message could explain what the user should do and the implications of doing so.  Links to the docs would be helpful.

See http://wilderness.apache.org/channels/?f=aurora/2014-08-14#1408055261 for context",Bug,Minor,jaybuff,2014-10-16T21:39:01.000+0000,5,Resolved,Complete,better error message when replicated log hasn't been initialized,2014-10-16T22:05:39.000+0000,MESOS-1703,1.0,mesos,Twitter Q4 Sprint 1
jieyu,2014-08-14T23:14:45.000+0000,jieyu,The doc should tell the user how to use the new network monitoring feature.,Documentation,Major,jieyu,2014-08-15T21:36:27.000+0000,5,Resolved,Complete,Add document for network monitoring.,2014-08-15T21:36:27.000+0000,MESOS-1702,2.0,mesos,Q3 Sprint 3
jieyu,2014-08-13T17:35:17.000+0000,vinodkone,"Observed this on Apache CI: https://builds.apache.org/job/Mesos-Trunk-Ubuntu-Build-Out-Of-Src-Set-JAVA_HOME/2331/consoleFull

It looks like the segfault happens before any tests are run. So I suspect somewhere in the setup phase of the tests.

{code}
mv -f .deps/tests-time_tests.Tpo .deps/tests-time_tests.Po
/bin/bash ./libtool  --tag=CXX   --mode=link g++  -g -g2 -O2 -Wno-unused-local-typedefs -std=c++11   -o tests tests-decoder_tests.o tests-encoder_tests.o tests-http_tests.o tests-io_tests.o tests-main.o tests-mutex_tests.o tests-metrics_tests.o tests-owned_tests.o tests-process_tests.o tests-queue_tests.o tests-reap_tests.o tests-sequence_tests.o tests-shared_tests.o tests-statistics_tests.o tests-subprocess_tests.o tests-system_tests.o tests-timeseries_tests.o tests-time_tests.o 3rdparty/libgmock.la libprocess.la 3rdparty/glog-0.3.3/libglog.la 3rdparty/libry_http_parser.la 3rdparty/libev-4.15/libev.la -lz  -lrt
libtool: link: g++ -g -g2 -O2 -Wno-unused-local-typedefs -std=c++11 -o tests tests-decoder_tests.o tests-encoder_tests.o tests-http_tests.o tests-io_tests.o tests-main.o tests-mutex_tests.o tests-metrics_tests.o tests-owned_tests.o tests-process_tests.o tests-queue_tests.o tests-reap_tests.o tests-sequence_tests.o tests-shared_tests.o tests-statistics_tests.o tests-subprocess_tests.o tests-system_tests.o tests-timeseries_tests.o tests-time_tests.o  3rdparty/.libs/libgmock.a ./.libs/libprocess.a /home/jenkins/jenkins-slave/workspace/Mesos-Trunk-Ubuntu-Build-Out-Of-Src-Set-JAVA_HOME/build/3rdparty/libprocess/3rdparty/glog-0.3.3/.libs/libglog.a /home/jenkins/jenkins-slave/workspace/Mesos-Trunk-Ubuntu-Build-Out-Of-Src-Set-JAVA_HOME/build/3rdparty/libprocess/3rdparty/libev-4.15/.libs/libev.a 3rdparty/glog-0.3.3/.libs/libglog.a -lpthread 3rdparty/.libs/libry_http_parser.a 3rdparty/libev-4.15/.libs/libev.a -lm -lz -lrt
make[5]: Leaving directory `/home/jenkins/jenkins-slave/workspace/Mesos-Trunk-Ubuntu-Build-Out-Of-Src-Set-JAVA_HOME/build/3rdparty/libprocess'
make  check-local
make[5]: Entering directory `/home/jenkins/jenkins-slave/workspace/Mesos-Trunk-Ubuntu-Build-Out-Of-Src-Set-JAVA_HOME/build/3rdparty/libprocess'
./tests
Note: Google Test filter = 
[==========] Running 0 tests from 0 test cases.
[==========] 0 tests from 0 test cases ran. (0 ms total)
[  PASSED  ] 0 tests.

  YOU HAVE 3 DISABLED TESTS

make[5]: *** [check-local] Segmentation fault
make[5]: Leaving directory `/home/jenkins/jenkins-slave/workspace/Mesos-Trunk-Ubuntu-Build-Out-Of-Src-Set-JAVA_HOME/build/3rdparty/libprocess'
make[4]: *** [check-am] Error 2
make[4]: Leaving directory `/home/jenkins/jenkins-slave/workspace/Mesos-Trunk-Ubuntu-Build-Out-Of-Src-Set-JAVA_HOME/build/3rdparty/libprocess'
make[3]: *** [check-recursive] Error 1
make[3]: Leaving directory `/home/jenkins/jenkins-slave/workspace/Mesos-Trunk-Ubuntu-Build-Out-Of-Src-Set-JAVA_HOME/build/3rdparty/libprocess'
make[2]: *** [check-recursive] Error 1
make[2]: Leaving directory `/home/jenkins/jenkins-slave/workspace/Mesos-Trunk-Ubuntu-Build-Out-Of-Src-Set-JAVA_HOME/build/3rdparty'
make[1]: *** [check] Error 2
make[1]: Leaving directory `/home/jenkins/jenkins-slave/workspace/Mesos-Trunk-Ubuntu-Build-Out-Of-Src-Set-JAVA_HOME/build/3rdparty'
make: *** [check-recursive] Error 1
Build step 'Execute shell' marked build as failure
Sending e-mails to: dev@mesos.apache.org benjamin.hindman@gmail.com dhamon@twitter.com yujie.jay@gmail.com
Finished: FAILURE
{code}",Bug,Major,vinodkone,2014-09-04T21:23:13.000+0000,5,Resolved,Complete,make check segfaults,2014-09-04T21:23:13.000+0000,MESOS-1698,2.0,mesos,Q3 Sprint 3
bmahler,2014-08-12T19:01:49.000+0000,bmahler,"As we update the Master to keep tasks in memory until they are both terminal and acknowledged (MESOS-1410), the lifetime of tasks in Mesos will look as follows:

{code}
Master           Slave
 {}               {}
{Tn}              {}  // Master receives Task T, non-terminal. Forwards to slave.
{Tn}             {Tn} // Slave receives Task T, non-terminal.
{Tn}             {Tt} // Task becomes terminal on slave. Update forwarded.
{Tt}             {Tt} // Master receives update, forwards to framework.
 {}              {Tt} // Master receives ack, forwards to slave.
 {}               {}  // Slave receives ack.
{code}

In the current form of reconciliation, the slave sends to the master all tasks that are not both terminal and acknowledged. At any point in the above lifecycle, the slave's re-registration message can reach the master.

Note the following properties:

*(1)* The master may have a non-terminal task, not present in the slave's re-registration message.
*(2)* The master may have a non-terminal task, present in the slave's re-registration message but in a different state.
*(3)* The slave's re-registration message may contain a terminal unacknowledged task unknown to the master.

In the current master / slave [reconciliation|https://github.com/apache/mesos/blob/0.19.1/src/master/master.cpp#L3146] code, the master assumes that case (1) is because a launch task message was dropped, and it sends TASK_LOST. We've seen above that (1) can happen even when the task reaches the slave correctly, so this can lead to inconsistency!

After chatting with [~vinodkone], we're considering updating the reconciliation to occur as follows:


→ Slave sends all tasks that are not both terminal and acknowledged, during re-registration. This is the same as before.

→ If the master sees tasks that are missing in the slave, the master sends the tasks that need to be reconciled to the slave for the tasks. This can be piggy-backed on the re-registration message.

→ The slave will send TASK_LOST if the task is not known to it. Preferably in a retried manner, unless we update socket closure on the slave to force a re-registration.",Bug,Major,bmahler,2014-10-09T01:32:47.000+0000,5,Resolved,Complete,Improve reconciliation between master and slave.,2014-10-09T01:32:47.000+0000,MESOS-1696,3.0,mesos,Mesos Q3 Sprint 5
vinodkone,2014-08-12T18:01:28.000+0000,bmahler,"The slave is currently exposing a string value for the ""registered"" statistic, this should be a number:

{code}
slave:5051/stats.json
{
  ""recovery_errors"": 0,
  ""registered"": ""1"",
  ""slave/executors_registering"": 0,
  ...
}
{code}

Should be a pretty straightforward fix, looks like this first originated back in 2013:

{code}
commit b8291304e1523eb67ea8dc5f195cdb0d8e7d8348
Author: Vinod Kone <vinod@twitter.com>
Date:   Wed Jul 3 12:37:36 2013 -0700

    Added a ""registered"" key/value pair to slave's stats.json.

    Review: https://reviews.apache.org/r/12256

diff --git a/src/slave/http.cpp b/src/slave/http.cpp
index dc2955f..dd51516 100644
--- a/src/slave/http.cpp
+++ b/src/slave/http.cpp
@@ -281,6 +281,8 @@ Future<Response> Slave::Http::stats(const Request& request)
   object.values[""lost_tasks""] = slave.stats.tasks[TASK_LOST];
   object.values[""valid_status_updates""] = slave.stats.validStatusUpdates;
   object.values[""invalid_status_updates""] = slave.stats.invalidStatusUpdates;
+  object.values[""registered""] = slave.master ? ""1"" : ""0"";
+

   return OK(object, request.query.get(""jsonp""));
 }
{code}",Bug,Minor,bmahler,2014-08-19T18:16:38.000+0000,5,Resolved,Complete,"The stats.json endpoint on the slave exposes ""registered"" as a string.",2014-08-19T18:16:38.000+0000,MESOS-1695,1.0,mesos,
vinodkone,2014-08-08T22:08:28.000+0000,idownes,Increment counter when container destroy fails.,Bug,Major,idownes,2015-02-24T20:44:16.000+0000,5,Resolved,Complete,Expose metric for container destroy failures,2015-02-24T20:44:16.000+0000,MESOS-1690,3.0,mesos,Twitter Mesos Q1 Sprint 2
xujyan,2014-08-07T18:59:27.000+0000,xujyan,Create a Markdown doc under /docs,Task,Major,xujyan,2014-08-15T05:32:56.000+0000,5,Resolved,Complete,Create user doc for framework rate limiting feature,2014-08-15T05:32:56.000+0000,MESOS-1683,2.0,mesos,Q3 Sprint 3
jieyu,2014-08-06T19:23:12.000+0000,jieyu,"{noformat}
GMOCK WARNING:
Uninteresting mock function call - taking default action specified at:
../../../mesos/src/tests/mesos.hpp:566:
    Function call: resourcesRecovered(@0x7f38f40043e8 20140806-190304-2081170186-36159-24511-0000, @0x7f38f40043c8 20140806-190304-2081170186-36159-24511-0, @0x7f38f40043b0 { cpus(*):2, mem(*):1024, disk(*):464204, ports(*):[31000-32000] })
{noformat}",Bug,Major,jieyu,2014-08-06T21:25:48.000+0000,5,Resolved,Complete,AllocatorTest.FrameworkReregistersFirst is flaky.,2014-08-06T21:25:48.000+0000,MESOS-1677,2.0,mesos,Q3 Sprint 2
xujyan,2014-08-06T18:21:32.000+0000,xujyan,"{noformat:title=}
[ RUN      ] ZooKeeperMasterContenderDetectorTest.MasterDetectorTimedoutSession
I0806 01:18:37.648684 17458 zookeeper_test_server.cpp:158] Started ZooKeeperTestServer on port 42069
2014-08-06 01:18:37,650:17458(0x2b4679ca5700):ZOO_INFO@log_env@712: Client environment:zookeeper.version=zookeeper C client 3.4.5
2014-08-06 01:18:37,650:17458(0x2b4679ca5700):ZOO_INFO@log_env@716: Client environment:host.name=lucid
2014-08-06 01:18:37,650:17458(0x2b4679ca5700):ZOO_INFO@log_env@723: Client environment:os.name=Linux
2014-08-06 01:18:37,650:17458(0x2b4679ca5700):ZOO_INFO@log_env@724: Client environment:os.arch=2.6.32-64-generic
2014-08-06 01:18:37,650:17458(0x2b4679ca5700):ZOO_INFO@log_env@725: Client environment:os.version=#128-Ubuntu SMP Tue Jul 15 08:32:40 UTC 2014
2014-08-06 01:18:37,651:17458(0x2b4679ca5700):ZOO_INFO@log_env@733: Client environment:user.name=(null)
2014-08-06 01:18:37,651:17458(0x2b4679ca5700):ZOO_INFO@log_env@741: Client environment:user.home=/home/jenkins
2014-08-06 01:18:37,651:17458(0x2b4679ca5700):ZOO_INFO@log_env@753: Client environment:user.dir=/var/jenkins/workspace/mesos-ubuntu-10.04-gcc/src
2014-08-06 01:18:37,651:17458(0x2b4679ca5700):ZOO_INFO@zookeeper_init@786: Initiating client connection, host=127.0.0.1:42069 sessionTimeout=5000 watcher=0x2b467450bc00 sessionId=0 sessionPasswd=<null> context=0x1682db0 flags=0
2014-08-06 01:18:37,656:17458(0x2b468638b700):ZOO_INFO@check_events@1703: initiated connection to server [127.0.0.1:42069]
2014-08-06 01:18:37,669:17458(0x2b468638b700):ZOO_INFO@check_events@1750: session establishment complete on server [127.0.0.1:42069], sessionId=0x147aa6601cf0000, negotiated timeout=6000
I0806 01:18:37.671725 17486 group.cpp:313] Group process (group(37)@127.0.1.1:55561) connected to ZooKeeper
I0806 01:18:37.671758 17486 group.cpp:787] Syncing group operations: queue size (joins, cancels, datas) = (0, 0, 0)
I0806 01:18:37.671771 17486 group.cpp:385] Trying to create path '/mesos' in ZooKeeper
2014-08-06 01:18:39,101:17458(0x2b4687394700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:36197] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2014-08-06 01:18:42,441:17458(0x2b4687394700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:36197] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
I0806 01:18:42.656673 17481 contender.cpp:131] Joining the ZK group
I0806 01:18:42.662484 17484 contender.cpp:247] New candidate (id='0') has entered the contest for leadership
I0806 01:18:42.663754 17481 detector.cpp:138] Detected a new leader: (id='0')
I0806 01:18:42.663884 17481 group.cpp:658] Trying to get '/mesos/info_0000000000' in ZooKeeper
I0806 01:18:42.664788 17483 detector.cpp:426] A new leading master (UPID=@128.150.152.0:10000) is detected
2014-08-06 01:18:42,666:17458(0x2b4679ea6700):ZOO_INFO@log_env@712: Client environment:zookeeper.version=zookeeper C client 3.4.5
2014-08-06 01:18:42,666:17458(0x2b4679ea6700):ZOO_INFO@log_env@716: Client environment:host.name=lucid
2014-08-06 01:18:42,666:17458(0x2b4679ea6700):ZOO_INFO@log_env@723: Client environment:os.name=Linux
2014-08-06 01:18:42,666:17458(0x2b4679ea6700):ZOO_INFO@log_env@724: Client environment:os.arch=2.6.32-64-generic
2014-08-06 01:18:42,666:17458(0x2b4679ea6700):ZOO_INFO@log_env@725: Client environment:os.version=#128-Ubuntu SMP Tue Jul 15 08:32:40 UTC 2014
2014-08-06 01:18:42,666:17458(0x2b4679ea6700):ZOO_INFO@log_env@733: Client environment:user.name=(null)
2014-08-06 01:18:42,666:17458(0x2b4679ea6700):ZOO_INFO@log_env@741: Client environment:user.home=/home/jenkins
2014-08-06 01:18:42,666:17458(0x2b4679ea6700):ZOO_INFO@log_env@753: Client environment:user.dir=/var/jenkins/workspace/mesos-ubuntu-10.04-gcc/src
2014-08-06 01:18:42,666:17458(0x2b4679ea6700):ZOO_INFO@zookeeper_init@786: Initiating client connection, host=127.0.0.1:42069 sessionTimeout=5000 watcher=0x2b467450bc00 sessionId=0 sessionPasswd=<null> context=0x15c00f0 flags=0
2014-08-06 01:18:42,668:17458(0x2b4686d91700):ZOO_INFO@check_events@1703: initiated connection to server [127.0.0.1:42069]
2014-08-06 01:18:42,672:17458(0x2b4686d91700):ZOO_INFO@check_events@1750: session establishment complete on server [127.0.0.1:42069], sessionId=0x147aa6601cf0001, negotiated timeout=6000
I0806 01:18:42.673542 17485 group.cpp:313] Group process (group(38)@127.0.1.1:55561) connected to ZooKeeper
I0806 01:18:42.673570 17485 group.cpp:787] Syncing group operations: queue size (joins, cancels, datas) = (0, 0, 0)
I0806 01:18:42.673580 17485 group.cpp:385] Trying to create path '/mesos' in ZooKeeper
2014-08-06 01:18:46,796:17458(0x2b468638b700):ZOO_WARN@zookeeper_interest@1557: Exceeded deadline by 2131ms
2014-08-06 01:18:46,796:17458(0x2b468638b700):ZOO_ERROR@handle_socket_error_msg@1643: Socket [127.0.0.1:42069] zk retcode=-7, errno=110(Connection timed out): connection to 127.0.0.1:42069 timed out (exceeded timeout by 131ms)
2014-08-06 01:18:46,796:17458(0x2b468638b700):ZOO_WARN@zookeeper_interest@1557: Exceeded deadline by 2131ms
2014-08-06 01:18:46,796:17458(0x2b4686d91700):ZOO_WARN@zookeeper_interest@1557: Exceeded deadline by 2115ms
2014-08-06 01:18:46,796:17458(0x2b4686d91700):ZOO_ERROR@handle_socket_error_msg@1643: Socket [127.0.0.1:42069] zk retcode=-7, errno=110(Connection timed out): connection to 127.0.0.1:42069 timed out (exceeded timeout by 115ms)
2014-08-06 01:18:46,796:17458(0x2b4686d91700):ZOO_WARN@zookeeper_interest@1557: Exceeded deadline by 2115ms
2014-08-06 01:18:46,799:17458(0x2b4687394700):ZOO_WARN@zookeeper_interest@1557: Exceeded deadline by 1025ms
2014-08-06 01:18:46,800:17458(0x2b4687394700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:36197] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
I0806 01:18:46.806895 17486 group.cpp:418] Lost connection to ZooKeeper, attempting to reconnect ...
I0806 01:18:46.807857 17479 group.cpp:418] Lost connection to ZooKeeper, attempting to reconnect ...
I0806 01:18:47.669064 17482 contender.cpp:131] Joining the ZK group
2014-08-06 01:18:47,669:17458(0x2b4686d91700):ZOO_WARN@zookeeper_interest@1557: Exceeded deadline by 2989ms
2014-08-06 01:18:47,669:17458(0x2b4686d91700):ZOO_INFO@check_events@1703: initiated connection to server [127.0.0.1:42069]
2014-08-06 01:18:47,671:17458(0x2b4686d91700):ZOO_INFO@check_events@1750: session establishment complete on server [127.0.0.1:42069], sessionId=0x147aa6601cf0001, negotiated timeout=6000
I0806 01:18:47.682868 17485 contender.cpp:247] New candidate (id='1') has entered the contest for leadership
I0806 01:18:47.683404 17482 group.cpp:313] Group process (group(38)@127.0.1.1:55561) reconnected to ZooKeeper
I0806 01:18:47.683445 17482 group.cpp:787] Syncing group operations: queue size (joins, cancels, datas) = (0, 0, 0)
I0806 01:18:47.685998 17482 detector.cpp:138] Detected a new leader: (id='0')
I0806 01:18:47.686142 17482 group.cpp:658] Trying to get '/mesos/info_0000000000' in ZooKeeper
I0806 01:18:47.687289 17479 detector.cpp:426] A new leading master (UPID=@128.150.152.0:10000) is detected
2014-08-06 01:18:47,687:17458(0x2b467a2a8700):ZOO_INFO@log_env@712: Client environment:zookeeper.version=zookeeper C client 3.4.5
2014-08-06 01:18:47,687:17458(0x2b467a2a8700):ZOO_INFO@log_env@716: Client environment:host.name=lucid
2014-08-06 01:18:47,687:17458(0x2b467a2a8700):ZOO_INFO@log_env@723: Client environment:os.name=Linux
2014-08-06 01:18:47,687:17458(0x2b467a2a8700):ZOO_INFO@log_env@724: Client environment:os.arch=2.6.32-64-generic
2014-08-06 01:18:47,687:17458(0x2b467a2a8700):ZOO_INFO@log_env@725: Client environment:os.version=#128-Ubuntu SMP Tue Jul 15 08:32:40 UTC 2014
2014-08-06 01:18:47,687:17458(0x2b467a2a8700):ZOO_INFO@log_env@733: Client environment:user.name=(null)
2014-08-06 01:18:47,687:17458(0x2b467a2a8700):ZOO_INFO@log_env@741: Client environment:user.home=/home/jenkins
2014-08-06 01:18:47,687:17458(0x2b467a2a8700):ZOO_INFO@log_env@753: Client environment:user.dir=/var/jenkins/workspace/mesos-ubuntu-10.04-gcc/src
2014-08-06 01:18:47,687:17458(0x2b467a2a8700):ZOO_INFO@zookeeper_init@786: Initiating client connection, host=127.0.0.1:42069 sessionTimeout=5000 watcher=0x2b467450bc00 sessionId=0 sessionPasswd=<null> context=0x2b467c0421c0 flags=0
2014-08-06 01:18:47,699:17458(0x2b4687de6700):ZOO_INFO@check_events@1703: initiated connection to server [127.0.0.1:42069]
2014-08-06 01:18:47,712:17458(0x2b4687de6700):ZOO_INFO@check_events@1750: session establishment complete on server [127.0.0.1:42069], sessionId=0x147aa6601cf0002, negotiated timeout=6000
I0806 01:18:47.712846 17479 group.cpp:313] Group process (group(39)@127.0.1.1:55561) connected to ZooKeeper
I0806 01:18:47.712873 17479 group.cpp:787] Syncing group operations: queue size (joins, cancels, datas) = (0, 0, 0)
I0806 01:18:47.712882 17479 group.cpp:385] Trying to create path '/mesos' in ZooKeeper
I0806 01:18:47.714648 17479 detector.cpp:138] Detected a new leader: (id='0')
I0806 01:18:47.714759 17479 group.cpp:658] Trying to get '/mesos/info_0000000000' in ZooKeeper
I0806 01:18:47.716130 17479 detector.cpp:426] A new leading master (UPID=@128.150.152.0:10000) is detected
2014-08-06 01:18:47,718:17458(0x2b4686d91700):ZOO_ERROR@handle_socket_error_msg@1721: Socket [127.0.0.1:42069] zk retcode=-4, errno=112(Host is down): failed while receiving a server response
I0806 01:18:47.718889 17479 group.cpp:418] Lost connection to ZooKeeper, attempting to reconnect ...
2014-08-06 01:18:47,720:17458(0x2b4687de6700):ZOO_ERROR@handle_socket_error_msg@1721: Socket [127.0.0.1:42069] zk retcode=-4, errno=112(Host is down): failed while receiving a server response
I0806 01:18:47.720788 17484 group.cpp:418] Lost connection to ZooKeeper, attempting to reconnect ...
I0806 01:18:47.724663 17458 zookeeper_test_server.cpp:122] Shutdown ZooKeeperTestServer on port 42069
2014-08-06 01:18:48,798:17458(0x2b468638b700):ZOO_WARN@zookeeper_interest@1557: Exceeded deadline by 4133ms
2014-08-06 01:18:48,798:17458(0x2b468638b700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:42069] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2014-08-06 01:18:49,720:17458(0x2b4686d91700):ZOO_WARN@zookeeper_interest@1557: Exceeded deadline by 33ms
2014-08-06 01:18:49,721:17458(0x2b4686d91700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:42069] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2014-08-06 01:18:49,722:17458(0x2b4687de6700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:42069] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2014-08-06 01:18:50,136:17458(0x2b4687394700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:36197] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2014-08-06 01:18:50,800:17458(0x2b468638b700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:42069] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2014-08-06 01:18:51,723:17458(0x2b4686d91700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:42069] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2014-08-06 01:18:51,723:17458(0x2b4687de6700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:42069] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2014-08-06 01:18:52,801:17458(0x2b468638b700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:42069] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
W0806 01:18:52.842553 17481 group.cpp:456] Timed out waiting to reconnect to ZooKeeper. Forcing ZooKeeper session (sessionId=147aa6601cf0000) expiration
I0806 01:18:52.842911 17481 group.cpp:472] ZooKeeper session expired
I0806 01:18:52.843468 17485 detector.cpp:126] The current leader (id=0) is lost
I0806 01:18:52.843483 17485 detector.cpp:138] Detected a new leader: None
I0806 01:18:52.843618 17485 contender.cpp:196] Membership cancelled: 0
2014-08-06 01:18:52,843:17458(0x2b4679aa4700):ZOO_INFO@zookeeper_close@2522: Freeing zookeeper resources for sessionId=0x147aa6601cf0000

2014-08-06 01:18:52,844:17458(0x2b4679aa4700):ZOO_INFO@log_env@712: Client environment:zookeeper.version=zookeeper C client 3.4.5
2014-08-06 01:18:52,844:17458(0x2b4679aa4700):ZOO_INFO@log_env@716: Client environment:host.name=lucid
2014-08-06 01:18:52,844:17458(0x2b4679aa4700):ZOO_INFO@log_env@723: Client environment:os.name=Linux
2014-08-06 01:18:52,844:17458(0x2b4679aa4700):ZOO_INFO@log_env@724: Client environment:os.arch=2.6.32-64-generic
2014-08-06 01:18:52,844:17458(0x2b4679aa4700):ZOO_INFO@log_env@725: Client environment:os.version=#128-Ubuntu SMP Tue Jul 15 08:32:40 UTC 2014
2014-08-06 01:18:52,844:17458(0x2b4679aa4700):ZOO_INFO@log_env@733: Client environment:user.name=(null)
2014-08-06 01:18:52,844:17458(0x2b4679aa4700):ZOO_INFO@log_env@741: Client environment:user.home=/home/jenkins
2014-08-06 01:18:52,844:17458(0x2b4679aa4700):ZOO_INFO@log_env@753: Client environment:user.dir=/var/jenkins/workspace/mesos-ubuntu-10.04-gcc/src
2014-08-06 01:18:52,844:17458(0x2b4679aa4700):ZOO_INFO@zookeeper_init@786: Initiating client connection, host=127.0.0.1:42069 sessionTimeout=5000 watcher=0x2b467450bc00 sessionId=0 sessionPasswd=<null> context=0x1349ad0 flags=0
2014-08-06 01:18:52,844:17458(0x2b468698f700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:42069] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2014-08-06 01:18:53,473:17458(0x2b4687394700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:36197] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
W0806 01:18:53.720684 17480 group.cpp:456] Timed out waiting to reconnect to ZooKeeper. Forcing ZooKeeper session (sessionId=147aa6601cf0001) expiration
I0806 01:18:53.721132 17480 group.cpp:472] ZooKeeper session expired
I0806 01:18:53.721516 17479 detector.cpp:126] The current leader (id=0) is lost
I0806 01:18:53.721534 17479 detector.cpp:138] Detected a new leader: None
I0806 01:18:53.721696 17479 contender.cpp:196] Membership cancelled: 1
2014-08-06 01:18:53,721:17458(0x2b46798a3700):ZOO_INFO@zookeeper_close@2522: Freeing zookeeper resources for sessionId=0x147aa6601cf0001

2014-08-06 01:18:53,722:17458(0x2b46798a3700):ZOO_INFO@log_env@712: Client environment:zookeeper.version=zookeeper C client 3.4.5
2014-08-06 01:18:53,722:17458(0x2b46798a3700):ZOO_INFO@log_env@716: Client environment:host.name=lucid
2014-08-06 01:18:53,722:17458(0x2b46798a3700):ZOO_INFO@log_env@723: Client environment:os.name=Linux
2014-08-06 01:18:53,722:17458(0x2b46798a3700):ZOO_INFO@log_env@724: Client environment:os.arch=2.6.32-64-generic
2014-08-06 01:18:53,722:17458(0x2b46798a3700):ZOO_INFO@log_env@725: Client environment:os.version=#128-Ubuntu SMP Tue Jul 15 08:32:40 UTC 2014
2014-08-06 01:18:53,722:17458(0x2b46798a3700):ZOO_INFO@log_env@733: Client environment:user.name=(null)
2014-08-06 01:18:53,722:17458(0x2b46798a3700):ZOO_INFO@log_env@741: Client environment:user.home=/home/jenkins
2014-08-06 01:18:53,722:17458(0x2b46798a3700):ZOO_INFO@log_env@753: Client environment:user.dir=/var/jenkins/workspace/mesos-ubuntu-10.04-gcc/src
2014-08-06 01:18:53,722:17458(0x2b46798a3700):ZOO_INFO@zookeeper_init@786: Initiating client connection, host=127.0.0.1:42069 sessionTimeout=5000 watcher=0x2b467450bc00 sessionId=0 sessionPasswd=<null> context=0x16a0550 flags=0
2014-08-06 01:18:53,723:17458(0x2b4686f92700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:42069] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2014-08-06 01:18:53,726:17458(0x2b4687de6700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:42069] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
W0806 01:18:53.730258 17479 group.cpp:456] Timed out waiting to reconnect to ZooKeeper. Forcing ZooKeeper session (sessionId=147aa6601cf0002) expiration
I0806 01:18:53.730736 17479 group.cpp:472] ZooKeeper session expired
I0806 01:18:53.731081 17481 detector.cpp:126] The current leader (id=0) is lost
I0806 01:18:53.731132 17481 detector.cpp:138] Detected a new leader: None
2014-08-06 01:18:53,731:17458(0x2b46796a2700):ZOO_INFO@zookeeper_close@2522: Freeing zookeeper resources for sessionId=0x147aa6601cf0002

2014-08-06 01:18:53,731:17458(0x2b46796a2700):ZOO_INFO@log_env@712: Client environment:zookeeper.version=zookeeper C client 3.4.5
2014-08-06 01:18:53,731:17458(0x2b46796a2700):ZOO_INFO@log_env@716: Client environment:host.name=lucid
2014-08-06 01:18:53,731:17458(0x2b46796a2700):ZOO_INFO@log_env@723: Client environment:os.name=Linux
2014-08-06 01:18:53,731:17458(0x2b46796a2700):ZOO_INFO@log_env@724: Client environment:os.arch=2.6.32-64-generic
2014-08-06 01:18:53,731:17458(0x2b46796a2700):ZOO_INFO@log_env@725: Client environment:os.version=#128-Ubuntu SMP Tue Jul 15 08:32:40 UTC 2014
2014-08-06 01:18:53,731:17458(0x2b46796a2700):ZOO_INFO@log_env@733: Client environment:user.name=(null)
2014-08-06 01:18:53,731:17458(0x2b46796a2700):ZOO_INFO@log_env@741: Client environment:user.home=/home/jenkins
2014-08-06 01:18:53,732:17458(0x2b46796a2700):ZOO_INFO@log_env@753: Client environment:user.dir=/var/jenkins/workspace/mesos-ubuntu-10.04-gcc/src
2014-08-06 01:18:53,732:17458(0x2b46796a2700):ZOO_INFO@zookeeper_init@786: Initiating client connection, host=127.0.0.1:42069 sessionTimeout=5000 watcher=0x2b467450bc00 sessionId=0 sessionPasswd=<null> context=0x2b467c035f30 flags=0
2014-08-06 01:18:53,733:17458(0x2b4687be5700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:42069] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2014-08-06 01:18:54,512:17458(0x2b468698f700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:42069] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2014-08-06 01:18:55,393:17458(0x2b4686f92700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:42069] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2014-08-06 01:18:55,403:17458(0x2b4687be5700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:42069] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2014-08-06 01:18:56,301:17458(0x2b468698f700):ZOO_WARN@zookeeper_interest@1557: Exceeded deadline by 122ms
2014-08-06 01:18:56,302:17458(0x2b468698f700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:42069] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2014-08-06 01:18:56,809:17458(0x2b4687394700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:36197] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2014-08-06 01:18:57,939:17458(0x2b4686f92700):ZOO_WARN@zookeeper_interest@1557: Exceeded deadline by 879ms
2014-08-06 01:18:57,940:17458(0x2b4686f92700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:42069] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2014-08-06 01:18:57,940:17458(0x2b4687be5700):ZOO_WARN@zookeeper_interest@1557: Exceeded deadline by 870ms
2014-08-06 01:18:57,940:17458(0x2b4687be5700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:42069] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
tests/master_contender_detector_tests.cpp:574: Failure
Failed to wait 10secs for leaderReconnecting
2014-08-06 01:18:57,941:17458(0x2b46794a0120):ZOO_INFO@zookeeper_close@2522: Freeing zookeeper resources for sessionId=0

I0806 01:18:57.949972 17458 contender.cpp:186] Now cancelling the membership: 1
2014-08-06 01:18:57,950:17458(0x2b46794a0120):ZOO_INFO@zookeeper_close@2522: Freeing zookeeper resources for sessionId=0

I0806 01:18:57.950731 17458 contender.cpp:186] Now cancelling the membership: 0
2014-08-06 01:18:57,951:17458(0x2b46794a0120):ZOO_INFO@zookeeper_close@2522: Freeing zookeeper resources for sessionId=0

../3rdparty/libprocess/include/process/gmock.hpp:298: Failure
Actual function call count doesn't match EXPECT_CALL(filter->mock, filter(testing::A<const DispatchEvent&>()))...
    Expected args: dispatch matcher (1, 16-byte object <50-20 4A-00 00-00 00-00 00-00 00-00 00-00 00-00>)
         Expected: to be called once
           Actual: never called - unsatisfied and active
[  FAILED  ] ZooKeeperMasterContenderDetectorTest.MasterDetectorTimedoutSession (20308 ms)
{noformat}",Bug,Major,xujyan,2014-09-11T05:38:29.000+0000,5,Resolved,Complete,ZooKeeperMasterContenderDetectorTest.MasterDetectorTimedoutSession is flaky,2014-09-11T05:38:29.000+0000,MESOS-1676,1.0,mesos,Mesos Q3 Sprint 5
jieyu,2014-08-06T16:44:25.000+0000,jieyu,"As the first step to solve MESOS-1654, we need to kill private_resources in SlaveInfo and add a 'ephemeral_ports' resource.

For now, the slave and the port mapping isolator will simply ignore the 'ephemeral_ports' resource in ExecutorInfo and TaskInfo, and make allocation by itself. We will revisit this once the overcommit race (MESOS-1466) is fixed.",Task,Major,jieyu,2014-08-06T16:47:34.000+0000,5,Resolved,Complete,Kill private_resources and treat 'ephemeral_ports' as a resource.,2014-08-06T16:47:34.000+0000,MESOS-1674,3.0,mesos,Q3 Sprint 2
jieyu,2014-08-05T19:17:57.000+0000,jieyu,"Right now, it is declared as follows:
{noformat}
const Duration MASTER_PING_TIMEOUT =
  master::SLAVE_PING_TIMEOUT * master::MAX_SLAVE_PING_TIMEOUTS
{noformat}

Since static initialization order in C++ is undefined, MASTER_PING_TIMEOUT's value is non-deterministic. We've already observed that in tests (where MASTER_PING_TIMEOUT == 0).",Bug,Critical,jieyu,2014-08-05T21:58:09.000+0000,5,Resolved,Complete,The value of MASTER_PING_TIMEOUT is non-deterministic,2014-08-06T16:37:19.000+0000,MESOS-1673,1.0,mesos,Q3 Sprint 2
dhamon,2014-08-05T18:32:47.000+0000,dhamon,The allocator already allows filters to be added when resources are unused. It is useful to also allow the same behaviour in {{resourcesRecovered}}.,Task,Major,dhamon,2014-08-06T19:24:50.000+0000,5,Resolved,Complete,Add filter to allocator resourcesRecovered method,2014-08-06T19:24:50.000+0000,MESOS-1672,2.0,mesos,Q3 Sprint 2
jieyu,2014-08-05T18:23:52.000+0000,jieyu,"Expose the following metrics:

slave/executors_registering
slave/executors_running
slave/executors_terminating
slave/executors_terminated",Task,Minor,jieyu,2014-08-06T19:59:46.000+0000,5,Resolved,Complete,Expose executor metrics for slave.,2014-08-06T19:59:46.000+0000,MESOS-1671,2.0,mesos,Q3 Sprint 2
vinodkone,2014-08-04T22:04:18.000+0000,bmahler,"In MESOS-1529, we realized that it's possible for a slave to remain disconnected in the master if the following occurs:

→ Master and Slave connected operating normally.
→ Temporary one-way network failure, master→slave link breaks.
→ Master marks slave as disconnected.
→ Network restored and health checking continues normally, slave is not removed as a result. Slave does not attempt to re-register since it is receiving pings once again.
→ Slave remains disconnected according to the master, and the slave does not try to re-register. Bad!

We were originally thinking of using a failover timeout in the master to remove these slaves that don't re-register. However, it can be dangerous when ZooKeeper issues are preventing the slave from re-registering with the master; we do not want to remove a ton of slaves in this situation.

Rather, when the slave is health checking correctly but does not re-register within a timeout, we could send a registration request from the master to the slave, telling the slave that it must re-register. This message could also be used when receiving status updates (or other messages) from slaves that are disconnected in the master.",Bug,Minor,bmahler,2014-09-25T20:56:28.000+0000,5,Resolved,Complete,Handle a temporary one-way master --> slave socket closure.,2014-10-08T21:45:56.000+0000,MESOS-1668,2.0,mesos,Mesos Q3 Sprint 5
dhamon,2014-08-04T21:42:55.000+0000,dhamon,"With network isolation, we statically assign ephemeral port ranges. As such there is a upper bound on the number of containers each slave can support.

We should avoid sending offers for slaves that have hit that limit as any tasks will fail to launch and will be LOST. ",Task,Major,dhamon,2014-08-05T22:13:10.000+0000,5,Resolved,Complete,Set maximum executors per slave to avoid overcommit of ephemeral ports,2014-08-05T22:13:10.000+0000,MESOS-1666,1.0,mesos,Q3 Sprint 2
,2014-08-04T19:07:54.000+0000,dhamon,"When we rate-limit messages from a framework, we should let them know so they can proactively back-off to avoid putting extra pressure on the master.",Improvement,Major,dhamon,,10020,Accepted,In Progress,Inform framework when rate limiting is active,2016-01-22T20:06:37.000+0000,MESOS-1664,3.0,mesos,
jieyu,2014-07-29T05:43:16.000+0000,jieyu,"A slave may crash while we are installing/removing filters. The slave recovery for the network isolator should tolerate those partially installed filters. Also, we want to avoid leaking a filter on host eth0 and host lo.

The current code cannot tolerate that, thus may cause the following error:

{noformat}
Failed to perform recovery: Collect failed: Failed to recover container d409a100-2afb-497c-864f-fe3002cf65d9 with pid 50405: No ephemeral ports found
To remedy this do as follows:
Step 1: rm -f /var/lib/mesos/meta/slaves/latest
       This ensures slave doesn't recover old live executors.
Step 2: Restart the slave.
{noformat}",Bug,Major,jieyu,2014-08-01T17:54:25.000+0000,5,Resolved,Complete,Network isolator should tolerate slave crashes while doing isolate/cleanup.,2014-08-06T16:36:39.000+0000,MESOS-1649,3.0,mesos,Q3 Sprint 2
jieyu,2014-07-28T21:41:05.000+0000,jieyu,"I would like to volunteer to be the release manager for 0.20.0, which will be releasing the following major features:

- Docker support in Mesos (MESOS-1524)

- Container level network monitoring for mesos containerizer (MESOS-1228)

- Authorization (MESOS-1342)

- Framework rate limiting (MESOS-1306)

- Enable building against installed third-party dependencies (MESOS-1071)

I would like to track blockers for the release on this ticket.",Task,Major,jieyu,2014-08-27T22:20:19.000+0000,5,Resolved,Complete,0.20.0 Release,2016-02-26T21:08:19.000+0000,MESOS-1645,5.0,mesos,Q3 Sprint 3
vinodkone,2014-07-22T22:24:35.000+0000,hitony,"{noformat}
Could not create logging file: No such file or directory
COULD NOT CREATE A LOGGINGFILE 20140722-205220.31450!F0722 20:52:20.494424 31450 utilities.cc:317] Check failed: !IsGoogleLoggingInitialized() You called InitGoogleLogging() twice!
*** Check failure stack trace: ***
    @           0x4399ce  google::LogMessage::Fail()
    @           0x43991d  google::LogMessage::SendToLog()
    @           0x43932e  google::LogMessage::Flush()
    @           0x43c0e5  google::LogMessageFatal::~LogMessageFatal()
    @           0x44089f  google::glog_internal_namespace_::InitGoogleLoggingUtilities()
    @           0x43c409  google::InitGoogleLogging()
    @     0x7f0bdd43b55c  mesos::internal::logging::initialize()
    @     0x7f0bdcf9564d  mesos::scheduler::MesosProcess::MesosProcess()
    @     0x7f0bdcf92de0  mesos::scheduler::Mesos::Mesos()
    @           0x421483  heron::mesos::Scheduler::Scheduler()
    @           0x4305dc  main
    @     0x7f0bd97159c4  __libc_start_main
    @           0x420869  (unknown)
Aborted
{noformat}",Bug,Major,hitony,2014-08-15T17:43:30.000+0000,5,Resolved,Complete,GLOG Initialized twice if the Framework Scheduler also uses GLOG,2014-08-15T17:43:30.000+0000,MESOS-1629,2.0,mesos,Q3 Sprint 3
vinodkone,2014-07-22T21:30:09.000+0000,vinodkone,"Playing with installed mesos headers, realized that we expect users to include the path to mesos directory (e.g., /usr/local/include/mesos) even though it is on the system path. This is because scheduler.pb.h etc include ""mesos.pb.h"" instead of ""mesos/mesos.pb.h"".",Bug,Major,vinodkone,2014-08-04T22:00:12.000+0000,5,Resolved,Complete,Installed protobuf header files include wrong path to mesos header file,2014-08-04T22:00:12.000+0000,MESOS-1627,2.0,mesos,Q3 Sprint 1
xujyan,2014-07-22T04:54:25.000+0000,xujyan,"The failed build: https://builds.apache.org/job/Mesos-Trunk-Ubuntu-Build-Out-Of-Src-Set-JAVA_HOME/2261/consoleFull
{noformat:title=the log where -lsnappy is used when compiling leveldb}
gzip -d -c ../../3rdparty/leveldb.tar.gz | tar xf -
test ! -e ../../3rdparty/leveldb.patch || patch -d leveldb -p1 <../../3rdparty/leveldb.patch
touch leveldb-stamp
cd leveldb && \
          make  CC=""gcc"" CXX=""g++"" OPT=""-g -g2 -O2 -Wno-unused-local-typedefs -std=c++11 -fPIC""
make[5]: Entering directory `/home/jenkins/jenkins-slave/workspace/Mesos-Trunk-Ubuntu-Build-Out-Of-Src-Set-JAVA_HOME/build/mesos-0.20.0/_build/3rdparty/leveldb'
g++ -pthread -lsnappy -shared -Wl,-soname -Wl,/home/jenkins/jenkins-slave/workspace/Mesos-Trunk-Ubuntu-Build-Out-Of-Src-Set-JAVA_HOME/build/mesos-0.20.0/_build/3rdparty/leveldb/libleveldb.so.1 -I. -I./include -fno-builtin-memcmp -pthread -DOS_LINUX -DLEVELDB_PLATFORM_POSIX -DSNAPPY -g -g2 -O2 -Wno-unused-local-typedefs -std=c++11 -fPIC -fPIC db/builder.cc db/c.cc db/db_impl.cc db/db_iter.cc db/dbformat.cc db/filename.cc db/log_reader.cc db/log_writer.cc db/memtable.cc db/repair.cc db/table_cache.cc db/version_edit.cc db/version_set.cc db/write_batch.cc table/block.cc table/block_builder.cc table/filter_block.cc table/format.cc table/iterator.cc table/merger.cc table/table.cc table/table_builder.cc table/two_level_iterator.cc util/arena.cc util/bloom.cc util/cache.cc util/coding.cc util/comparator.cc util/crc32c.cc util/env.cc util/env_posix.cc util/filter_policy.cc util/hash.cc util/histogram.cc util/logging.cc util/options.cc util/status.cc  port/port_posix.cc -o libleveldb.so.1.4
ln -fs libleveldb.so.1.4 libleveldb.so
ln -fs libleveldb.so.1.4 libleveldb.so.1
g++ -I. -I./include -fno-builtin-memcmp -pthread -DOS_LINUX -DLEVELDB_PLATFORM_POSIX -DSNAPPY -g -g2 -O2 -Wno-unused-local-typedefs -std=c++11 -fPIC -c db/builder.cc -o db/builder.o
{noformat}

{noformat:title=the error}
/bin/bash ../libtool  --tag=CXX   --mode=link g++ -pthread -g -g2 -O2 -Wno-unused-local-typedefs -std=c++11   -o mesos-local local/mesos_local-main.o libmesos.la -lsasl2 -lcurl -lz  -lrt
libtool: link: g++ -pthread -g -g2 -O2 -Wno-unused-local-typedefs -std=c++11 -o .libs/mesos-local local/mesos_local-main.o  ./.libs/libmesos.so -lsasl2 /usr/lib/x86_64-linux-gnu/libcurl.so -lz -lrt -pthread -Wl,-rpath -Wl,/home/jenkins/jenkins-slave/workspace/Mesos-Trunk-Ubuntu-Build-Out-Of-Src-Set-JAVA_HOME/build/mesos-0.20.0/_inst/lib
./.libs/libmesos.so: undefined reference to `snappy::RawCompress(char const*, unsigned long, char*, unsigned long*)'
./.libs/libmesos.so: undefined reference to `snappy::RawUncompress(char const*, unsigned long, char*)'
./.libs/libmesos.so: undefined reference to `snappy::GetUncompressedLength(char const*, unsigned long, unsigned long*)'
./.libs/libmesos.so: undefined reference to `snappy::MaxCompressedLength(unsigned long)'
{noformat}",Bug,Major,xujyan,2014-07-29T18:28:42.000+0000,5,Resolved,Complete,Apache Jenkins build fails due to -lsnappy is set when building leveldb,2014-08-26T16:31:53.000+0000,MESOS-1624,1.0,mesos,Q3 Sprint 2
bmahler,2014-07-21T19:38:38.000+0000,bmahler,"Per Vinod's feedback on https://reviews.apache.org/r/23542/, we do not send back TASK_STAGING for those tasks that are pending in the Master (validation / authorization still in progress).

For both implicit and explicit task reconciliation, the master could reply with TASK_STAGING for these tasks, as this provides additional information to the framework.",Improvement,Major,bmahler,2014-08-13T19:10:02.000+0000,5,Resolved,Complete,Reconciliation does not send back tasks pending validation / authorization.,2014-08-13T19:10:02.000+0000,MESOS-1620,3.0,mesos,Q3 Sprint 2
kaysoky,2014-07-17T23:04:41.000+0000,dhamon,"As a first step toward Optimistic Offers, take the description from the epic and build an implementation design doc that can be shared for comments.

Note: the links to the working group notes and design doc are located in the [JIRA Epic|MESOS-1607].",Documentation,Major,dhamon,2015-10-12T20:05:04.000+0000,5,Resolved,Complete,Create design document for Optimistic Offers,2016-03-17T13:14:31.000+0000,MESOS-1615,8.0,mesos,Mesosphere Sprint 20
,2014-07-16T04:54:40.000+0000,vinodkone,"While investigating stout build setup for making it installable, I came across some discrepancies.

stout tests are included in libprocess's Makefile instead of stout Makefile.

stout's 3rd party dependencies (e.g., picojson) live in libprocess's 3rdparty directory instead of living in stout's (non-existent) 3rd party directory.

It would be nice to fix these issues before making stout installable.",Improvement,Major,vinodkone,,1,Open,New,Cleanup stout build setup,2014-10-24T16:45:34.000+0000,MESOS-1605,3.0,mesos,Q3 Sprint 1
greggomann,2014-07-15T01:01:28.000+0000,vinodkone,"Observed this on Jenkins.

{code}
[ RUN      ] SlaveRecoveryTest/0.ReconcileKillTask
Using temporary directory '/tmp/SlaveRecoveryTest_0_ReconcileKillTask_3zJ6DG'
I0714 15:08:43.915114 27216 leveldb.cpp:176] Opened db in 474.695188ms
I0714 15:08:43.933645 27216 leveldb.cpp:183] Compacted db in 18.068942ms
I0714 15:08:43.934129 27216 leveldb.cpp:198] Created db iterator in 7860ns
I0714 15:08:43.934439 27216 leveldb.cpp:204] Seeked to beginning of db in 2560ns
I0714 15:08:43.934779 27216 leveldb.cpp:273] Iterated through 0 keys in the db in 1400ns
I0714 15:08:43.935098 27216 replica.cpp:741] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned
I0714 15:08:43.936027 27238 recover.cpp:425] Starting replica recovery
I0714 15:08:43.936225 27238 recover.cpp:451] Replica is in EMPTY status
I0714 15:08:43.936867 27238 replica.cpp:638] Replica in EMPTY status received a broadcasted recover request
I0714 15:08:43.937049 27238 recover.cpp:188] Received a recover response from a replica in EMPTY status
I0714 15:08:43.937232 27238 recover.cpp:542] Updating replica status to STARTING
I0714 15:08:43.945600 27235 master.cpp:288] Master 20140714-150843-16842879-55850-27216 (quantal) started on 127.0.1.1:55850
I0714 15:08:43.945643 27235 master.cpp:325] Master only allowing authenticated frameworks to register
I0714 15:08:43.945651 27235 master.cpp:330] Master only allowing authenticated slaves to register
I0714 15:08:43.945658 27235 credentials.hpp:36] Loading credentials for authentication from '/tmp/SlaveRecoveryTest_0_ReconcileKillTask_3zJ6DG/credentials'
I0714 15:08:43.945808 27235 master.cpp:359] Authorization enabled
I0714 15:08:43.946369 27235 hierarchical_allocator_process.hpp:301] Initializing hierarchical allocator process with master : master@127.0.1.1:55850
I0714 15:08:43.946419 27235 master.cpp:122] No whitelist given. Advertising offers for all slaves
I0714 15:08:43.946614 27235 master.cpp:1128] The newly elected leader is master@127.0.1.1:55850 with id 20140714-150843-16842879-55850-27216
I0714 15:08:43.946630 27235 master.cpp:1141] Elected as the leading master!
I0714 15:08:43.946637 27235 master.cpp:959] Recovering from registrar
I0714 15:08:43.946707 27235 registrar.cpp:313] Recovering registrar
I0714 15:08:43.957895 27238 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 20.529301ms
I0714 15:08:43.957978 27238 replica.cpp:320] Persisted replica status to STARTING
I0714 15:08:43.958142 27238 recover.cpp:451] Replica is in STARTING status
I0714 15:08:43.958664 27238 replica.cpp:638] Replica in STARTING status received a broadcasted recover request
I0714 15:08:43.958762 27238 recover.cpp:188] Received a recover response from a replica in STARTING status
I0714 15:08:43.958945 27238 recover.cpp:542] Updating replica status to VOTING
I0714 15:08:43.975685 27238 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 16.646136ms
I0714 15:08:43.976367 27238 replica.cpp:320] Persisted replica status to VOTING
I0714 15:08:43.976824 27241 recover.cpp:556] Successfully joined the Paxos group
I0714 15:08:43.977072 27242 recover.cpp:440] Recover process terminated
I0714 15:08:43.980590 27236 log.cpp:656] Attempting to start the writer
I0714 15:08:43.981385 27236 replica.cpp:474] Replica received implicit promise request with proposal 1
I0714 15:08:43.999141 27236 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 17.705787ms
I0714 15:08:43.999222 27236 replica.cpp:342] Persisted promised to 1
I0714 15:08:44.004451 27240 coordinator.cpp:230] Coordinator attemping to fill missing position
I0714 15:08:44.004914 27240 replica.cpp:375] Replica received explicit promise request for position 0 with proposal 2
I0714 15:08:44.021456 27240 leveldb.cpp:343] Persisting action (8 bytes) to leveldb took 16.499775ms
I0714 15:08:44.021533 27240 replica.cpp:676] Persisted action at 0
I0714 15:08:44.022006 27240 replica.cpp:508] Replica received write request for position 0
I0714 15:08:44.022043 27240 leveldb.cpp:438] Reading position from leveldb took 21376ns
I0714 15:08:44.035969 27240 leveldb.cpp:343] Persisting action (14 bytes) to leveldb took 13.885907ms
I0714 15:08:44.036365 27240 replica.cpp:676] Persisted action at 0
I0714 15:08:44.040156 27238 replica.cpp:655] Replica received learned notice for position 0
I0714 15:08:44.058082 27238 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 17.860707ms
I0714 15:08:44.058161 27238 replica.cpp:676] Persisted action at 0
I0714 15:08:44.058176 27238 replica.cpp:661] Replica learned NOP action at position 0
I0714 15:08:44.058526 27238 log.cpp:672] Writer started with ending position 0
I0714 15:08:44.058872 27238 leveldb.cpp:438] Reading position from leveldb took 25660ns
I0714 15:08:44.060556 27238 registrar.cpp:346] Successfully fetched the registry (0B)
I0714 15:08:44.060845 27238 registrar.cpp:422] Attempting to update the 'registry'
I0714 15:08:44.062304 27238 log.cpp:680] Attempting to append 120 bytes to the log
I0714 15:08:44.062866 27236 coordinator.cpp:340] Coordinator attempting to write APPEND action at position 1
I0714 15:08:44.063154 27236 replica.cpp:508] Replica received write request for position 1
I0714 15:08:44.082813 27236 leveldb.cpp:343] Persisting action (137 bytes) to leveldb took 19.61683ms
I0714 15:08:44.082890 27236 replica.cpp:676] Persisted action at 1
I0714 15:08:44.083256 27236 replica.cpp:655] Replica received learned notice for position 1
I0714 15:08:44.097398 27236 leveldb.cpp:343] Persisting action (139 bytes) to leveldb took 14.104796ms
I0714 15:08:44.097475 27236 replica.cpp:676] Persisted action at 1
I0714 15:08:44.097488 27236 replica.cpp:661] Replica learned APPEND action at position 1
I0714 15:08:44.098569 27236 registrar.cpp:479] Successfully updated 'registry'
I0714 15:08:44.098906 27240 log.cpp:699] Attempting to truncate the log to 1
I0714 15:08:44.099608 27240 coordinator.cpp:340] Coordinator attempting to write TRUNCATE action at position 2
I0714 15:08:44.100005 27240 replica.cpp:508] Replica received write request for position 2
I0714 15:08:44.100566 27236 registrar.cpp:372] Successfully recovered registrar
I0714 15:08:44.101227 27239 master.cpp:986] Recovered 0 slaves from the Registry (84B) ; allowing 10mins for slaves to re-register
I0714 15:08:44.118376 27240 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 18.329495ms
I0714 15:08:44.118455 27240 replica.cpp:676] Persisted action at 2
I0714 15:08:44.122258 27242 replica.cpp:655] Replica received learned notice for position 2
I0714 15:08:44.137336 27242 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 15.023553ms
I0714 15:08:44.137460 27242 leveldb.cpp:401] Deleting ~1 keys from leveldb took 55049ns
I0714 15:08:44.137480 27242 replica.cpp:676] Persisted action at 2
I0714 15:08:44.137492 27242 replica.cpp:661] Replica learned TRUNCATE action at position 2
I0714 15:08:44.143729 27216 containerizer.cpp:124] Using isolation: posix/cpu,posix/mem
I0714 15:08:44.145934 27242 slave.cpp:168] Slave started on 43)@127.0.1.1:55850
I0714 15:08:44.145953 27242 credentials.hpp:84] Loading credential for authentication from '/tmp/SlaveRecoveryTest_0_ReconcileKillTask_Zl9DUt/credential'
I0714 15:08:44.146040 27242 slave.cpp:266] Slave using credential for: test-principal
I0714 15:08:44.146136 27242 slave.cpp:279] Slave resources: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I0714 15:08:44.146198 27242 slave.cpp:324] Slave hostname: quantal
I0714 15:08:44.146209 27242 slave.cpp:325] Slave checkpoint: true
I0714 15:08:44.146708 27242 state.cpp:33] Recovering state from '/tmp/SlaveRecoveryTest_0_ReconcileKillTask_Zl9DUt/meta'
I0714 15:08:44.146824 27242 status_update_manager.cpp:193] Recovering status update manager
I0714 15:08:44.146901 27242 containerizer.cpp:287] Recovering containerizer
I0714 15:08:44.147228 27242 slave.cpp:3126] Finished recovery
I0714 15:08:44.147531 27242 slave.cpp:599] New master detected at master@127.0.1.1:55850
I0714 15:08:44.147562 27242 slave.cpp:675] Authenticating with master master@127.0.1.1:55850
I0714 15:08:44.147614 27242 slave.cpp:648] Detecting new master
I0714 15:08:44.147652 27242 status_update_manager.cpp:167] New master detected at master@127.0.1.1:55850
I0714 15:08:44.147691 27242 authenticatee.hpp:128] Creating new client SASL connection
I0714 15:08:44.148533 27235 master.cpp:3507] Authenticating slave(43)@127.0.1.1:55850
I0714 15:08:44.148666 27235 authenticator.hpp:156] Creating new server SASL connection
I0714 15:08:44.149054 27242 authenticatee.hpp:219] Received SASL authentication mechanisms: CRAM-MD5
I0714 15:08:44.149447 27242 authenticatee.hpp:245] Attempting to authenticate with mechanism 'CRAM-MD5'
I0714 15:08:44.149917 27236 authenticator.hpp:262] Received SASL authentication start
I0714 15:08:44.149974 27236 authenticator.hpp:384] Authentication requires more steps
I0714 15:08:44.150208 27242 authenticatee.hpp:265] Received SASL authentication step
I0714 15:08:44.150720 27239 authenticator.hpp:290] Received SASL authentication step
I0714 15:08:44.150749 27239 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'quantal' server FQDN: 'quantal' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0714 15:08:44.150758 27239 auxprop.cpp:153] Looking up auxiliary property '*userPassword'
I0714 15:08:44.150771 27239 auxprop.cpp:153] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0714 15:08:44.150781 27239 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'quantal' server FQDN: 'quantal' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0714 15:08:44.150787 27239 auxprop.cpp:103] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0714 15:08:44.150792 27239 auxprop.cpp:103] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0714 15:08:44.150804 27239 authenticator.hpp:376] Authentication success
I0714 15:08:44.150848 27239 master.cpp:3547] Successfully authenticated principal 'test-principal' at slave(43)@127.0.1.1:55850
I0714 15:08:44.157696 27242 authenticatee.hpp:305] Authentication success
I0714 15:08:44.158855 27242 slave.cpp:732] Successfully authenticated with master master@127.0.1.1:55850
I0714 15:08:44.158936 27242 slave.cpp:970] Will retry registration in 10.352612ms if necessary
I0714 15:08:44.161813 27216 sched.cpp:139] Version: 0.20.0
I0714 15:08:44.162608 27236 sched.cpp:235] New master detected at master@127.0.1.1:55850
I0714 15:08:44.162637 27236 sched.cpp:285] Authenticating with master master@127.0.1.1:55850
I0714 15:08:44.162747 27236 authenticatee.hpp:128] Creating new client SASL connection
I0714 15:08:44.163506 27239 master.cpp:2789] Registering slave at slave(43)@127.0.1.1:55850 (quantal) with id 20140714-150843-16842879-55850-27216-0
I0714 15:08:44.164086 27238 registrar.cpp:422] Attempting to update the 'registry'
I0714 15:08:44.165694 27238 log.cpp:680] Attempting to append 295 bytes to the log
I0714 15:08:44.166231 27240 coordinator.cpp:340] Coordinator attempting to write APPEND action at position 3
I0714 15:08:44.166517 27240 replica.cpp:508] Replica received write request for position 3
I0714 15:08:44.167199 27239 master.cpp:3507] Authenticating scheduler-225679c4-a9fd-4119-9deb-c7712eba37e1@127.0.1.1:55850
I0714 15:08:44.167867 27241 authenticator.hpp:156] Creating new server SASL connection
I0714 15:08:44.168058 27241 authenticatee.hpp:219] Received SASL authentication mechanisms: CRAM-MD5
I0714 15:08:44.168081 27241 authenticatee.hpp:245] Attempting to authenticate with mechanism 'CRAM-MD5'
I0714 15:08:44.168107 27241 authenticator.hpp:262] Received SASL authentication start
I0714 15:08:44.168149 27241 authenticator.hpp:384] Authentication requires more steps
I0714 15:08:44.168176 27241 authenticatee.hpp:265] Received SASL authentication step
I0714 15:08:44.168215 27241 authenticator.hpp:290] Received SASL authentication step
I0714 15:08:44.168233 27241 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'quantal' server FQDN: 'quantal' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0714 15:08:44.168793 27241 auxprop.cpp:153] Looking up auxiliary property '*userPassword'
I0714 15:08:44.168820 27241 auxprop.cpp:153] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0714 15:08:44.168834 27241 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'quantal' server FQDN: 'quantal' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0714 15:08:44.168840 27241 auxprop.cpp:103] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0714 15:08:44.168845 27241 auxprop.cpp:103] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0714 15:08:44.168858 27241 authenticator.hpp:376] Authentication success
I0714 15:08:44.168895 27241 authenticatee.hpp:305] Authentication success
I0714 15:08:44.168970 27241 sched.cpp:359] Successfully authenticated with master master@127.0.1.1:55850
I0714 15:08:44.168987 27241 sched.cpp:478] Sending registration request to master@127.0.1.1:55850
I0714 15:08:44.169426 27239 master.cpp:1239] Queuing up registration request from scheduler-225679c4-a9fd-4119-9deb-c7712eba37e1@127.0.1.1:55850 because authentication is still in progress
I0714 15:08:44.169958 27239 master.cpp:3547] Successfully authenticated principal 'test-principal' at scheduler-225679c4-a9fd-4119-9deb-c7712eba37e1@127.0.1.1:55850
I0714 15:08:44.170440 27241 slave.cpp:970] Will retry registration in 8.76707ms if necessary
I0714 15:08:44.175359 27239 master.cpp:2777] Ignoring register slave message from slave(43)@127.0.1.1:55850 (quantal) as admission is already in progress
I0714 15:08:44.175916 27239 master.cpp:1247] Received registration request from scheduler-225679c4-a9fd-4119-9deb-c7712eba37e1@127.0.1.1:55850
I0714 15:08:44.176298 27239 master.cpp:1207] Authorizing framework principal 'test-principal' to receive offers for role '*'
I0714 15:08:44.176858 27239 master.cpp:1306] Registering framework 20140714-150843-16842879-55850-27216-0000 at scheduler-225679c4-a9fd-4119-9deb-c7712eba37e1@127.0.1.1:55850
I0714 15:08:44.177408 27236 sched.cpp:409] Framework registered with 20140714-150843-16842879-55850-27216-0000
I0714 15:08:44.177443 27236 sched.cpp:423] Scheduler::registered took 12527ns
I0714 15:08:44.177727 27241 hierarchical_allocator_process.hpp:331] Added framework 20140714-150843-16842879-55850-27216-0000
I0714 15:08:44.177747 27241 hierarchical_allocator_process.hpp:724] No resources available to allocate!
I0714 15:08:44.177753 27241 hierarchical_allocator_process.hpp:686] Performed allocation for 0 slaves in 8120ns
I0714 15:08:44.179908 27241 slave.cpp:970] Will retry registration in 66.781028ms if necessary
I0714 15:08:44.180007 27241 master.cpp:2777] Ignoring register slave message from slave(43)@127.0.1.1:55850 (quantal) as admission is already in progress
I0714 15:08:44.183082 27240 leveldb.cpp:343] Persisting action (314 bytes) to leveldb took 16.533189ms
I0714 15:08:44.183125 27240 replica.cpp:676] Persisted action at 3
I0714 15:08:44.183465 27240 replica.cpp:655] Replica received learned notice for position 3
I0714 15:08:44.203276 27240 leveldb.cpp:343] Persisting action (316 bytes) to leveldb took 19.768951ms
I0714 15:08:44.203376 27240 replica.cpp:676] Persisted action at 3
I0714 15:08:44.203392 27240 replica.cpp:661] Replica learned APPEND action at position 3
I0714 15:08:44.204033 27240 registrar.cpp:479] Successfully updated 'registry'
I0714 15:08:44.204138 27240 log.cpp:699] Attempting to truncate the log to 3
I0714 15:08:44.204221 27240 master.cpp:2829] Registered slave 20140714-150843-16842879-55850-27216-0 at slave(43)@127.0.1.1:55850 (quantal)
I0714 15:08:44.204241 27240 master.cpp:3975] Adding slave 20140714-150843-16842879-55850-27216-0 at slave(43)@127.0.1.1:55850 (quantal) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I0714 15:08:44.204387 27240 coordinator.cpp:340] Coordinator attempting to write TRUNCATE action at position 4
I0714 15:08:44.204489 27240 slave.cpp:766] Registered with master master@127.0.1.1:55850; given slave ID 20140714-150843-16842879-55850-27216-0
I0714 15:08:44.204745 27240 slave.cpp:779] Checkpointing SlaveInfo to '/tmp/SlaveRecoveryTest_0_ReconcileKillTask_Zl9DUt/meta/slaves/20140714-150843-16842879-55850-27216-0/slave.info'
I0714 15:08:44.204954 27240 hierarchical_allocator_process.hpp:444] Added slave 20140714-150843-16842879-55850-27216-0 (quantal) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (and cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] available)
I0714 15:08:44.205023 27240 hierarchical_allocator_process.hpp:750] Offering cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave 20140714-150843-16842879-55850-27216-0 to framework 20140714-150843-16842879-55850-27216-0000
I0714 15:08:44.205122 27240 hierarchical_allocator_process.hpp:706] Performed allocation for slave 20140714-150843-16842879-55850-27216-0 in 131192ns
I0714 15:08:44.205189 27240 slave.cpp:2323] Received ping from slave-observer(32)@127.0.1.1:55850
I0714 15:08:44.205258 27240 master.hpp:801] Adding offer 20140714-150843-16842879-55850-27216-0 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave 20140714-150843-16842879-55850-27216-0 (quantal)
I0714 15:08:44.205303 27240 master.cpp:3454] Sending 1 offers to framework 20140714-150843-16842879-55850-27216-0000
I0714 15:08:44.205469 27240 sched.cpp:546] Scheduler::resourceOffers took 23591ns
I0714 15:08:44.206351 27241 replica.cpp:508] Replica received write request for position 4
I0714 15:08:44.208353 27237 master.hpp:811] Removing offer 20140714-150843-16842879-55850-27216-0 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave 20140714-150843-16842879-55850-27216-0 (quantal)
I0714 15:08:44.208436 27237 master.cpp:2133] Processing reply for offers: [ 20140714-150843-16842879-55850-27216-0 ] on slave 20140714-150843-16842879-55850-27216-0 at slave(43)@127.0.1.1:55850 (quantal) for framework 20140714-150843-16842879-55850-27216-0000
I0714 15:08:44.208472 27237 master.cpp:2219] Authorizing framework principal 'test-principal' to launch task 4a6783aa-8d07-46e3-8399-2a5d047f0021 as user 'jenkins'
I0714 15:08:44.208909 27237 master.hpp:773] Adding task 4a6783aa-8d07-46e3-8399-2a5d047f0021 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave 20140714-150843-16842879-55850-27216-0 (quantal)
I0714 15:08:44.208947 27237 master.cpp:2285] Launching task 4a6783aa-8d07-46e3-8399-2a5d047f0021 of framework 20140714-150843-16842879-55850-27216-0000 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave 20140714-150843-16842879-55850-27216-0 at slave(43)@127.0.1.1:55850 (quantal)
I0714 15:08:44.209090 27237 slave.cpp:1001] Got assigned task 4a6783aa-8d07-46e3-8399-2a5d047f0021 for framework 20140714-150843-16842879-55850-27216-0000
I0714 15:08:44.209190 27237 slave.cpp:3398] Checkpointing FrameworkInfo to '/tmp/SlaveRecoveryTest_0_ReconcileKillTask_Zl9DUt/meta/slaves/20140714-150843-16842879-55850-27216-0/frameworks/20140714-150843-16842879-55850-27216-0000/framework.info'
I0714 15:08:44.209413 27237 slave.cpp:3405] Checkpointing framework pid 'scheduler-225679c4-a9fd-4119-9deb-c7712eba37e1@127.0.1.1:55850' to '/tmp/SlaveRecoveryTest_0_ReconcileKillTask_Zl9DUt/meta/slaves/20140714-150843-16842879-55850-27216-0/frameworks/20140714-150843-16842879-55850-27216-0000/framework.pid'
I0714 15:08:44.209710 27237 slave.cpp:1111] Launching task 4a6783aa-8d07-46e3-8399-2a5d047f0021 for framework 20140714-150843-16842879-55850-27216-0000
I0714 15:08:44.210978 27237 slave.cpp:3720] Checkpointing ExecutorInfo to '/tmp/SlaveRecoveryTest_0_ReconcileKillTask_Zl9DUt/meta/slaves/20140714-150843-16842879-55850-27216-0/frameworks/20140714-150843-16842879-55850-27216-0000/executors/4a6783aa-8d07-46e3-8399-2a5d047f0021/executor.info'
I0714 15:08:44.211520 27237 slave.cpp:3835] Checkpointing TaskInfo to '/tmp/SlaveRecoveryTest_0_ReconcileKillTask_Zl9DUt/meta/slaves/20140714-150843-16842879-55850-27216-0/frameworks/20140714-150843-16842879-55850-27216-0000/executors/4a6783aa-8d07-46e3-8399-2a5d047f0021/runs/19c466f8-bb5a-4842-a152-f585ff88762a/tasks/4a6783aa-8d07-46e3-8399-2a5d047f0021/task.info'
I0714 15:08:44.211714 27237 slave.cpp:1221] Queuing task '4a6783aa-8d07-46e3-8399-2a5d047f0021' for executor 4a6783aa-8d07-46e3-8399-2a5d047f0021 of framework '20140714-150843-16842879-55850-27216-0000
I0714 15:08:44.211937 27236 containerizer.cpp:427] Starting container '19c466f8-bb5a-4842-a152-f585ff88762a' for executor '4a6783aa-8d07-46e3-8399-2a5d047f0021' of framework '20140714-150843-16842879-55850-27216-0000'
I0714 15:08:44.212242 27236 slave.cpp:560] Successfully attached file '/tmp/SlaveRecoveryTest_0_ReconcileKillTask_Zl9DUt/slaves/20140714-150843-16842879-55850-27216-0/frameworks/20140714-150843-16842879-55850-27216-0000/executors/4a6783aa-8d07-46e3-8399-2a5d047f0021/runs/19c466f8-bb5a-4842-a152-f585ff88762a'
I0714 15:08:44.216187 27236 launcher.cpp:137] Forked child with pid '28451' for container '19c466f8-bb5a-4842-a152-f585ff88762a'
I0714 15:08:44.217281 27236 containerizer.cpp:705] Checkpointing executor's forked pid 28451 to '/tmp/SlaveRecoveryTest_0_ReconcileKillTask_Zl9DUt/meta/slaves/20140714-150843-16842879-55850-27216-0/frameworks/20140714-150843-16842879-55850-27216-0000/executors/4a6783aa-8d07-46e3-8399-2a5d047f0021/runs/19c466f8-bb5a-4842-a152-f585ff88762a/pids/forked.pid'
I0714 15:08:44.219408 27236 containerizer.cpp:537] Fetching URIs for container '19c466f8-bb5a-4842-a152-f585ff88762a' using command '/var/jenkins/workspace/mesos-ubuntu-12.10-gcc/src/mesos-fetcher'
I0714 15:08:44.223963 27241 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 17.554461ms
I0714 15:08:44.224501 27241 replica.cpp:676] Persisted action at 4
I0714 15:08:44.225051 27241 replica.cpp:655] Replica received learned notice for position 4
I0714 15:08:44.242923 27241 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 17.806547ms
I0714 15:08:44.243057 27241 leveldb.cpp:401] Deleting ~2 keys from leveldb took 57154ns
I0714 15:08:44.243078 27241 replica.cpp:676] Persisted action at 4
I0714 15:08:44.243096 27241 replica.cpp:661] Replica learned TRUNCATE action at position 4
I0714 15:08:44.401140 27241 slave.cpp:2468] Monitoring executor '4a6783aa-8d07-46e3-8399-2a5d047f0021' of framework '20140714-150843-16842879-55850-27216-0000' in container '19c466f8-bb5a-4842-a152-f585ff88762a'
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0714 15:08:44.434221 28486 process.cpp:1671] libprocess is initialized on 127.0.1.1:34669 for 8 cpus
I0714 15:08:44.436146 28486 exec.cpp:131] Version: 0.20.0
I0714 15:08:44.438555 28500 exec.cpp:181] Executor started at: executor(1)@127.0.1.1:34669 with pid 28486
I0714 15:08:44.440846 27241 slave.cpp:1732] Got registration for executor '4a6783aa-8d07-46e3-8399-2a5d047f0021' of framework 20140714-150843-16842879-55850-27216-0000
I0714 15:08:44.440917 27241 slave.cpp:1817] Checkpointing executor pid 'executor(1)@127.0.1.1:34669' to '/tmp/SlaveRecoveryTest_0_ReconcileKillTask_Zl9DUt/meta/slaves/20140714-150843-16842879-55850-27216-0/frameworks/20140714-150843-16842879-55850-27216-0000/executors/4a6783aa-8d07-46e3-8399-2a5d047f0021/runs/19c466f8-bb5a-4842-a152-f585ff88762a/pids/libprocess.pid'
I0714 15:08:44.442373 27243 process.cpp:1098] Socket closed while receiving
I0714 15:08:44.442790 27241 slave.cpp:1851] Flushing queued task 4a6783aa-8d07-46e3-8399-2a5d047f0021 for executor '4a6783aa-8d07-46e3-8399-2a5d047f0021' of framework 20140714-150843-16842879-55850-27216-0000
I0714 15:08:44.443192 27243 process.cpp:1098] Socket closed while receiving
I0714 15:08:44.443994 28508 process.cpp:1037] Socket closed while receiving
I0714 15:08:44.444144 28508 process.cpp:1037] Socket closed while receiving
I0714 15:08:44.444741 28500 exec.cpp:205] Executor registered on slave 20140714-150843-16842879-55850-27216-0
Registered executor on quantal
I0714 15:08:44.446338 28500 exec.cpp:217] Executor::registered took 534236ns
I0714 15:08:44.446715 28500 exec.cpp:292] Executor asked to run task '4a6783aa-8d07-46e3-8399-2a5d047f0021'
Starting task 4a6783aa-8d07-46e3-8399-2a5d047f0021
I0714 15:08:44.447548 28500 exec.cpp:301] Executor::launchTask took 584306ns
sh -c 'sleep 1000'
Forked command at 28509
I0714 15:08:44.451202 28506 exec.cpp:524] Executor sending status update TASK_RUNNING (UUID: 323fc20a-b5b8-475d-8752-b1f853797f55) for task 4a6783aa-8d07-46e3-8399-2a5d047f0021 of framework 20140714-150843-16842879-55850-27216-0000
I0714 15:08:44.452327 27239 slave.cpp:2086] Handling status update TASK_RUNNING (UUID: 323fc20a-b5b8-475d-8752-b1f853797f55) for task 4a6783aa-8d07-46e3-8399-2a5d047f0021 of framework 20140714-150843-16842879-55850-27216-0000 from executor(1)@127.0.1.1:34669
I0714 15:08:44.452503 27239 status_update_manager.cpp:320] Received status update TASK_RUNNING (UUID: 323fc20a-b5b8-475d-8752-b1f853797f55) for task 4a6783aa-8d07-46e3-8399-2a5d047f0021 of framework 20140714-150843-16842879-55850-27216-0000
I0714 15:08:44.452520 27239 status_update_manager.cpp:499] Creating StatusUpdate stream for task 4a6783aa-8d07-46e3-8399-2a5d047f0021 of framework 20140714-150843-16842879-55850-27216-0000
I0714 15:08:44.452775 27239 status_update_manager.hpp:342] Checkpointing UPDATE for status update TASK_RUNNING (UUID: 323fc20a-b5b8-475d-8752-b1f853797f55) for task 4a6783aa-8d07-46e3-8399-2a5d047f0021 of framework 20140714-150843-16842879-55850-27216-0000
I0714 15:08:44.472384 27239 status_update_manager.cpp:373] Forwarding status update TASK_RUNNING (UUID: 323fc20a-b5b8-475d-8752-b1f853797f55) for task 4a6783aa-8d07-46e3-8399-2a5d047f0021 of framework 20140714-150843-16842879-55850-27216-0000 to master@127.0.1.1:55850
I0714 15:08:44.472764 27237 master.cpp:3115] Status update TASK_RUNNING (UUID: 323fc20a-b5b8-475d-8752-b1f853797f55) for task 4a6783aa-8d07-46e3-8399-2a5d047f0021 of framework 20140714-150843-16842879-55850-27216-0000 from slave 20140714-150843-16842879-55850-27216-0 at slave(43)@127.0.1.1:55850 (quantal)
I0714 15:08:44.472854 27237 sched.cpp:637] Scheduler::statusUpdate took 17656ns
I0714 15:08:44.472920 27237 master.cpp:2639] Forwarding status update acknowledgement 323fc20a-b5b8-475d-8752-b1f853797f55 for task 4a6783aa-8d07-46e3-8399-2a5d047f0021 of framework 20140714-150843-16842879-55850-27216-0000 to slave 20140714-150843-16842879-55850-27216-0 at slave(43)@127.0.1.1:55850 (quantal)
I0714 15:08:44.473122 27239 status_update_manager.cpp:398] Received status update acknowledgement (UUID: 323fc20a-b5b8-475d-8752-b1f853797f55) for task 4a6783aa-8d07-46e3-8399-2a5d047f0021 of framework 20140714-150843-16842879-55850-27216-0000
I0714 15:08:44.473146 27239 status_update_manager.hpp:342] Checkpointing ACK for status update TASK_RUNNING (UUID: 323fc20a-b5b8-475d-8752-b1f853797f55) for task 4a6783aa-8d07-46e3-8399-2a5d047f0021 of framework 20140714-150843-16842879-55850-27216-0000
I0714 15:08:44.473244 27237 slave.cpp:2244] Status update manager successfully handled status update TASK_RUNNING (UUID: 323fc20a-b5b8-475d-8752-b1f853797f55) for task 4a6783aa-8d07-46e3-8399-2a5d047f0021 of framework 20140714-150843-16842879-55850-27216-0000
I0714 15:08:44.473258 27237 slave.cpp:2250] Sending acknowledgement for status update TASK_RUNNING (UUID: 323fc20a-b5b8-475d-8752-b1f853797f55) for task 4a6783aa-8d07-46e3-8399-2a5d047f0021 of framework 20140714-150843-16842879-55850-27216-0000 to executor(1)@127.0.1.1:34669
I0714 15:08:44.473567 27243 process.cpp:1098] Socket closed while receiving
I0714 15:08:44.474095 28508 process.cpp:1037] Socket closed while receiving
I0714 15:08:44.474676 28502 exec.cpp:338] Executor received status update acknowledgement 323fc20a-b5b8-475d-8752-b1f853797f55 for task 4a6783aa-8d07-46e3-8399-2a5d047f0021 of framework 20140714-150843-16842879-55850-27216-0000
I0714 15:08:44.491111 27239 slave.cpp:1672] Status update manager successfully handled status update acknowledgement (UUID: 323fc20a-b5b8-475d-8752-b1f853797f55) for task 4a6783aa-8d07-46e3-8399-2a5d047f0021 of framework 20140714-150843-16842879-55850-27216-0000
I0714 15:08:44.491761 27216 slave.cpp:484] Slave terminating
I0714 15:08:44.492559 27216 containerizer.cpp:124] Using isolation: posix/cpu,posix/mem
I0714 15:08:44.494635 27237 master.cpp:766] Slave 20140714-150843-16842879-55850-27216-0 at slave(43)@127.0.1.1:55850 (quantal) disconnected
I0714 15:08:44.494663 27237 master.cpp:1608] Disconnecting slave 20140714-150843-16842879-55850-27216-0
I0714 15:08:44.495120 27237 slave.cpp:168] Slave started on 44)@127.0.1.1:55850
I0714 15:08:44.495133 27237 credentials.hpp:84] Loading credential for authentication from '/tmp/SlaveRecoveryTest_0_ReconcileKillTask_Zl9DUt/credential'
I0714 15:08:44.495226 27237 slave.cpp:266] Slave using credential for: test-principal
I0714 15:08:44.495322 27237 slave.cpp:279] Slave resources: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I0714 15:08:44.495407 27237 slave.cpp:324] Slave hostname: quantal
I0714 15:08:44.495419 27237 slave.cpp:325] Slave checkpoint: true
I0714 15:08:44.495939 27242 master.cpp:2469] Asked to kill task 4a6783aa-8d07-46e3-8399-2a5d047f0021 of framework 20140714-150843-16842879-55850-27216-0000
I0714 15:08:44.496207 27238 state.cpp:33] Recovering state from '/tmp/SlaveRecoveryTest_0_ReconcileKillTask_Zl9DUt/meta'
I0714 15:08:44.498291 27240 slave.cpp:3194] Recovering framework 20140714-150843-16842879-55850-27216-0000
I0714 15:08:44.498325 27240 slave.cpp:3570] Recovering executor '4a6783aa-8d07-46e3-8399-2a5d047f0021' of framework 20140714-150843-16842879-55850-27216-0000
I0714 15:08:44.498940 27240 status_update_manager.cpp:193] Recovering status update manager
I0714 15:08:44.498956 27240 status_update_manager.cpp:201] Recovering executor '4a6783aa-8d07-46e3-8399-2a5d047f0021' of framework 20140714-150843-16842879-55850-27216-0000
I0714 15:08:44.498975 27240 status_update_manager.cpp:499] Creating StatusUpdate stream for task 4a6783aa-8d07-46e3-8399-2a5d047f0021 of framework 20140714-150843-16842879-55850-27216-0000
I0714 15:08:44.499092 27240 status_update_manager.hpp:306] Replaying status update stream for task 4a6783aa-8d07-46e3-8399-2a5d047f0021
I0714 15:08:44.499241 27240 slave.cpp:560] Successfully attached file '/tmp/SlaveRecoveryTest_0_ReconcileKillTask_Zl9DUt/slaves/20140714-150843-16842879-55850-27216-0/frameworks/20140714-150843-16842879-55850-27216-0000/executors/4a6783aa-8d07-46e3-8399-2a5d047f0021/runs/19c466f8-bb5a-4842-a152-f585ff88762a'
I0714 15:08:44.499433 27240 containerizer.cpp:287] Recovering containerizer
I0714 15:08:44.499457 27240 containerizer.cpp:329] Recovering container '19c466f8-bb5a-4842-a152-f585ff88762a' for executor '4a6783aa-8d07-46e3-8399-2a5d047f0021' of framework 20140714-150843-16842879-55850-27216-0000
I0714 15:08:44.495811 27237 hierarchical_allocator_process.hpp:483] Slave 20140714-150843-16842879-55850-27216-0 disconnected
I0714 15:08:44.501255 27240 slave.cpp:3067] Sending reconnect request to executor 4a6783aa-8d07-46e3-8399-2a5d047f0021 of framework 20140714-150843-16842879-55850-27216-0000 at executor(1)@127.0.1.1:34669
I0714 15:08:44.502030 28501 exec.cpp:251] Received reconnect request from slave 20140714-150843-16842879-55850-27216-0
I0714 15:08:44.502627 27243 process.cpp:1098] Socket closed while receiving
I0714 15:08:44.502681 28508 process.cpp:1037] Socket closed while receiving
I0714 15:08:44.503211 27240 slave.cpp:1911] Re-registering executor 4a6783aa-8d07-46e3-8399-2a5d047f0021 of framework 20140714-150843-16842879-55850-27216-0000
I0714 15:08:44.504238 28501 exec.cpp:228] Executor re-registered on slave 20140714-150843-16842879-55850-27216-0
I0714 15:08:44.505033 28501 exec.cpp:240] Executor::reregistered took 45053ns
Re-registered executor on quantal
I0714 15:08:44.505507 27243 process.cpp:1098] Socket closed while receiving
I0714 15:08:44.505558 28508 process.cpp:1037] Socket closed while receiving
I0714 15:08:44.948043 27241 hierarchical_allocator_process.hpp:686] Performed allocation for 1 slaves in 124255ns
I0714 15:08:45.948671 27237 hierarchical_allocator_process.hpp:686] Performed allocation for 1 slaves in 61521ns
I0714 15:08:46.503978 27238 slave.cpp:2035] Cleaning up un-reregistered executors
I0714 15:08:46.504050 27238 slave.cpp:3126] Finished recovery
I0714 15:08:46.504590 27238 slave.cpp:599] New master detected at master@127.0.1.1:55850
I0714 15:08:46.504639 27238 slave.cpp:675] Authenticating with master master@127.0.1.1:55850
I0714 15:08:46.504729 27238 slave.cpp:648] Detecting new master
I0714 15:08:46.504772 27238 status_update_manager.cpp:167] New master detected at master@127.0.1.1:55850
I0714 15:08:46.504863 27238 authenticatee.hpp:128] Creating new client SASL connection
I0714 15:08:46.505091 27238 master.cpp:3507] Authenticating slave(44)@127.0.1.1:55850
I0714 15:08:46.505239 27238 authenticator.hpp:156] Creating new server SASL connection
I0714 15:08:46.505363 27238 authenticatee.hpp:219] Received SASL authentication mechanisms: CRAM-MD5
I0714 15:08:46.505393 27238 authenticatee.hpp:245] Attempting to authenticate with mechanism 'CRAM-MD5'
I0714 15:08:46.505420 27238 authenticator.hpp:262] Received SASL authentication start
I0714 15:08:46.505476 27238 authenticator.hpp:384] Authentication requires more steps
I0714 15:08:46.505506 27238 authenticatee.hpp:265] Received SASL authentication step
I0714 15:08:46.505542 27238 authenticator.hpp:290] Received SASL authentication step
I0714 15:08:46.505558 27238 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'quantal' server FQDN: 'quantal' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0714 15:08:46.505566 27238 auxprop.cpp:153] Looking up auxiliary property '*userPassword'
I0714 15:08:46.505584 27238 auxprop.cpp:153] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0714 15:08:46.505595 27238 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'quantal' server FQDN: 'quantal' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0714 15:08:46.505601 27238 auxprop.cpp:103] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0714 15:08:46.505606 27238 auxprop.cpp:103] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0714 15:08:46.505616 27238 authenticator.hpp:376] Authentication success
I0714 15:08:46.505646 27238 authenticatee.hpp:305] Authentication success
I0714 15:08:46.505671 27238 master.cpp:3547] Successfully authenticated principal 'test-principal' at slave(44)@127.0.1.1:55850
I0714 15:08:46.505769 27238 slave.cpp:732] Successfully authenticated with master master@127.0.1.1:55850
I0714 15:08:46.505873 27238 slave.cpp:970] Will retry registration in 17.903094ms if necessary
W0714 15:08:46.505991 27238 master.cpp:2904] Slave at slave(44)@127.0.1.1:55850 (quantal) is being allowed to re-register with an already in use id (20140714-150843-16842879-55850-27216-0)
W0714 15:08:46.506063 27238 master.cpp:3679]  Slave 20140714-150843-16842879-55850-27216-0 at slave(44)@127.0.1.1:55850 (quantal) has non-terminal task 4a6783aa-8d07-46e3-8399-2a5d047f0021 that is supposed to be killed. Killing it now!
I0714 15:08:46.506150 27238 slave.cpp:816] Re-registered with master master@127.0.1.1:55850
I0714 15:08:46.506186 27238 slave.cpp:1277] Asked to kill task 4a6783aa-8d07-46e3-8399-2a5d047f0021 of framework 20140714-150843-16842879-55850-27216-0000
I0714 15:08:46.507275 27241 hierarchical_allocator_process.hpp:497] Slave 20140714-150843-16842879-55850-27216-0 reconnected
I0714 15:08:46.508061 28504 exec.cpp:312] Executor asked to kill task '4a6783aa-8d07-46e3-8399-2a5d047f0021'
I0714 15:08:46.508117 28504 exec.cpp:321] Executor::killTask took 24954ns
Shutting down
Sending SIGTERM to process tree at pid 28509
I0714 15:08:46.512238 27243 process.cpp:1098] Socket closed while receiving
I0714 15:08:46.512508 27238 slave.cpp:1582] Updating framework 20140714-150843-16842879-55850-27216-0000 pid to scheduler-225679c4-a9fd-4119-9deb-c7712eba37e1@127.0.1.1:55850
I0714 15:08:46.512846 27238 slave.cpp:1590] Checkpointing framework pid 'scheduler-225679c4-a9fd-4119-9deb-c7712eba37e1@127.0.1.1:55850' to '/tmp/SlaveRecoveryTest_0_ReconcileKillTask_Zl9DUt/meta/slaves/20140714-150843-16842879-55850-27216-0/frameworks/20140714-150843-16842879-55850-27216-0000/framework.pid'
I0714 15:08:46.513419 28508 process.cpp:1037] Socket closed while receiving
Killing the following process trees:
[ 
-+- 28509 sh -c sleep 1000 
 \--- 28510 sleep 1000 
]
Command terminated with signal Terminated (pid: 28509)
I0714 15:08:46.940232 28506 exec.cpp:524] Executor sending status update TASK_KILLED (UUID: e3a5f8fd-eefc-42c6-94a7-086c93c01968) for task 4a6783aa-8d07-46e3-8399-2a5d047f0021 of framework 20140714-150843-16842879-55850-27216-0000
I0714 15:08:46.940918 27240 slave.cpp:2086] Handling status update TASK_KILLED (UUID: e3a5f8fd-eefc-42c6-94a7-086c93c01968) for task 4a6783aa-8d07-46e3-8399-2a5d047f0021 of framework 20140714-150843-16842879-55850-27216-0000 from executor(1)@127.0.1.1:34669
I0714 15:08:46.940979 27240 slave.cpp:3768] Terminating task 4a6783aa-8d07-46e3-8399-2a5d047f0021
I0714 15:08:46.941603 27240 status_update_manager.cpp:320] Received status update TASK_KILLED (UUID: e3a5f8fd-eefc-42c6-94a7-086c93c01968) for task 4a6783aa-8d07-46e3-8399-2a5d047f0021 of framework 20140714-150843-16842879-55850-27216-0000
I0714 15:08:46.941644 27240 status_update_manager.hpp:342] Checkpointing UPDATE for status update TASK_KILLED (UUID: e3a5f8fd-eefc-42c6-94a7-086c93c01968) for task 4a6783aa-8d07-46e3-8399-2a5d047f0021 of framework 20140714-150843-16842879-55850-27216-0000
I0714 15:08:46.949417 27236 hierarchical_allocator_process.hpp:686] Performed allocation for 1 slaves in 63926ns
I0714 15:08:46.965200 27240 status_update_manager.cpp:373] Forwarding status update TASK_KILLED (UUID: e3a5f8fd-eefc-42c6-94a7-086c93c01968) for task 4a6783aa-8d07-46e3-8399-2a5d047f0021 of framework 20140714-150843-16842879-55850-27216-0000 to master@127.0.1.1:55850
I0714 15:08:46.965625 27239 master.cpp:3115] Status update TASK_KILLED (UUID: e3a5f8fd-eefc-42c6-94a7-086c93c01968) for task 4a6783aa-8d07-46e3-8399-2a5d047f0021 of framework 20140714-150843-16842879-55850-27216-0000 from slave 20140714-150843-16842879-55850-27216-0 at slave(44)@127.0.1.1:55850 (quantal)
I0714 15:08:46.965724 27239 master.hpp:791] Removing task 4a6783aa-8d07-46e3-8399-2a5d047f0021 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave 20140714-150843-16842879-55850-27216-0 (quantal)
I0714 15:08:46.965903 27239 sched.cpp:637] Scheduler::statusUpdate took 39326ns
I0714 15:08:46.966022 27239 hierarchical_allocator_process.hpp:635] Recovered cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (total allocatable: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]) on slave 20140714-150843-16842879-55850-27216-0 from framework 20140714-150843-16842879-55850-27216-0000
I0714 15:08:46.966120 27239 master.cpp:2639] Forwarding status update acknowledgement e3a5f8fd-eefc-42c6-94a7-086c93c01968 for task 4a6783aa-8d07-46e3-8399-2a5d047f0021 of framework 20140714-150843-16842879-55850-27216-0000 to slave 20140714-150843-16842879-55850-27216-0 at slave(44)@127.0.1.1:55850 (quantal)
I0714 15:08:46.966501 27241 slave.cpp:2244] Status update manager successfully handled status update TASK_KILLED (UUID: e3a5f8fd-eefc-42c6-94a7-086c93c01968) for task 4a6783aa-8d07-46e3-8399-2a5d047f0021 of framework 20140714-150843-16842879-55850-27216-0000
I0714 15:08:46.966519 27241 slave.cpp:2250] Sending acknowledgement for status update TASK_KILLED (UUID: e3a5f8fd-eefc-42c6-94a7-086c93c01968) for task 4a6783aa-8d07-46e3-8399-2a5d047f0021 of framework 20140714-150843-16842879-55850-27216-0000 to executor(1)@127.0.1.1:34669
I0714 15:08:46.966754 27240 status_update_manager.cpp:398] Received status update acknowledgement (UUID: e3a5f8fd-eefc-42c6-94a7-086c93c01968) for task 4a6783aa-8d07-46e3-8399-2a5d047f0021 of framework 20140714-150843-16842879-55850-27216-0000
I0714 15:08:46.966785 27240 status_update_manager.hpp:342] Checkpointing ACK for status update TASK_KILLED (UUID: e3a5f8fd-eefc-42c6-94a7-086c93c01968) for task 4a6783aa-8d07-46e3-8399-2a5d047f0021 of framework 20140714-150843-16842879-55850-27216-0000
I0714 15:08:46.967386 28500 exec.cpp:338] Executor received status update acknowledgement e3a5f8fd-eefc-42c6-94a7-086c93c01968 for task 4a6783aa-8d07-46e3-8399-2a5d047f0021 of framework 20140714-150843-16842879-55850-27216-0000
I0714 15:08:46.967562 27243 process.cpp:1098] Socket closed while receiving
I0714 15:08:46.968147 28508 process.cpp:1037] Socket closed while receiving
I0714 15:08:46.984608 27240 status_update_manager.cpp:530] Cleaning up status update stream for task 4a6783aa-8d07-46e3-8399-2a5d047f0021 of framework 20140714-150843-16842879-55850-27216-0000
I0714 15:08:46.985239 27236 slave.cpp:1672] Status update manager successfully handled status update acknowledgement (UUID: e3a5f8fd-eefc-42c6-94a7-086c93c01968) for task 4a6783aa-8d07-46e3-8399-2a5d047f0021 of framework 20140714-150843-16842879-55850-27216-0000
I0714 15:08:46.985280 27236 slave.cpp:3810] Completing task 4a6783aa-8d07-46e3-8399-2a5d047f0021
I0714 15:08:47.940703 27243 process.cpp:1037] Socket closed while receiving
I0714 15:08:47.940984 27238 containerizer.cpp:1019] Executor for container '19c466f8-bb5a-4842-a152-f585ff88762a' has exited
I0714 15:08:47.941007 27238 containerizer.cpp:903] Destroying container '19c466f8-bb5a-4842-a152-f585ff88762a'
I0714 15:08:47.950192 27241 hierarchical_allocator_process.hpp:750] Offering cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave 20140714-150843-16842879-55850-27216-0 to framework 20140714-150843-16842879-55850-27216-0000
I0714 15:08:47.950405 27241 hierarchical_allocator_process.hpp:686] Performed allocation for 1 slaves in 320604ns
I0714 15:08:47.950518 27241 master.hpp:801] Adding offer 20140714-150843-16842879-55850-27216-1 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave 20140714-150843-16842879-55850-27216-0 (quantal)
I0714 15:08:47.950572 27241 master.cpp:3454] Sending 1 offers to framework 20140714-150843-16842879-55850-27216-0000
I0714 15:08:47.950774 27241 sched.cpp:546] Scheduler::resourceOffers took 37944ns
I0714 15:08:47.951179 27216 master.cpp:625] Master terminating
I0714 15:08:47.951263 27216 master.hpp:811] Removing offer 20140714-150843-16842879-55850-27216-1 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave 20140714-150843-16842879-55850-27216-0 (quantal)
I0714 15:08:47.953447 27242 sched.cpp:747] Stopping framework '20140714-150843-16842879-55850-27216-0000'
I0714 15:08:47.953547 27242 slave.cpp:2330] master@127.0.1.1:55850 exited
W0714 15:08:47.953567 27242 slave.cpp:2333] Master disconnected! Waiting for a new master to be elected
I0714 15:08:47.964512 27238 slave.cpp:2526] Executor '4a6783aa-8d07-46e3-8399-2a5d047f0021' of framework 20140714-150843-16842879-55850-27216-0000 exited with status 0
I0714 15:08:47.968690 27238 slave.cpp:2660] Cleaning up executor '4a6783aa-8d07-46e3-8399-2a5d047f0021' of framework 20140714-150843-16842879-55850-27216-0000
I0714 15:08:47.969348 27236 gc.cpp:56] Scheduling '/tmp/SlaveRecoveryTest_0_ReconcileKillTask_Zl9DUt/slaves/20140714-150843-16842879-55850-27216-0/frameworks/20140714-150843-16842879-55850-27216-0000/executors/4a6783aa-8d07-46e3-8399-2a5d047f0021/runs/19c466f8-bb5a-4842-a152-f585ff88762a' for gc 6.99998878298667days in the future
I0714 15:08:47.969751 27241 gc.cpp:56] Scheduling '/tmp/SlaveRecoveryTest_0_ReconcileKillTask_Zl9DUt/slaves/20140714-150843-16842879-55850-27216-0/frameworks/20140714-150843-16842879-55850-27216-0000/executors/4a6783aa-8d07-46e3-8399-2a5d047f0021' for gc 6.99998877682963days in the future
I0714 15:08:47.970082 27239 gc.cpp:56] Scheduling '/tmp/SlaveRecoveryTest_0_ReconcileKillTask_Zl9DUt/meta/slaves/20140714-150843-16842879-55850-27216-0/frameworks/20140714-150843-16842879-55850-27216-0000/executors/4a6783aa-8d07-46e3-8399-2a5d047f0021/runs/19c466f8-bb5a-4842-a152-f585ff88762a' for gc 6.99998877336889days in the future
I0714 15:08:47.970379 27242 gc.cpp:56] Scheduling '/tmp/SlaveRecoveryTest_0_ReconcileKillTask_Zl9DUt/meta/slaves/20140714-150843-16842879-55850-27216-0/frameworks/20140714-150843-16842879-55850-27216-0000/executors/4a6783aa-8d07-46e3-8399-2a5d047f0021' for gc 6.99998876968889days in the future
I0714 15:08:47.970587 27238 slave.cpp:2735] Cleaning up framework 20140714-150843-16842879-55850-27216-0000
I0714 15:08:47.970960 27237 status_update_manager.cpp:282] Closing status update streams for framework 20140714-150843-16842879-55850-27216-0000
I0714 15:08:47.971225 27236 gc.cpp:56] Scheduling '/tmp/SlaveRecoveryTest_0_ReconcileKillTask_Zl9DUt/slaves/20140714-150843-16842879-55850-27216-0/frameworks/20140714-150843-16842879-55850-27216-0000' for gc 6.99998875966519days in the future
I0714 15:08:47.971549 27241 gc.cpp:56] Scheduling '/tmp/SlaveRecoveryTest_0_ReconcileKillTask_Zl9DUt/meta/slaves/20140714-150843-16842879-55850-27216-0/frameworks/20140714-150843-16842879-55850-27216-0000' for gc 6.99998875612148days in the future
W0714 15:08:47.975971 27235 containerizer.cpp:893] Ignoring destroy of unknown container: 19c466f8-bb5a-4842-a152-f585ff88762a
./tests/cluster.hpp:530: Failure
(wait).failure(): Unknown container: 19c466f8-bb5a-4842-a152-f585ff88762a
#
# A fatal error has been detected by the Java Runtime Environment:
#
#  SIGSEGV (0xb) at pc=0x00000000005a0299, pid=27216, tid=47907931709760
#
# JRE version: OpenJDK Runtime Environment (7.0_55-b14) (build 1.7.0_55-b14)
# Java VM: OpenJDK 64-Bit Server VM (24.51-b03 mixed mode linux-amd64 compressed oops)
# Problematic frame:
# C  [lt-mesos-tests+0x1a0299]  mlock@@GLIBC_2.2.5+0x1a0299
#
# Failed to write core dump. Core dumps have been disabled. To enable core dumping, try ""ulimit -c unlimited"" before starting Java again
#
# An error report file with more information is saved as:
# /var/jenkins/workspace/mesos-ubuntu-12.10-gcc/src/hs_err_pid27216.log
#
# If you would like to submit a bug report, please include
# instructions on how to reproduce the bug and visit:
#   http://icedtea.classpath.org/bugzilla
# The crash happened outside the Java Virtual Machine in native code.
# See problematic frame for where to report the bug.
#
make[3]: *** [check-local] Aborted
make[3]: Leaving directory `/var/jenkins/workspace/mesos-ubuntu-12.10-gcc/src'
make[2]: *** [check-am] Error 2
make[2]: Leaving directory `/var/jenkins/workspace/mesos-ubuntu-12.10-gcc/src'
make[1]: *** [check] Error 2
make[1]: Leaving directory `/var/jenkins/workspace/mesos-ubuntu-12.10-gcc/src'
make: *** [check-recursive] Error 1
Build step 'Execute shell' marked build as failure
erreicht: 1854109
Sending e-mails to: kernel-test@twitter.com apache-mesos@twitter.com
Finished: FAILURE
 Help us localize this page Page generated: Jul 14, 2014 5:57:17 PMREST 
{code}",Bug,Major,vinodkone,2016-01-21T19:17:28.000+0000,5,Resolved,Complete,SlaveRecoveryTest/0.ReconcileKillTask is flaky,2016-01-21T19:17:28.000+0000,MESOS-1594,1.0,mesos,Mesosphere Sprint 27
alexandra.sava,2014-07-14T21:56:04.000+0000,bmahler,"An ""inverse"" resource offer means that Mesos is requesting resources back from the framework, possibly within some time interval.

This can be leveraged initially to provide more automated cluster maintenance, by offering schedulers the opportunity to move tasks to compensate for planned maintenance. Operators can set a time limit on how long to wait for schedulers to relocate tasks before the tasks are forcibly terminated.

Inverse resource offers have many other potential uses, as it opens the opportunity for the allocator to attempt to move tasks in the cluster through the co-operation of the framework, possibly providing better over-subscription, fairness, etc.",Task,Major,bmahler,2014-09-13T00:05:28.000+0000,5,Resolved,Complete,Design inverse resource offer support,2014-09-13T00:05:28.000+0000,MESOS-1592,5.0,mesos,Q3 Sprint 3
xujyan,2014-07-14T21:22:39.000+0000,xujyan,It currently just reads the flag as the value of the password.,Improvement,Major,xujyan,2014-07-25T21:02:21.000+0000,5,Resolved,Complete,Allow LoadGeneratorFramework to read password from a file,2014-07-25T21:02:21.000+0000,MESOS-1590,1.0,mesos,
jieyu,2014-07-14T17:28:24.000+0000,idownes,We should report disk usage for the executor work directory from MesosContainerizer and include in the ResourceStatistics protobuf.,Improvement,Major,idownes,2015-01-20T19:35:41.000+0000,5,Resolved,Complete,Report disk usage from MesosContainerizer,2015-03-05T01:30:30.000+0000,MESOS-1587,5.0,mesos,Twitter Mesos Q1 Sprint 1
idownes,2014-07-14T17:26:47.000+0000,idownes,"Ideally, tasks should not write outside their sandbox (executor work directory) but pragmatically they may need to write to /tmp, /var/tmp, or some other directory.

1) We should include any such files in disk usage and quota.
2) We should make these ""shared"" directories private, i.e., each container has their own.
3) We should make the lifetime of any such files the same as the executor work directory.",Improvement,Major,idownes,2014-10-27T17:39:46.000+0000,5,Resolved,Complete,"Isolate system directories, e.g., per-container /tmp",2014-11-05T18:59:30.000+0000,MESOS-1586,3.0,mesos,Q3 Sprint 1
xujyan,2014-07-11T19:05:00.000+0000,xujyan,"* Rate limits config takes a configurable *capacity* for each principal.
* To ensure that Master maintain the message order of a framework it's important that Master sends an FrameworkErrorMessage back to the scheduler to ask it to abort.",Bug,Major,xujyan,2014-08-07T07:20:43.000+0000,5,Resolved,Complete,Improve framework rate limiting by imposing the max number of outstanding messages per framework principal,2014-08-07T07:20:43.000+0000,MESOS-1578,5.0,mesos,Q3 Sprint 1
alexr,2014-07-08T19:11:39.000+0000,nnielsen,"Even though the executor shutdown grace period is set to a larger interval, the signal escalation timeout will still be 3 seconds. It should either be configurable or dependent on EXECUTOR_SHUTDOWN_GRACE_PERIOD.

Thoughts?",Improvement,Major,nnielsen,2016-03-24T17:37:42.000+0000,5,Resolved,Complete,Signal escalation timeout is not configurable.,2016-03-24T17:37:42.000+0000,MESOS-1571,8.0,mesos,Mesosphere Q4 Sprint 2 - 11/14
alexandra.sava,2014-07-08T03:04:29.000+0000,bmahler,"We currently do not log the user id when receiving a SIGTERM, this makes debugging a bit difficult. It's easy to get this information through sigaction.",Improvement,Major,bmahler,2014-08-26T00:54:20.000+0000,5,Resolved,Complete,Add logging of the user uid when receiving SIGTERM.,2014-08-26T00:54:20.000+0000,MESOS-1567,1.0,mesos,Q3 Sprint 1
xujyan,2014-07-02T21:33:49.000+0000,jieyu,"Many of the time, when jenkins build times out, we know that some test freezes at some place. However, most of the time, it's very hard to reproduce the deadlock on dev machines.

I would be cool if we can dump the stack traces of all threads when jenkins build times out. Some command like the following:

{noformat}
echo thread apply all bt > tmp; gdb attach `pgrep lt-mesos-tests` < tmp
{noformat}",Improvement,Minor,jieyu,2014-08-07T22:59:20.000+0000,5,Resolved,Complete,Allow jenkins build machine to dump stack traces of all threads when timeout,2014-08-07T22:59:20.000+0000,MESOS-1559,5.0,mesos,Q3 Sprint 1
,2014-06-26T00:42:48.000+0000,vinodkone,"{code}
[ RUN      ] SlaveRecoveryTest/0.MultipleFrameworks
Using temporary directory '/tmp/SlaveRecoveryTest_0_MultipleFrameworks_6dJqxr'
I0626 00:04:39.557339  5450 leveldb.cpp:176] Opened db in 179.857593ms
I0626 00:04:39.565433  5450 leveldb.cpp:183] Compacted db in 8.071041ms
I0626 00:04:39.565457  5450 leveldb.cpp:198] Created db iterator in 4065ns
I0626 00:04:39.565466  5450 leveldb.cpp:204] Seeked to beginning of db in 596ns
I0626 00:04:39.565474  5450 leveldb.cpp:273] Iterated through 0 keys in the db in 396ns
I0626 00:04:39.565490  5450 replica.cpp:741] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned
I0626 00:04:39.565827  5476 recover.cpp:425] Starting replica recovery
I0626 00:04:39.566033  5474 recover.cpp:451] Replica is in EMPTY status
I0626 00:04:39.566504  5474 replica.cpp:638] Replica in EMPTY status received a broadcasted recover request
I0626 00:04:39.566686  5477 recover.cpp:188] Received a recover response from a replica in EMPTY status
I0626 00:04:39.566905  5472 recover.cpp:542] Updating replica status to STARTING
I0626 00:04:39.568307  5471 master.cpp:288] Master 20140626-000439-1032504131-55423-5450 (juno.apache.org) started on 67.195.138.61:55423
I0626 00:04:39.568332  5471 master.cpp:325] Master only allowing authenticated frameworks to register
I0626 00:04:39.568339  5471 master.cpp:330] Master only allowing authenticated slaves to register
I0626 00:04:39.568348  5471 credentials.hpp:35] Loading credentials for authentication from '/tmp/SlaveRecoveryTest_0_MultipleFrameworks_6dJqxr/credentials'
I0626 00:04:39.568461  5471 master.cpp:356] Authorization enabled
I0626 00:04:39.568739  5478 master.cpp:122] No whitelist given. Advertising offers for all slaves
I0626 00:04:39.568814  5475 hierarchical_allocator_process.hpp:301] Initializing hierarchical allocator process with master : master@67.195.138.61:55423
I0626 00:04:39.569206  5478 master.cpp:1122] The newly elected leader is master@67.195.138.61:55423 with id 20140626-000439-1032504131-55423-5450
I0626 00:04:39.569223  5478 master.cpp:1135] Elected as the leading master!
I0626 00:04:39.569231  5478 master.cpp:953] Recovering from registrar
I0626 00:04:39.569286  5475 registrar.cpp:313] Recovering registrar
I0626 00:04:39.600639  5477 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 33.682136ms
I0626 00:04:39.600661  5477 replica.cpp:320] Persisted replica status to STARTING
I0626 00:04:39.600790  5476 recover.cpp:451] Replica is in STARTING status
I0626 00:04:39.601184  5474 replica.cpp:638] Replica in STARTING status received a broadcasted recover request
I0626 00:04:39.601274  5477 recover.cpp:188] Received a recover response from a replica in STARTING status
I0626 00:04:39.601465  5471 recover.cpp:542] Updating replica status to VOTING
I0626 00:04:39.610605  5471 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 9.076262ms
I0626 00:04:39.610638  5471 replica.cpp:320] Persisted replica status to VOTING
I0626 00:04:39.610683  5471 recover.cpp:556] Successfully joined the Paxos group
I0626 00:04:39.610780  5471 recover.cpp:440] Recover process terminated
I0626 00:04:39.610946  5474 log.cpp:656] Attempting to start the writer
I0626 00:04:39.611486  5475 replica.cpp:474] Replica received implicit promise request with proposal 1
I0626 00:04:39.618924  5475 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 7.418789ms
I0626 00:04:39.618942  5475 replica.cpp:342] Persisted promised to 1
I0626 00:04:39.619220  5476 coordinator.cpp:230] Coordinator attemping to fill missing position
I0626 00:04:39.619763  5476 replica.cpp:375] Replica received explicit promise request for position 0 with proposal 2
I0626 00:04:39.627267  5476 leveldb.cpp:343] Persisting action (8 bytes) to leveldb took 7.485492ms
I0626 00:04:39.627295  5476 replica.cpp:676] Persisted action at 0
I0626 00:04:39.627822  5473 replica.cpp:508] Replica received write request for position 0
I0626 00:04:39.627861  5473 leveldb.cpp:438] Reading position from leveldb took 17132ns
I0626 00:04:39.635592  5473 leveldb.cpp:343] Persisting action (14 bytes) to leveldb took 7.714322ms
I0626 00:04:39.635612  5473 replica.cpp:676] Persisted action at 0
I0626 00:04:39.635797  5473 replica.cpp:655] Replica received learned notice for position 0
I0626 00:04:39.643941  5473 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 8.129347ms
I0626 00:04:39.643960  5473 replica.cpp:676] Persisted action at 0
I0626 00:04:39.643970  5473 replica.cpp:661] Replica learned NOP action at position 0
I0626 00:04:39.644207  5473 log.cpp:672] Writer started with ending position 0
I0626 00:04:39.644625  5471 leveldb.cpp:438] Reading position from leveldb took 9128ns
I0626 00:04:39.646010  5476 registrar.cpp:346] Successfully fetched the registry (0B)
I0626 00:04:39.646044  5476 registrar.cpp:422] Attempting to update the 'registry'
I0626 00:04:39.647274  5471 log.cpp:680] Attempting to append 136 bytes to the log
I0626 00:04:39.647337  5471 coordinator.cpp:340] Coordinator attempting to write APPEND action at position 1
I0626 00:04:39.647687  5476 replica.cpp:508] Replica received write request for position 1
I0626 00:04:39.655206  5476 leveldb.cpp:343] Persisting action (155 bytes) to leveldb took 7.499736ms
I0626 00:04:39.655225  5476 replica.cpp:676] Persisted action at 1
I0626 00:04:39.655467  5476 replica.cpp:655] Replica received learned notice for position 1
I0626 00:04:39.663534  5476 leveldb.cpp:343] Persisting action (157 bytes) to leveldb took 8.054929ms
I0626 00:04:39.663554  5476 replica.cpp:676] Persisted action at 1
I0626 00:04:39.663563  5476 replica.cpp:661] Replica learned APPEND action at position 1
I0626 00:04:39.663890  5478 registrar.cpp:479] Successfully updated 'registry'
I0626 00:04:39.663947  5478 registrar.cpp:372] Successfully recovered registrar
I0626 00:04:39.663969  5476 log.cpp:699] Attempting to truncate the log to 1
I0626 00:04:39.664044  5478 master.cpp:980] Recovered 0 slaves from the Registry (98B) ; allowing 10mins for slaves to re-register
I0626 00:04:39.664057  5476 coordinator.cpp:340] Coordinator attempting to write TRUNCATE action at position 2
I0626 00:04:39.664341  5476 replica.cpp:508] Replica received write request for position 2
I0626 00:04:39.664681  5450 containerizer.cpp:124] Using isolation: posix/cpu,posix/mem
I0626 00:04:39.666721  5471 slave.cpp:168] Slave started on 173)@67.195.138.61:55423
I0626 00:04:39.666741  5471 credentials.hpp:35] Loading credentials for authentication from '/tmp/SlaveRecoveryTest_0_MultipleFrameworks_G6ObtK/credential'
I0626 00:04:39.666806  5471 slave.cpp:268] Slave using credential for: test-principal
I0626 00:04:39.666936  5471 slave.cpp:281] Slave resources: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I0626 00:04:39.667000  5471 slave.cpp:326] Slave hostname: juno.apache.org
I0626 00:04:39.667009  5471 slave.cpp:327] Slave checkpoint: true
I0626 00:04:39.667572  5478 state.cpp:33] Recovering state from '/tmp/SlaveRecoveryTest_0_MultipleFrameworks_G6ObtK/meta'
I0626 00:04:39.667703  5475 status_update_manager.cpp:193] Recovering status update manager
I0626 00:04:39.667840  5475 containerizer.cpp:287] Recovering containerizer
I0626 00:04:39.668478  5471 slave.cpp:3128] Finished recovery
I0626 00:04:39.668712  5471 slave.cpp:601] New master detected at master@67.195.138.61:55423
I0626 00:04:39.668738  5471 slave.cpp:677] Authenticating with master master@67.195.138.61:55423
I0626 00:04:39.668802  5471 slave.cpp:650] Detecting new master
I0626 00:04:39.668861  5471 status_update_manager.cpp:167] New master detected at master@67.195.138.61:55423
I0626 00:04:39.668916  5471 authenticatee.hpp:128] Creating new client SASL connection
I0626 00:04:39.669087  5471 master.cpp:3499] Authenticating slave(173)@67.195.138.61:55423
I0626 00:04:39.669203  5471 authenticator.hpp:156] Creating new server SASL connection
I0626 00:04:39.669340  5471 authenticatee.hpp:219] Received SASL authentication mechanisms: CRAM-MD5
I0626 00:04:39.669359  5471 authenticatee.hpp:245] Attempting to authenticate with mechanism 'CRAM-MD5'
I0626 00:04:39.669386  5471 authenticator.hpp:262] Received SASL authentication start
I0626 00:04:39.669414  5471 authenticator.hpp:384] Authentication requires more steps
I0626 00:04:39.669457  5471 authenticatee.hpp:265] Received SASL authentication step
I0626 00:04:39.669514  5471 authenticator.hpp:290] Received SASL authentication step
I0626 00:04:39.669534  5471 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'juno.apache.org' server FQDN: 'juno.apache.org' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0626 00:04:39.669543  5471 auxprop.cpp:153] Looking up auxiliary property '*userPassword'
I0626 00:04:39.669567  5471 auxprop.cpp:153] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0626 00:04:39.669580  5471 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'juno.apache.org' server FQDN: 'juno.apache.org' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0626 00:04:39.669589  5471 auxprop.cpp:103] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0626 00:04:39.669594  5471 auxprop.cpp:103] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0626 00:04:39.669606  5471 authenticator.hpp:376] Authentication success
I0626 00:04:39.669641  5471 authenticatee.hpp:305] Authentication success
I0626 00:04:39.669669  5471 master.cpp:3539] Successfully authenticated principal 'test-principal' at slave(173)@67.195.138.61:55423
I0626 00:04:39.669761  5450 sched.cpp:139] Version: 0.20.0
I0626 00:04:39.669764  5478 slave.cpp:734] Successfully authenticated with master master@67.195.138.61:55423
I0626 00:04:39.669826  5478 slave.cpp:972] Will retry registration in 3.190666ms if necessary
I0626 00:04:39.669950  5471 master.cpp:2781] Registering slave at slave(173)@67.195.138.61:55423 (juno.apache.org) with id 20140626-000439-1032504131-55423-5450-0
I0626 00:04:39.669960  5475 sched.cpp:235] New master detected at master@67.195.138.61:55423
I0626 00:04:39.669977  5475 sched.cpp:285] Authenticating with master master@67.195.138.61:55423
I0626 00:04:39.670073  5471 registrar.cpp:422] Attempting to update the 'registry'
I0626 00:04:39.670114  5475 authenticatee.hpp:128] Creating new client SASL connection
I0626 00:04:39.670263  5475 master.cpp:3499] Authenticating scheduler-e66c50d2-2790-4d20-bc77-a57af0e1780b@67.195.138.61:55423
I0626 00:04:39.670361  5474 authenticator.hpp:156] Creating new server SASL connection
I0626 00:04:39.670506  5475 authenticatee.hpp:219] Received SASL authentication mechanisms: CRAM-MD5
I0626 00:04:39.670526  5475 authenticatee.hpp:245] Attempting to authenticate with mechanism 'CRAM-MD5'
I0626 00:04:39.670559  5475 authenticator.hpp:262] Received SASL authentication start
I0626 00:04:39.670590  5475 authenticator.hpp:384] Authentication requires more steps
I0626 00:04:39.670619  5475 authenticatee.hpp:265] Received SASL authentication step
I0626 00:04:39.670650  5475 authenticator.hpp:290] Received SASL authentication step
I0626 00:04:39.670670  5475 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'juno.apache.org' server FQDN: 'juno.apache.org' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0626 00:04:39.670677  5475 auxprop.cpp:153] Looking up auxiliary property '*userPassword'
I0626 00:04:39.670687  5475 auxprop.cpp:153] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0626 00:04:39.670697  5475 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'juno.apache.org' server FQDN: 'juno.apache.org' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0626 00:04:39.670706  5475 auxprop.cpp:103] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0626 00:04:39.670712  5475 auxprop.cpp:103] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0626 00:04:39.670723  5475 authenticator.hpp:376] Authentication success
I0626 00:04:39.670749  5475 authenticatee.hpp:305] Authentication success
I0626 00:04:39.670773  5475 master.cpp:3539] Successfully authenticated principal 'test-principal' at scheduler-e66c50d2-2790-4d20-bc77-a57af0e1780b@67.195.138.61:55423
I0626 00:04:39.670845  5475 sched.cpp:359] Successfully authenticated with master master@67.195.138.61:55423
I0626 00:04:39.670858  5475 sched.cpp:478] Sending registration request to master@67.195.138.61:55423
I0626 00:04:39.670899  5475 master.cpp:1241] Received registration request from scheduler-e66c50d2-2790-4d20-bc77-a57af0e1780b@67.195.138.61:55423
I0626 00:04:39.670922  5475 master.cpp:1201] Authorizing framework principal 'test-principal' to receive offers for role '*'
I0626 00:04:39.671052  5475 master.cpp:1300] Registering framework 20140626-000439-1032504131-55423-5450-0000 at scheduler-e66c50d2-2790-4d20-bc77-a57af0e1780b@67.195.138.61:55423
I0626 00:04:39.671159  5474 sched.cpp:409] Framework registered with 20140626-000439-1032504131-55423-5450-0000
I0626 00:04:39.671185  5474 sched.cpp:423] Scheduler::registered took 10223ns
I0626 00:04:39.671226  5474 hierarchical_allocator_process.hpp:331] Added framework 20140626-000439-1032504131-55423-5450-0000
I0626 00:04:39.671241  5474 hierarchical_allocator_process.hpp:724] No resources available to allocate!
I0626 00:04:39.671247  5474 hierarchical_allocator_process.hpp:686] Performed allocation for 0 slaves in 8574ns
I0626 00:04:39.671879  5476 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 7.48781ms
I0626 00:04:39.671900  5476 replica.cpp:676] Persisted action at 2
I0626 00:04:39.672164  5471 replica.cpp:655] Replica received learned notice for position 2
I0626 00:04:39.674092  5472 slave.cpp:972] Will retry registration in 25.467893ms if necessary
I0626 00:04:39.674108  5476 master.cpp:2769] Ignoring register slave message from slave(173)@67.195.138.61:55423 (juno.apache.org) as admission is already in progress
I0626 00:04:39.680193  5471 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 8.01285ms
I0626 00:04:39.680223  5471 leveldb.cpp:401] Deleting ~1 keys from leveldb took 11393ns
I0626 00:04:39.680234  5471 replica.cpp:676] Persisted action at 2
I0626 00:04:39.680245  5471 replica.cpp:661] Replica learned TRUNCATE action at position 2
I0626 00:04:39.680585  5472 log.cpp:680] Attempting to append 326 bytes to the log
I0626 00:04:39.680670  5477 coordinator.cpp:340] Coordinator attempting to write APPEND action at position 3
I0626 00:04:39.680953  5474 replica.cpp:508] Replica received write request for position 3
I0626 00:04:39.688521  5474 leveldb.cpp:343] Persisting action (345 bytes) to leveldb took 7.548316ms
I0626 00:04:39.688542  5474 replica.cpp:676] Persisted action at 3
I0626 00:04:39.688750  5474 replica.cpp:655] Replica received learned notice for position 3
I0626 00:04:39.696851  5474 leveldb.cpp:343] Persisting action (347 bytes) to leveldb took 8.088289ms
I0626 00:04:39.696869  5474 replica.cpp:676] Persisted action at 3
I0626 00:04:39.696878  5474 replica.cpp:661] Replica learned APPEND action at position 3
I0626 00:04:39.697268  5474 registrar.cpp:479] Successfully updated 'registry'
I0626 00:04:39.697350  5474 log.cpp:699] Attempting to truncate the log to 3
I0626 00:04:39.697412  5474 master.cpp:2821] Registered slave 20140626-000439-1032504131-55423-5450-0 at slave(173)@67.195.138.61:55423 (juno.apache.org)
I0626 00:04:39.697423  5474 master.cpp:3967] Adding slave 20140626-000439-1032504131-55423-5450-0 at slave(173)@67.195.138.61:55423 (juno.apache.org) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I0626 00:04:39.697535  5474 coordinator.cpp:340] Coordinator attempting to write TRUNCATE action at position 4
I0626 00:04:39.697618  5474 slave.cpp:768] Registered with master master@67.195.138.61:55423; given slave ID 20140626-000439-1032504131-55423-5450-0
I0626 00:04:39.697754  5474 slave.cpp:781] Checkpointing SlaveInfo to '/tmp/SlaveRecoveryTest_0_MultipleFrameworks_G6ObtK/meta/slaves/20140626-000439-1032504131-55423-5450-0/slave.info'
I0626 00:04:39.697762  5471 hierarchical_allocator_process.hpp:444] Added slave 20140626-000439-1032504131-55423-5450-0 (juno.apache.org) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (and cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] available)
I0626 00:04:39.697845  5471 hierarchical_allocator_process.hpp:750] Offering cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave 20140626-000439-1032504131-55423-5450-0 to framework 20140626-000439-1032504131-55423-5450-0000
I0626 00:04:39.697854  5474 slave.cpp:2325] Received ping from slave-observer(142)@67.195.138.61:55423
I0626 00:04:39.698040  5471 hierarchical_allocator_process.hpp:706] Performed allocation for slave 20140626-000439-1032504131-55423-5450-0 in 231333ns
I0626 00:04:39.698051  5474 replica.cpp:508] Replica received write request for position 4
I0626 00:04:39.698118  5471 master.hpp:794] Adding offer 20140626-000439-1032504131-55423-5450-0 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave 20140626-000439-1032504131-55423-5450-0 (juno.apache.org)
I0626 00:04:39.698170  5471 master.cpp:3446] Sending 1 offers to framework 20140626-000439-1032504131-55423-5450-0000
I0626 00:04:39.698318  5471 sched.cpp:546] Scheduler::resourceOffers took 24371ns
I0626 00:04:39.699718  5477 master.hpp:804] Removing offer 20140626-000439-1032504131-55423-5450-0 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave 20140626-000439-1032504131-55423-5450-0 (juno.apache.org)
I0626 00:04:39.699787  5477 master.cpp:2125] Processing reply for offers: [ 20140626-000439-1032504131-55423-5450-0 ] on slave 20140626-000439-1032504131-55423-5450-0 at slave(173)@67.195.138.61:55423 (juno.apache.org) for framework 20140626-000439-1032504131-55423-5450-0000
I0626 00:04:39.699812  5477 master.cpp:2211] Authorizing framework principal 'test-principal' to launch task 897522cc-4ec5-4904-aed0-00b6b8c41028 as user 'jenkins'
I0626 00:04:39.700160  5477 master.hpp:766] Adding task 897522cc-4ec5-4904-aed0-00b6b8c41028 with resources cpus(*):1; mem(*):512 on slave 20140626-000439-1032504131-55423-5450-0 (juno.apache.org)
I0626 00:04:39.700188  5477 master.cpp:2277] Launching task 897522cc-4ec5-4904-aed0-00b6b8c41028 of framework 20140626-000439-1032504131-55423-5450-0000 with resources cpus(*):1; mem(*):512 on slave 20140626-000439-1032504131-55423-5450-0 at slave(173)@67.195.138.61:55423 (juno.apache.org)
I0626 00:04:39.700392  5471 slave.cpp:1003] Got assigned task 897522cc-4ec5-4904-aed0-00b6b8c41028 for framework 20140626-000439-1032504131-55423-5450-0000
I0626 00:04:39.700479  5477 hierarchical_allocator_process.hpp:546] Framework 20140626-000439-1032504131-55423-5450-0000 left cpus(*):1; mem(*):512; disk(*):1024; ports(*):[31000-32000] unused on slave 20140626-000439-1032504131-55423-5450-0
I0626 00:04:39.700505  5471 slave.cpp:3400] Checkpointing FrameworkInfo to '/tmp/SlaveRecoveryTest_0_MultipleFrameworks_G6ObtK/meta/slaves/20140626-000439-1032504131-55423-5450-0/frameworks/20140626-000439-1032504131-55423-5450-0000/framework.info'
I0626 00:04:39.700597  5477 hierarchical_allocator_process.hpp:588] Framework 20140626-000439-1032504131-55423-5450-0000 filtered slave 20140626-000439-1032504131-55423-5450-0 for 5secs
I0626 00:04:39.700686  5471 slave.cpp:3407] Checkpointing framework pid 'scheduler-e66c50d2-2790-4d20-bc77-a57af0e1780b@67.195.138.61:55423' to '/tmp/SlaveRecoveryTest_0_MultipleFrameworks_G6ObtK/meta/slaves/20140626-000439-1032504131-55423-5450-0/frameworks/20140626-000439-1032504131-55423-5450-0000/framework.pid'
I0626 00:04:39.700960  5471 slave.cpp:1113] Launching task 897522cc-4ec5-4904-aed0-00b6b8c41028 for framework 20140626-000439-1032504131-55423-5450-0000
I0626 00:04:39.702287  5471 slave.cpp:3722] Checkpointing ExecutorInfo to '/tmp/SlaveRecoveryTest_0_MultipleFrameworks_G6ObtK/meta/slaves/20140626-000439-1032504131-55423-5450-0/frameworks/20140626-000439-1032504131-55423-5450-0000/executors/897522cc-4ec5-4904-aed0-00b6b8c41028/executor.info'
I0626 00:04:39.702738  5471 slave.cpp:3837] Checkpointing TaskInfo to '/tmp/SlaveRecoveryTest_0_MultipleFrameworks_G6ObtK/meta/slaves/20140626-000439-1032504131-55423-5450-0/frameworks/20140626-000439-1032504131-55423-5450-0000/executors/897522cc-4ec5-4904-aed0-00b6b8c41028/runs/9ad3a5ac-3587-47df-96c2-df76ea09328c/tasks/897522cc-4ec5-4904-aed0-00b6b8c41028/task.info'
I0626 00:04:39.702744  5476 containerizer.cpp:427] Starting container '9ad3a5ac-3587-47df-96c2-df76ea09328c' for executor '897522cc-4ec5-4904-aed0-00b6b8c41028' of framework '20140626-000439-1032504131-55423-5450-0000'
I0626 00:04:39.702987  5471 slave.cpp:1223] Queuing task '897522cc-4ec5-4904-aed0-00b6b8c41028' for executor 897522cc-4ec5-4904-aed0-00b6b8c41028 of framework '20140626-000439-1032504131-55423-5450-0000
I0626 00:04:39.703039  5471 slave.cpp:562] Successfully attached file '/tmp/SlaveRecoveryTest_0_MultipleFrameworks_G6ObtK/slaves/20140626-000439-1032504131-55423-5450-0/frameworks/20140626-000439-1032504131-55423-5450-0000/executors/897522cc-4ec5-4904-aed0-00b6b8c41028/runs/9ad3a5ac-3587-47df-96c2-df76ea09328c'
I0626 00:04:39.704654  5477 launcher.cpp:137] Forked child with pid '7596' for container '9ad3a5ac-3587-47df-96c2-df76ea09328c'
I0626 00:04:39.704891  5477 containerizer.cpp:705] Checkpointing executor's forked pid 7596 to '/tmp/SlaveRecoveryTest_0_MultipleFrameworks_G6ObtK/meta/slaves/20140626-000439-1032504131-55423-5450-0/frameworks/20140626-000439-1032504131-55423-5450-0000/executors/897522cc-4ec5-4904-aed0-00b6b8c41028/runs/9ad3a5ac-3587-47df-96c2-df76ea09328c/pids/forked.pid'
I0626 00:04:39.705301  5474 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 7.183865ms
I0626 00:04:39.705343  5474 replica.cpp:676] Persisted action at 4
I0626 00:04:39.705912  5476 containerizer.cpp:537] Fetching URIs for container '9ad3a5ac-3587-47df-96c2-df76ea09328c' using command '/home/jenkins/jenkins-slave/workspace/Mesos-Trunk-Ubuntu-Build-Out-Of-Src-Disable-Java-Disable-Python-Disable-Webui/build/src/mesos-fetcher'
I0626 00:04:39.706073  5471 replica.cpp:655] Replica received learned notice for position 4
I0626 00:04:39.713664  5471 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 6.238172ms
I0626 00:04:39.713762  5471 leveldb.cpp:401] Deleting ~2 keys from leveldb took 42244ns
I0626 00:04:39.713788  5471 replica.cpp:676] Persisted action at 4
I0626 00:04:39.713810  5471 replica.cpp:661] Replica learned TRUNCATE action at position 4
I0626 00:04:40.378677  5475 slave.cpp:2470] Monitoring executor '897522cc-4ec5-4904-aed0-00b6b8c41028' of framework '20140626-000439-1032504131-55423-5450-0000' in container '9ad3a5ac-3587-47df-96c2-df76ea09328c'
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0626 00:04:40.413177  7631 process.cpp:1671] libprocess is initialized on 67.195.138.61:40619 for 8 cpus
I0626 00:04:40.414454  7631 exec.cpp:131] Version: 0.20.0
I0626 00:04:40.415856  7649 exec.cpp:181] Executor started at: executor(1)@67.195.138.61:40619 with pid 7631
I0626 00:04:40.416453  5471 slave.cpp:1734] Got registration for executor '897522cc-4ec5-4904-aed0-00b6b8c41028' of framework 20140626-000439-1032504131-55423-5450-0000
I0626 00:04:40.416527  5471 slave.cpp:1819] Checkpointing executor pid 'executor(1)@67.195.138.61:40619' to '/tmp/SlaveRecoveryTest_0_MultipleFrameworks_G6ObtK/meta/slaves/20140626-000439-1032504131-55423-5450-0/frameworks/20140626-000439-1032504131-55423-5450-0000/executors/897522cc-4ec5-4904-aed0-00b6b8c41028/runs/9ad3a5ac-3587-47df-96c2-df76ea09328c/pids/libprocess.pid'
I0626 00:04:40.416998  5471 slave.cpp:1853] Flushing queued task 897522cc-4ec5-4904-aed0-00b6b8c41028 for executor '897522cc-4ec5-4904-aed0-00b6b8c41028' of framework 20140626-000439-1032504131-55423-5450-0000
I0626 00:04:40.417186  5479 process.cpp:1098] Socket closed while receiving
I0626 00:04:40.417322  7648 exec.cpp:205] Executor registered on slave 20140626-000439-1032504131-55423-5450-0
I0626 00:04:40.417368  7653 process.cpp:1037] Socket closed while receiving
I0626 00:04:40.418385  7648 exec.cpp:217] Executor::registered took 115121ns
Registered executor on juno.apache.org
I0626 00:04:40.418544  7648 exec.cpp:292] Executor asked to run task '897522cc-4ec5-4904-aed0-00b6b8c41028'
Starting task 897522cc-4ec5-4904-aed0-00b6b8c41028
I0626 00:04:40.418609  7648 exec.cpp:301] Executor::launchTask took 35936ns
Forked command at 7654
sh -c 'sleep 1000'
I0626 00:04:40.420611  7650 exec.cpp:524] Executor sending status update TASK_RUNNING (UUID: 6d952e6d-b7d7-4f40-9f44-f7c3f81757af) for task 897522cc-4ec5-4904-aed0-00b6b8c41028 of framework 20140626-000439-1032504131-55423-5450-0000
I0626 00:04:40.420953  5473 slave.cpp:2088] Handling status update TASK_RUNNING (UUID: 6d952e6d-b7d7-4f40-9f44-f7c3f81757af) for task 897522cc-4ec5-4904-aed0-00b6b8c41028 of framework 20140626-000439-1032504131-55423-5450-0000 from executor(1)@67.195.138.61:40619
I0626 00:04:40.421188  5474 status_update_manager.cpp:320] Received status update TASK_RUNNING (UUID: 6d952e6d-b7d7-4f40-9f44-f7c3f81757af) for task 897522cc-4ec5-4904-aed0-00b6b8c41028 of framework 20140626-000439-1032504131-55423-5450-0000
I0626 00:04:40.421206  5474 status_update_manager.cpp:499] Creating StatusUpdate stream for task 897522cc-4ec5-4904-aed0-00b6b8c41028 of framework 20140626-000439-1032504131-55423-5450-0000
I0626 00:04:40.421469  5474 status_update_manager.hpp:342] Checkpointing UPDATE for status update TASK_RUNNING (UUID: 6d952e6d-b7d7-4f40-9f44-f7c3f81757af) for task 897522cc-4ec5-4904-aed0-00b6b8c41028 of framework 20140626-000439-1032504131-55423-5450-0000
I0626 00:04:40.525890  5474 status_update_manager.cpp:373] Forwarding status update TASK_RUNNING (UUID: 6d952e6d-b7d7-4f40-9f44-f7c3f81757af) for task 897522cc-4ec5-4904-aed0-00b6b8c41028 of framework 20140626-000439-1032504131-55423-5450-0000 to master@67.195.138.61:55423
I0626 00:04:40.526053  5474 master.cpp:3107] Status update TASK_RUNNING (UUID: 6d952e6d-b7d7-4f40-9f44-f7c3f81757af) for task 897522cc-4ec5-4904-aed0-00b6b8c41028 of framework 20140626-000439-1032504131-55423-5450-0000 from slave 20140626-000439-1032504131-55423-5450-0 at slave(173)@67.195.138.61:55423 (juno.apache.org)
I0626 00:04:40.526087  5474 slave.cpp:2246] Status update manager successfully handled status update TASK_RUNNING (UUID: 6d952e6d-b7d7-4f40-9f44-f7c3f81757af) for task 897522cc-4ec5-4904-aed0-00b6b8c41028 of framework 20140626-000439-1032504131-55423-5450-0000
I0626 00:04:40.526100  5474 slave.cpp:2252] Sending acknowledgement for status update TASK_RUNNING (UUID: 6d952e6d-b7d7-4f40-9f44-f7c3f81757af) for task 897522cc-4ec5-4904-aed0-00b6b8c41028 of framework 20140626-000439-1032504131-55423-5450-0000 to executor(1)@67.195.138.61:40619
I0626 00:04:40.526252  5474 sched.cpp:637] Scheduler::statusUpdate took 17393ns
I0626 00:04:40.526294  5474 master.cpp:2631] Forwarding status update acknowledgement 6d952e6d-b7d7-4f40-9f44-f7c3f81757af for task 897522cc-4ec5-4904-aed0-00b6b8c41028 of framework 20140626-000439-1032504131-55423-5450-0000 to slave 20140626-000439-1032504131-55423-5450-0 at slave(173)@67.195.138.61:55423 (juno.apache.org)
I0626 00:04:40.526371  5474 status_update_manager.cpp:398] Received status update acknowledgement (UUID: 6d952e6d-b7d7-4f40-9f44-f7c3f81757af) for task 897522cc-4ec5-4904-aed0-00b6b8c41028 of framework 20140626-000439-1032504131-55423-5450-0000
I0626 00:04:40.526384  5474 status_update_manager.hpp:342] Checkpointing ACK for status update TASK_RUNNING (UUID: 6d952e6d-b7d7-4f40-9f44-f7c3f81757af) for task 897522cc-4ec5-4904-aed0-00b6b8c41028 of framework 20140626-000439-1032504131-55423-5450-0000
I0626 00:04:40.526468  5479 process.cpp:1098] Socket closed while receiving
I0626 00:04:40.526574  7651 exec.cpp:338] Executor received status update acknowledgement 6d952e6d-b7d7-4f40-9f44-f7c3f81757af for task 897522cc-4ec5-4904-aed0-00b6b8c41028 of framework 20140626-000439-1032504131-55423-5450-0000
I0626 00:04:40.526679  7653 process.cpp:1037] Socket closed while receiving
I0626 00:04:40.569715  5473 hierarchical_allocator_process.hpp:833] Filtered cpus(*):1; mem(*):512; disk(*):1024; ports(*):[31000-32000] on slave 20140626-000439-1032504131-55423-5450-0 for framework 20140626-000439-1032504131-55423-5450-0000
I0626 00:04:40.569749  5473 hierarchical_allocator_process.hpp:686] Performed allocation for 1 slaves in 105698ns
I0626 00:04:40.576212  5477 slave.cpp:1674] Status update manager successfully handled status update acknowledgement (UUID: 6d952e6d-b7d7-4f40-9f44-f7c3f81757af) for task 897522cc-4ec5-4904-aed0-00b6b8c41028 of framework 20140626-000439-1032504131-55423-5450-0000
I0626 00:04:40.578642  5450 sched.cpp:139] Version: 0.20.0
I0626 00:04:40.578886  5475 sched.cpp:235] New master detected at master@67.195.138.61:55423
I0626 00:04:40.578902  5475 sched.cpp:285] Authenticating with master master@67.195.138.61:55423
I0626 00:04:40.579040  5475 authenticatee.hpp:128] Creating new client SASL connection
I0626 00:04:40.579202  5475 master.cpp:3499] Authenticating scheduler-bb54dd52-95dc-4ed9-b69c-7a65f1661180@67.195.138.61:55423
I0626 00:04:40.579313  5475 authenticator.hpp:156] Creating new server SASL connection
I0626 00:04:40.579414  5475 authenticatee.hpp:219] Received SASL authentication mechanisms: CRAM-MD5
I0626 00:04:40.579430  5475 authenticatee.hpp:245] Attempting to authenticate with mechanism 'CRAM-MD5'
I0626 00:04:40.579457  5475 authenticator.hpp:262] Received SASL authentication start
I0626 00:04:40.579488  5475 authenticator.hpp:384] Authentication requires more steps
I0626 00:04:40.579514  5475 authenticatee.hpp:265] Received SASL authentication step
I0626 00:04:40.579551  5475 authenticator.hpp:290] Received SASL authentication step
I0626 00:04:40.579573  5475 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'juno.apache.org' server FQDN: 'juno.apache.org' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0626 00:04:40.579586  5475 auxprop.cpp:153] Looking up auxiliary property '*userPassword'
I0626 00:04:40.579601  5475 auxprop.cpp:153] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0626 00:04:40.579612  5475 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'juno.apache.org' server FQDN: 'juno.apache.org' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0626 00:04:40.579619  5475 auxprop.cpp:103] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0626 00:04:40.579624  5475 auxprop.cpp:103] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0626 00:04:40.579638  5475 authenticator.hpp:376] Authentication success
I0626 00:04:40.579664  5475 authenticatee.hpp:305] Authentication success
I0626 00:04:40.579687  5475 master.cpp:3539] Successfully authenticated principal 'test-principal' at scheduler-bb54dd52-95dc-4ed9-b69c-7a65f1661180@67.195.138.61:55423
I0626 00:04:40.579768  5475 sched.cpp:359] Successfully authenticated with master master@67.195.138.61:55423
I0626 00:04:40.579781  5475 sched.cpp:478] Sending registration request to master@67.195.138.61:55423
I0626 00:04:40.579825  5475 master.cpp:1241] Received registration request from scheduler-bb54dd52-95dc-4ed9-b69c-7a65f1661180@67.195.138.61:55423
I0626 00:04:40.579845  5475 master.cpp:1201] Authorizing framework principal 'test-principal' to receive offers for role '*'
I0626 00:04:40.579984  5475 master.cpp:1300] Registering framework 20140626-000439-1032504131-55423-5450-0001 at scheduler-bb54dd52-95dc-4ed9-b69c-7a65f1661180@67.195.138.61:55423
I0626 00:04:40.580056  5475 sched.cpp:409] Framework registered with 20140626-000439-1032504131-55423-5450-0001
I0626 00:04:40.580075  5475 sched.cpp:423] Scheduler::registered took 8994ns
I0626 00:04:40.580117  5475 hierarchical_allocator_process.hpp:331] Added framework 20140626-000439-1032504131-55423-5450-0001
I0626 00:04:40.580173  5475 hierarchical_allocator_process.hpp:750] Offering cpus(*):1; mem(*):512; disk(*):1024; ports(*):[31000-32000] on slave 20140626-000439-1032504131-55423-5450-0 to framework 20140626-000439-1032504131-55423-5450-0001
I0626 00:04:40.580366  5475 hierarchical_allocator_process.hpp:833] Filtered  on slave 20140626-000439-1032504131-55423-5450-0 for framework 20140626-000439-1032504131-55423-5450-0000
I0626 00:04:40.580378  5475 hierarchical_allocator_process.hpp:686] Performed allocation for 1 slaves in 251520ns
I0626 00:04:40.580454  5475 master.hpp:794] Adding offer 20140626-000439-1032504131-55423-5450-1 with resources cpus(*):1; mem(*):512; disk(*):1024; ports(*):[31000-32000] on slave 20140626-000439-1032504131-55423-5450-0 (juno.apache.org)
I0626 00:04:40.580509  5475 master.cpp:3446] Sending 1 offers to framework 20140626-000439-1032504131-55423-5450-0001
I0626 00:04:40.580796  5476 sched.cpp:546] Scheduler::resourceOffers took 36436ns
I0626 00:04:40.582280  5476 master.hpp:804] Removing offer 20140626-000439-1032504131-55423-5450-1 with resources cpus(*):1; mem(*):512; disk(*):1024; ports(*):[31000-32000] on slave 20140626-000439-1032504131-55423-5450-0 (juno.apache.org)
I0626 00:04:40.582362  5476 master.cpp:2125] Processing reply for offers: [ 20140626-000439-1032504131-55423-5450-1 ] on slave 20140626-000439-1032504131-55423-5450-0 at slave(173)@67.195.138.61:55423 (juno.apache.org) for framework 20140626-000439-1032504131-55423-5450-0001
I0626 00:04:40.582402  5476 master.cpp:2211] Authorizing framework principal 'test-principal' to launch task b1f40647-a2ff-475d-a56b-d2a5db9c1229 as user 'jenkins'
I0626 00:04:40.582823  5475 master.hpp:766] Adding task b1f40647-a2ff-475d-a56b-d2a5db9c1229 with resources cpus(*):1; mem(*):512; disk(*):1024; ports(*):[31000-32000] on slave 20140626-000439-1032504131-55423-5450-0 (juno.apache.org)
I0626 00:04:40.582892  5475 master.cpp:2277] Launching task b1f40647-a2ff-475d-a56b-d2a5db9c1229 of framework 20140626-000439-1032504131-55423-5450-0001 with resources cpus(*):1; mem(*):512; disk(*):1024; ports(*):[31000-32000] on slave 20140626-000439-1032504131-55423-5450-0 at slave(173)@67.195.138.61:55423 (juno.apache.org)
I0626 00:04:40.583001  5474 slave.cpp:1003] Got assigned task b1f40647-a2ff-475d-a56b-d2a5db9c1229 for framework 20140626-000439-1032504131-55423-5450-0001
I0626 00:04:40.583097  5474 slave.cpp:3400] Checkpointing FrameworkInfo to '/tmp/SlaveRecoveryTest_0_MultipleFrameworks_G6ObtK/meta/slaves/20140626-000439-1032504131-55423-5450-0/frameworks/20140626-000439-1032504131-55423-5450-0001/framework.info'
I0626 00:04:40.583204  5474 slave.cpp:3407] Checkpointing framework pid 'scheduler-bb54dd52-95dc-4ed9-b69c-7a65f1661180@67.195.138.61:55423' to '/tmp/SlaveRecoveryTest_0_MultipleFrameworks_G6ObtK/meta/slaves/20140626-000439-1032504131-55423-5450-0/frameworks/20140626-000439-1032504131-55423-5450-0001/framework.pid'
I0626 00:04:40.583442  5474 slave.cpp:1113] Launching task b1f40647-a2ff-475d-a56b-d2a5db9c1229 for framework 20140626-000439-1032504131-55423-5450-0001
I0626 00:04:40.584455  5474 slave.cpp:3722] Checkpointing ExecutorInfo to '/tmp/SlaveRecoveryTest_0_MultipleFrameworks_G6ObtK/meta/slaves/20140626-000439-1032504131-55423-5450-0/frameworks/20140626-000439-1032504131-55423-5450-0001/executors/b1f40647-a2ff-475d-a56b-d2a5db9c1229/executor.info'
I0626 00:04:40.584846  5474 slave.cpp:3837] Checkpointing TaskInfo to '/tmp/SlaveRecoveryTest_0_MultipleFrameworks_G6ObtK/meta/slaves/20140626-000439-1032504131-55423-5450-0/frameworks/20140626-000439-1032504131-55423-5450-0001/executors/b1f40647-a2ff-475d-a56b-d2a5db9c1229/runs/44b9f0a1-fcf4-4b33-b6dc-2d886304e8b3/tasks/b1f40647-a2ff-475d-a56b-d2a5db9c1229/task.info'
I0626 00:04:40.584866  5476 containerizer.cpp:427] Starting container '44b9f0a1-fcf4-4b33-b6dc-2d886304e8b3' for executor 'b1f40647-a2ff-475d-a56b-d2a5db9c1229' of framework '20140626-000439-1032504131-55423-5450-0001'
I0626 00:04:40.584976  5474 slave.cpp:1223] Queuing task 'b1f40647-a2ff-475d-a56b-d2a5db9c1229' for executor b1f40647-a2ff-475d-a56b-d2a5db9c1229 of framework '20140626-000439-1032504131-55423-5450-0001
I0626 00:04:40.585026  5474 slave.cpp:562] Successfully attached file '/tmp/SlaveRecoveryTest_0_MultipleFrameworks_G6ObtK/slaves/20140626-000439-1032504131-55423-5450-0/frameworks/20140626-000439-1032504131-55423-5450-0001/executors/b1f40647-a2ff-475d-a56b-d2a5db9c1229/runs/44b9f0a1-fcf4-4b33-b6dc-2d886304e8b3'
I0626 00:04:40.586937  5476 launcher.cpp:137] Forked child with pid '7656' for container '44b9f0a1-fcf4-4b33-b6dc-2d886304e8b3'
I0626 00:04:40.587131  5476 containerizer.cpp:705] Checkpointing executor's forked pid 7656 to '/tmp/SlaveRecoveryTest_0_MultipleFrameworks_G6ObtK/meta/slaves/20140626-000439-1032504131-55423-5450-0/frameworks/20140626-000439-1032504131-55423-5450-0001/executors/b1f40647-a2ff-475d-a56b-d2a5db9c1229/runs/44b9f0a1-fcf4-4b33-b6dc-2d886304e8b3/pids/forked.pid'
I0626 00:04:40.587872  5477 containerizer.cpp:537] Fetching URIs for container '44b9f0a1-fcf4-4b33-b6dc-2d886304e8b3' using command '/home/jenkins/jenkins-slave/workspace/Mesos-Trunk-Ubuntu-Build-Out-Of-Src-Disable-Java-Disable-Python-Disable-Webui/build/src/mesos-fetcher'
I0626 00:04:41.384660  5472 slave.cpp:2470] Monitoring executor 'b1f40647-a2ff-475d-a56b-d2a5db9c1229' of framework '20140626-000439-1032504131-55423-5450-0001' in container '44b9f0a1-fcf4-4b33-b6dc-2d886304e8b3'
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0626 00:04:41.417649  7691 process.cpp:1671] libprocess is initialized on 67.195.138.61:40524 for 8 cpus
I0626 00:04:41.418674  7691 exec.cpp:131] Version: 0.20.0
I0626 00:04:41.420272  7712 exec.cpp:181] Executor started at: executor(1)@67.195.138.61:40524 with pid 7691
I0626 00:04:41.420771  5477 slave.cpp:1734] Got registration for executor 'b1f40647-a2ff-475d-a56b-d2a5db9c1229' of framework 20140626-000439-1032504131-55423-5450-0001
I0626 00:04:41.420871  5477 slave.cpp:1819] Checkpointing executor pid 'executor(1)@67.195.138.61:40524' to '/tmp/SlaveRecoveryTest_0_MultipleFrameworks_G6ObtK/meta/slaves/20140626-000439-1032504131-55423-5450-0/frameworks/20140626-000439-1032504131-55423-5450-0001/executors/b1f40647-a2ff-475d-a56b-d2a5db9c1229/runs/44b9f0a1-fcf4-4b33-b6dc-2d886304e8b3/pids/libprocess.pid'
I0626 00:04:41.421335  5477 slave.cpp:1853] Flushing queued task b1f40647-a2ff-475d-a56b-d2a5db9c1229 for executor 'b1f40647-a2ff-475d-a56b-d2a5db9c1229' of framework 20140626-000439-1032504131-55423-5450-0001
I0626 00:04:41.421401  5479 process.cpp:1098] Socket closed while receiving
I0626 00:04:41.421506  5479 process.cpp:1098] Socket closed while receiving
I0626 00:04:41.421622  7709 exec.cpp:205] Executor registered on slave 20140626-000439-1032504131-55423-5450-0
I0626 00:04:41.421701  7713 process.cpp:1037] Socket closed while receiving
I0626 00:04:41.421891  7713 process.cpp:1037] Socket closed while receiving
I0626 00:04:41.422695  7709 exec.cpp:217] Executor::registered took 116729ns
Registered executor on juno.apache.org
I0626 00:04:41.422817  7709 exec.cpp:292] Executor asked to run task 'b1f40647-a2ff-475d-a56b-d2a5db9c1229'
Starting task b1f40647-a2ff-475d-a56b-d2a5db9c1229
I0626 00:04:41.422878  7709 exec.cpp:301] Executor::launchTask took 44617ns
Forked command at 7714
sh -c 'sleep 1000'
I0626 00:04:41.424744  7710 exec.cpp:524] Executor sending status update TASK_RUNNING (UUID: 7994ad88-77f5-45a5-91bf-b1f4957fba87) for task b1f40647-a2ff-475d-a56b-d2a5db9c1229 of framework 20140626-000439-1032504131-55423-5450-0001
I0626 00:04:41.425102  5473 slave.cpp:2088] Handling status update TASK_RUNNING (UUID: 7994ad88-77f5-45a5-91bf-b1f4957fba87) for task b1f40647-a2ff-475d-a56b-d2a5db9c1229 of framework 20140626-000439-1032504131-55423-5450-0001 from executor(1)@67.195.138.61:40524
I0626 00:04:41.425271  5472 status_update_manager.cpp:320] Received status update TASK_RUNNING (UUID: 7994ad88-77f5-45a5-91bf-b1f4957fba87) for task b1f40647-a2ff-475d-a56b-d2a5db9c1229 of framework 20140626-000439-1032504131-55423-5450-0001
I0626 00:04:41.425309  5472 status_update_manager.cpp:499] Creating StatusUpdate stream for task b1f40647-a2ff-475d-a56b-d2a5db9c1229 of framework 20140626-000439-1032504131-55423-5450-0001
I0626 00:04:41.425585  5472 status_update_manager.hpp:342] Checkpointing UPDATE for status update TASK_RUNNING (UUID: 7994ad88-77f5-45a5-91bf-b1f4957fba87) for task b1f40647-a2ff-475d-a56b-d2a5db9c1229 of framework 20140626-000439-1032504131-55423-5450-0001
I0626 00:04:41.517669  5472 status_update_manager.cpp:373] Forwarding status update TASK_RUNNING (UUID: 7994ad88-77f5-45a5-91bf-b1f4957fba87) for task b1f40647-a2ff-475d-a56b-d2a5db9c1229 of framework 20140626-000439-1032504131-55423-5450-0001 to master@67.195.138.61:55423
I0626 00:04:41.517848  5474 slave.cpp:2246] Status update manager successfully handled status update TASK_RUNNING (UUID: 7994ad88-77f5-45a5-91bf-b1f4957fba87) for task b1f40647-a2ff-475d-a56b-d2a5db9c1229 of framework 20140626-000439-1032504131-55423-5450-0001
I0626 00:04:41.517870  5474 slave.cpp:2252] Sending acknowledgement for status update TASK_RUNNING (UUID: 7994ad88-77f5-45a5-91bf-b1f4957fba87) for task b1f40647-a2ff-475d-a56b-d2a5db9c1229 of framework 20140626-000439-1032504131-55423-5450-0001 to executor(1)@67.195.138.61:40524
I0626 00:04:41.517985  5471 master.cpp:3107] Status update TASK_RUNNING (UUID: 7994ad88-77f5-45a5-91bf-b1f4957fba87) for task b1f40647-a2ff-475d-a56b-d2a5db9c1229 of framework 20140626-000439-1032504131-55423-5450-0001 from slave 20140626-000439-1032504131-55423-5450-0 at slave(173)@67.195.138.61:55423 (juno.apache.org)
I0626 00:04:41.518061  5473 sched.cpp:637] Scheduler::statusUpdate took 30727ns
I0626 00:04:41.518087  5479 process.cpp:1098] Socket closed while receiving
I0626 00:04:41.518188  5473 master.cpp:2631] Forwarding status update acknowledgement 7994ad88-77f5-45a5-91bf-b1f4957fba87 for task b1f40647-a2ff-475d-a56b-d2a5db9c1229 of framework 20140626-000439-1032504131-55423-5450-0001 to slave 20140626-000439-1032504131-55423-5450-0 at slave(173)@67.195.138.61:55423 (juno.apache.org)
I0626 00:04:41.518209  7705 exec.cpp:338] Executor received status update acknowledgement 7994ad88-77f5-45a5-91bf-b1f4957fba87 for task b1f40647-a2ff-475d-a56b-d2a5db9c1229 of framework 20140626-000439-1032504131-55423-5450-0001
I0626 00:04:41.518237  7713 process.cpp:1037] Socket closed while receiving
I0626 00:04:41.518332  5477 status_update_manager.cpp:398] Received status update acknowledgement (UUID: 7994ad88-77f5-45a5-91bf-b1f4957fba87) for task b1f40647-a2ff-475d-a56b-d2a5db9c1229 of framework 20140626-000439-1032504131-55423-5450-0001
I0626 00:04:41.518358  5477 status_update_manager.hpp:342] Checkpointing ACK for status update TASK_RUNNING (UUID: 7994ad88-77f5-45a5-91bf-b1f4957fba87) for task b1f40647-a2ff-475d-a56b-d2a5db9c1229 of framework 20140626-000439-1032504131-55423-5450-0001
I0626 00:04:41.565961  5477 slave.cpp:1674] Status update manager successfully handled status update acknowledgement (UUID: 7994ad88-77f5-45a5-91bf-b1f4957fba87) for task b1f40647-a2ff-475d-a56b-d2a5db9c1229 of framework 20140626-000439-1032504131-55423-5450-0001
I0626 00:04:41.566172  5450 slave.cpp:486] Slave terminating
I0626 00:04:41.566315  5476 master.cpp:760] Slave 20140626-000439-1032504131-55423-5450-0 at slave(173)@67.195.138.61:55423 (juno.apache.org) disconnected
I0626 00:04:41.566337  5476 master.cpp:1602] Disconnecting slave 20140626-000439-1032504131-55423-5450-0
I0626 00:04:41.566411  5473 hierarchical_allocator_process.hpp:483] Slave 20140626-000439-1032504131-55423-5450-0 disconnected
I0626 00:04:41.567461  5450 containerizer.cpp:124] Using isolation: posix/cpu,posix/mem
I0626 00:04:41.569854  5477 slave.cpp:168] Slave started on 174)@67.195.138.61:55423
I0626 00:04:41.569874  5477 credentials.hpp:35] Loading credentials for authentication from '/tmp/SlaveRecoveryTest_0_MultipleFrameworks_G6ObtK/credential'
I0626 00:04:41.569941  5477 slave.cpp:268] Slave using credential for: test-principal
I0626 00:04:41.570065  5477 slave.cpp:281] Slave resources: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I0626 00:04:41.570139  5477 slave.cpp:326] Slave hostname: juno.apache.org
I0626 00:04:41.570148  5477 slave.cpp:327] Slave checkpoint: true
I0626 00:04:41.570361  5478 hierarchical_allocator_process.hpp:833] Filtered  on slave 20140626-000439-1032504131-55423-5450-0 for framework 20140626-000439-1032504131-55423-5450-0000
I0626 00:04:41.570382  5478 hierarchical_allocator_process.hpp:686] Performed allocation for 1 slaves in 97062ns
I0626 00:04:41.570710  5476 state.cpp:33] Recovering state from '/tmp/SlaveRecoveryTest_0_MultipleFrameworks_G6ObtK/meta'
I0626 00:04:41.572727  5475 slave.cpp:3196] Recovering framework 20140626-000439-1032504131-55423-5450-0001
I0626 00:04:41.572752  5475 slave.cpp:3572] Recovering executor 'b1f40647-a2ff-475d-a56b-d2a5db9c1229' of framework 20140626-000439-1032504131-55423-5450-0001
I0626 00:04:41.572877  5475 slave.cpp:3196] Recovering framework 20140626-000439-1032504131-55423-5450-0000
I0626 00:04:41.572904  5475 slave.cpp:3572] Recovering executor '897522cc-4ec5-4904-aed0-00b6b8c41028' of framework 20140626-000439-1032504131-55423-5450-0000
I0626 00:04:41.573421  5478 status_update_manager.cpp:193] Recovering status update manager
I0626 00:04:41.573436  5478 status_update_manager.cpp:201] Recovering executor 'b1f40647-a2ff-475d-a56b-d2a5db9c1229' of framework 20140626-000439-1032504131-55423-5450-0001
I0626 00:04:41.573470  5478 status_update_manager.cpp:499] Creating StatusUpdate stream for task b1f40647-a2ff-475d-a56b-d2a5db9c1229 of framework 20140626-000439-1032504131-55423-5450-0001
I0626 00:04:41.573627  5478 status_update_manager.hpp:306] Replaying status update stream for task b1f40647-a2ff-475d-a56b-d2a5db9c1229
I0626 00:04:41.573662  5478 status_update_manager.cpp:201] Recovering executor '897522cc-4ec5-4904-aed0-00b6b8c41028' of framework 20140626-000439-1032504131-55423-5450-0000
I0626 00:04:41.573689  5478 status_update_manager.cpp:499] Creating StatusUpdate stream for task 897522cc-4ec5-4904-aed0-00b6b8c41028 of framework 20140626-000439-1032504131-55423-5450-0000
I0626 00:04:41.573804  5478 status_update_manager.hpp:306] Replaying status update stream for task 897522cc-4ec5-4904-aed0-00b6b8c41028
I0626 00:04:41.573848  5475 slave.cpp:562] Successfully attached file '/tmp/SlaveRecoveryTest_0_MultipleFrameworks_G6ObtK/slaves/20140626-000439-1032504131-55423-5450-0/frameworks/20140626-000439-1032504131-55423-5450-0001/executors/b1f40647-a2ff-475d-a56b-d2a5db9c1229/runs/44b9f0a1-fcf4-4b33-b6dc-2d886304e8b3'
I0626 00:04:41.573881  5475 slave.cpp:562] Successfully attached file '/tmp/SlaveRecoveryTest_0_MultipleFrameworks_G6ObtK/slaves/20140626-000439-1032504131-55423-5450-0/frameworks/20140626-000439-1032504131-55423-5450-0000/executors/897522cc-4ec5-4904-aed0-00b6b8c41028/runs/9ad3a5ac-3587-47df-96c2-df76ea09328c'
I0626 00:04:41.574369  5477 containerizer.cpp:287] Recovering containerizer
I0626 00:04:41.574404  5477 containerizer.cpp:329] Recovering container '44b9f0a1-fcf4-4b33-b6dc-2d886304e8b3' for executor 'b1f40647-a2ff-475d-a56b-d2a5db9c1229' of framework 20140626-000439-1032504131-55423-5450-0001
I0626 00:04:41.574440  5477 containerizer.cpp:329] Recovering container '9ad3a5ac-3587-47df-96c2-df76ea09328c' for executor '897522cc-4ec5-4904-aed0-00b6b8c41028' of framework 20140626-000439-1032504131-55423-5450-0000
I0626 00:04:41.575889  5476 slave.cpp:3069] Sending reconnect request to executor 897522cc-4ec5-4904-aed0-00b6b8c41028 of framework 20140626-000439-1032504131-55423-5450-0000 at executor(1)@67.195.138.61:40619
I0626 00:04:41.576014  5476 slave.cpp:3069] Sending reconnect request to executor b1f40647-a2ff-475d-a56b-d2a5db9c1229 of framework 20140626-000439-1032504131-55423-5450-0001 at executor(1)@67.195.138.61:40524
I0626 00:04:41.576128  5479 process.cpp:1098] Socket closed while receiving
I0626 00:04:41.576170  7645 exec.cpp:251] Received reconnect request from slave 20140626-000439-1032504131-55423-5450-0
I0626 00:04:41.576202  7653 process.cpp:1037] Socket closed while receiving
I0626 00:04:41.576230  5479 process.cpp:1098] Socket closed while receiving
I0626 00:04:41.576308  7705 exec.cpp:251] Received reconnect request from slave 20140626-000439-1032504131-55423-5450-0
I0626 00:04:41.576328  7713 process.cpp:1037] Socket closed while receiving
I0626 00:04:41.576519  5472 slave.cpp:1913] Re-registering executor 897522cc-4ec5-4904-aed0-00b6b8c41028 of framework 20140626-000439-1032504131-55423-5450-0000
I0626 00:04:41.576658  5479 process.cpp:1098] Socket closed while receiving
I0626 00:04:41.576730  7653 process.cpp:1037] Socket closed while receiving
I0626 00:04:41.576750  7650 exec.cpp:228] Executor re-registered on slave 20140626-000439-1032504131-55423-5450-0
IRe-registered executor on juno.apache.org
0626 00:04:41.577729  7650 exec.cpp:240] Executor::reregistered took 50146ns
I0626 00:04:41.590677  5476 hierarchical_allocator_process.hpp:833] Filtered  on slave 20140626-000439-1032504131-55423-5450-0 for framework 20140626-000439-1032504131-55423-5450-0000
I0626 00:04:41.590695  5475 slave.cpp:2037] Cleaning up un-reregistered executors
I0626 00:04:41.590701  5476 hierarchical_allocator_process.hpp:686] Performed allocation for 1 slaves in 56695ns
I0626 00:04:41.590706  5475 slave.cpp:2055] Killing un-reregistered executor 'b1f40647-a2ff-475d-a56b-d2a5db9c1229' of framework 20140626-000439-1032504131-55423-5450-0001
I0626 00:04:41.590744  5475 slave.cpp:3128] Finished recovery
I0626 00:04:41.590900  5474 containerizer.cpp:903] Destroying container '44b9f0a1-fcf4-4b33-b6dc-2d886304e8b3'
I0626 00:04:41.592074  5472 slave.cpp:601] New master detected at master@67.195.138.61:55423
I0626 00:04:41.592099  5472 slave.cpp:677] Authenticating with master master@67.195.138.61:55423
I0626 00:04:41.592154  5472 slave.cpp:650] Detecting new master
I0626 00:04:41.592196  5472 status_update_manager.cpp:167] New master detected at master@67.195.138.61:55423
W0626 00:04:41.592607  5477 slave.cpp:1906] Shutting down executor 'b1f40647-a2ff-475d-a56b-d2a5db9c1229' of framework 20140626-000439-1032504131-55423-5450-0001 because the slave is not in recovery mode
I0626 00:04:41.592816  5479 process.cpp:1098] Socket closed while receiving
I0626 00:04:41.592881  7711 exec.cpp:378] Executor asked to shutdown
I0626 00:04:41.592921  7713 process.cpp:1037] Socket closed while receiving
I0626 00:04:41.592954  7705 exec.cpp:77] Scheduling shutdown of the executor
IShutting down
0626 00:04:41.592994  7711 exec.cpp:393] Executor::shutdown took 49357ns
Sending SIGTERM to process tree at pid 7714
I0626 00:04:41.594029  5471 authenticatee.hpp:128] Creating new client SASL connection
I0626 00:04:41.594419  5472 master.cpp:3499] Authenticating slave(174)@67.195.138.61:55423
I0626 00:04:41.594646  5476 authenticator.hpp:156] Creating new server SASL connection
I0626 00:04:41.594898  5476 authenticatee.hpp:219] Received SASL authentication mechanisms: CRAM-MD5
I0626 00:04:41.594923  5476 authenticatee.hpp:245] Attempting to authenticate with mechanism 'CRAM-MD5'
I0626 00:04:41.594960  5476 authenticator.hpp:262] Received SASL authentication start
I0626 00:04:41.595002  5476 authenticator.hpp:384] Authentication requires more steps
I0626 00:04:41.595039  5476 authenticatee.hpp:265] Received SASL authentication step
I0626 00:04:41.595095  5476 authenticator.hpp:290] Received SASL authentication step
I0626 00:04:41.595115  5476 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'juno.apache.org' server FQDN: 'juno.apache.org' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0626 00:04:41.595124  5476 auxprop.cpp:153] Looking up auxiliary property '*userPassword'
I0626 00:04:41.595141  5476 auxprop.cpp:153] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0626 00:04:41.595155  5476 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'juno.apache.org' server FQDN: 'juno.apache.org' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0626 00:04:41.595162  5476 auxprop.cpp:103] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0626 00:04:41.595168  5476 auxprop.cpp:103] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0626 00:04:41.595181  5476 authenticator.hpp:376] Authentication success
I0626 00:04:41.595219  5476 authenticatee.hpp:305] Authentication success
I0626 00:04:41.595252  5476 master.cpp:3539] Successfully authenticated principal 'test-principal' at slave(174)@67.195.138.61:55423
I0626 00:04:41.595978  5471 slave.cpp:734] Successfully authenticated with master master@67.195.138.61:55423
I0626 00:04:41.596087  5471 slave.cpp:972] Will retry registration in 5.904051ms if necessary
W0626 00:04:41.596179  5476 master.cpp:2896] Slave at slave(174)@67.195.138.61:55423 (juno.apache.org) is being allowed to re-register with an already in use id (20140626-000439-1032504131-55423-5450-0)
I0626 00:04:41.596371  5476 slave.cpp:818] Re-registered with master master@67.195.138.61:55423
I0626 00:04:41.596407  5476 slave.cpp:1584] Updating framework 20140626-000439-1032504131-55423-5450-0000 pid to scheduler-e66c50d2-2790-4d20-bc77-a57af0e1780b@67.195.138.61:55423
I0626 00:04:41.596454  5476 slave.cpp:1592] Checkpointing framework pid 'scheduler-e66c50d2-2790-4d20-bc77-a57af0e1780b@67.195.138.61:55423' to '/tmp/SlaveRecoveryTest_0_MultipleFrameworks_G6ObtK/meta/slaves/20140626-000439-1032504131-55423-5450-0/frameworks/20140626-000439-1032504131-55423-5450-0000/framework.pid'
I0626 00:04:41.596570  5476 slave.cpp:1584] Updating framework 20140626-000439-1032504131-55423-5450-0001 pid to scheduler-bb54dd52-95dc-4ed9-b69c-7a65f1661180@67.195.138.61:55423
I0626 00:04:41.596608  5476 slave.cpp:1592] Checkpointing framework pid 'scheduler-bb54dd52-95dc-4ed9-b69c-7a65f1661180@67.195.138.61:55423' to '/tmp/SlaveRecoveryTest_0_MultipleFrameworks_G6ObtK/meta/slaves/20140626-000439-1032504131-55423-5450-0/frameworks/20140626-000439-1032504131-55423-5450-0001/framework.pid'
I0626 00:04:41.596710  5476 hierarchical_allocator_process.hpp:497] Slave 20140626-000439-1032504131-55423-5450-0 reconnected
I0626 00:04:41.597498  5476 master.cpp:2461] Asked to kill task 897522cc-4ec5-4904-aed0-00b6b8c41028 of framework 20140626-000439-1032504131-55423-5450-0000
I0626 00:04:41.597523  5476 master.cpp:2562] Telling slave 20140626-000439-1032504131-55423-5450-0 at slave(174)@67.195.138.61:55423 (juno.apache.org) to kill task 897522cc-4ec5-4904-aed0-00b6b8c41028 of framework 20140626-000439-1032504131-55423-5450-0000
I0626 00:04:41.597580  5476 slave.cpp:1279] Asked to kill task 897522cc-4ec5-4904-aed0-00b6b8c41028 of framework 20140626-000439-1032504131-55423-5450-0000
I0626 00:04:41.597724  5479 process.cpp:1098] Socket closed while receiving
I0626 00:04:41.597790  7645 exec.cpp:312] Executor asked to kill task '897522cc-4ec5-4904-aed0-00b6b8c41028'
I0626 00:04:41.597796  7653 process.cpp:1037] Socket closed while receiving
I0626 00:04:41.597843  7645 exec.cpp:321] Executor::killTask took 26639ns
Shutting down
Sending SIGTERM to process tree at pid 7654
Killing the following process trees:
[ 
-+- 7654 sh -c sleep 1000 
 \--- 7655 sleep 1000 
]
I0626 00:04:41.656000  5479 process.cpp:1037] Socket closed while receiving
Command terminated with signal Terminated (pid: 7654)
I0626 00:04:42.421964  7649 exec.cpp:524] Executor sending status update TASK_KILLED (UUID: 3bd1b60e-8496-4254-8188-c160b6a7e498) for task 897522cc-4ec5-4904-aed0-00b6b8c41028 of framework 20140626-000439-1032504131-55423-5450-0000
I0626 00:04:42.422332  5477 slave.cpp:2088] Handling status update TASK_KILLED (UUID: 3bd1b60e-8496-4254-8188-c160b6a7e498) for task 897522cc-4ec5-4904-aed0-00b6b8c41028 of framework 20140626-000439-1032504131-55423-5450-0000 from executor(1)@67.195.138.61:40619
I0626 00:04:42.422384  5477 slave.cpp:3770] Terminating task 897522cc-4ec5-4904-aed0-00b6b8c41028
I0626 00:04:42.422912  5472 status_update_manager.cpp:320] Received status update TASK_KILLED (UUID: 3bd1b60e-8496-4254-8188-c160b6a7e498) for task 897522cc-4ec5-4904-aed0-00b6b8c41028 of framework 20140626-000439-1032504131-55423-5450-0000
I0626 00:04:42.422946  5472 status_update_manager.hpp:342] Checkpointing UPDATE for status update TASK_KILLED (UUID: 3bd1b60e-8496-4254-8188-c160b6a7e498) for task 897522cc-4ec5-4904-aed0-00b6b8c41028 of framework 20140626-000439-1032504131-55423-5450-0000
I0626 00:04:42.558498  5472 status_update_manager.cpp:373] Forwarding status update TASK_KILLED (UUID: 3bd1b60e-8496-4254-8188-c160b6a7e498) for task 897522cc-4ec5-4904-aed0-00b6b8c41028 of framework 20140626-000439-1032504131-55423-5450-0000 to master@67.195.138.61:55423
I0626 00:04:42.558712  5477 slave.cpp:2246] Status update manager successfully handled status update TASK_KILLED (UUID: 3bd1b60e-8496-4254-8188-c160b6a7e498) for task 897522cc-4ec5-4904-aed0-00b6b8c41028 of framework 20140626-000439-1032504131-55423-5450-0000
I0626 00:04:42.558743  5477 slave.cpp:2252] Sending acknowledgement for status update TASK_KILLED (UUID: 3bd1b60e-8496-4254-8188-c160b6a7e498) for task 897522cc-4ec5-4904-aed0-00b6b8c41028 of framework 20140626-000439-1032504131-55423-5450-0000 to executor(1)@67.195.138.61:40619
I0626 00:04:42.558749  5476 master.cpp:3107] Status update TASK_KILLED (UUID: 3bd1b60e-8496-4254-8188-c160b6a7e498) for task 897522cc-4ec5-4904-aed0-00b6b8c41028 of framework 20140626-000439-1032504131-55423-5450-0000 from slave 20140626-000439-1032504131-55423-5450-0 at slave(174)@67.195.138.61:55423 (juno.apache.org)
I0626 00:04:42.558820  5476 master.hpp:784] Removing task 897522cc-4ec5-4904-aed0-00b6b8c41028 with resources cpus(*):1; mem(*):512 on slave 20140626-000439-1032504131-55423-5450-0 (juno.apache.org)
I0626 00:04:42.558917  5478 sched.cpp:637] Scheduler::statusUpdate took 40786ns
I0626 00:04:42.559017  5479 process.cpp:1098] Socket closed while receiving
I0626 00:04:42.559092  7653 process.cpp:1037] Socket closed while receiving
I0626 00:04:42.559100  7650 exec.cpp:338] Executor received status update acknowledgement 3bd1b60e-8496-4254-8188-c160b6a7e498 for task 897522cc-4ec5-4904-aed0-00b6b8c41028 of framework 20140626-000439-1032504131-55423-5450-0000
I0626 00:04:42.559386  5471 master.cpp:2631] Forwarding status update acknowledgement 3bd1b60e-8496-4254-8188-c160b6a7e498 for task 897522cc-4ec5-4904-aed0-00b6b8c41028 of framework 20140626-000439-1032504131-55423-5450-0000 to slave 20140626-000439-1032504131-55423-5450-0 at slave(174)@67.195.138.61:55423 (juno.apache.org)
I0626 00:04:42.559453  5474 hierarchical_allocator_process.hpp:635] Recovered cpus(*):1; mem(*):512 (total allocatable: cpus(*):1; mem(*):512) on slave 20140626-000439-1032504131-55423-5450-0 from framework 20140626-000439-1032504131-55423-5450-0000
I0626 00:04:42.559494  5471 master.cpp:2461] Asked to kill task b1f40647-a2ff-475d-a56b-d2a5db9c1229 of framework 20140626-000439-1032504131-55423-5450-0001
I0626 00:04:42.559516  5471 master.cpp:2562] Telling slave 20140626-000439-1032504131-55423-5450-0 at slave(174)@67.195.138.61:55423 (juno.apache.org) to kill task b1f40647-a2ff-475d-a56b-d2a5db9c1229 of framework 20140626-000439-1032504131-55423-5450-0001
I0626 00:04:42.559541  5474 status_update_manager.cpp:398] Received status update acknowledgement (UUID: 3bd1b60e-8496-4254-8188-c160b6a7e498) for task 897522cc-4ec5-4904-aed0-00b6b8c41028 of framework 20140626-000439-1032504131-55423-5450-0000
I0626 00:04:42.559577  5474 status_update_manager.hpp:342] Checkpointing ACK for status update TASK_KILLED (UUID: 3bd1b60e-8496-4254-8188-c160b6a7e498) for task 897522cc-4ec5-4904-aed0-00b6b8c41028 of framework 20140626-000439-1032504131-55423-5450-0000
I0626 00:04:42.559608  5472 slave.cpp:1279] Asked to kill task b1f40647-a2ff-475d-a56b-d2a5db9c1229 of framework 20140626-000439-1032504131-55423-5450-0001
W0626 00:04:42.559625  5472 slave.cpp:1364] Ignoring kill task b1f40647-a2ff-475d-a56b-d2a5db9c1229 of framework 20140626-000439-1032504131-55423-5450-0001 because the executor 'b1f40647-a2ff-475d-a56b-d2a5db9c1229' is terminating/terminated
I0626 00:04:42.569269  5476 master.cpp:122] No whitelist given. Advertising offers for all slaves
I0626 00:04:42.591553  5478 containerizer.cpp:1019] Executor for container '44b9f0a1-fcf4-4b33-b6dc-2d886304e8b3' has exited
I0626 00:04:42.591665  5477 hierarchical_allocator_process.hpp:833] Filtered cpus(*):1; mem(*):512 on slave 20140626-000439-1032504131-55423-5450-0 for framework 20140626-000439-1032504131-55423-5450-0000
I0626 00:04:42.591794  5477 hierarchical_allocator_process.hpp:750] Offering cpus(*):1; mem(*):512 on slave 20140626-000439-1032504131-55423-5450-0 to framework 20140626-000439-1032504131-55423-5450-0001
I0626 00:04:42.591970  5477 hierarchical_allocator_process.hpp:686] Performed allocation for 1 slaves in 352174ns
I0626 00:04:42.592067  5471 master.hpp:794] Adding offer 20140626-000439-1032504131-55423-5450-2 with resources cpus(*):1; mem(*):512 on slave 20140626-000439-1032504131-55423-5450-0 (juno.apache.org)
I0626 00:04:42.592103  5471 master.cpp:3446] Sending 1 offers to framework 20140626-000439-1032504131-55423-5450-0001
I0626 00:04:42.592118  5473 slave.cpp:2528] Executor 'b1f40647-a2ff-475d-a56b-d2a5db9c1229' of framework 20140626-000439-1032504131-55423-5450-0001 terminated with signal Killed
E0626 00:04:42.592233  5477 slave.cpp:2796] Failed to unmonitor container for executor b1f40647-a2ff-475d-a56b-d2a5db9c1229 of framework 20140626-000439-1032504131-55423-5450-0001: Not monitored
I0626 00:04:42.592279  5472 sched.cpp:546] Scheduler::resourceOffers took 32048ns
I0626 00:04:42.592439  5472 master.hpp:804] Removing offer 20140626-000439-1032504131-55423-5450-2 with resources cpus(*):1; mem(*):512 on slave 20140626-000439-1032504131-55423-5450-0 (juno.apache.org)
I0626 00:04:42.592495  5472 master.cpp:2125] Processing reply for offers: [ 20140626-000439-1032504131-55423-5450-2 ] on slave 20140626-000439-1032504131-55423-5450-0 at slave(174)@67.195.138.61:55423 (juno.apache.org) for framework 20140626-000439-1032504131-55423-5450-0001
I0626 00:04:42.592707  5475 hierarchical_allocator_process.hpp:546] Framework 20140626-000439-1032504131-55423-5450-0001 left cpus(*):1; mem(*):512 unused on slave 20140626-000439-1032504131-55423-5450-0
I0626 00:04:42.592865  5475 hierarchical_allocator_process.hpp:588] Framework 20140626-000439-1032504131-55423-5450-0001 filtered slave 20140626-000439-1032504131-55423-5450-0 for 5secs
I0626 00:04:42.593211  5473 slave.cpp:2088] Handling status update TASK_FAILED (UUID: 69181ee5-c620-4d1a-b5d2-d7cd03a0bc7e) for task b1f40647-a2ff-475d-a56b-d2a5db9c1229 of framework 20140626-000439-1032504131-55423-5450-0001 from @0.0.0.0:0
I0626 00:04:42.593237  5473 slave.cpp:3770] Terminating task b1f40647-a2ff-475d-a56b-d2a5db9c1229
W0626 00:04:42.593387  5472 containerizer.cpp:809] Ignoring update for unknown container: 44b9f0a1-fcf4-4b33-b6dc-2d886304e8b3
I0626 00:04:42.600702  5474 status_update_manager.cpp:530] Cleaning up status update stream for task 897522cc-4ec5-4904-aed0-00b6b8c41028 of framework 20140626-000439-1032504131-55423-5450-0000
I0626 00:04:42.600874  5473 slave.cpp:1674] Status update manager successfully handled status update acknowledgement (UUID: 3bd1b60e-8496-4254-8188-c160b6a7e498) for task 897522cc-4ec5-4904-aed0-00b6b8c41028 of framework 20140626-000439-1032504131-55423-5450-0000
I0626 00:04:42.600895  5473 slave.cpp:3812] Completing task 897522cc-4ec5-4904-aed0-00b6b8c41028
I0626 00:04:42.600913  5474 status_update_manager.cpp:320] Received status update TASK_FAILED (UUID: 69181ee5-c620-4d1a-b5d2-d7cd03a0bc7e) for task b1f40647-a2ff-475d-a56b-d2a5db9c1229 of framework 20140626-000439-1032504131-55423-5450-0001
I0626 00:04:42.600939  5474 status_update_manager.hpp:342] Checkpointing UPDATE for status update TASK_FAILED (UUID: 69181ee5-c620-4d1a-b5d2-d7cd03a0bc7e) for task b1f40647-a2ff-475d-a56b-d2a5db9c1229 of framework 20140626-000439-1032504131-55423-5450-0001
I0626 00:04:42.634199  5474 status_update_manager.cpp:373] Forwarding status update TASK_FAILED (UUID: 69181ee5-c620-4d1a-b5d2-d7cd03a0bc7e) for task b1f40647-a2ff-475d-a56b-d2a5db9c1229 of framework 20140626-000439-1032504131-55423-5450-0001 to master@67.195.138.61:55423
I0626 00:04:42.634354  5475 master.cpp:3107] Status update TASK_FAILED (UUID: 69181ee5-c620-4d1a-b5d2-d7cd03a0bc7e) for task b1f40647-a2ff-475d-a56b-d2a5db9c1229 of framework 20140626-000439-1032504131-55423-5450-0001 from slave 20140626-000439-1032504131-55423-5450-0 at slave(174)@67.195.138.61:55423 (juno.apache.org)
I0626 00:04:42.634373  5477 slave.cpp:2246] Status update manager successfully handled status update TASK_FAILED (UUID: 69181ee5-c620-4d1a-b5d2-d7cd03a0bc7e) for task b1f40647-a2ff-475d-a56b-d2a5db9c1229 of framework 20140626-000439-1032504131-55423-5450-0001
I0626 00:04:42.634428  5473 sched.cpp:637] Scheduler::statusUpdate took 22610ns
I0626 00:04:42.634520  5475 master.hpp:784] Removing task b1f40647-a2ff-475d-a56b-d2a5db9c1229 with resources cpus(*):1; mem(*):512; disk(*):1024; ports(*):[31000-32000] on slave 20140626-000439-1032504131-55423-5450-0 (juno.apache.org)
../../src/tests/slave_recovery_tests.cpp:2930: Failure
Value of: status2.get().state()
  Actual: TASK_FAILED
Expected: TASK_KILLED
I0626 00:04:42.634699  5475 master.cpp:2631] Forwarding status update acknowledgement 69181ee5-c620-4d1a-b5d2-d7cd03a0bc7e for task b1f40647-a2ff-475d-a56b-d2a5db9c1229 of framework 20140626-000439-1032504131-55423-5450-0001 to slave 20140626-000439-1032504131-55423-5450-0 at slave(174)@67.195.138.61:55423 (juno.apache.org)
I0626 00:04:42.634778  5472 hierarchical_allocator_process.hpp:635] Recovered cpus(*):1; mem(*):512; disk(*):1024; ports(*):[31000-32000] (total allocatable: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]) on slave 20140626-000439-1032504131-55423-5450-0 from framework 20140626-000439-1032504131-55423-5450-0001
I0626 00:04:42.634804  5475 status_update_manager.cpp:398] Received status update acknowledgement (UUID: 69181ee5-c620-4d1a-b5d2-d7cd03a0bc7e) for task b1f40647-a2ff-475d-a56b-d2a5db9c1229 of framework 20140626-000439-1032504131-55423-5450-0001
I0626 00:04:42.634836  5475 status_update_manager.hpp:342] Checkpointing ACK for status update TASK_FAILED (UUID: 69181ee5-c620-4d1a-b5d2-d7cd03a0bc7e) for task b1f40647-a2ff-475d-a56b-d2a5db9c1229 of framework 20140626-000439-1032504131-55423-5450-0001
I0626 00:04:42.634843  5472 master.cpp:710] Framework 20140626-000439-1032504131-55423-5450-0001 disconnected
I0626 00:04:42.634857  5472 master.cpp:1577] Deactivating framework 20140626-000439-1032504131-55423-5450-0001
I0626 00:04:42.634881  5472 master.cpp:732] Giving framework 20140626-000439-1032504131-55423-5450-0001 0ns to failover
I0626 00:04:42.635025  5472 hierarchical_allocator_process.hpp:407] Deactivated framework 20140626-000439-1032504131-55423-5450-0001
I0626 00:04:42.635056  5472 master.cpp:3362] Framework failover timeout, removing framework 20140626-000439-1032504131-55423-5450-0001
I0626 00:04:42.635066  5472 master.cpp:3821] Removing framework 20140626-000439-1032504131-55423-5450-0001
I0626 00:04:42.635154  5472 master.cpp:710] Framework 20140626-000439-1032504131-55423-5450-0000 disconnected
I0626 00:04:42.635167  5472 master.cpp:1577] Deactivating framework 20140626-000439-1032504131-55423-5450-0000
I0626 00:04:42.635226  5472 master.cpp:732] Giving framework 20140626-000439-1032504131-55423-5450-0000 0ns to failover
I0626 00:04:42.635254  5478 hierarchical_allocator_process.hpp:362] Removed framework 20140626-000439-1032504131-55423-5450-0001
I0626 00:04:42.635267  5476 slave.cpp:1407] Asked to shut down framework 20140626-000439-1032504131-55423-5450-0001 by master@67.195.138.61:55423
I0626 00:04:42.635285  5476 slave.cpp:1432] Shutting down framework 20140626-000439-1032504131-55423-5450-0001
I0626 00:04:42.635301  5476 slave.cpp:2662] Cleaning up executor 'b1f40647-a2ff-475d-a56b-d2a5db9c1229' of framework 20140626-000439-1032504131-55423-5450-0001
I0626 00:04:42.635308  5478 hierarchical_allocator_process.hpp:407] Deactivated framework 20140626-000439-1032504131-55423-5450-0000
I0626 00:04:42.635340  5478 master.cpp:3362] Framework failover timeout, removing framework 20140626-000439-1032504131-55423-5450-0000
I0626 00:04:42.635349  5478 master.cpp:3821] Removing framework 20140626-000439-1032504131-55423-5450-0000
I0626 00:04:42.635469  5478 hierarchical_allocator_process.hpp:362] Removed framework 20140626-000439-1032504131-55423-5450-0000
I0626 00:04:42.635601  5450 master.cpp:619] Master terminating
I0626 00:04:42.635840  5472 gc.cpp:56] Scheduling '/tmp/SlaveRecoveryTest_0_MultipleFrameworks_G6ObtK/slaves/20140626-000439-1032504131-55423-5450-0/frameworks/20140626-000439-1032504131-55423-5450-0001/executors/b1f40647-a2ff-475d-a56b-d2a5db9c1229/runs/44b9f0a1-fcf4-4b33-b6dc-2d886304e8b3' for gc 6.99999264157333days in the future
I0626 00:04:42.635916  5476 slave.cpp:2737] Cleaning up framework 20140626-000439-1032504131-55423-5450-0001
I0626 00:04:42.635916  5472 gc.cpp:56] Scheduling '/tmp/SlaveRecoveryTest_0_MultipleFrameworks_G6ObtK/slaves/20140626-000439-1032504131-55423-5450-0/frameworks/20140626-000439-1032504131-55423-5450-0001/executors/b1f40647-a2ff-475d-a56b-d2a5db9c1229' for gc 6.99999264090074days in the future
I0626 00:04:42.635960  5472 gc.cpp:56] Scheduling '/tmp/SlaveRecoveryTest_0_MultipleFrameworks_G6ObtK/meta/slaves/20140626-000439-1032504131-55423-5450-0/frameworks/20140626-000439-1032504131-55423-5450-0001/executors/b1f40647-a2ff-475d-a56b-d2a5db9c1229/runs/44b9f0a1-fcf4-4b33-b6dc-2d886304e8b3' for gc 6.99999264048593days in the future
I0626 00:04:42.636015  5472 gc.cpp:56] Scheduling '/tmp/SlaveRecoveryTest_0_MultipleFrameworks_G6ObtK/meta/slaves/20140626-000439-1032504131-55423-5450-0/frameworks/20140626-000439-1032504131-55423-5450-0001/executors/b1f40647-a2ff-475d-a56b-d2a5db9c1229' for gc 6.99999264009185days in the future
I0626 00:04:42.636034  5476 slave.cpp:1407] Asked to shut down framework 20140626-000439-1032504131-55423-5450-0000 by master@67.195.138.61:55423
I0626 00:04:42.636049  5476 slave.cpp:1432] Shutting down framework 20140626-000439-1032504131-55423-5450-0000
I0626 00:04:42.636061  5476 slave.cpp:2808] Shutting down executor '897522cc-4ec5-4904-aed0-00b6b8c41028' of framework 20140626-000439-1032504131-55423-5450-0000
I0626 00:04:42.636064  5472 gc.cpp:56] Scheduling '/tmp/SlaveRecoveryTest_0_MultipleFrameworks_G6ObtK/slaves/20140626-000439-1032504131-55423-5450-0/frameworks/20140626-000439-1032504131-55423-5450-0001' for gc 6.99999263944593days in the future
I0626 00:04:42.636107  5472 gc.cpp:56] Scheduling '/tmp/SlaveRecoveryTest_0_MultipleFrameworks_G6ObtK/meta/slaves/20140626-000439-1032504131-55423-5450-0/frameworks/20140626-000439-1032504131-55423-5450-0001' for gc 6.9999926390963days in the future
I0626 00:04:42.636207  5476 slave.cpp:2332] master@67.195.138.61:55423 exited
W0626 00:04:42.636220  5476 slave.cpp:2335] Master disconnected! Waiting for a new master to be elected
I0626 00:04:42.636307  5479 process.cpp:1098] Socket closed while receiving
I0626 00:04:42.636379  7653 process.cpp:1037] Socket closed while receiving
I0626 00:04:42.636382  7648 exec.cpp:378] Executor asked to shutdown
I0626 00:04:42.636535  7648 exec.cpp:393] Executor::shutdown took 6684ns
I0626 00:04:42.636545  7649 exec.cpp:77] Scheduling shutdown of the executor
I0626 00:04:42.637948  5472 containerizer.cpp:903] Destroying container '9ad3a5ac-3587-47df-96c2-df76ea09328c'
I0626 00:04:42.672613  5479 process.cpp:1037] Socket closed while receiving
I0626 00:04:42.692251  5475 status_update_manager.cpp:530] Cleaning up status update stream for task b1f40647-a2ff-475d-a56b-d2a5db9c1229 of framework 20140626-000439-1032504131-55423-5450-0001
I0626 00:04:42.692435  5475 status_update_manager.cpp:282] Closing status update streams for framework 20140626-000439-1032504131-55423-5450-0001
I0626 00:04:42.692450  5471 slave.cpp:1674] Status update manager successfully handled status update acknowledgement (UUID: 69181ee5-c620-4d1a-b5d2-d7cd03a0bc7e) for task b1f40647-a2ff-475d-a56b-d2a5db9c1229 of framework 20140626-000439-1032504131-55423-5450-0001
E0626 00:04:42.692477  5471 slave.cpp:1685] Status update acknowledgement (UUID: 69181ee5-c620-4d1a-b5d2-d7cd03a0bc7e) for task b1f40647-a2ff-475d-a56b-d2a5db9c1229 of unknown framework 20140626-000439-1032504131-55423-5450-0001
I0626 00:04:43.592118  5473 containerizer.cpp:1019] Executor for container '9ad3a5ac-3587-47df-96c2-df76ea09328c' has exited
I0626 00:04:43.592550  5475 slave.cpp:2528] Executor '897522cc-4ec5-4904-aed0-00b6b8c41028' of framework 20140626-000439-1032504131-55423-5450-0000 terminated with signal Killed
I0626 00:04:43.592599  5475 slave.cpp:2662] Cleaning up executor '897522cc-4ec5-4904-aed0-00b6b8c41028' of framework 20140626-000439-1032504131-55423-5450-0000
I0626 00:04:43.592901  5475 slave.cpp:2737] Cleaning up framework 20140626-000439-1032504131-55423-5450-0000
I0626 00:04:43.592900  5472 gc.cpp:56] Scheduling '/tmp/SlaveRecoveryTest_0_MultipleFrameworks_G6ObtK/slaves/20140626-000439-1032504131-55423-5450-0/frameworks/20140626-000439-1032504131-55423-5450-0000/executors/897522cc-4ec5-4904-aed0-00b6b8c41028/runs/9ad3a5ac-3587-47df-96c2-df76ea09328c' for gc 6.99999313928296days in the future
I0626 00:04:43.592991  5472 gc.cpp:56] Scheduling '/tmp/SlaveRecoveryTest_0_MultipleFrameworks_G6ObtK/slaves/20140626-000439-1032504131-55423-5450-0/frameworks/20140626-000439-1032504131-55423-5450-0000/executors/897522cc-4ec5-4904-aed0-00b6b8c41028' for gc 6.99999313866963days in the future
I0626 00:04:43.592985  5471 status_update_manager.cpp:282] Closing status update streams for framework 20140626-000439-1032504131-55423-5450-0000
I0626 00:04:43.593022  5475 slave.cpp:486] Slave terminating
I0626 00:04:43.593040  5472 gc.cpp:56] Scheduling '/tmp/SlaveRecoveryTest_0_MultipleFrameworks_G6ObtK/meta/slaves/20140626-000439-1032504131-55423-5450-0/frameworks/20140626-000439-1032504131-55423-5450-0000/executors/897522cc-4ec5-4904-aed0-00b6b8c41028/runs/9ad3a5ac-3587-47df-96c2-df76ea09328c' for gc 6.99999313827556days in the future
I0626 00:04:43.593086  5472 gc.cpp:56] Scheduling '/tmp/SlaveRecoveryTest_0_MultipleFrameworks_G6ObtK/meta/slaves/20140626-000439-1032504131-55423-5450-0/frameworks/20140626-000439-1032504131-55423-5450-0000/executors/897522cc-4ec5-4904-aed0-00b6b8c41028' for gc 6.99999313791704days in the future
I0626 00:04:43.593125  5472 gc.cpp:56] Scheduling '/tmp/SlaveRecoveryTest_0_MultipleFrameworks_G6ObtK/slaves/20140626-000439-1032504131-55423-5450-0/frameworks/20140626-000439-1032504131-55423-5450-0000' for gc 6.99999313702518days in the future
I0626 00:04:43.593166  5472 gc.cpp:56] Scheduling '/tmp/SlaveRecoveryTest_0_MultipleFrameworks_G6ObtK/meta/slaves/20140626-000439-1032504131-55423-5450-0/frameworks/20140626-000439-1032504131-55423-5450-0000' for gc 6.99999313664296days in the future
[  FAILED  ] SlaveRecoveryTest/0.MultipleFrameworks, where TypeParam = mesos::internal::slave::MesosContainerizer (4218 ms)

{code}",Bug,Major,vinodkone,2016-01-17T08:37:18.000+0000,5,Resolved,Complete,SlaveRecoveryTest/0.MultipleFrameworks is flaky,2016-01-17T08:37:18.000+0000,MESOS-1545,1.0,mesos,Q2 Sprint 5
bmahler,2014-06-22T23:42:15.000+0000,dhamon,"If a network partition occurs between a Master and Slave, the Master will remove the Slave (as it fails health check) and mark the tasks being run there as LOST. However, the Slave is not aware that it has been removed so the tasks will continue to run.

(To clarify a little bit: neither the master nor the slave receives 'exited' event, indicating that the connection between the master and slave is not closed).

There are at least two possible approaches to solving this issue:

1. Introduce a health check from Slave to Master so they have a consistent view of a network partition. We may still see this issue should a one-way connection error occur.

2. Be less aggressive about marking tasks and Slaves as lost. Wait until the Slave reappears and reconcile then. We'd still need to mark Slaves and tasks as potentially lost (zombie state) but maybe the Scheduler can make a more intelligent decision.",Bug,Major,dhamon,2014-08-04T22:04:58.000+0000,5,Resolved,Complete,Handle a network partition between Master and Slave,2015-01-08T07:06:01.000+0000,MESOS-1529,5.0,mesos,Q3 Sprint 1
benjaminhindman,2014-06-21T00:05:00.000+0000,jaybuff,"Currently you have to choose the containerizer at mesos-slave start time via the --isolation option.  I'd like to be able to specify the containerizer in the request to launch the job. This could be specified by a new ""Provider"" field in the ContainerInfo proto buf.",Improvement,Major,jaybuff,2014-08-14T00:57:54.000+0000,5,Resolved,Complete,Choose containerizer at runtime,2014-08-14T00:57:54.000+0000,MESOS-1527,3.0,mesos,Q3 Sprint 1
xujyan,2014-06-19T21:27:29.000+0000,xujyan,"- Usage
- Design
- Implementation Notes",Improvement,Major,xujyan,2014-08-20T21:41:40.000+0000,5,Resolved,Complete,Update Rate Limiting Design doc to reflect the latest changes,2014-08-20T21:41:40.000+0000,MESOS-1518,2.0,mesos,Q2 Sprint 4
idownes,2014-06-11T22:29:09.000+0000,idownes,"When restarting many slaves there's a reasonable chance that a slave will be restarted between the fork and exec stages of launching an executor in the MesosContainerizer.

The forked child correctly detects this however rather than abort it should safely log and then exit non-zero cleanly.",Bug,Major,idownes,2014-06-12T19:28:37.000+0000,5,Resolved,Complete,Improve child exit if slave dies during executor launch in MC,2014-06-12T19:28:37.000+0000,MESOS-1472,1.0,mesos,Q2 Sprint 3
neilc,2014-06-11T03:48:02.000+0000,bmahler,"The replicated log could benefit from some documentation. In particular, how does it work? What do operators need to know? Possibly there is some overlap with our future maintenance documentation in MESOS-1470.

I believe [~jieyu] has some unpublished work that could be leveraged here!",Documentation,Major,bmahler,2016-02-18T19:28:04.000+0000,5,Resolved,Complete,Document replicated log design/internals,2016-02-18T19:28:04.000+0000,MESOS-1471,5.0,mesos,Q3 Sprint 1
vinodkone,2014-06-10T17:35:42.000+0000,dhamon,"When the mesos review build times out, likely due to a long-running failing test, we have no output to debug. We should find a way to stream the output from the build instead of waiting for the build to finish.",Bug,Minor,dhamon,2016-01-29T22:08:41.000+0000,5,Resolved,Complete,No output from review bot on timeout,2016-01-29T22:08:41.000+0000,MESOS-1469,2.0,mesos,Q2 Sprint 3
,2014-06-10T00:32:36.000+0000,vinodkone,"The following sequence of events can cause an overcommit

--> Launch task is called for a task whose executor is already running

--> Executor's resources are not accounted for on the master

--> Executor exits and the event is enqueued behind launch tasks on the master

--> Master sends the task to the slave which needs to commit for resources for task and the (new) executor.

--> Master processes the executor exited event and re-offers the executor's resources causing an overcommit of resources.",Bug,Major,vinodkone,,1,Open,New,Race between executor exited event and launch task can cause overcommit of resources,2014-10-27T18:44:18.000+0000,MESOS-1466,8.0,mesos,Q3 Sprint 3
dhamon,2014-06-06T17:34:37.000+0000,dhamon,"In file included from launcher/main.cpp:19:
In file included from ./launcher/launcher.hpp:24:
In file included from ../3rdparty/libprocess/include/process/future.hpp:23:
../3rdparty/libprocess/include/process/owned.hpp:188:5: error: delete called on 'mesos::internal::launcher::Operation' that is abstract but has non-virtual destructor [-Werror,-Wdelete-non-virtual-dtor]
    delete t;
    ^
/usr/bin/../lib/gcc/x86_64-linux-gnu/4.8/../../../../include/c++/4.8/bits/shared_ptr_base.h:456:8: note: in instantiation of member function 'process::Owned<mesos::internal::launcher::Operation>::Data::~Data' requested here
              delete __p;
              ^
/usr/bin/../lib/gcc/x86_64-linux-gnu/4.8/../../../../include/c++/4.8/bits/shared_ptr_base.h:768:24: note: in instantiation of function template specialization 'std::__shared_count<2>::__shared_count<process::Owned<mesos::internal::launcher::Operation>::Data *>' requested here
        : _M_ptr(__p), _M_refcount(__p)
                       ^
/usr/bin/../lib/gcc/x86_64-linux-gnu/4.8/../../../../include/c++/4.8/bits/shared_ptr_base.h:919:4: note: in instantiation of function template specialization 'std::__shared_ptr<process::Owned<mesos::internal::launcher::Operation>::Data, 2>::__shared_ptr<process::Owned<mesos::internal::launcher::Operation>::Data>' requested here
          __shared_ptr(__p).swap(*this);
          ^
../3rdparty/libprocess/include/process/owned.hpp:68:10: note: in instantiation of function template specialization 'std::__shared_ptr<process::Owned<mesos::internal::launcher::Operation>::Data, 2>::reset<process::Owned<mesos::internal::launcher::Operation>::Data>' requested here
    data.reset(new Data(t));
         ^
./launcher/launcher.hpp:101:7: note: in instantiation of member function 'process::Owned<mesos::internal::launcher::Operation>::Owned' requested here
  add(process::Owned<Operation>(new T()));
      ^
launcher/main.cpp:26:3: note: in instantiation of function template specialization 'mesos::internal::launcher::add<mesos::internal::launcher::ShellOperation>' requested here
  launcher::add<launcher::ShellOperation>();
  ^
1 error generated.",Bug,Major,dhamon,2014-06-06T17:45:13.000+0000,5,Resolved,Complete,Build failure: Ubuntu 13.10/clang due to missing virtual destructor,2014-06-06T17:45:13.000+0000,MESOS-1459,1.0,mesos,Q2 Sprint 3
,2014-05-28T01:01:15.000+0000,vinodkone,"{code}
[ RUN      ] LogZooKeeperTest.WriteRead
I0527 23:23:48.286031  1352 zookeeper_test_server.cpp:158] Started ZooKeeperTestServer on port 39446
I0527 23:23:48.293916  1352 log_tests.cpp:1945] Using temporary directory '/tmp/LogZooKeeperTest_WriteRead_Vyty8g'
I0527 23:23:48.296430  1352 leveldb.cpp:176] Opened db in 2.459713ms
I0527 23:23:48.296740  1352 leveldb.cpp:183] Compacted db in 286843ns
I0527 23:23:48.296761  1352 leveldb.cpp:198] Created db iterator in 3083ns
I0527 23:23:48.296772  1352 leveldb.cpp:204] Seeked to beginning of db in 4541ns
I0527 23:23:48.296777  1352 leveldb.cpp:273] Iterated through 0 keys in the db in 87ns
I0527 23:23:48.296788  1352 replica.cpp:741] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned
I0527 23:23:48.297499  1383 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 505340ns
I0527 23:23:48.297513  1383 replica.cpp:320] Persisted replica status to VOTING
I0527 23:23:48.299492  1352 leveldb.cpp:176] Opened db in 1.73582ms
I0527 23:23:48.299773  1352 leveldb.cpp:183] Compacted db in 263937ns
I0527 23:23:48.299793  1352 leveldb.cpp:198] Created db iterator in 7494ns
I0527 23:23:48.299806  1352 leveldb.cpp:204] Seeked to beginning of db in 235ns
I0527 23:23:48.299813  1352 leveldb.cpp:273] Iterated through 0 keys in the db in 93ns
I0527 23:23:48.299821  1352 replica.cpp:741] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned
I0527 23:23:48.300503  1380 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 492309ns
I0527 23:23:48.300516  1380 replica.cpp:320] Persisted replica status to VOTING
I0527 23:23:48.302500  1352 leveldb.cpp:176] Opened db in 1.793829ms
I0527 23:23:48.303642  1352 leveldb.cpp:183] Compacted db in 1.123929ms
I0527 23:23:48.303669  1352 leveldb.cpp:198] Created db iterator in 5865ns
I0527 23:23:48.303689  1352 leveldb.cpp:204] Seeked to beginning of db in 8811ns
I0527 23:23:48.303705  1352 leveldb.cpp:273] Iterated through 1 keys in the db in 9545ns
I0527 23:23:48.303715  1352 replica.cpp:741] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned
2014-05-27 23:23:48,303:1352(0x2b1173a29700):ZOO_INFO@log_env@712: Client environment:zookeeper.version=zookeeper C client 3.4.5
2014-05-27 23:23:48,303:1352(0x2b1173a29700):ZOO_INFO@log_env@716: Client environment:host.name=minerva
2014-05-27 23:23:48,303:1352(0x2b1173a29700):ZOO_INFO@log_env@723: Client environment:os.name=Linux
2014-05-27 23:23:48,303:1352(0x2b1173a29700):ZOO_INFO@log_env@724: Client environment:os.arch=3.2.0-57-generic
2014-05-27 23:23:48,303:1352(0x2b1173a29700):ZOO_INFO@log_env@725: Client environment:os.version=#87-Ubuntu SMP Tue Nov 12 21:35:10 UTC 2013
2014-05-27 23:23:48,303:1352(0x2b1173e2b700):ZOO_INFO@log_env@712: Client environment:zookeeper.version=zookeeper C client 3.4.5
2014-05-27 23:23:48,304:1352(0x2b1173e2b700):ZOO_INFO@log_env@716: Client environment:host.name=minerva
2014-05-27 23:23:48,304:1352(0x2b1173e2b700):ZOO_INFO@log_env@723: Client environment:os.name=Linux
2014-05-27 23:23:48,304:1352(0x2b1173e2b700):ZOO_INFO@log_env@724: Client environment:os.arch=3.2.0-57-generic
2014-05-27 23:23:48,304:1352(0x2b1173e2b700):ZOO_INFO@log_env@725: Client environment:os.version=#87-Ubuntu SMP Tue Nov 12 21:35:10 UTC 2013
2014-05-27 23:23:48,304:1352(0x2b1173a29700):ZOO_INFO@log_env@733: Client environment:user.name=(null)
I0527 23:23:48.303988  1380 log.cpp:238] Attempting to join replica to ZooKeeper group
2014-05-27 23:23:48,304:1352(0x2b1173e2b700):ZOO_INFO@log_env@733: Client environment:user.name=(null)
2014-05-27 23:23:48,304:1352(0x2b1173a29700):ZOO_INFO@log_env@741: Client environment:user.home=/home/jenkins
I0527 23:23:48.304198  1385 recover.cpp:425] Starting replica recovery
2014-05-27 23:23:48,304:1352(0x2b1173e2b700):ZOO_INFO@log_env@741: Client environment:user.home=/home/jenkins
2014-05-27 23:23:48,304:1352(0x2b1173a29700):ZOO_INFO@log_env@753: Client environment:user.dir=/tmp/LogZooKeeperTest_WriteRead_Vyty8g
2014-05-27 23:23:48,304:1352(0x2b1173a29700):ZOO_INFO@zookeeper_init@786: Initiating client connection, host=127.0.0.1:39446 sessionTimeout=5000 watcher=0x2b11708e98d0 sessionId=0 sessionPasswd=<null> context=0x2b118002f4e0 flags=0
2014-05-27 23:23:48,304:1352(0x2b1173e2b700):ZOO_INFO@log_env@753: Client environment:user.dir=/tmp/LogZooKeeperTest_WriteRead_Vyty8g
I0527 23:23:48.304352  1385 recover.cpp:451] Replica is in VOTING status
2014-05-27 23:23:48,304:1352(0x2b1173e2b700):ZOO_INFO@zookeeper_init@786: Initiating client connection, host=127.0.0.1:39446 sessionTimeout=5000 watcher=0x2b11708e98d0 sessionId=0 sessionPasswd=<null> context=0x2b1198015ca0 flags=0
I0527 23:23:48.304417  1385 recover.cpp:440] Recover process terminated
2014-05-27 23:23:48,304:1352(0x2b12897b8700):ZOO_INFO@check_events@1703: initiated connection to server [127.0.0.1:39446]
2014-05-27 23:23:48,304:1352(0x2b12891b5700):ZOO_INFO@check_events@1703: initiated connection to server [127.0.0.1:39446]
I0527 23:23:48.311262  1352 leveldb.cpp:176] Opened db in 7.261703ms
2014-05-27 23:23:48,311:1352(0x2b12897b8700):ZOO_INFO@check_events@1750: session establishment complete on server [127.0.0.1:39446], sessionId=0x1463fff34bd0000, negotiated timeout=6000
I0527 23:23:48.312379  1381 group.cpp:310] Group process ((614)@67.195.138.8:35151) connected to ZooKeeper
I0527 23:23:48.312407  1381 group.cpp:784] Syncing group operations: queue size (joins, cancels, datas) = (0, 0, 0)
I0527 23:23:48.312417  1381 group.cpp:382] Trying to create path '/log' in ZooKeeper
I0527 23:23:48.312422  1352 leveldb.cpp:183] Compacted db in 1.119843ms
I0527 23:23:48.312505  1352 leveldb.cpp:198] Created db iterator in 3901ns
I0527 23:23:48.312526  1352 leveldb.cpp:204] Seeked to beginning of db in 7398ns
I0527 23:23:48.312541  1352 leveldb.cpp:273] Iterated through 1 keys in the db in 6345ns
I0527 23:23:48.312553  1352 replica.cpp:741] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned
2014-05-27 23:23:48,312:1352(0x2b1173627700):ZOO_INFO@log_env@712: Client environment:zookeeper.version=zookeeper C client 3.4.5
2014-05-27 23:23:48,312:1352(0x2b1173627700):ZOO_INFO@log_env@716: Client environment:host.name=minerva
2014-05-27 23:23:48,312:1352(0x2b1173627700):ZOO_INFO@log_env@723: Client environment:os.name=Linux
2014-05-27 23:23:48,312:1352(0x2b1173627700):ZOO_INFO@log_env@724: Client environment:os.arch=3.2.0-57-generic
2014-05-27 23:23:48,312:1352(0x2b1173627700):ZOO_INFO@log_env@725: Client environment:os.version=#87-Ubuntu SMP Tue Nov 12 21:35:10 UTC 2013
2014-05-27 23:23:48,312:1352(0x2b1173627700):ZOO_INFO@log_env@733: Client environment:user.name=(null)
2014-05-27 23:23:48,312:1352(0x2b12891b5700):ZOO_INFO@check_events@1750: session establishment complete on server [127.0.0.1:39446], sessionId=0x1463fff34bd0001, negotiated timeout=6000
2014-05-27 23:23:48,313:1352(0x2b1173627700):ZOO_INFO@log_env@741: Client environment:user.home=/home/jenkins
2014-05-27 23:23:48,313:1352(0x2b1173627700):ZOO_INFO@log_env@753: Client environment:user.dir=/tmp/LogZooKeeperTest_WriteRead_Vyty8g
2014-05-27 23:23:48,313:1352(0x2b1173627700):ZOO_INFO@zookeeper_init@786: Initiating client connection, host=127.0.0.1:39446 sessionTimeout=5000 watcher=0x2b11708e98d0 sessionId=0 sessionPasswd=<null> context=0x2b119001fd20 flags=0
I0527 23:23:48.313247  1380 group.cpp:310] Group process ((616)@67.195.138.8:35151) connected to ZooKeeper
I0527 23:23:48.313266  1380 group.cpp:784] Syncing group operations: queue size (joins, cancels, datas) = (1, 0, 0)
I0527 23:23:48.313273  1380 group.cpp:382] Trying to create path '/log' in ZooKeeper
2014-05-27 23:23:48,313:1352(0x2b12889b0700):ZOO_INFO@check_events@1703: initiated connection to server [127.0.0.1:39446]
2014-05-27 23:23:48,313:1352(0x2b1173828700):ZOO_INFO@log_env@712: Client environment:zookeeper.version=zookeeper C client 3.4.5
2014-05-27 23:23:48,313:1352(0x2b1173828700):ZOO_INFO@log_env@716: Client environment:host.name=minerva
2014-05-27 23:23:48,313:1352(0x2b1173828700):ZOO_INFO@log_env@723: Client environment:os.name=Linux
2014-05-27 23:23:48,313:1352(0x2b1173828700):ZOO_INFO@log_env@724: Client environment:os.arch=3.2.0-57-generic
2014-05-27 23:23:48,313:1352(0x2b1173828700):ZOO_INFO@log_env@725: Client environment:os.version=#87-Ubuntu SMP Tue Nov 12 21:35:10 UTC 2013
I0527 23:23:48.313436  1387 log.cpp:238] Attempting to join replica to ZooKeeper group
2014-05-27 23:23:48,313:1352(0x2b1173828700):ZOO_INFO@log_env@733: Client environment:user.name=(null)
2014-05-27 23:23:48,313:1352(0x2b1173828700):ZOO_INFO@log_env@741: Client environment:user.home=/home/jenkins
2014-05-27 23:23:48,313:1352(0x2b1173828700):ZOO_INFO@log_env@753: Client environment:user.dir=/tmp/LogZooKeeperTest_WriteRead_Vyty8g
2014-05-27 23:23:48,313:1352(0x2b1173828700):ZOO_INFO@zookeeper_init@786: Initiating client connection, host=127.0.0.1:39446 sessionTimeout=5000 watcher=0x2b11708e98d0 sessionId=0 sessionPasswd=<null> context=0x2b1190011ea0 flags=0
I0527 23:23:48.313601  1387 recover.cpp:425] Starting replica recovery
I0527 23:23:48.313721  1382 recover.cpp:451] Replica is in VOTING status
I0527 23:23:48.313794  1382 recover.cpp:440] Recover process terminated
2014-05-27 23:23:48,313:1352(0x2b1288bb1700):ZOO_INFO@check_events@1703: initiated connection to server [127.0.0.1:39446]
I0527 23:23:48.313973  1383 log.cpp:656] Attempting to start the writer
2014-05-27 23:23:48,315:1352(0x2b12889b0700):ZOO_INFO@check_events@1750: session establishment complete on server [127.0.0.1:39446], sessionId=0x1463fff34bd0002, negotiated timeout=6000
I0527 23:23:48.315682  1387 group.cpp:310] Group process ((619)@67.195.138.8:35151) connected to ZooKeeper
2014-05-27 23:23:48,315:1352(0x2b1288bb1700):ZOO_INFO@check_events@1750: session establishment complete on server [127.0.0.1:39446], sessionId=0x1463fff34bd0003, negotiated timeout=6000
I0527 23:23:48.315709  1387 group.cpp:784] Syncing group operations: queue size (joins, cancels, datas) = (0, 0, 0)
I0527 23:23:48.315738  1387 group.cpp:382] Trying to create path '/log' in ZooKeeper
I0527 23:23:48.315964  1386 group.cpp:310] Group process ((621)@67.195.138.8:35151) connected to ZooKeeper
I0527 23:23:48.315981  1386 group.cpp:784] Syncing group operations: queue size (joins, cancels, datas) = (1, 0, 0)
I0527 23:23:48.315989  1386 group.cpp:382] Trying to create path '/log' in ZooKeeper
I0527 23:23:48.317881  1385 network.hpp:423] ZooKeeper group memberships changed
I0527 23:23:48.317937  1381 group.cpp:655] Trying to get '/log/0000000000' in ZooKeeper
I0527 23:23:48.318205  1382 network.hpp:423] ZooKeeper group memberships changed
I0527 23:23:48.318317  1383 group.cpp:655] Trying to get '/log/0000000000' in ZooKeeper
I0527 23:23:48.319154  1382 network.hpp:461] ZooKeeper group PIDs: { log-replica(22)@67.195.138.8:35151 }
I0527 23:23:48.319541  1386 network.hpp:461] ZooKeeper group PIDs: { log-replica(22)@67.195.138.8:35151 }
I0527 23:23:48.319851  1381 replica.cpp:474] Replica received implicit promise request with proposal 1
I0527 23:23:48.319905  1387 replica.cpp:474] Replica received implicit promise request with proposal 1
I0527 23:23:48.319907  1384 network.hpp:423] ZooKeeper group memberships changed
I0527 23:23:48.320091  1385 group.cpp:655] Trying to get '/log/0000000000' in ZooKeeper
I0527 23:23:48.320384  1383 network.hpp:423] ZooKeeper group memberships changed
I0527 23:23:48.320441  1381 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 568396ns
I0527 23:23:48.320456  1384 group.cpp:655] Trying to get '/log/0000000000' in ZooKeeper
I0527 23:23:48.320461  1381 replica.cpp:342] Persisted promised to 1
I0527 23:23:48.320446  1387 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 516015ns
I0527 23:23:48.320497  1387 replica.cpp:342] Persisted promised to 1
I0527 23:23:48.320814  1383 coordinator.cpp:230] Coordinator attemping to fill missing position
I0527 23:23:48.321050  1384 group.cpp:655] Trying to get '/log/0000000001' in ZooKeeper
I0527 23:23:48.321063  1385 group.cpp:655] Trying to get '/log/0000000001' in ZooKeeper
I0527 23:23:48.321341  1387 replica.cpp:375] Replica received explicit promise request for position 0 with proposal 2
I0527 23:23:48.321375  1381 replica.cpp:375] Replica received explicit promise request for position 0 with proposal 2
I0527 23:23:48.321506  1387 leveldb.cpp:343] Persisting action (8 bytes) to leveldb took 89us
I0527 23:23:48.321530  1387 replica.cpp:676] Persisted action at 0
I0527 23:23:48.321584  1381 leveldb.cpp:343] Persisting action (8 bytes) to leveldb took 122910ns
I0527 23:23:48.321602  1381 replica.cpp:676] Persisted action at 0
I0527 23:23:48.321775  1383 network.hpp:461] ZooKeeper group PIDs: { log-replica(22)@67.195.138.8:35151, log-replica(23)@67.195.138.8:35151 }
I0527 23:23:48.321961  1381 replica.cpp:508] Replica received write request for position 0
I0527 23:23:48.321984  1381 leveldb.cpp:438] Reading position from leveldb took 7813ns
I0527 23:23:48.322064  1380 network.hpp:461] ZooKeeper group PIDs: { log-replica(22)@67.195.138.8:35151, log-replica(23)@67.195.138.8:35151 }
I0527 23:23:48.322073  1381 leveldb.cpp:343] Persisting action (14 bytes) to leveldb took 78683ns
I0527 23:23:48.322077  1383 replica.cpp:508] Replica received write request for position 0
I0527 23:23:48.322084  1381 replica.cpp:676] Persisted action at 0
I0527 23:23:48.322111  1383 leveldb.cpp:438] Reading position from leveldb took 17416ns
I0527 23:23:48.322330  1383 leveldb.cpp:343] Persisting action (14 bytes) to leveldb took 157199ns
I0527 23:23:48.322345  1383 replica.cpp:676] Persisted action at 0
I0527 23:23:48.322522  1386 replica.cpp:655] Replica received learned notice for position 0
I0527 23:23:48.322523  1382 replica.cpp:655] Replica received learned notice for position 0
I0527 23:23:48.322638  1386 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 86907ns
I0527 23:23:48.322661  1386 replica.cpp:676] Persisted action at 0
I0527 23:23:48.322670  1386 replica.cpp:661] Replica learned NOP action at position 0
I0527 23:23:48.322682  1382 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 85031ns
I0527 23:23:48.322693  1382 replica.cpp:676] Persisted action at 0
I0527 23:23:48.322700  1382 replica.cpp:661] Replica learned NOP action at position 0
I0527 23:23:48.322790  1380 log.cpp:672] Writer started with ending position 0
I0527 23:23:48.322898  1380 log.cpp:680] Attempting to append 11 bytes to the log
I0527 23:23:48.322978  1383 coordinator.cpp:340] Coordinator attempting to write APPEND action at position 1
I0527 23:23:48.323122  1380 replica.cpp:508] Replica received write request for position 1
I0527 23:23:48.323158  1381 replica.cpp:508] Replica received write request for position 1
I0527 23:23:48.323202  1380 leveldb.cpp:343] Persisting action (27 bytes) to leveldb took 66527ns
I0527 23:23:48.323215  1380 replica.cpp:676] Persisted action at 1
I0527 23:23:48.323238  1381 leveldb.cpp:343] Persisting action (27 bytes) to leveldb took 67074ns
I0527 23:23:48.323252  1381 replica.cpp:676] Persisted action at 1
I0527 23:23:48.323354  1380 replica.cpp:655] Replica received learned notice for position 1
I0527 23:23:48.323362  1382 replica.cpp:655] Replica received learned notice for position 1
I0527 23:23:48.323443  1380 leveldb.cpp:343] Persisting action (29 bytes) to leveldb took 77398ns
I0527 23:23:48.323461  1380 replica.cpp:676] Persisted action at 1
I0527 23:23:48.323463  1382 leveldb.cpp:343] Persisting action (29 bytes) to leveldb took 90567ns
I0527 23:23:48.323467  1380 replica.cpp:661] Replica learned APPEND action at position 1
I0527 23:23:48.323477  1382 replica.cpp:676] Persisted action at 1
I0527 23:23:48.323484  1382 replica.cpp:661] Replica learned APPEND action at position 1
I0527 23:23:48.323729  1380 leveldb.cpp:438] Reading position from leveldb took 7224ns
2014-05-27 23:23:48,324:1352(0x2b1173c2a700):ZOO_INFO@zookeeper_close@2505: Closing zookeeper sessionId=0x1463fff34bd0003 to [127.0.0.1:39446]

2014-05-27 23:23:48,324:1352(0x2b117301ff80):ZOO_INFO@zookeeper_close@2505: Closing zookeeper sessionId=0x1463fff34bd0002 to [127.0.0.1:39446]

I0527 23:23:48.326591  1386 network.hpp:423] ZooKeeper group memberships changed
I0527 23:23:48.326690  1382 group.cpp:655] Trying to get '/log/0000000000' in ZooKeeper
I0527 23:23:48.327450  1384 network.hpp:461] ZooKeeper group PIDs: { log-replica(22)@67.195.138.8:35151 }
2014-05-27 23:23:48,446:1352(0x2b12bc401700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:51020] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2014-05-27 23:23:51,782:1352(0x2b12bc401700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:51020] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2014-05-27 23:23:55,118:1352(0x2b12bc401700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:51020] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
I0527 23:23:57.002908  1381 network.hpp:423] ZooKeeper group memberships changed
I0527 23:23:57.003042  1381 network.hpp:461] ZooKeeper group PIDs: {  }
2014-05-27 23:23:58,455:1352(0x2b12bc401700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:51020] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2014-05-27 23:24:01,791:1352(0x2b12bc401700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:51020] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2014-05-27 23:24:05,127:1352(0x2b12bc401700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:51020] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2014-05-27 23:24:08,464:1352(0x2b12bc401700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:51020] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2014-05-27 23:24:11,800:1352(0x2b12bc401700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:51020] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2014-05-27 23:24:15,136:1352(0x2b12bc401700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:51020] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2014-05-27 23:24:18,473:1352(0x2b12bc401700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:51020] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2014-05-27 23:24:21,809:1352(0x2b12bc401700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:51020] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2014-05-27 23:24:25,146:1352(0x2b12bc401700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:51020] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2014-05-27 23:24:28,482:1352(0x2b12bc401700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:51020] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2014-05-27 23:24:31,818:1352(0x2b12bc401700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:51020] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2014-05-27 23:24:35,155:1352(0x2b12bc401700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:51020] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2014-05-27 23:24:38,491:1352(0x2b12bc401700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:51020] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2014-05-27 23:24:41,827:1352(0x2b12bc401700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:51020] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2014-05-27 23:24:45,164:1352(0x2b12bc401700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:51020] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2014-05-27 23:24:48,500:1352(0x2b12bc401700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:51020] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2014-05-27 23:24:51,834:1352(0x2b12bc401700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:51020] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2014-05-27 23:24:55,171:1352(0x2b12bc401700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:51020] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2014-05-27 23:24:58,507:1352(0x2b12bc401700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:51020] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2014-05-27 23:25:01,844:1352(0x2b12bc401700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:51020] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2014-05-27 23:25:05,180:1352(0x2b12bc401700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:51020] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2014-05-27 23:25:08,516:1352(0x2b12bc401700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:51020] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2014-05-27 23:25:11,853:1352(0x2b12bc401700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:51020] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2014-05-27 23:25:15,186:1352(0x2b12bc401700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:51020] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2014-05-27 23:25:18,523:1352(0x2b12bc401700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:51020] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2014-05-27 23:25:21,859:1352(0x2b12bc401700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:51020] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2014-05-27 23:25:25,195:1352(0x2b12bc401700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:51020] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2014-05-27 23:25:28,530:1352(0x2b12bc401700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:51020] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2014-05-27 23:25:31,866:1352(0x2b12bc401700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:51020] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2014-05-27 23:25:35,203:1352(0x2b12bc401700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:51020] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2014-05-27 23:25:38,539:1352(0x2b12bc401700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:51020] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2014-05-27 23:25:41,875:1352(0x2b12bc401700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:51020] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2014-05-27 23:25:45,212:1352(0x2b12bc401700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:51020] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2014-05-27 23:25:48,548:1352(0x2b12bc401700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:51020] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2014-05-27 23:25:51,885:1352(0x2b12bc401700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:51020] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2014-05-27 23:25:55,221:1352(0x2b12bc401700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:51020] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2014-05-27 23:25:58,557:1352(0x2b12bc401700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:51020] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2014-05-27 23:26:01,894:1352(0x2b12bc401700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:51020] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2014-05-27 23:26:05,230:1352(0x2b12bc401700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:51020] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2014-05-27 23:26:08,567:1352(0x2b12bc401700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:51020] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2014-05-27 23:26:11,903:1352(0x2b12bc401700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:51020] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2014-05-27 23:26:15,239:1352(0x2b12bc401700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:51020] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2014-05-27 23:26:18,576:1352(0x2b12bc401700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:51020] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2014-05-27 23:26:21,912:1352(0x2b12bc401700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:51020] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2014-05-27 23:26:25,248:1352(0x2b12bc401700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:51020] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2014-05-27 23:26:28,585:1352(0x2b12bc401700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:51020] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2014-05-27 23:26:31,921:1352(0x2b12bc401700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:51020] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2014-05-27 23:26:35,257:1352(0x2b12bc401700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:51020] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2014-05-27 23:26:38,594:1352(0x2b12bc401700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:51020] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2014-05-27 23:26:41,930:1352(0x2b12bc401700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:51020] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2014-05-27 23:26:45,267:1352(0x2b12bc401700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:51020] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2014-05-27 23:26:48,603:1352(0x2b12bc401700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:51020] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2014-05-27 23:26:51,939:1352(0x2b12bc401700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:51020] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2014-05-27 23:26:55,276:1352(0x2b12bc401700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:51020] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2014-05-27 23:26:58,612:1352(0x2b12bc401700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:51020] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2014-05-27 23:27:01,948:1352(0x2b12bc401700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:51020] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2014-05-27 23:27:05,285:1352(0x2b12bc401700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:51020] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2014-05-27 23:27:08,621:1352(0x2b12bc401700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:51020] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2014-05-27 23:27:11,958:1352(0x2b12bc401700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:51020] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2014-05-27 23:27:15,294:1352(0x2b12bc401700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:51020] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2014-05-27 23:27:18,630:1352(0x2b12bc401700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:51020] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2014-05-27 23:27:21,967:1352(0x2b12bc401700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:51020] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2014-05-27 23:27:25,303:1352(0x2b12bc401700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:51020] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2014-05-27 23:27:28,639:1352(0x2b12bc401700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:51020] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2014-05-27 23:27:31,976:1352(0x2b12bc401700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:51020] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2014-05-27 23:27:35,312:1352(0x2b12bc401700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:51020] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2014-05-27 23:27:38,649:1352(0x2b12bc401700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:51020] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2014-05-27 23:27:41,985:1352(0x2b12bc401700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:51020] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2014-05-27 23:27:45,321:1352(0x2b12bc401700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:51020] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2014-05-27 23:27:48,658:1352(0x2b12bc401700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:51020] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2014-05-27 23:27:51,994:1352(0x2b12bc401700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:51020] zk retcode=-4, errno=111(Connection refused): server refused to accept the client

{code}",Bug,Major,vinodkone,,10020,Accepted,In Progress,LogZooKeeperTest.WriteRead test is flaky,2015-09-20T00:57:39.000+0000,MESOS-1425,1.0,mesos,Q3 Sprint 1
pbrett,2014-05-27T22:31:20.000+0000,tillt,"Triggered by MESOS-1413 I would like to propose changing our tests to not rely on {{echo}} but to use {{printf}} instead.

This seems to be useful as {{echo}} is introducing an extra linefeed after the supplied string whereas {{printf}} does not. The {{-n}} switch preventing that extra linefeed is unfortunately not portable - it is not supported by the builtin {{echo}} of the BSD / OSX {{/bin/sh}}.
",Improvement,Minor,tillt,2015-04-07T17:30:40.000+0000,5,Resolved,Complete,Mesos tests should not rely on echo,2015-04-07T17:49:57.000+0000,MESOS-1424,1.0,mesos,Twitter Mesos Q1 Sprint 6
bmahler,2014-05-22T19:33:18.000+0000,bmahler,"Once we are sending acknowledgments through the master as per MESOS-1409, we need to keep terminal tasks that are *unacknowledged* in the Master's memory.

This will allow us to identify these tasks to frameworks when we haven't yet forwarded them an update. Without this, we're susceptible to MESOS-1389.",Task,Major,bmahler,2014-09-19T01:27:30.000+0000,5,Resolved,Complete,Keep terminal unacknowledged tasks in the master's state.,2014-09-19T01:27:30.000+0000,MESOS-1410,5.0,mesos,Mesos Q3 Sprint 5
idownes,2014-05-20T22:10:58.000+0000,idownes,"Document interval, duration and the event flags. Document event name normalization for the protobuf.",Improvement,Major,idownes,2014-06-13T22:35:03.000+0000,5,Resolved,Complete,Document perf isolator flags,2014-06-13T22:35:03.000+0000,MESOS-1398,1.0,mesos,Q2 Sprint 3
,2014-05-20T22:10:13.000+0000,idownes,Rename ContainerStatistics which includes optional ResourceStatistics and optional PerfStatistics.,Improvement,Major,idownes,,1,Open,New,Rename ResourceStatistics for containers,2015-02-23T20:04:27.000+0000,MESOS-1397,8.0,mesos,Q2 Sprint 3
idownes,2014-05-20T22:09:26.000+0000,idownes,"Field names from `perf list` normalized to convert hyphens to underscores and down-cased. Start with just the hardware and software events, not raw hardware, breakpoints or tracepoints, 

All fields should be optional. Include as an optional field to ResourceStatistics.",Improvement,Major,idownes,2014-06-13T22:33:59.000+0000,5,Resolved,Complete,Introduce a PerfStatistics protobuf,2014-06-13T22:33:59.000+0000,MESOS-1396,2.0,mesos,Q2 Sprint 3
idownes,2014-05-20T22:07:16.000+0000,idownes,"Test that changes to add/remove perf isolator will be handled through slave recovery, e.g., containers started without the perf isolator continue to report resource statistics and containers started with the perf isolator will include perf statistics.",Improvement,Major,idownes,2014-06-13T22:34:55.000+0000,5,Resolved,Complete,Test perf isolator for slave roll forward/roll back,2014-06-13T22:34:55.000+0000,MESOS-1395,2.0,mesos,Q2 Sprint 3
idownes,2014-05-20T22:05:38.000+0000,idownes,"Test across different kernel versions (at least 2.6.XX and 3.X) and across different distributions.

Test input flags and parsing output.",Improvement,Major,idownes,2014-06-26T00:15:31.000+0000,5,Resolved,Complete,Test different versions of perf,2016-01-13T10:51:47.000+0000,MESOS-1394,3.0,mesos,Q2 Sprint 3
idownes,2014-05-20T22:04:02.000+0000,idownes,"1. Should support output from pid and cgroup targets.
2. Should support output for the same events from >= 1 cgroup
3. Should return as PerfStatistics protobuf.
",Improvement,Major,idownes,2014-06-13T22:34:18.000+0000,5,Resolved,Complete,Write parser for perf output.,2014-06-13T22:34:18.000+0000,MESOS-1393,3.0,mesos,Q2 Sprint 3
xujyan,2014-05-20T18:51:25.000+0000,bmahler,"Looks like the following can occur when a znode goes away right before we can read it's contents:

{noformat: title=Slave exit}
I0520 16:33:45.721727 29155 group.cpp:382] Trying to create path '/home/mesos/test/master' in ZooKeeper
I0520 16:33:48.600837 29155 detector.cpp:134] Detected a new leader: (id='2617')
I0520 16:33:48.601428 29147 group.cpp:655] Trying to get '/home/mesos/test/master/info_0000002617' in ZooKeeper
Failed to detect a master: Failed to get data for ephemeral node '/home/mesos/test/master/info_0000002617' in ZooKeeper: no node
Slave Exit Status: 1
{noformat}",Bug,Major,bmahler,2014-09-18T16:41:43.000+0000,5,Resolved,Complete,Failure when znode is removed before we can read its contents.,2014-09-18T16:42:25.000+0000,MESOS-1392,3.0,mesos,Mesos Q3 Sprint 5
xujyan,2014-05-14T23:29:38.000+0000,clambert,Need to add a 'principal' field to FrameworkInfo and verify if the Framework has the claimed principal during registration.,Task,Major,clambert,2014-05-27T17:44:44.000+0000,5,Resolved,Complete,Keep track of the principals for authenticated pids in Master.,2014-06-03T04:34:47.000+0000,MESOS-1373,3.0,mesos,Q2'14 Sprint 2
dhamon,2014-05-14T20:59:14.000+0000,dhamon,We expose the master's event queue length and we should do the same for the scheduler driver.,Task,Major,dhamon,2014-05-27T21:54:21.000+0000,5,Resolved,Complete,Expose libprocess queue length from scheduler driver to metrics endpoint,2014-05-30T19:26:52.000+0000,MESOS-1371,1.0,mesos,Q2'14 Sprint 2
greggomann,2014-05-13T22:57:15.000+0000,dhamon,"--gtest_repeat=-1 --gtest_shuffle --gtest_break_on_failure

{noformat}
[ RUN      ] SlaveRecoveryTest/0.MultipleFrameworks
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0513 15:42:05.931761  4320 exec.cpp:131] Version: 0.19.0
I0513 15:42:05.936698  4340 exec.cpp:205] Executor registered on slave 20140513-154204-16842879-51872-13062-0
Registered executor on artoo
Starting task 51991f97-f5fd-4905-ad0f-02668083af7c
Forked command at 4367
sh -c 'sleep 1000'
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0513 15:42:06.915061  4408 exec.cpp:131] Version: 0.19.0
I0513 15:42:06.931149  4435 exec.cpp:205] Executor registered on slave 20140513-154204-16842879-51872-13062-0
Registered executor on artoo
Starting task eaf5d8d6-3a6c-4ee1-84c1-fae20fb1df83
sh -c 'sleep 1000'
Forked command at 4439
I0513 15:42:06.998332  4340 exec.cpp:251] Received reconnect request from slave 20140513-154204-16842879-51872-13062-0
I0513 15:42:06.998414  4436 exec.cpp:251] Received reconnect request from slave 20140513-154204-16842879-51872-13062-0
I0513 15:42:07.006350  4437 exec.cpp:228] Executor re-registered on slave 20140513-154204-16842879-51872-13062-0
Re-registered executor on artoo
I0513 15:42:07.027039  4337 exec.cpp:378] Executor asked to shutdown
Shutting down
Sending SIGTERM to process tree at pid 4367
Killing the following process trees:
[ 
-+- 4367 sh -c sleep 1000 
 \--- 4368 sleep 1000 
]
../../src/tests/slave_recovery_tests.cpp:2807: Failure
Value of: status1.get().state()
  Actual: TASK_FAILED
Expected: TASK_KILLED

Program received signal SIGSEGV, Segmentation fault.
testing::UnitTest::AddTestPartResult (this=0x154dac0 <testing::UnitTest::GetInstance()::instance>, result_type=testing::TestPartResult::kFatalFailure, file_name=0xeb6b6c ""../../src/tests/slave_recovery_tests.cpp"", line_number=2807, message=..., os_stack_trace=...) at gmock-1.6.0/gtest/src/gtest.cc:3795
3795          *static_cast<volatile int*>(NULL) = 1;
(gdb) bt
#0  testing::UnitTest::AddTestPartResult (this=0x154dac0 <testing::UnitTest::GetInstance()::instance>, result_type=testing::TestPartResult::kFatalFailure, file_name=0xeb6b6c ""../../src/tests/slave_recovery_tests.cpp"", line_number=2807, message=..., os_stack_trace=...) at gmock-1.6.0/gtest/src/gtest.cc:3795
#1  0x0000000000df98b9 in testing::internal::AssertHelper::operator= (this=0x7fffffffb860, message=...) at gmock-1.6.0/gtest/src/gtest.cc:356
#2  0x0000000000cdfa57 in SlaveRecoveryTest_MultipleFrameworks_Test<mesos::internal::slave::MesosContainerizer>::TestBody (this=0x1954db0) at ../../src/tests/slave_recovery_tests.cpp:2807
#3  0x0000000000e22583 in testing::internal::HandleSehExceptionsInMethodIfSupported<testing::Test, void> (object=0x1954db0, method=&virtual testing::Test::TestBody(), location=0xed0af0 ""the test body"") at gmock-1.6.0/gtest/src/gtest.cc:2090
#4  0x0000000000e12467 in testing::internal::HandleExceptionsInMethodIfSupported<testing::Test, void> (object=0x1954db0, method=&virtual testing::Test::TestBody(), location=0xed0af0 ""the test body"") at gmock-1.6.0/gtest/src/gtest.cc:2126
#5  0x0000000000e010d5 in testing::Test::Run (this=0x1954db0) at gmock-1.6.0/gtest/src/gtest.cc:2161
#6  0x0000000000e01ceb in testing::TestInfo::Run (this=0x158cf80) at gmock-1.6.0/gtest/src/gtest.cc:2338
#7  0x0000000000e02387 in testing::TestCase::Run (this=0x158a880) at gmock-1.6.0/gtest/src/gtest.cc:2445
#8  0x0000000000e079ed in testing::internal::UnitTestImpl::RunAllTests (this=0x1558b40) at gmock-1.6.0/gtest/src/gtest.cc:4237
#9  0x0000000000e1ec83 in testing::internal::HandleSehExceptionsInMethodIfSupported<testing::internal::UnitTestImpl, bool> (object=0x1558b40, method=(bool (testing::internal::UnitTestImpl::*)(testing::internal::UnitTestImpl * const)) 0xe07700 <testing::internal::UnitTestImpl::RunAllTests()>, 
    location=0xed1219 ""auxiliary test code (environments or event listeners)"") at gmock-1.6.0/gtest/src/gtest.cc:2090
#10 0x0000000000e14217 in testing::internal::HandleExceptionsInMethodIfSupported<testing::internal::UnitTestImpl, bool> (object=0x1558b40, method=(bool (testing::internal::UnitTestImpl::*)(testing::internal::UnitTestImpl * const)) 0xe07700 <testing::internal::UnitTestImpl::RunAllTests()>, 
    location=0xed1219 ""auxiliary test code (environments or event listeners)"") at gmock-1.6.0/gtest/src/gtest.cc:2126
#11 0x0000000000e076d7 in testing::UnitTest::Run (this=0x154dac0 <testing::UnitTest::GetInstance()::instance>) at gmock-1.6.0/gtest/src/gtest.cc:3872
#12 0x0000000000b99887 in main (argc=1, argv=0x7fffffffd9f8) at ../../src/tests/main.cpp:107
(gdb) frame 2
#2  0x0000000000cdfa57 in SlaveRecoveryTest_MultipleFrameworks_Test<mesos::internal::slave::MesosContainerizer>::TestBody (this=0x1954db0) at ../../src/tests/slave_recovery_tests.cpp:2807
2807      ASSERT_EQ(TASK_KILLED, status1.get().state());
(gdb) p status1
$1 = {data = {<std::__shared_ptr<process::Future<mesos::TaskStatus>::Data, 2>> = {_M_ptr = 0x1963140, _M_refcount = {_M_pi = 0x198a620}}, <No data fields>}}
(gdb) p status1.get()
$2 = (const mesos::TaskStatus &) @0x7fffdc5bf5f0: {<google::protobuf::Message> = {<google::protobuf::MessageLite> = {_vptr$MessageLite = 0x7ffff74bc940 <vtable for mesos::TaskStatus+16>}, <No data fields>}, static kTaskIdFieldNumber = 1, static kStateFieldNumber = 2, static kMessageFieldNumber = 4, 
  static kDataFieldNumber = 3, static kSlaveIdFieldNumber = 5, static kTimestampFieldNumber = 6, _unknown_fields_ = {fields_ = 0x0}, task_id_ = 0x7fffdc5ce9a0, message_ = 0x7fffdc5f5880, data_ = 0x154b4b0 <google::protobuf::internal::kEmptyString>, slave_id_ = 0x7fffdc59c4f0, timestamp_ = 1429688582.046252, 
  state_ = 3, _cached_size_ = 0, _has_bits_ = {55}, static default_instance_ = 0x0}
(gdb) p status1.get().state()
$3 = mesos::TASK_FAILED
(gdb) list
2802      // Kill task 1.
2803      driver1.killTask(task1.task_id());
2804
2805      // Wait for TASK_KILLED update.
2806      AWAIT_READY(status1);
2807      ASSERT_EQ(TASK_KILLED, status1.get().state());
2808
2809      // Kill task 2.
2810      driver2.killTask(task2.task_id());
2811
{noformat}",Bug,Minor,dhamon,2016-01-29T22:36:14.000+0000,5,Resolved,Complete,SlaveRecoveryTest/0.MultipleFrameworks is flaky,2016-01-29T22:36:14.000+0000,MESOS-1365,1.0,mesos,Q2'14 Sprint 2
vinodkone,2014-05-13T20:58:48.000+0000,vinodkone,This would be nice to have during debugging.,Improvement,Minor,vinodkone,2014-05-19T19:16:48.000+0000,5,Resolved,Complete,Show when the leading master was elected in the webui,2014-05-30T19:27:01.000+0000,MESOS-1358,1.0,mesos,Q2'14 Sprint 2
,2014-05-12T19:14:42.000+0000,bmahler,"From Jenkins:
https://builds.apache.org/job/Mesos-Ubuntu-distcheck/79/consoleFull

{noformat}
[ RUN      ] GarbageCollectorIntegrationTest.DiskUsage
Using temporary directory '/tmp/GarbageCollectorIntegrationTest_DiskUsage_pU3Ym7'
I0507 03:27:38.775058  5758 leveldb.cpp:174] Opened db in 44.343989ms
I0507 03:27:38.787498  5758 leveldb.cpp:181] Compacted db in 12.411065ms
I0507 03:27:38.787533  5758 leveldb.cpp:196] Created db iterator in 4008ns
I0507 03:27:38.787545  5758 leveldb.cpp:202] Seeked to beginning of db in 598ns
I0507 03:27:38.787552  5758 leveldb.cpp:271] Iterated through 0 keys in the db in 173ns
I0507 03:27:38.787564  5758 replica.cpp:741] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned
I0507 03:27:38.787858  5777 recover.cpp:425] Starting replica recovery
I0507 03:27:38.788352  5793 master.cpp:267] Master 20140507-032738-453759884-58462-5758 (hemera.apache.org) started on 140.211.11.27:58462
I0507 03:27:38.788377  5793 master.cpp:304] Master only allowing authenticated frameworks to register
I0507 03:27:38.788383  5793 master.cpp:309] Master only allowing authenticated slaves to register
I0507 03:27:38.788389  5793 credentials.hpp:35] Loading credentials for authentication
I0507 03:27:38.789064  5779 recover.cpp:451] Replica is in EMPTY status
W0507 03:27:38.789115  5793 credentials.hpp:48] Failed to stat credentials file 'file:///tmp/GarbageCollectorIntegrationTest_DiskUsage_pU3Ym7/credentials': No such file or directory
I0507 03:27:38.789489  5779 master.cpp:104] No whitelist given. Advertising offers for all slaves
I0507 03:27:38.789531  5778 hierarchical_allocator_process.hpp:301] Initializing hierarchical allocator process with master : master@140.211.11.27:58462
I0507 03:27:38.791007  5788 replica.cpp:638] Replica in EMPTY status received a broadcasted recover request
I0507 03:27:38.791177  5780 master.cpp:921] The newly elected leader is master@140.211.11.27:58462 with id 20140507-032738-453759884-58462-5758
I0507 03:27:38.791198  5780 master.cpp:931] Elected as the leading master!
I0507 03:27:38.791205  5780 master.cpp:752] Recovering from registrar
I0507 03:27:38.791251  5796 recover.cpp:188] Received a recover response from a replica in EMPTY status
I0507 03:27:38.791323  5797 registrar.cpp:313] Recovering registrar
I0507 03:27:38.792137  5795 recover.cpp:542] Updating replica status to STARTING
I0507 03:27:38.807531  5781 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 15.124092ms
I0507 03:27:38.807559  5781 replica.cpp:320] Persisted replica status to STARTING
I0507 03:27:38.807621  5781 recover.cpp:451] Replica is in STARTING status
I0507 03:27:38.809319  5799 replica.cpp:638] Replica in STARTING status received a broadcasted recover request
I0507 03:27:38.809983  5795 recover.cpp:188] Received a recover response from a replica in STARTING status
I0507 03:27:38.811204  5778 recover.cpp:542] Updating replica status to VOTING
I0507 03:27:38.827595  5795 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 16.011355ms
I0507 03:27:38.827627  5795 replica.cpp:320] Persisted replica status to VOTING
I0507 03:27:38.827683  5795 recover.cpp:556] Successfully joined the Paxos group
I0507 03:27:38.827775  5795 recover.cpp:440] Recover process terminated
I0507 03:27:38.828966  5780 log.cpp:656] Attempting to start the writer
I0507 03:27:38.831114  5782 replica.cpp:474] Replica received implicit promise request with proposal 1
I0507 03:27:38.847708  5782 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 16.573137ms
I0507 03:27:38.847739  5782 replica.cpp:342] Persisted promised to 1
I0507 03:27:38.848141  5797 coordinator.cpp:230] Coordinator attemping to fill missing position
I0507 03:27:38.849684  5790 replica.cpp:375] Replica received explicit promise request for position 0 with proposal 2
I0507 03:27:38.863777  5790 leveldb.cpp:341] Persisting action (8 bytes) to leveldb took 14.076775ms
I0507 03:27:38.863801  5790 replica.cpp:676] Persisted action at 0
I0507 03:27:38.864915  5798 replica.cpp:508] Replica received write request for position 0
I0507 03:27:38.864949  5798 leveldb.cpp:436] Reading position from leveldb took 11807ns
I0507 03:27:38.879945  5798 leveldb.cpp:341] Persisting action (14 bytes) to leveldb took 14.978446ms
I0507 03:27:38.879976  5798 replica.cpp:676] Persisted action at 0
I0507 03:27:38.880491  5797 replica.cpp:655] Replica received learned notice for position 0
I0507 03:27:38.895969  5797 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 15.459949ms
I0507 03:27:38.895992  5797 replica.cpp:676] Persisted action at 0
I0507 03:27:38.896003  5797 replica.cpp:661] Replica learned NOP action at position 0
I0507 03:27:38.896411  5783 log.cpp:672] Writer started with ending position 0
I0507 03:27:38.898058  5798 leveldb.cpp:436] Reading position from leveldb took 11910ns
I0507 03:27:38.899749  5777 registrar.cpp:346] Successfully fetched the registry (0B)
I0507 03:27:38.899766  5777 registrar.cpp:422] Attempting to update the 'registry'
I0507 03:27:38.901458  5791 log.cpp:680] Attempting to append 137 bytes to the log
I0507 03:27:38.901666  5780 coordinator.cpp:340] Coordinator attempting to write APPEND action at position 1
I0507 03:27:38.902773  5783 replica.cpp:508] Replica received write request for position 1
I0507 03:27:38.916127  5783 leveldb.cpp:341] Persisting action (156 bytes) to leveldb took 13.225715ms
I0507 03:27:38.916152  5783 replica.cpp:676] Persisted action at 1
I0507 03:27:38.916534  5790 replica.cpp:655] Replica received learned notice for position 1
I0507 03:27:38.928203  5790 leveldb.cpp:341] Persisting action (158 bytes) to leveldb took 11.652434ms
I0507 03:27:38.928225  5790 replica.cpp:676] Persisted action at 1
I0507 03:27:38.928236  5790 replica.cpp:661] Replica learned APPEND action at position 1
I0507 03:27:38.928546  5790 registrar.cpp:479] Successfully updated 'registry'
I0507 03:27:38.928642  5790 registrar.cpp:372] Successfully recovered registrar
I0507 03:27:38.929044  5783 master.cpp:779] Recovered 0 slaves from the Registry (99B) ; allowing 10mins for slaves to re-register
I0507 03:27:38.929502  5799 log.cpp:699] Attempting to truncate the log to 1
I0507 03:27:38.929888  5797 coordinator.cpp:340] Coordinator attempting to write TRUNCATE action at position 2
I0507 03:27:38.930161  5781 replica.cpp:508] Replica received write request for position 2
I0507 03:27:38.932977  5789 slave.cpp:140] Slave started on 56)@140.211.11.27:58462
I0507 03:27:38.932991  5789 credentials.hpp:35] Loading credentials for authentication
W0507 03:27:38.933567  5789 credentials.hpp:48] Failed to stat credentials file 'file:///tmp/GarbageCollectorIntegrationTest_DiskUsage_A9Pxks/credential': No such file or directory
I0507 03:27:38.933585  5789 slave.cpp:230] Slave using credential for: test-principal
I0507 03:27:38.933765  5789 slave.cpp:243] Slave resources: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I0507 03:27:38.933854  5789 slave.cpp:271] Slave hostname: hemera.apache.org
I0507 03:27:38.933863  5789 slave.cpp:272] Slave checkpoint: false
I0507 03:27:38.934239  5778 state.cpp:33] Recovering state from '/tmp/GarbageCollectorIntegrationTest_DiskUsage_A9Pxks/meta'
I0507 03:27:38.934960  5792 status_update_manager.cpp:193] Recovering status update manager
I0507 03:27:38.935123  5779 slave.cpp:2945] Finished recovery
I0507 03:27:38.936998  5779 slave.cpp:526] New master detected at master@140.211.11.27:58462
I0507 03:27:38.937021  5779 slave.cpp:586] Authenticating with master master@140.211.11.27:58462
I0507 03:27:38.937077  5798 status_update_manager.cpp:167] New master detected at master@140.211.11.27:58462
I0507 03:27:38.937306 5779 slave.cpp:559] Detecting new master
I0507 03:27:38.937335  5800 authenticatee.hpp:128] Creating new client SASL connection
I0507 03:27:38.938030  5778 master.cpp:2798] Authenticating slave(56)@140.211.11.27:58462
I0507 03:27:38.938742 5783 authenticator.hpp:148] Creating new server SASL connection
I0507 03:27:38.939312  5786 authenticatee.hpp:219] Received SASL authentication mechanisms: CRAM-MD5
I0507 03:27:38.939340  5786 authenticatee.hpp:245] Attempting to authenticate with mechanism 'CRAM-MD5'
I0507 03:27:38.939390  5786 authenticator.hpp:254] Received SASL authentication start
I0507 03:27:38.939553  5786 authenticator.hpp:342] Authentication requires more steps
I0507 03:27:38.939592  5786 authenticatee.hpp:265] Received SASL authentication step
I0507 03:27:38.939715  5786 authenticator.hpp:282] Received SASL authentication step
I0507 03:27:38.939803  5786 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'hemera.apache.org' server FQDN: 'hemera.apache.org' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false
I0507 03:27:38.939821 5786 auxprop.cpp:153] Looking up auxiliary property '*userPassword'
I0507 03:27:38.939831  5786 auxprop.cpp:153] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0507 03:27:38.939841  5786 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'hemera.apache.org' server FQDN: 'hemera.apache.org' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true
I0507 03:27:38.939851 5786 auxprop.cpp:103] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0507 03:27:38.939857  5786 auxprop.cpp:103] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0507 03:27:38.939870  5786 authenticator.hpp:334] Authentication success
I0507 03:27:38.939937  5786 authenticatee.hpp:305] Authentication success
I0507 03:27:38.940016  5778 master.cpp:2838] Successfully authenticated slave(56)@140.211.11.27:58462
I0507 03:27:38.940449 5799 slave.cpp:643] Successfully authenticated with master master@140.211.11.27:58462
I0507 03:27:38.940513 5799 slave.cpp:872] Will retry registration in 5.176207635secs if necessary
I0507 03:27:38.940625  5794 master.cpp:2134] Registering slave at slave(56)@140.211.11.27:58462 (hemera.apache.org) with id 20140507-032738-453759884-58462-5758-0
I0507 03:27:38.940800 5796 registrar.cpp:422] Attempting to update the 'registry'
I0507 03:27:38.940850  5781 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 10.659152ms
I0507 03:27:38.940871  5781 replica.cpp:676] Persisted action at 2
I0507 03:27:38.941843  5788 replica.cpp:655] Replica received learned notice for position 2
I0507 03:27:38.953193  5788 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 11.291343ms
I0507 03:27:38.953258  5788 leveldb.cpp:399] Deleting ~1 keys from leveldb took 33725ns
I0507 03:27:38.953274  5788 replica.cpp:676] Persisted action at 2
I0507 03:27:38.953282  5788 replica.cpp:661] Replica learned TRUNCATE action at position 2
I0507 03:27:38.953541  5797 log.cpp:680] Attempting to append 330 bytes to the log
I0507 03:27:38.953614  5797 coordinator.cpp:340] Coordinator attempting to write APPEND action at position 3
I0507 03:27:38.954731  5789 replica.cpp:508] Replica received write request for position 3
I0507 03:27:38.965240  5789 leveldb.cpp:341] Persisting action (349 bytes) to leveldb took 10.489719ms
I0507 03:27:38.965261  5789 replica.cpp:676] Persisted action at 3
I0507 03:27:38.966253  5780 replica.cpp:655] Replica received learned notice for position 3
I0507 03:27:38.977375  5780 leveldb.cpp:341] Persisting action (351 bytes) to leveldb took 11.098798ms
I0507 03:27:38.977408  5780 replica.cpp:676] Persisted action at 3
I0507 03:27:38.977421  5780 replica.cpp:661] Replica learned APPEND action at position 3
I0507 03:27:38.977859  5792 registrar.cpp:479] Successfully updated 'registry'
I0507 03:27:38.977926  5780 log.cpp:699] Attempting to truncate the log to 3
I0507 03:27:38.978060  5792 master.cpp:2174] Registered slave 20140507-032738-453759884-58462-5758-0 at slave(56)@140.211.11.27:58462 (hemera.apache.org)
I0507 03:27:38.978112  5792 master.cpp:3283] Adding slave 20140507-032738-453759884-58462-5758-0 at slave(56)@140.211.11.27:58462 (hemera.apache.org) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I0507 03:27:38.978134  5784 coordinator.cpp:340] Coordinator attempting to write TRUNCATE action at position 4
I0507 03:27:38.978508  5785 slave.cpp:676] Registered with master master@140.211.11.27:58462; given slave ID 20140507-032738-453759884-58462-5758-0
I0507 03:27:38.978631 5786 hierarchical_allocator_process.hpp:444] Added slave 20140507-032738-453759884-58462-5758-0 (hemera.apache.org) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (and cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] available)
I0507 03:27:38.978677  5786 hierarchical_allocator_process.hpp:707] Performed allocation for slave 20140507-032738-453759884-58462-5758-0 in 5421ns
I0507 03:27:38.979872  5796 replica.cpp:508] Replica received write request for position 4
I0507 03:27:38.982084  5758 sched.cpp:121] Version: 0.19.0
I0507 03:27:38.982213  5789 sched.cpp:217] New master detected at master@140.211.11.27:58462
I0507 03:27:38.982228  5789 sched.cpp:268] Authenticating with master master@140.211.11.27:58462
I0507 03:27:38.982347  5788 authenticatee.hpp:128] Creating new client SASL connection
I0507 03:27:38.982676  5788 master.cpp:2798] Authenticating scheduler(59)@140.211.11.27:58462
I0507 03:27:38.983100  5788 authenticator.hpp:148] Creating new server SASL connection
I0507 03:27:38.983294  5788 authenticatee.hpp:219] Received SASL authentication mechanisms: CRAM-MD5
I0507 03:27:38.983312  5788 authenticatee.hpp:245] Attempting to authenticate with mechanism 'CRAM-MD5'
I0507 03:27:38.983360  5788 authenticator.hpp:254] Received SASL authentication start
I0507 03:27:38.983505  5788 authenticator.hpp:342] Authentication requires more steps
I0507 03:27:38.984220  5782 authenticatee.hpp:265] Received SASL authentication step
I0507 03:27:38.984275  5782 authenticator.hpp:282] Received SASL authentication step
I0507 03:27:38.984315  5782 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'hemera.apache.org' server FQDN: 'hemera.apache.org' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false
I0507 03:27:38.984347 5782 auxprop.cpp:153] Looking up auxiliary property '*userPassword'
I0507 03:27:38.984359  5782 auxprop.cpp:153] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0507 03:27:38.984370  5782 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'hemera.apache.org' server FQDN: 'hemera.apache.org' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true
I0507 03:27:38.984377 5782 auxprop.cpp:103] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0507 03:27:38.984383  5782 auxprop.cpp:103] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0507 03:27:38.984397  5782 authenticator.hpp:334] Authentication success
I0507 03:27:38.984429  5782 authenticatee.hpp:305] Authentication success
I0507 03:27:38.984469  5795 master.cpp:2838] Successfully authenticated scheduler(59)@140.211.11.27:58462
I0507 03:27:38.985110  5782 sched.cpp:342] Successfully authenticated with master master@140.211.11.27:58462
I0507 03:27:38.985133  5782 sched.cpp:461] Sending registration request to master@140.211.11.27:58462
I0507 03:27:38.985326 5795 master.cpp:980] Received registration request from scheduler(59)@140.211.11.27:58462
I0507 03:27:38.985357  5795 master.cpp:998] Registering framework 20140507-032738-453759884-58462-5758-0000 at scheduler(59)@140.211.11.27:58462
I0507 03:27:38.985424  5795 sched.cpp:392] Framework registered with 20140507-032738-453759884-58462-5758-0000
I0507 03:27:38.985471  5792 hierarchical_allocator_process.hpp:331] Added framework 20140507-032738-453759884-58462-5758-0000
I0507 03:27:38.985610  5795 sched.cpp:406] Scheduler::registered took 36702ns
I0507 03:27:38.985646  5792 hierarchical_allocator_process.hpp:751] Offering cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave 20140507-032738-453759884-58462-5758-0 to framework 20140507-032738-453759884-58462-5758-0000
I0507 03:27:38.985954  5792 hierarchical_allocator_process.hpp:687] Performed allocation for 1 slaves in 330895ns
I0507 03:27:38.986001  5789 master.hpp:612] Adding offer 20140507-032738-453759884-58462-5758-0 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave 20140507-032738-453759884-58462-5758-0 (hemera.apache.org)
I0507 03:27:38.986090  5789 master.cpp:2747] Sending 1 offers to framework 20140507-032738-453759884-58462-5758-0000
I0507 03:27:38.986548  5792 sched.cpp:529] Scheduler::resourceOffers took 162873ns
I0507 03:27:38.986721  5792 master.hpp:622] Removing offer 20140507-032738-453759884-58462-5758-0 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave 20140507-032738-453759884-58462-5758-0 (hemera.apache.org)
I0507 03:27:38.986781  5792 master.cpp:1812] Processing reply for offers: [ 20140507-032738-453759884-58462-5758-0 ] on slave 20140507-032738-453759884-58462-5758-0 at slave(56)@140.211.11.27:58462 (hemera.apache.org) for framework 20140507-032738-453759884-58462-5758-0000
I0507 03:27:38.986843  5792 master.hpp:584] Adding task 0 with resources cpus(*):2; mem(*):1024 on slave 20140507-032738-453759884-58462-5758-0 (hemera.apache.org)
I0507 03:27:38.986876  5792 master.cpp:2922] Launching task 0 of framework 20140507-032738-453759884-58462-5758-0000 with resources cpus(*):2; mem(*):1024 on slave 20140507-032738-453759884-58462-5758-0 at slave(56)@140.211.11.27:58462 (hemera.apache.org)
I0507 03:27:38.986981  5795 slave.cpp:906] Got assigned task 0 for framework 20140507-032738-453759884-58462-5758-0000
I0507 03:27:38.987180  5795 slave.cpp:1016] Launching task 0 for framework 20140507-032738-453759884-58462-5758-0000
I0507 03:27:38.987203  5787 hierarchical_allocator_process.hpp:546] Framework 20140507-032738-453759884-58462-5758-0000 left disk(*):1024; ports(*):[31000-32000] unused on slave 20140507-032738-453759884-58462-5758-0
I0507 03:27:38.987287  5787 hierarchical_allocator_process.hpp:589] Framework 20140507-032738-453759884-58462-5758-0000 filtered slave 20140507-032738-453759884-58462-5758-0 for 5secs
I0507 03:27:38.991395  5795 exec.cpp:131] Version: 0.19.0
I0507 03:27:38.991497  5779 exec.cpp:181] Executor started at: executor(27)@140.211.11.27:58462 with pid 5758
I0507 03:27:38.991510  5795 slave.cpp:1126] Queuing task '0' for executor default of framework '20140507-032738-453759884-58462-5758-0000
I0507 03:27:38.991566  5795 slave.cpp:487] Successfully attached file '/tmp/GarbageCollectorIntegrationTest_DiskUsage_A9Pxks/slaves/20140507-032738-453759884-58462-5758-0/frameworks/20140507-032738-453759884-58462-5758-0000/executors/default/runs/de776bec-2822-4bbc-befc-eec40eb5f674'
I0507 03:27:38.991595  5795 slave.cpp:2283] Monitoring executor 'default' of framework '20140507-032738-453759884-58462-5758-0000' in container 'de776bec-2822-4bbc-befc-eec40eb5f674'
I0507 03:27:38.991778  5795 slave.cpp:1599] Got registration for executor 'default' of framework 20140507-032738-453759884-58462-5758-0000
I0507 03:27:38.991874  5795 slave.cpp:1718] Flushing queued task 0 for executor 'default' of framework 20140507-032738-453759884-58462-5758-0000
I0507 03:27:38.991935  5780 exec.cpp:205] Executor registered on slave 20140507-032738-453759884-58462-5758-0
I0507 03:27:38.993419  5796 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 13.489998ms
I0507 03:27:38.993449  5796 replica.cpp:676] Persisted action at 4
I0507 03:27:38.994510  5777 replica.cpp:655] Replica received learned notice for position 4
I0507 03:27:38.994753  5780 exec.cpp:217] Executor::registered took 14516ns
I0507 03:27:38.994818  5780 exec.cpp:292] Executor asked to run task '0'
I0507 03:27:38.994849  5780 exec.cpp:301] Executor::launchTask took 18872ns
I0507 03:27:38.996703  5780 exec.cpp:524] Executor sending status update TASK_RUNNING (UUID: be7346ad-e198-4b38-9252-421ff759fdee) for task 0 of framework 20140507-032738-453759884-58462-5758-0000
I0507 03:27:38.996793  5780 slave.cpp:1954] Handling status update TASK_RUNNING (UUID: be7346ad-e198-4b38-9252-421ff759fdee) for task 0 of framework 20140507-032738-453759884-58462-5758-0000 from executor(27)@140.211.11.27:58462
I0507 03:27:38.996888  5780 status_update_manager.cpp:320] Received status update TASK_RUNNING (UUID: be7346ad-e198-4b38-9252-421ff759fdee) for task 0 of framework 20140507-032738-453759884-58462-5758-0000
I0507 03:27:38.996920  5780 status_update_manager.cpp:499] Creating StatusUpdate stream for task 0 of framework 20140507-032738-453759884-58462-5758-0000
I0507 03:27:38.996968  5780 status_update_manager.cpp:373] Forwarding status update TASK_RUNNING (UUID: be7346ad-e198-4b38-9252-421ff759fdee) for task 0 of framework 20140507-032738-453759884-58462-5758-0000 to master@140.211.11.27:58462
I0507 03:27:38.997189  5790 master.cpp:2450] Status update TASK_RUNNING (UUID: be7346ad-e198-4b38-9252-421ff759fdee) for task 0 of framework 20140507-032738-453759884-58462-5758-0000 from slave 20140507-032738-453759884-58462-5758-0 at slave(56)@140.211.11.27:58462 (hemera.apache.org)
I0507 03:27:38.997268  5780 slave.cpp:2071] Status update manager successfully handled status update TASK_RUNNING (UUID: be7346ad-e198-4b38-9252-421ff759fdee) for task 0 of framework 20140507-032738-453759884-58462-5758-0000
I0507 03:27:38.997321  5797 sched.cpp:620] Scheduler::statusUpdate took 77906ns
I0507 03:27:38.997336  5780 slave.cpp:2077] Sending acknowledgement for status update TASK_RUNNING (UUID: be7346ad-e198-4b38-9252-421ff759fdee) for task 0 of framework 20140507-032738-453759884-58462-5758-0000 to executor(27)@140.211.11.27:58462
I0507 03:27:38.998700  5797 slave.cpp:2341] Executor 'default' of framework 20140507-032738-453759884-58462-5758-0000 has exited with status 0
I0507 03:27:38.998814  5793 exec.cpp:338] Executor received status update acknowledgement be7346ad-e198-4b38-9252-421ff759fdee for task 0 of framework 20140507-032738-453759884-58462-5758-0000
I0507 03:27:39.000041  5797 slave.cpp:1954] Handling status update TASK_LOST (UUID: 4c8e572c-3fa7-43f3-aaf8-f82e77a70c1b) for task 0 of framework 20140507-032738-453759884-58462-5758-0000 from @0.0.0.0:0
I0507 03:27:39.000063  5797 slave.cpp:3446] Terminating task 0
I0507 03:27:39.000190  5797 status_update_manager.cpp:320] Received status update TASK_LOST (UUID: 4c8e572c-3fa7-43f3-aaf8-f82e77a70c1b) for task 0 of framework 20140507-032738-453759884-58462-5758-0000
I0507 03:27:39.000229  5779 master.cpp:2523] Executor default of framework 20140507-032738-453759884-58462-5758-0000 on slave 20140507-032738-453759884-58462-5758-0 at slave(56)@140.211.11.27:58462 (hemera.apache.org) has exited with status 0
I0507 03:27:39.000341  5797 status_update_manager.cpp:398] Received status update acknowledgement (UUID: be7346ad-e198-4b38-9252-421ff759fdee) for task 0 of framework 20140507-032738-453759884-58462-5758-0000
I0507 03:27:39.000385  5797 status_update_manager.cpp:373] Forwarding status update TASK_LOST (UUID: 4c8e572c-3fa7-43f3-aaf8-f82e77a70c1b) for task 0 of framework 20140507-032738-453759884-58462-5758-0000 to master@140.211.11.27:58462
I0507 03:27:39.000516  5791 slave.cpp:2071] Status update manager successfully handled status update TASK_LOST (UUID: 4c8e572c-3fa7-43f3-aaf8-f82e77a70c1b) for task 0 of framework 20140507-032738-453759884-58462-5758-0000
I0507 03:27:39.000686  5791 slave.cpp:1539] Status update manager successfully handled status update acknowledgement (UUID: be7346ad-e198-4b38-9252-421ff759fdee) for task 0 of framework 20140507-032738-453759884-58462-5758-0000
I0507 03:27:39.000759  5795 master.cpp:2450] Status update TASK_LOST (UUID: 4c8e572c-3fa7-43f3-aaf8-f82e77a70c1b) for task 0 of framework 20140507-032738-453759884-58462-5758-0000 from slave 20140507-032738-453759884-58462-5758-0 at slave(56)@140.211.11.27:58462 (hemera.apache.org)
I0507 03:27:39.000841  5784 sched.cpp:620] Scheduler::statusUpdate took 11418ns
I0507 03:27:39.000849  5795 master.hpp:602] Removing task 0 with resources cpus(*):2; mem(*):1024 on slave 20140507-032738-453759884-58462-5758-0 (hemera.apache.org)
I0507 03:27:39.001313  5799 hierarchical_allocator_process.hpp:636] Recovered cpus(*):2; mem(*):1024 (total allocatable: disk(*):1024; ports(*):[31000-32000]; cpus(*):2; mem(*):1024) on slave 20140507-032738-453759884-58462-5758-0 from framework 20140507-032738-453759884-58462-5758-0000
I0507 03:27:39.002792  5778 status_update_manager.cpp:398] Received status update acknowledgement (UUID: 4c8e572c-3fa7-43f3-aaf8-f82e77a70c1b) for task 0 of framework 20140507-032738-453759884-58462-5758-0000
I0507 03:27:39.002831  5778 status_update_manager.cpp:530] Cleaning up status update stream for task 0 of framework 20140507-032738-453759884-58462-5758-0000
I0507 03:27:39.002903  5778 slave.cpp:1539] Status update manager successfully handled status update acknowledgement (UUID: 4c8e572c-3fa7-43f3-aaf8-f82e77a70c1b) for task 0 of framework 20140507-032738-453759884-58462-5758-0000
I0507 03:27:39.002976  5778 slave.cpp:3470] Completing task 0
I0507 03:27:39.002991  5778 slave.cpp:2480] Cleaning up executor 'default' of framework 20140507-032738-453759884-58462-5758-0000
I0507 03:27:39.006098  5778 slave.cpp:2555] Cleaning up framework 20140507-032738-453759884-58462-5758-0000
I0507 03:27:39.006105  5800 gc.cpp:56] Scheduling '/tmp/GarbageCollectorIntegrationTest_DiskUsage_A9Pxks/slaves/20140507-032738-453759884-58462-5758-0/frameworks/20140507-032738-453759884-58462-5758-0000/executors/default/runs/de776bec-2822-4bbc-befc-eec40eb5f674' for gc 1.00000000231788weeks in the future
I0507 03:27:39.006146  5800 gc.cpp:56] Scheduling '/tmp/GarbageCollectorIntegrationTest_DiskUsage_A9Pxks/slaves/20140507-032738-453759884-58462-5758-0/frameworks/20140507-032738-453759884-58462-5758-0000/executors/default' for gc 1.00000000231788weeks in the future
I0507 03:27:39.006211  5786 status_update_manager.cpp:282] Closing status update streams for framework 20140507-032738-453759884-58462-5758-0000
I0507 03:27:39.006299  5786 gc.cpp:56] Scheduling '/tmp/GarbageCollectorIntegrationTest_DiskUsage_A9Pxks/slaves/20140507-032738-453759884-58462-5758-0/frameworks/20140507-032738-453759884-58462-5758-0000' for gc 1.00000000231788weeks in the future
I0507 03:27:39.010058  5777 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 15.533184ms
I0507 03:27:39.010144  5777 leveldb.cpp:399] Deleting ~2 keys from leveldb took 64787ns
I0507 03:27:39.010154  5777 replica.cpp:676] Persisted action at 4
I0507 03:27:39.010160  5777 replica.cpp:661] Replica learned TRUNCATE action at position 4
I0507 03:27:39.029413  5789 slave.cpp:2801] Current usage 90.00%. Max allowed age: 0ns
../../src/tests/gc_tests.cpp:658: Failure
Value of: os::exists(executorDir)
  Actual: true
Expected: false
{noformat}",Bug,Major,bmahler,,1,Open,New,GarbageCollectorIntegrationTest.DiskUsage is flaky.,2015-05-13T23:30:49.000+0000,MESOS-1347,2.0,mesos,Q2'14 Sprint 2
xujyan,2014-05-09T21:03:18.000+0000,xujyan,Framework::principal is used identify one or more frameworks. If multiple frameworks use the same principal they'll have one counter showing their combined message count.,Improvement,Major,xujyan,2014-06-17T21:24:02.000+0000,5,Resolved,Complete,"Add ""per-framework-principal"" counters for all messages from a scheduler on Master",2014-06-17T21:24:03.000+0000,MESOS-1339,3.0,mesos,Q2 Sprint 3
dhamon,2014-05-08T23:40:36.000+0000,dhamon,"As we move the metrics to a new endpoint, we should consider revisiting the names of some of the current metrics to make them clearer.

It may also be worth considering changing some existing counter-style metrics to gauges.
",Improvement,Major,dhamon,2014-05-27T22:09:21.000+0000,5,Resolved,Complete,Improve Master and Slave metric names,2015-06-05T08:22:16.000+0000,MESOS-1332,3.0,mesos,Q2'14 Sprint 2
bernd-mesos,2014-05-06T21:43:00.000+0000,tarnfeld,"There are current no tests that cover the {{mesos-fetcher}} tool itself, and hence bugs like MESOS-1313 have accidentally slipped though.",Improvement,Major,tarnfeld,2014-11-08T03:29:23.000+0000,5,Resolved,Complete,Implement decent unit test coverage for the mesos-fetcher tool,2014-11-08T03:29:23.000+0000,MESOS-1316,2.0,mesos,Q3 Sprint 1
vinodkone,2014-05-05T20:46:31.000+0000,vinodkone,"When frameworks register or reregister they should authorize their roles.  

Split register framework / reregister framework.  
",Task,Major,vinodkone,2014-06-10T23:36:13.000+0000,5,Resolved,Complete,Authorize offer allocations,2014-06-10T23:36:14.000+0000,MESOS-1307,8.0,mesos,Q2'14 Sprint 2
tillt,2014-05-05T19:05:43.000+0000,idownes,"I'm having trouble reproducing this but I did observe it once on my OSX system:

{noformat}
[==========] Running 2 tests from 1 test case.
[----------] Global test environment set-up.
[----------] 2 tests from ExamplesTest
[ RUN      ] ExamplesTest.TestFramework
../../src/tests/script.cpp:81: Failure
Failed
test_framework_test.sh terminated with signal 'Abort trap: 6'
[  FAILED  ] ExamplesTest.TestFramework (953 ms)
[ RUN      ] ExamplesTest.NoExecutorFramework
[       OK ] ExamplesTest.NoExecutorFramework (10162 ms)
[----------] 2 tests from ExamplesTest (11115 ms total)

[----------] Global test environment tear-down
[==========] 2 tests from 1 test case ran. (11121 ms total)
[  PASSED  ] 1 test.
[  FAILED  ] 1 test, listed below:
[  FAILED  ] ExamplesTest.TestFramework
{noformat}

when investigating a failed make check for https://reviews.apache.org/r/20971/
{noformat}
[----------] 6 tests from ExamplesTest
[ RUN      ] ExamplesTest.TestFramework
[       OK ] ExamplesTest.TestFramework (8643 ms)
[ RUN      ] ExamplesTest.NoExecutorFramework
tests/script.cpp:81: Failure
Failed
no_executor_framework_test.sh terminated with signal 'Aborted'
[  FAILED  ] ExamplesTest.NoExecutorFramework (7220 ms)
[ RUN      ] ExamplesTest.JavaFramework
[       OK ] ExamplesTest.JavaFramework (11181 ms)
[ RUN      ] ExamplesTest.JavaException
[       OK ] ExamplesTest.JavaException (5624 ms)
[ RUN      ] ExamplesTest.JavaLog
[       OK ] ExamplesTest.JavaLog (6472 ms)
[ RUN      ] ExamplesTest.PythonFramework
[       OK ] ExamplesTest.PythonFramework (14467 ms)
[----------] 6 tests from ExamplesTest (53607 ms total)
{noformat}",Bug,Major,idownes,2015-06-24T18:11:08.000+0000,5,Resolved,Complete,"ExamplesTest.{TestFramework, NoExecutorFramework} flaky",2015-07-02T06:51:15.000+0000,MESOS-1303,1.0,mesos,Mesosphere Sprint 13
dhamon,2014-04-23T21:13:47.000+0000,dhamon,stout's os::ls returns a list that can be empty - instead it should return a Try<list...> to be consistent.,Improvement,Minor,dhamon,2014-07-30T19:05:03.000+0000,5,Resolved,Complete,stout's os::ls should return a Try<>,2014-07-30T19:05:03.000+0000,MESOS-1237,2.0,mesos,Q3 Sprint 2
dhamon,2014-04-23T21:11:45.000+0000,dhamon,stout's os module should use Try<Nothing> for return values throughout.,Improvement,Minor,dhamon,2014-07-29T21:46:47.000+0000,5,Resolved,Complete,stout's os module uses a mix of Try<Nothing> and bool returns,2014-07-29T21:46:47.000+0000,MESOS-1236,2.0,mesos,Q3 Sprint 2
vinodkone,2014-04-17T05:49:06.000+0000,rlacroix,"When a scheduler reconnects after the failover timeout has exceeded, the framework id is usually reused because the scheduler doesn't know that the timeout exceeded and it is actually handled as a new framework.

The /framework/:framework_id route of the Web UI doesn't handle those cases very well because its key is reused. It only shows the terminated one.

Would it make sense to ignore the provided framework id when a scheduler reconnects to a terminated framework and generate a new id to make sure it's unique?",Bug,Major,rlacroix,2014-08-19T01:54:12.000+0000,5,Resolved,Complete,Master should disallow frameworks that reconnect after failover timeout.,2014-09-17T00:42:44.000+0000,MESOS-1219,2.0,mesos,Q3 Sprint 1
idownes,2014-04-08T18:20:27.000+0000,idownes,"Subprocess uses process::reap to wait on the subprocess pid and set the exit status. However, process::reap polls with a one second interval resulting in a delay up to the interval duration before the status future is set.

This means if you need to wait for the subprocess to complete you get hit with E(delay) = 0.5 seconds, independent of the execution time. For example, the MesosContainerizer uses mesos-fetcher in a Subprocess to fetch the executor during launch. At Twitter we fetch a local file, i.e., a very fast operation, but the launch is blocked until the mesos-fetcher pid is reaped -> adding 0 to 1 seconds for every launch!

The problem is even worse with a chain of short Subprocesses because after the first Subprocess completes you'll be synchronized with the reap interval and you'll see nearly the full interval before notification, i.e., 10 Subprocesses each of << 1 second duration with take ~10 seconds!

This has become particularly apparent in some new tests I'm working on where test durations are now greatly extended with each taking several seconds.",Improvement,Major,idownes,2014-09-26T17:22:38.000+0000,5,Resolved,Complete,"Subprocess is ""slow"" -> gated by process::reap poll interval",2015-07-31T00:58:23.000+0000,MESOS-1199,1.0,mesos,Mesos Q3 Sprint 6
tstclair,2014-04-07T18:24:53.000+0000,tstclair,"When attempting to configure mesos to use systemd slices on a 'rawhide/f21' machine, it fails creating the isolator: 

I0407 12:39:28.035354 14916 containerizer.cpp:180] Using isolation: cgroups/cpu,cgroups/mem
Failed to create a containerizer: Could not create isolator cgroups/cpu: Failed to create isolator: The cpu subsystem is co-mounted at /sys/fs/cgroup/cpu with other subsytems

------ details ------
/sys/fs/cgroup
total 0
drwxr-xr-x. 12 root root 280 Mar 18 08:47 .
drwxr-xr-x.  6 root root   0 Mar 18 08:47 ..
drwxr-xr-x.  2 root root   0 Mar 18 08:47 blkio
lrwxrwxrwx.  1 root root  11 Mar 18 08:47 cpu -> cpu,cpuacct
lrwxrwxrwx.  1 root root  11 Mar 18 08:47 cpuacct -> cpu,cpuacct
drwxr-xr-x.  2 root root   0 Mar 18 08:47 cpu,cpuacct
drwxr-xr-x.  2 root root   0 Mar 18 08:47 cpuset
drwxr-xr-x.  2 root root   0 Mar 18 08:47 devices
drwxr-xr-x.  2 root root   0 Mar 18 08:47 freezer
drwxr-xr-x.  2 root root   0 Mar 18 08:47 hugetlb
drwxr-xr-x.  3 root root   0 Apr  3 11:26 memory
drwxr-xr-x.  2 root root   0 Mar 18 08:47 net_cls
drwxr-xr-x.  2 root root   0 Mar 18 08:47 perf_event
drwxr-xr-x.  4 root root   0 Mar 18 08:47 systemd
",Bug,Major,tstclair,2014-09-22T20:39:45.000+0000,5,Resolved,Complete,systemd.slice + cgroup enablement fails in multiple ways. ,2015-07-07T20:12:41.000+0000,MESOS-1195,3.0,mesos,Mesos Q3 Sprint 6
vinodkone,2014-03-26T20:48:43.000+0000,wfarner,"To safeguard against unforeseen bugs leading to widespread slave removal, it would be nice to allow for rate limiting of the decision to remove slaves and/or send TASK_LOST messages for tasks on those slaves.  Ideally this would allow an operator to be notified soon enough to intervene before causing cluster impact.",Improvement,Major,wfarner,2015-02-05T00:28:57.000+0000,5,Resolved,Complete,Add support for rate limiting slave removal,2015-02-25T00:16:44.000+0000,MESOS-1148,3.0,mesos,Twitter Mesos Q1 Sprint 1
dhamon,2014-03-24T16:13:43.000+0000,benjaminhindman,"During task validation we drop tasks that have errors and send TASK_LOST status updates. In most circumstances a framework will want to relaunch a task that has gone lost, and in the event the task is actually malformed (thus invalid) this will result in an infinite loop of sending a task and having it go lost.",Improvement,Major,benjaminhindman,2014-11-12T18:30:51.000+0000,5,Resolved,Complete,Add a TASK_ERROR task status.,2015-02-24T20:22:03.000+0000,MESOS-1143,2.0,mesos,Twitter Mesos Q4 Sprint 3
vinodkone,2014-03-20T17:23:15.000+0000,benjaminhindman,"The default scheduler/executor interface and implementation in Mesos have a few drawbacks:

(1) The interface is fairly high-level which makes it hard to do certain things, for example, handle events (callbacks) in batch. This can have a big impact on the performance of schedulers (for example, writing task updates that need to be persisted).

(2) The implementation requires writing a lot of boilerplate JNI and native Python wrappers when adding additional API components.

The plan is to provide a lower-level API that can easily be used to implement the higher-level API that is currently provided. This will also open the door to more easily building native-language Mesos libraries (i.e., not needing the C++ shim layer) and building new higher-level abstractions on top of the lower-level API.",Task,Major,benjaminhindman,2015-04-25T21:43:45.000+0000,5,Resolved,Complete,Implement the protobufs for the scheduler API,2015-04-25T21:43:45.000+0000,MESOS-1127,8.0,mesos,Twitter Mesos Q1 Sprint 5
,2014-03-18T20:00:09.000+0000,ijimenez,Integrate HTTP auth into the CLI programs,Improvement,Minor,ijimenez,,1,Open,New,HTTP auth for CLI,2015-10-19T06:40:43.000+0000,MESOS-1120,3.0,mesos,Mesosphere Sprint 10
vinodkone,2014-03-18T18:25:15.000+0000,vinodkone,"Currently the Allocator::allocate() code loops through roles and frameworks (based on DRF sort) and allocates *all* slaves resources to the first framework.

This logic should be a bit inversed. Instead, the slave should go through each slave, allocate it a role/framework and update the DRF shares.",Bug,Major,vinodkone,2014-08-08T06:33:33.000+0000,5,Resolved,Complete,Allocator should make an allocation decision per slave instead of per framework/role.,2014-08-08T06:33:33.000+0000,MESOS-1119,2.0,mesos,Q3 Sprint 1
vinodkone,2014-03-11T21:48:00.000+0000,adam-mesos,"Master should not deactivate an authenticated framework/slave upon receiving a new AuthenticateMessage unless new authentication succeeds. As it stands now, a malicious user could spoof the pid of an authenticated framework/slave and send an AuthenticateMessage to knock a valid framework/slave off the authenticated list, forcing the valid framework/slave to re-authenticate and re-register. This could be used in a DoS attack.
But how should we handle the scenario when the actual authenticated framework/slave sends an AuthenticateMessage that fails authentication?",Bug,Major,adam-mesos,2014-09-25T20:55:34.000+0000,5,Resolved,Complete,Master should not deactivate authenticated framework/slave on new AuthenticateMessage unless new authentication succeeds.,2014-09-25T20:55:34.000+0000,MESOS-1081,1.0,mesos,Mesos Q3 Sprint 5
greggomann,2014-02-18T21:34:32.000+0000,vinodkone,"The {{ExamplesTest.JavaLog}} test framework is flaky, possibly related to a race condition between mutexes.
{noformat}
[ RUN      ] ExamplesTest.JavaLog
Using temporary directory '/tmp/ExamplesTest_JavaLog_WBWEb9'
Feb 18, 2014 12:10:57 PM TestLog main
INFO: Starting a local ZooKeeper server
...
F0218 12:10:58.575036 17450 coordinator.cpp:394] Check failed: !missing Not expecting local replica to be missing position 3 after the writing is done
*** Check failure stack trace: ***
tests/script.cpp:81: Failure
Failed
java_log_test.sh terminated with signal 'Aborted'
[  FAILED  ] ExamplesTest.JavaLog (2166 ms)
{noformat}

Full logs attached.",Bug,Major,vinodkone,2015-08-16T17:54:02.000+0000,5,Resolved,Complete,ExamplesTest.JavaLog is flaky,2015-08-16T17:54:02.000+0000,MESOS-1013,2.0,mesos,Mesosphere Sprint 16
greggomann,2014-02-17T21:24:16.000+0000,nekto0n,"In my environment mesos build from master results in broken python api module {{_mesos.so}}:
{noformat}
nekto0n@ya-darkstar ~/workspace/mesos/src/python $ PYTHONPATH=build/lib.linux-x86_64-2.7/ python -c ""import _mesos""
Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
ImportError: /home/nekto0n/workspace/mesos/src/python/build/lib.linux-x86_64-2.7/_mesos.so: undefined symbol: _ZN6google14FlagRegistererC1EPKcS2_S2_S2_PvS3_
{noformat}
Unmangled version of symbol looks like this:
{noformat}
google::FlagRegisterer::FlagRegisterer(char const*, char const*, char const*, char const*, void*, void*)
{noformat}
During {{./configure}} step {{glog}} finds {{gflags}} development files and starts using them, thus *implicitly* adding dependency on {{libgflags.so}}. This breaks Python extensions module and perhaps can break other mesos subsystems when moved to hosts without {{gflags}} installed.

This task is done when the ExamplesTest.PythonFramework test will pass on a system with gflags installed.",Bug,Major,nekto0n,2015-08-20T01:38:15.000+0000,5,Resolved,Complete,Python extension build is broken if gflags-dev is installed,2015-08-20T01:38:15.000+0000,MESOS-1010,3.0,mesos,Mesosphere Sprint 16
,2014-02-12T23:57:59.000+0000,idownes,The current code will start launch a container and wait on it before the launch is complete. We should do this only after the container has successfully launched. Likewise for the executor registration timeout.,Bug,Minor,idownes,,10020,Accepted,In Progress,Slave should wait() and start executor registration timeout after launch ,2015-07-15T18:36:55.000+0000,MESOS-999,3.0,mesos,Twitter Mesos Q2 Sprint 5
jieyu,2014-02-12T23:54:28.000+0000,idownes,Container resources are updated in several places in the slave and we don't check the update was successful or even wait until it completes.,Bug,Major,idownes,2015-02-18T21:29:56.000+0000,5,Resolved,Complete,Slave should wait until Containerizer::update() completes successfully,2015-03-31T23:04:33.000+0000,MESOS-998,5.0,mesos,Twitter Mesos Q1 Sprint 2
vinodkone,2014-02-11T22:25:26.000+0000,vinodkone,"Looks like a SEGFAULT during shutdown.

{noformat}
[ RUN      ] ExamplesTest.PythonFramework
Using temporary directory '/tmp/ExamplesTest_PythonFramework_RZ4yaf'
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0211 21:14:47.861803 21045 process.cpp:1591] libprocess is initialized on 67.195.138.9:53443 for 8 cpus
I0211 21:14:47.861884 21045 logging.cpp:140] Logging to STDERR
I0211 21:14:47.862761 21045 master.cpp:240] Master ID: 2014-02-11-21:14:47-160088899-53443-21045 Hostname: vesta.apache.org
I0211 21:14:47.862897 21054 master.cpp:322] Master started on 67.195.138.9:53443
I0211 21:14:47.862908 21054 master.cpp:325] Master only allowing authenticated frameworks to register!
I0211 21:14:47.864362 21053 master.cpp:86] No whitelist given. Advertising offers for all slaves
I0211 21:14:47.864506 21055 slave.cpp:112] Slave started on 1)@67.195.138.9:53443
I0211 21:14:47.864522 21059 slave.cpp:112] Slave started on 2)@67.195.138.9:53443
I0211 21:14:47.864749 21055 slave.cpp:212] Slave resources: cpus(*):8; mem(*):6961; disk(*):1.38501e+06; ports(*):[31000-32000]
I0211 21:14:47.864778 21059 slave.cpp:212] Slave resources: cpus(*):8; mem(*):6961; disk(*):1.38501e+06; ports(*):[31000-32000]
I0211 21:14:47.864819 21055 slave.cpp:240] Slave hostname: vesta.apache.org
I0211 21:14:47.864827 21055 slave.cpp:241] Slave checkpoint: true
I0211 21:14:47.864850 21059 slave.cpp:240] Slave hostname: vesta.apache.org
I0211 21:14:47.864858 21059 slave.cpp:241] Slave checkpoint: true
I0211 21:14:47.865329 21055 master.cpp:760] The newly elected leader is master@67.195.138.9:53443 with id 2014-02-11-21:14:47-160088899-53443-21045
I0211 21:14:47.865350 21055 master.cpp:770] Elected as the leading master!
I0211 21:14:47.865399 21055 state.cpp:33] Recovering state from '/tmp/mesos-Z8v6cu/1/meta'
I0211 21:14:47.865407 21059 state.cpp:33] Recovering state from '/tmp/mesos-Z8v6cu/0/meta'
I0211 21:14:47.865502 21052 hierarchical_allocator_process.hpp:302] Initializing hierarchical allocator process with master : master@67.195.138.9:53443
I0211 21:14:47.865540 21054 status_update_manager.cpp:188] Recovering status update manager
I0211 21:14:47.865619 21053 process_isolator.cpp:319] Recovering isolator
I0211 21:14:47.865674 21057 status_update_manager.cpp:188] Recovering status update manager
I0211 21:14:47.865699 21059 slave.cpp:2760] Finished recovery
I0211 21:14:47.865733 21053 process_isolator.cpp:319] Recovering isolator
I0211 21:14:47.865789 21053 slave.cpp:2760] Finished recovery
I0211 21:14:47.865921 21059 slave.cpp:508] New master detected at master@67.195.138.9:53443
I0211 21:14:47.865958 21053 status_update_manager.cpp:162] New master detected at master@67.195.138.9:53443
I0211 21:14:47.865978 21059 slave.cpp:533] Detecting new master
I0211 21:14:47.866019 21053 slave.cpp:508] New master detected at master@67.195.138.9:53443
I0211 21:14:47.866063 21053 slave.cpp:533] Detecting new master
I0211 21:14:47.866070 21055 status_update_manager.cpp:162] New master detected at master@67.195.138.9:53443
I0211 21:14:47.866077 21059 master.cpp:1840] Attempting to register slave on vesta.apache.org at slave(2)@67.195.138.9:53443
I0211 21:14:47.866092 21059 master.cpp:2810] Adding slave 2014-02-11-21:14:47-160088899-53443-21045-0 at vesta.apache.org with cpus(*):8; mem(*):6961; disk(*):1.38501e+06; ports(*):[31000-32000]
I0211 21:14:47.866216 21059 master.cpp:1840] Attempting to register slave on vesta.apache.org at slave(1)@67.195.138.9:53443
I0211 21:14:47.866225 21053 slave.cpp:551] Registered with master master@67.195.138.9:53443; given slave ID 2014-02-11-21:14:47-160088899-53443-21045-0
I0211 21:14:47.866228 21059 master.cpp:2810] Adding slave 2014-02-11-21:14:47-160088899-53443-21045-1 at vesta.apache.org with cpus(*):8; mem(*):6961; disk(*):1.38501e+06; ports(*):[31000-32000]
I0211 21:14:47.866278 21055 hierarchical_allocator_process.hpp:445] Added slave 2014-02-11-21:14:47-160088899-53443-21045-0 (vesta.apache.org) with cpus(*):8; mem(*):6961; disk(*):1.38501e+06; ports(*):[31000-32000] (and cpus(*):8; mem(*):6961; disk(*):1.38501e+06; ports(*):[31000-32000] available)
I0211 21:14:47.866297 21059 slave.cpp:551] Registered with master master@67.195.138.9:53443; given slave ID 2014-02-11-21:14:47-160088899-53443-21045-1
I0211 21:14:47.866327 21055 hierarchical_allocator_process.hpp:708] Performed allocation for slave 2014-02-11-21:14:47-160088899-53443-21045-0 in 11us
I0211 21:14:47.866330 21053 slave.cpp:564] Checkpointing SlaveInfo to '/tmp/mesos-Z8v6cu/1/meta/slaves/2014-02-11-21:14:47-160088899-53443-21045-0/slave.info'
I0211 21:14:47.866400 21059 slave.cpp:564] Checkpointing SlaveInfo to '/tmp/mesos-Z8v6cu/0/meta/slaves/2014-02-11-21:14:47-160088899-53443-21045-1/slave.info'
I0211 21:14:47.866399 21055 hierarchical_allocator_process.hpp:445] Added slave 2014-02-11-21:14:47-160088899-53443-21045-1 (vesta.apache.org) with cpus(*):8; mem(*):6961; disk(*):1.38501e+06; ports(*):[31000-32000] (and cpus(*):8; mem(*):6961; disk(*):1.38501e+06; ports(*):[31000-32000] available)
I0211 21:14:47.866423 21055 hierarchical_allocator_process.hpp:708] Performed allocation for slave 2014-02-11-21:14:47-160088899-53443-21045-1 in 2505ns
I0211 21:14:47.866636 21059 slave.cpp:112] Slave started on 3)@67.195.138.9:53443
I0211 21:14:47.866727 21059 slave.cpp:212] Slave resources: cpus(*):8; mem(*):6961; disk(*):1.38501e+06; ports(*):[31000-32000]
I0211 21:14:47.866766 21059 slave.cpp:240] Slave hostname: vesta.apache.org
I0211 21:14:47.866772 21059 slave.cpp:241] Slave checkpoint: true
I0211 21:14:47.867300 21052 state.cpp:33] Recovering state from '/tmp/mesos-Z8v6cu/2/meta'
I0211 21:14:47.867368 21052 status_update_manager.cpp:188] Recovering status update manager
I0211 21:14:47.867419 21055 process_isolator.cpp:319] Recovering isolator
I0211 21:14:47.867544 21052 slave.cpp:2760] Finished recovery
I0211 21:14:47.867729 21052 slave.cpp:508] New master detected at master@67.195.138.9:53443
I0211 21:14:47.867770 21054 status_update_manager.cpp:162] New master detected at master@67.195.138.9:53443
I0211 21:14:47.867777 21052 slave.cpp:533] Detecting new master
I0211 21:14:47.867815 21055 master.cpp:1840] Attempting to register slave on vesta.apache.org at slave(3)@67.195.138.9:53443
I0211 21:14:47.867827 21055 master.cpp:2810] Adding slave 2014-02-11-21:14:47-160088899-53443-21045-2 at vesta.apache.org with cpus(*):8; mem(*):6961; disk(*):1.38501e+06; ports(*):[31000-32000]
I0211 21:14:47.867885 21052 slave.cpp:551] Registered with master master@67.195.138.9:53443; given slave ID 2014-02-11-21:14:47-160088899-53443-21045-2
I0211 21:14:47.867961 21055 hierarchical_allocator_process.hpp:445] Added slave 2014-02-11-21:14:47-160088899-53443-21045-2 (vesta.apache.org) with cpus(*):8; mem(*):6961; disk(*):1.38501e+06; ports(*):[31000-32000] (and cpus(*):8; mem(*):6961; disk(*):1.38501e+06; ports(*):[31000-32000] available)
I0211 21:14:47.867985 21052 slave.cpp:564] Checkpointing SlaveInfo to '/tmp/mesos-Z8v6cu/2/meta/slaves/2014-02-11-21:14:47-160088899-53443-21045-2/slave.info'
I0211 21:14:47.867987 21055 hierarchical_allocator_process.hpp:708] Performed allocation for slave 2014-02-11-21:14:47-160088899-53443-21045-2 in 3308ns
I0211 21:14:47.868468 21045 sched.cpp:121] Version: 0.18.0
I0211 21:14:47.868633 21055 sched.cpp:217] New master detected at master@67.195.138.9:53443
I0211 21:14:47.868651 21055 sched.cpp:268] Authenticating with master master@67.195.138.9:53443
I0211 21:14:47.868696 21055 sched.cpp:237] Detecting new master
I0211 21:14:47.868708 21054 authenticatee.hpp:100] Initializing client SASL
I0211 21:14:47.869549 21054 authenticatee.hpp:124] Creating new client SASL connection
I0211 21:14:47.869633 21055 master.cpp:2323] Authenticating framework at scheduler(1)@67.195.138.9:53443
I0211 21:14:47.869818 21059 authenticator.hpp:83] Initializing server SASL
I0211 21:14:47.870029 21059 auxprop.cpp:45] Initialized in-memory auxiliary property plugin
I0211 21:14:47.870040 21059 authenticator.hpp:140] Creating new server SASL connection
I0211 21:14:47.870144 21057 authenticatee.hpp:212] Received SASL authentication mechanisms: CRAM-MD5
I0211 21:14:47.870174 21057 authenticatee.hpp:238] Attempting to authenticate with mechanism 'CRAM-MD5'
I0211 21:14:47.870203 21057 authenticator.hpp:243] Received SASL authentication start
I0211 21:14:47.870256 21057 authenticator.hpp:325] Authentication requires more steps
I0211 21:14:47.870282 21057 authenticatee.hpp:258] Received SASL authentication step
I0211 21:14:47.870348 21057 authenticator.hpp:271] Received SASL authentication step
I0211 21:14:47.870376 21057 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'vesta.apache.org' server FQDN: 'vesta.apache.org' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0211 21:14:47.870384 21057 auxprop.cpp:153] Looking up auxiliary property '*userPassword'
I0211 21:14:47.870396 21057 auxprop.cpp:153] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0211 21:14:47.870405 21057 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'vesta.apache.org' server FQDN: 'vesta.apache.org' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0211 21:14:47.870411 21057 auxprop.cpp:103] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0211 21:14:47.870415 21057 auxprop.cpp:103] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0211 21:14:47.870425 21057 authenticator.hpp:317] Authentication success
I0211 21:14:47.870445 21057 master.cpp:2363] Successfully authenticated framework at scheduler(1)@67.195.138.9:53443
I0211 21:14:47.870448 21055 authenticatee.hpp:298] Authentication success
I0211 21:14:47.870492 21055 sched.cpp:342] Successfully authenticated with master master@67.195.138.9:53443
I0211 21:14:47.870538 21057 master.cpp:818] Received registration request from scheduler(1)@67.195.138.9:53443
I0211 21:14:47.870590 21057 master.cpp:836] Registering framework 2014-02-11-21:14:47-160088899-53443-21045-0000 at scheduler(1)@67.195.138.9:53443
I0211 21:14:47.870661 21055 sched.cpp:391] Framework registered with 2014-02-11-21:14:47-160088899-53443-21045-0000
I0211 21:14:47.870661 21057 hierarchical_allocator_process.hpp:332] Added framework 2014-02-11-21:14:47-160088899-53443-21045-0000
I0211 21:14:47.870707 21057 hierarchical_allocator_process.hpp:752] Offering cpus(*):8; mem(*):6961; disk(*):1.38501e+06; ports(*):[31000-32000] on slave 2014-02-11-21:14:47-160088899-53443-21045-0 to framework 2014-02-11-21:14:47-160088899-53443-21045-0000
I0211 21:14:47.870798 21057 hierarchical_allocator_process.hpp:752] Offering cpus(*):8; mem(*):6961; disk(*):1.38501e+06; ports(*):[31000-32000] on slave 2014-02-11-21:14:47-160088899-53443-21045-1 to framework 2014-02-11-21:14:47-160088899-53443-21045-0000
I0211 21:14:47.870869 21057 hierarchical_allocator_process.hpp:752] Offering cpus(*):8; mem(*):6961; disk(*):1.38501e+06; ports(*):[31000-32000] on slave 2014-02-11-21:14:47-160088899-53443-21045-2 to framework 2014-02-11-21:14:47-160088899-53443-21045-0000
I0211 21:14:47.870894 21055 sched.cpp:405] Scheduler::registered took 222149ns
I0211 21:14:47.871038 21057 hierarchical_allocator_process.hpp:688] Performed allocation for 3 slaves in 351098ns
I0211 21:14:47.871106 21058 master.hpp:439] Adding offer 2014-02-11-21:14:47-160088899-53443-21045-0 with resources cpus(*):8; mem(*):6961; disk(*):1.38501e+06; ports(*):[31000-32000] on slave 2014-02-11-21:14:47-160088899-53443-21045-2 (vesta.apache.org)
I0211 21:14:47.871215 21058 master.hpp:439] Adding offer 2014-02-11-21:14:47-160088899-53443-21045-1 with resources cpus(*):8; mem(*):6961; disk(*):1.38501e+06; ports(*):[31000-32000] on slave 2014-02-11-21:14:47-160088899-53443-21045-1 (vesta.apache.org)
I0211 21:14:47.871296 21058 master.hpp:439] Adding offer 2014-02-11-21:14:47-160088899-53443-21045-2 with resources cpus(*):8; mem(*):6961; disk(*):1.38501e+06; ports(*):[31000-32000] on slave 2014-02-11-21:14:47-160088899-53443-21045-0 (vesta.apache.org)
I0211 21:14:47.871333 21058 master.cpp:2278] Sending 3 offers to framework 2014-02-11-21:14:47-160088899-53443-21045-0000
I0211 21:14:47.873667 21055 sched.cpp:525] Scheduler::resourceOffers took 2.150843ms
I0211 21:14:47.873884 21053 master.hpp:449] Removing offer 2014-02-11-21:14:47-160088899-53443-21045-0 with resources cpus(*):8; mem(*):6961; disk(*):1.38501e+06; ports(*):[31000-32000] on slave 2014-02-11-21:14:47-160088899-53443-21045-2 (vesta.apache.org)
I0211 21:14:47.873934 21053 master.cpp:1574] Processing reply for offers: [ 2014-02-11-21:14:47-160088899-53443-21045-0 ] on slave 2014-02-11-21:14:47-160088899-53443-21045-2 (vesta.apache.org) for framework 2014-02-11-21:14:47-160088899-53443-21045-0000
I0211 21:14:47.874035 21053 master.hpp:411] Adding task 0 with resources cpus(*):1; mem(*):32 on slave 2014-02-11-21:14:47-160088899-53443-21045-2 (vesta.apache.org)
I0211 21:14:47.874059 21053 master.cpp:2447] Launching task 0 of framework 2014-02-11-21:14:47-160088899-53443-21045-0000 with resources cpus(*):1; mem(*):32 on slave 2014-02-11-21:14:47-160088899-53443-21045-2 (vesta.apache.org)
I0211 21:14:47.874150 21059 slave.cpp:736] Got assigned task 0 for framework 2014-02-11-21:14:47-160088899-53443-21045-0000
I0211 21:14:47.874200 21058 hierarchical_allocator_process.hpp:547] Framework 2014-02-11-21:14:47-160088899-53443-21045-0000 left cpus(*):7; mem(*):6929; disk(*):1.38501e+06; ports(*):[31000-32000] unused on slave 2014-02-11-21:14:47-160088899-53443-21045-2
I0211 21:14:47.874250 21053 master.hpp:449] Removing offer 2014-02-11-21:14:47-160088899-53443-21045-1 with resources cpus(*):8; mem(*):6961; disk(*):1.38501e+06; ports(*):[31000-32000] on slave 2014-02-11-21:14:47-160088899-53443-21045-1 (vesta.apache.org)
I0211 21:14:47.874307 21053 master.cpp:1574] Processing reply for offers: [ 2014-02-11-21:14:47-160088899-53443-21045-1 ] on slave 2014-02-11-21:14:47-160088899-53443-21045-1 (vesta.apache.org) for framework 2014-02-11-21:14:47-160088899-53443-21045-0000
I0211 21:14:47.874322 21058 hierarchical_allocator_process.hpp:590] Framework 2014-02-11-21:14:47-160088899-53443-21045-0000 filtered slave 2014-02-11-21:14:47-160088899-53443-21045-2 for 5secs
I0211 21:14:47.874354 21059 slave.cpp:845] Launching task 0 for framework 2014-02-11-21:14:47-160088899-53443-21045-0000
I0211 21:14:47.874404 21053 master.hpp:411] Adding task 1 with resources cpus(*):1; mem(*):32 on slave 2014-02-11-21:14:47-160088899-53443-21045-1 (vesta.apache.org)
I0211 21:14:47.874428 21053 master.cpp:2447] Launching task 1 of framework 2014-02-11-21:14:47-160088899-53443-21045-0000 with resources cpus(*):1; mem(*):32 on slave 2014-02-11-21:14:47-160088899-53443-21045-1 (vesta.apache.org)
I0211 21:14:47.874479 21058 slave.cpp:736] Got assigned task 1 for framework 2014-02-11-21:14:47-160088899-53443-21045-0000
I0211 21:14:47.874586 21053 master.hpp:449] Removing offer 2014-02-11-21:14:47-160088899-53443-21045-2 with resources cpus(*):8; mem(*):6961; disk(*):1.38501e+06; ports(*):[31000-32000] on slave 2014-02-11-21:14:47-160088899-53443-21045-0 (vesta.apache.org)
I0211 21:14:47.874646 21053 master.cpp:1574] Processing reply for offers: [ 2014-02-11-21:14:47-160088899-53443-21045-2 ] on slave 2014-02-11-21:14:47-160088899-53443-21045-0 (vesta.apache.org) for framework 2014-02-11-21:14:47-160088899-53443-21045-0000
I0211 21:14:47.874690 21058 slave.cpp:845] Launching task 1 for framework 2014-02-11-21:14:47-160088899-53443-21045-0000
I0211 21:14:47.874694 21053 master.hpp:411] Adding task 2 with resources cpus(*):1; mem(*):32 on slave 2014-02-11-21:14:47-160088899-53443-21045-0 (vesta.apache.org)
I0211 21:14:47.874716 21053 master.cpp:2447] Launching task 2 of framework 2014-02-11-21:14:47-160088899-53443-21045-0000 with resources cpus(*):1; mem(*):32 on slave 2014-02-11-21:14:47-160088899-53443-21045-0 (vesta.apache.org)
I0211 21:14:47.874820 21053 hierarchical_allocator_process.hpp:547] Framework 2014-02-11-21:14:47-160088899-53443-21045-0000 left cpus(*):7; mem(*):6929; disk(*):1.38501e+06; ports(*):[31000-32000] unused on slave 2014-02-11-21:14:47-160088899-53443-21045-1
I0211 21:14:47.874892 21053 hierarchical_allocator_process.hpp:590] Framework 2014-02-11-21:14:47-160088899-53443-21045-0000 filtered slave 2014-02-11-21:14:47-160088899-53443-21045-1 for 5secs
I0211 21:14:47.874922 21053 hierarchical_allocator_process.hpp:547] Framework 2014-02-11-21:14:47-160088899-53443-21045-0000 left cpus(*):7; mem(*):6929; disk(*):1.38501e+06; ports(*):[31000-32000] unused on slave 2014-02-11-21:14:47-160088899-53443-21045-0
I0211 21:14:47.874980 21053 hierarchical_allocator_process.hpp:590] Framework 2014-02-11-21:14:47-160088899-53443-21045-0000 filtered slave 2014-02-11-21:14:47-160088899-53443-21045-0 for 5secs
I0211 21:14:47.875012 21053 slave.cpp:736] Got assigned task 2 for framework 2014-02-11-21:14:47-160088899-53443-21045-0000
I0211 21:14:47.875151 21053 slave.cpp:845] Launching task 2 for framework 2014-02-11-21:14:47-160088899-53443-21045-0000
I0211 21:14:47.875527 21059 slave.cpp:955] Queuing task '0' for executor default of framework '2014-02-11-21:14:47-160088899-53443-21045-0000
I0211 21:14:47.875608 21059 process_isolator.cpp:102] Launching default (/home/hudson/jenkins-slave/workspace/Mesos-Trunk-Ubuntu-Build-In-Src-Set-JAVA_HOME/src/examples/python/test-executor) in /tmp/mesos-Z8v6cu/2/slaves/2014-02-11-21:14:47-160088899-53443-21045-2/frameworks/2014-02-11-21:14:47-160088899-53443-21045-0000/executors/default/runs/02cdf8bd-0757-4a40-8e77-af60bb202d71 with resources cpus(*):1; mem(*):32' for framework 2014-02-11-21:14:47-160088899-53443-21045-0000
I0211 21:14:47.876787 21054 slave.cpp:469] Successfully attached file '/tmp/mesos-Z8v6cu/2/slaves/2014-02-11-21:14:47-160088899-53443-21045-2/frameworks/2014-02-11-21:14:47-160088899-53443-21045-0000/executors/default/runs/02cdf8bd-0757-4a40-8e77-af60bb202d71'
I0211 21:14:47.876852 21059 process_isolator.cpp:165] Forked executor at 21061
I0211 21:14:47.876940 21058 slave.cpp:955] Queuing task '1' for executor default of framework '2014-02-11-21:14:47-160088899-53443-21045-0000
I0211 21:14:47.877095 21057 process_isolator.cpp:102] Launching default (/home/hudson/jenkins-slave/workspace/Mesos-Trunk-Ubuntu-Build-In-Src-Set-JAVA_HOME/src/examples/python/test-executor) in /tmp/mesos-Z8v6cu/0/slaves/2014-02-11-21:14:47-160088899-53443-21045-1/frameworks/2014-02-11-21:14:47-160088899-53443-21045-0000/executors/default/runs/568b657d-839d-483f-aff1-4872fbfc27dc with resources cpus(*):1; mem(*):32' for framework 2014-02-11-21:14:47-160088899-53443-21045-0000
I0211 21:14:47.877102 21052 slave.cpp:469] Successfully attached file '/tmp/mesos-Z8v6cu/0/slaves/2014-02-11-21:14:47-160088899-53443-21045-1/frameworks/2014-02-11-21:14:47-160088899-53443-21045-0000/executors/default/runs/568b657d-839d-483f-aff1-4872fbfc27dc'
I0211 21:14:47.878783 21057 process_isolator.cpp:165] Forked executor at 21062
I0211 21:14:47.879032 21053 slave.cpp:955] Queuing task '2' for executor default of framework '2014-02-11-21:14:47-160088899-53443-21045-0000
I0211 21:14:47.879192 21054 slave.cpp:2098] Monitoring executor default of framework 2014-02-11-21:14:47-160088899-53443-21045-0000 forked at pid 21062
I0211 21:14:47.879192 21058 slave.cpp:469] Successfully attached file '/tmp/mesos-Z8v6cu/1/slaves/2014-02-11-21:14:47-160088899-53443-21045-0/frameworks/2014-02-11-21:14:47-160088899-53443-21045-0000/executors/default/runs/a7c4170a-f40b-4493-81b3-0ea8c70e3977'
I0211 21:14:47.879166 21052 process_isolator.cpp:102] Launching default (/home/hudson/jenkins-slave/workspace/Mesos-Trunk-Ubuntu-Build-In-Src-Set-JAVA_HOME/src/examples/python/test-executor) in /tmp/mesos-Z8v6cu/1/slaves/2014-02-11-21:14:47-160088899-53443-21045-0/frameworks/2014-02-11-21:14:47-160088899-53443-21045-0000/executors/default/runs/a7c4170a-f40b-4493-81b3-0ea8c70e3977 with resources cpus(*):1; mem(*):32' for framework 2014-02-11-21:14:47-160088899-53443-21045-0000
I0211 21:14:47.880775 21057 slave.cpp:2098] Monitoring executor default of framework 2014-02-11-21:14:47-160088899-53443-21045-0000 forked at pid 21061
I0211 21:14:47.880959 21052 process_isolator.cpp:165] Forked executor at 21064
E0211 21:14:47.881386 21054 slave.cpp:2124] Failed to watch executor default of framework 2014-02-11-21:14:47-160088899-53443-21045-0000: Already watched
I0211 21:14:47.881474 21055 slave.cpp:2098] Monitoring executor default of framework 2014-02-11-21:14:47-160088899-53443-21045-0000 forked at pid 21064
E0211 21:14:47.881516 21055 slave.cpp:2124] Failed to watch executor default of framework 2014-02-11-21:14:47-160088899-53443-21045-0000: Already watched
Fetching resources into '/tmp/mesos-Z8v6cu/2/slaves/2014-02-11-21:14:47-160088899-53443-21045-2/frameworks/2014-02-11-21:14:47-160088899-53443-21045-0000/executors/default/runs/02cdf8bd-0757-4a40-8e77-af60bb202d71'
Fetching resources into '/tmp/mesos-Z8v6cu/0/slaves/2014-02-11-21:14:47-160088899-53443-21045-1/frameworks/2014-02-11-21:14:47-160088899-53443-21045-0000/executors/default/runs/568b657d-839d-483f-aff1-4872fbfc27dc'
Fetching resources into '/tmp/mesos-Z8v6cu/1/slaves/2014-02-11-21:14:47-160088899-53443-21045-0/frameworks/2014-02-11-21:14:47-160088899-53443-21045-0000/executors/default/runs/a7c4170a-f40b-4493-81b3-0ea8c70e3977'
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0211 21:14:48.154657 21117 process.cpp:1591] libprocess is initialized on 67.195.138.9:60148 for 8 cpus
I0211 21:14:48.155632 21117 exec.cpp:131] Version: 0.18.0
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0211 21:14:48.156184 21116 process.cpp:1591] libprocess is initialized on 67.195.138.9:55901 for 8 cpus
I0211 21:14:48.157078 21119 exec.cpp:181] Executor started at: executor(1)@67.195.138.9:60148 with pid 21117
I0211 21:14:48.157146 21116 exec.cpp:131] Version: 0.18.0
I0211 21:14:48.157536 21052 slave.cpp:1431] Got registration for executor 'default' of framework 2014-02-11-21:14:47-160088899-53443-21045-0000
I0211 21:14:48.157784 21052 slave.cpp:1552] Flushing queued task 2 for executor 'default' of framework 2014-02-11-21:14:47-160088899-53443-21045-0000
I0211 21:14:48.158042 21126 process.cpp:1010] Socket closed while receiving
I0211 21:14:48.158088 21124 exec.cpp:205] Executor registered on slave 2014-02-11-21:14:47-160088899-53443-21045-0
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0211 21:14:48.158324 21113 process.cpp:1591] libprocess is initialized on 67.195.138.9:43514 for 8 cpus
I0211 21:14:48.158526 21128 exec.cpp:181] Executor started at: executor(1)@67.195.138.9:55901 with pid 21116
I0211 21:14:48.158803 21055 slave.cpp:1431] Got registration for executor 'default' of framework 2014-02-11-21:14:47-160088899-53443-21045-0000
I0211 21:14:48.159018 21055 slave.cpp:1552] Flushing queued task 1 for executor 'default' of framework 2014-02-11-21:14:47-160088899-53443-21045-0000
I0211 21:14:48.159241 21133 exec.cpp:205] Executor registered on slave 2014-02-11-21:14:47-160088899-53443-21045-1
I0211 21:14:48.159246 21135 process.cpp:1010] Socket closed while receiving
I0211 21:14:48.159283 21113 exec.cpp:131] Version: 0.18.0
I0211 21:14:48.159543 21124 exec.cpp:217] Executor::registered took 575493ns
I0211 21:14:48.159593 21124 exec.cpp:292] Executor asked to run task '2'
Starting executor
Running task 2
I0211 21:14:48.160181 21124 exec.cpp:301] Executor::launchTask took 569794ns
I0211 21:14:48.160450 21133 exec.cpp:217] Executor::registered took 454612ns
I0211 21:14:48.160522 21133 exec.cpp:292] Executor asked to run task '1'
Sending status update...
I0211 21:14:48.160640 21137 exec.cpp:181] Executor started at: executor(1)@67.195.138.9:43514 with pid 21113
Sent status update
I0211 21:14:48.160894 21052 slave.cpp:1431] Got registration for executor 'default' of framework 2014-02-11-21:14:47-160088899-53443-21045-0000
Starting executor
Running task 1
I0211 21:14:48.161001 21133 exec.cpp:301] Executor::launchTask took 466392ns
I0211 21:14:48.161068 21052 slave.cpp:1552] Flushing queued task 0 for executor 'default' of framework 2014-02-11-21:14:47-160088899-53443-21045-0000
I0211 21:14:48.161222 21144 process.cpp:1010] Socket closed while receiving
I0211 21:14:48.161273 21137 exec.cpp:205] Executor registered on slave 2014-02-11-21:14:47-160088899-53443-21045-2
ISending status update...
0211 21:14:48.161321 21144 process.cpp:1010] Socket closed while receiving
Sent status update
I0211 21:14:48.161535 21125 exec.cpp:524] Executor sending status update TASK_RUNNING (UUID: bd0018b7-0742-42bc-a0a0-1d90f87e7d3b) for task 2 of framework 2014-02-11-21:14:47-160088899-53443-21045-0000
I0211 21:14:48.161744 21058 slave.cpp:1765] Handling status update TASK_RUNNING (UUID: bd0018b7-0742-42bc-a0a0-1d90f87e7d3b) for task 2 of framework 2014-02-11-21:14:47-160088899-53443-21045-0000 from executor(1)@67.195.138.9:60148
I0211 21:14:48.161859 21058 status_update_manager.cpp:314] Received status update TASK_RUNNING (UUID: bd0018b7-0742-42bc-a0a0-1d90f87e7d3b) for task 2 of framework 2014-02-11-21:14:47-160088899-53443-21045-0000
I0211 21:14:48.161874 21058 status_update_manager.cpp:493] Creating StatusUpdate stream for task 2 of framework 2014-02-11-21:14:47-160088899-53443-21045-0000
I0211 21:14:48.161938 21058 status_update_manager.cpp:367] Forwarding status update TASK_RUNNING (UUID: bd0018b7-0742-42bc-a0a0-1d90f87e7d3b) for task 2 of framework 2014-02-11-21:14:47-160088899-53443-21045-0000 to master@67.195.138.9:53443
I0211 21:14:48.162057 21058 master.cpp:2026] Status update TASK_RUNNING (UUID: bd0018b7-0742-42bc-a0a0-1d90f87e7d3b) for task 2 of framework 2014-02-11-21:14:47-160088899-53443-21045-0000 from slave(2)@67.195.138.9:53443
I0211 21:14:48.162080 21058 slave.cpp:1884] Status update manager successfully handled status update TASK_RUNNING (UUID: bd0018b7-0742-42bc-a0a0-1d90f87e7d3b) for task 2 of framework 2014-02-11-21:14:47-160088899-53443-21045-0000
I0211 21:14:48.162088 21058 slave.cpp:1890] Sending acknowledgement for status update TASK_RUNNING (UUID: bd0018b7-0742-42bc-a0a0-1d90f87e7d3b) for task 2 of framework 2014-02-11-21:14:47-160088899-53443-21045-0000 to executor(1)@67.195.138.9:60148
I0211 21:14:48.162555 21058 sched.cpp:616] Scheduler::statusUpdate took 351553ns
I0211 21:14:48.162623 21058 status_update_manager.cpp:392] Received status update acknowledgement (UUID: bd0018b7-0742-42bc-a0a0-1d90f87e7d3b) for task 2 of framework 2014-02-11-21:14:47-160088899-53443-21045-0000
I0211 21:14:48.162669 21058 slave.cpp:1371] Status update manager successfully handled status update acknowledgement (UUID: bd0018b7-0742-42bc-a0a0-1d90f87e7d3b) for task 2 of framework 2014-02-11-21:14:47-160088899-53443-21045-0000
I0211 21:14:48.162766 21126 process.cpp:1010] Socket closed while receiving
I0211 21:14:48.163368 21131 exec.cpp:524] Executor sending status update TASK_RUNNING (UUID: e2254a60-ebc8-4553-9ed8-e44cc4d84eb8) for task 1 of framework 2014-02-11-21:14:47-160088899-53443-21045-0000
I0211 21:14:48.163434 21125 exec.cpp:524] Executor sending status update TASK_FINISHED (UUID: 1d57909c-8b68-45f9-9785-c4b6ad29e664) for task 2 of framework 2014-02-11-21:14:47-160088899-53443-21045-0000
I0211 21:14:48.163486 21125 exec.cpp:338] Executor received status update acknowledgement bd0018b7-0742-42bc-a0a0-1d90f87e7d3b for task 2 of framework 2014-02-11-21:14:47-160088899-53443-21045-0000
I0211 21:14:48.163565 21058 slave.cpp:1765] Handling status update TASK_FINISHED (UUID: 1d57909c-8b68-45f9-9785-c4b6ad29e664) for task 2 of framework 2014-02-11-21:14:47-160088899-53443-21045-0000 from executor(1)@67.195.138.9:60148
I0211 21:14:48.163583 21058 slave.cpp:3214] Terminating task 2
I0211 21:14:48.163662 21058 slave.cpp:1765] Handling status update TASK_RUNNING (UUID: e2254a60-ebc8-4553-9ed8-e44cc4d84eb8) for task 1 of framework 2014-02-11-21:14:47-160088899-53443-21045-0000 from executor(1)@67.195.138.9:55901
I0211 21:14:48.163676 21137 exec.cpp:217] Executor::registered took 548316ns
II0211 21:14:48.163739 21058 status_update_manager.cpp:314] Received status update TASK_FINISHED (UUID: 1d57909c-8b68-45f9-9785-c4b6ad29e664) for task 2 of framework 2014-02-11-21:14:47-160088899-53443-21045-0000
0211 21:14:48.163740 21137 exec.cpp:292] Executor asked to run task '0'
I0211 21:14:48.163756 21058 status_update_manager.cpp:367] Forwarding status update TASK_FINISHED (UUID: 1d57909c-8b68-45f9-9785-c4b6ad29e664) for task 2 of framework 2014-02-11-21:14:47-160088899-53443-21045-0000 to master@67.195.138.9:53443
I0211 21:14:48.163813 21058 status_update_manager.cpp:314] Received status update TASK_RUNNING (UUID: e2254a60-ebc8-4553-9ed8-e44cc4d84eb8) for task 1 of framework 2014-02-11-21:14:47-160088899-53443-21045-0000
I0211 21:14:48.163825 21058 status_update_manager.cpp:493] Creating StatusUpdate stream for task 1 of framework 2014-02-11-21:14:47-160088899-53443-21045-0000
I0211 21:14:48.163868 21058 status_update_manager.cpp:367] Forwarding status update TASK_RUNNING (UUID: e2254a60-ebc8-4553-9ed8-e44cc4d84eb8) for task 1 of framework 2014-02-11-21:14:47-160088899-53443-21045-0000 to master@67.195.138.9:53443
I0211 21:14:48.163954 21058 master.cpp:2026] Status update TASK_FINISHED (UUID: 1d57909c-8b68-45f9-9785-c4b6ad29e664) for task 2 of framework 2014-02-11-21:14:47-160088899-53443-21045-0000 from slave(2)@67.195.138.9:53443
I0211 21:14:48.163998 21058 master.hpp:429] Removing task 2 with resources cpus(*):1; mem(*):32 on slave 2014-02-11-21:14:47-160088899-53443-21045-0 (vesta.apache.org)
I0211 21:14:48.164083 21058 master.cpp:2026] Status update TASK_RUNNING (UUID: e2254a60-ebc8-4553-9ed8-e44cc4d84eb8) for task 1 of framework 2014-02-11-21:14:47-160088899-53443-21045-0000 from slave(1)@67.195.138.9:53443
I0211 21:14:48.164103 21058 slave.cpp:1884] Status update manager successfully handled status update TASK_FINISHED (UUID: 1d57909c-8b68-45f9-9785-c4b6ad29e664) for task 2 of framework 2014-02-11-21:14:47-160088899-53443-21045-0000
I0211 21:14:48.164113 21058 slave.cpp:1890] Sending acknowledgement for status update TASK_FINISHED (UUID: 1d57909c-8b68-45f9-9785-c4b6ad29e664) for task 2 of framework 2014-02-11-21:14:47-160088899-53443-21045-0000 to executor(1)@67.195.138.9:60148
I0211 21:14:48.164181 21058 slave.cpp:1884] Status update manager successfully handled status update TASK_RUNNING (UUID: e2254a60-ebc8-4553-9ed8-e44cc4d84eb8) for task 1 of framework 2014-02-11-21:14:47-160088899-53443-21045-0000
II0211 21:14:48.164193 21058 slave.cpp:1890] Sending acknowledgement for status update TASK_RUNNING (UUID: e2254a60-ebc8-4553-9ed8-e44cc4d84eb8) for task 1 of framework 2014-02-11-21:14:47-160088899-53443-21045-0000 to executor(1)@67.195.138.9:55901
0211 21:14:48.164191 21131 exec.cpp:524] Executor sending status update TASK_FINISHED (UUID: 37fd5f35-c3b3-4c16-b836-3cab90ed6874) for task 1 of framework 2014-02-11-21:14:47-160088899-53443-21045-0000
Starting executor
Running task 0
I0211 21:14:48.164299 21052 hierarchical_allocator_process.hpp:637] Recovered cpus(*):1; mem(*):32 (total allocatable: cpus(*):8; mem(*):6961; disk(*):1.38501e+06; ports(*):[31000-32000]) on slave 2014-02-11-21:14:47-160088899-53443-21045-0 from framework 2014-02-11-21:14:47-160088899-53443-21045-0000
I0211 21:14:48.164361 21058 slave.cpp:1765] Handling status update TASK_FINISHED (UUID: 37fd5f35-c3b3-4c16-b836-3cab90ed6874) for task 1 of framework 2014-02-11-21:14:47-160088899-53443-21045-0000 from executor(1)@67.195.138.9:55901
I0211 21:14:48.164392 21058 slave.cpp:3214] Terminating task 1
I0211 21:14:48.164505 21052 status_update_manager.cpp:314] Received status update TASK_FINISHED (UUID: 37fd5f35-c3b3-4c16-b836-3cab90ed6874) for task 1 of framework 2014-02-11-21:14:47-160088899-53443-21045-0000
I0211 21:14:48.164512 21057 sched.cpp:616] Scheduler::statusUpdate took 238156ns
I0211 21:14:48.164559 21052 slave.cpp:1884] Status update manager successfully handled status update TASK_FINISHED (UUID: 37fd5f35-c3b3-4c16-b836-3cab90ed6874) for task 1 of framework 2014-02-11-21:14:47-160088899-53443-21045-0000
II0211 21:14:48.164572 21052 slave.cpp:1890] Sending acknowledgement for status update TASK_FINISHED (UUID: 37fd5f35-c3b3-4c16-b836-3cab90ed6874) for task 1 of framework 2014-02-11-21:14:47-160088899-53443-21045-0000 to executor(1)@67.195.138.9:55901
0211 21:14:48.164571 21135 process.cpp:1010] Socket closed while receiving
I0211 21:14:48.164600 21130 exec.cpp:338] Executor received status update acknowledgement e2254a60-ebc8-4553-9ed8-e44cc4d84eb8 for task 1 of framework 2014-02-11-21:14:47-160088899-53443-21045-0000
I0211 21:14:48.164635 21057 sched.cpp:616] Scheduler::statusUpdate took 76837ns
Sending status update...
I0211 21:14:48.164715 21135 process.cpp:1010] Socket closed while receiving
I0211 21:14:48.164726 21057 status_update_manager.cpp:392] Received status update acknowledgement (UUID: e2254a60-ebc8-4553-9ed8-e44cc4d84eb8) for task 1 of framework 2014-02-11-21:14:47-160088899-53443-21045-0000
I0211 21:14:48.164728 21130 exec.cpp:338] Executor received status update acknowledgement 37fd5f35-c3b3-4c16-b836-3cab90ed6874 for task 1 of framework 2014-02-11-21:14:47-160088899-53443-21045-0000
I0211 21:14:48.164749 21057 status_update_manager.cpp:367] Forwarding status update TASK_FINISHED (UUID: 37fd5f35-c3b3-4c16-b836-3cab90ed6874) for task 1 of framework 2014-02-11-21:14:47-160088899-53443-21045-0000 to master@67.195.138.9:53443
Sent status update
I0211 21:14:48.164818 21057 slave.cpp:1371] Status update manager successfully handled status update acknowledgement (UUID: e2254a60-ebc8-4553-9ed8-e44cc4d84eb8) for task 1 of framework 2014-02-11-21:14:47-160088899-53443-21045-0000
I0211 21:14:48.164842 21053 status_update_manager.cpp:392] Received status update acknowledgement (UUID: 1d57909c-8b68-45f9-9785-c4b6ad29e664) for task 2 of framework 2014-02-11-21:14:47-160088899-53443-21045-0000
I0211 21:14:48.164829 21137 exec.cpp:301] Executor::launchTask took 1.068244ms
I0211 21:14:48.164872 21053 status_update_manager.cpp:524] Cleaning up status update stream for task 2 of framework 2014-02-11-21:14:47-160088899-53443-21045-0000
I0211 21:14:48.164911 21126 process.cpp:1010] Socket closed while receiving
I0211 21:14:48.164952 21122 exec.cpp:338] Executor received status update acknowledgement 1d57909c-8b68-45f9-9785-c4b6ad29e664 for task 2 of framework 2014-02-11-21:14:47-160088899-53443-21045-0000
I0211 21:14:48.164995 21126 process.cpp:1010] Socket closed while receiving
I0211 21:14:48.165006 21122 exec.cpp:359] Executor received framework message
I0211 21:14:48.165004 21058 master.cpp:2026] Status update TASK_FINISHED (UUID: 37fd5f35-c3b3-4c16-b836-3cab90ed6874) for task 1 of framework 2014-02-11-21:14:47-160088899-53443-21045-0000 from slave(1)@67.195.138.9:53443
I0211 21:14:48.165043 21057 slave.cpp:1371] Status update manager successfully handled status update acknowledgement (UUID: 1d57909c-8b68-45f9-9785-c4b6ad29e664) for task 2 of framework 2014-02-11-21:14:47-160088899-53443-21045-0000
I0211 21:14:48.165052 21122 exec.cpp:368] Executor::frameworkMessage took 34533ns
I0211 21:14:48.165058 21057 slave.cpp:3237] Completing task 2
I0211 21:14:48.165057 21058 master.hpp:429] Removing task 1 with resources cpus(*):1; mem(*):32 on slave 2014-02-11-21:14:47-160088899-53443-21045-1 (vesta.apache.org)
I0211 21:14:48.165175 21053 sched.cpp:616] Scheduler::statusUpdate took 162784ns
I0211 21:14:48.165220 21058 slave.cpp:1943] Sending message for framework 2014-02-11-21:14:47-160088899-53443-21045-0000 to scheduler(1)@67.195.138.9:53443
I0211 21:14:48.165211 21055 hierarchical_allocator_process.hpp:637] Recovered cpus(*):1; mem(*):32 (total allocatable: cpus(*):8; mem(*):6961; disk(*):1.38501e+06; ports(*):[31000-32000]) on slave 2014-02-11-21:14:47-160088899-53443-21045-1 from framework 2014-02-11-21:14:47-160088899-53443-21045-0000
I0211 21:14:48.165316 21057 status_update_manager.cpp:392] Received status update acknowledgement (UUID: 37fd5f35-c3b3-4c16-b836-3cab90ed6874) for task 1 of framework 2014-02-11-21:14:47-160088899-53443-21045-0000
I0211 21:14:48.165386 21057 status_update_manager.cpp:524] Cleaning up status update stream for task 1 of framework 2014-02-11-21:14:47-160088899-53443-21045-0000
I0211 21:14:48.165427 21135 process.cpp:1010] Socket closed while receiving
I0211 21:14:48.165451 21055 sched.cpp:701] Scheduler::frameworkMessage took 170832ns
I0211 21:14:48.165462 21053 slave.cpp:1371] Status update manager successfully handled status update acknowledgement (UUID: 37fd5f35-c3b3-4c16-b836-3cab90ed6874) for task 1 of framework 2014-02-11-21:14:47-160088899-53443-21045-0000
I0211 21:14:48.165468 21132 exec.cpp:359] Executor received framework message
I0211 21:14:48.165475 21053 slave.cpp:3237] Completing task 1
I0211 21:14:48.165493 21132 exec.cpp:368] Executor::frameworkMessage took 11572ns
I0211 21:14:48.165654 21057 slave.cpp:1943] Sending message for framework 2014-02-11-21:14:47-160088899-53443-21045-0000 to scheduler(1)@67.195.138.9:53443
I0211 21:14:48.165786 21057 sched.cpp:701] Scheduler::frameworkMessage took 96059ns
I0211 21:14:48.165777 21137 exec.cpp:524] Executor sending status update TASK_RUNNING (UUID: f955480b-856b-4f79-8d92-63edea7ad97d) for task 0 of framework 2014-02-11-21:14:47-160088899-53443-21045-0000
I0211 21:14:48.165974 21055 slave.cpp:1765] Handling status update TASK_RUNNING (UUID: f955480b-856b-4f79-8d92-63edea7ad97d) for task 0 of framework 2014-02-11-21:14:47-160088899-53443-21045-0000 from executor(1)@67.195.138.9:43514
I0211 21:14:48.166085 21053 status_update_manager.cpp:314] Received status update TASK_RUNNING (UUID: f955480b-856b-4f79-8d92-63edea7ad97d) for task 0 of framework 2014-02-11-21:14:47-160088899-53443-21045-0000
I0211 21:14:48.166113 21053 status_update_manager.cpp:493] Creating StatusUpdate stream for task 0 of framework 2014-02-11-21:14:47-160088899-53443-21045-0000
I0211 21:14:48.166160 21053 status_update_manager.cpp:367] Forwarding status update TASK_RUNNING (UUID: f955480b-856b-4f79-8d92-63edea7ad97d) for task 0 of framework 2014-02-11-21:14:47-160088899-53443-21045-0000 to master@67.195.138.9:53443
I0211 21:14:48.166240 21052 master.cpp:2026] Status update TASK_RUNNING (UUID: f955480b-856b-4f79-8d92-63edea7ad97d) for task 0 of framework 2014-02-11-21:14:47-160088899-53443-21045-0000 from slave(3)@67.195.138.9:53443
I0211 21:14:48.166244 21055 slave.cpp:1884] Status update manager successfully handled status update TASK_RUNNING (UUID: f955480b-856b-4f79-8d92-63edea7ad97d) for task 0 of framework 2014-02-11-21:14:47-160088899-53443-21045-0000
I0211 21:14:48.166278 21055 slave.cpp:1890] Sending acknowledgement for status update TASK_RUNNING (UUID: f955480b-856b-4f79-8d92-63edea7ad97d) for task 0 of framework 2014-02-11-21:14:47-160088899-53443-21045-0000 to executor(1)@67.195.138.9:43514
I0211 21:14:48.166385 21058 sched.cpp:616] Scheduler::statusUpdate took 88496ns
I0211 21:14:48.166489 21144 process.cpp:1010] Socket closed while receiving
I0211 21:14:48.166631 21052 status_update_manager.cpp:392] Received status update acknowledgement (UUID: f955480b-856b-4f79-8d92-63edea7ad97d) for task 0 of framework 2014-02-11-21:14:47-160088899-53443-21045-0000
I0211 21:14:48.166653 21137 exec.cpp:524] Executor sending status update TASK_FINISHED (UUID: 9af4cdff-74ae-40ab-9788-0d5f8b7435ec) for task 0 of framework 2014-02-11-21:14:47-160088899-53443-21045-0000
I0211 21:14:48.166679 21052 slave.cpp:1371] Status update manager successfully handled status update acknowledgement (UUID: f955480b-856b-4f79-8d92-63edea7ad97d) for task 0 of framework 2014-02-11-21:14:47-160088899-53443-21045-0000
I0211 21:14:48.166699 21137 exec.cpp:338] Executor received status update acknowledgement f955480b-856b-4f79-8d92-63edea7ad97d for task 0 of framework 2014-02-11-21:14:47-160088899-53443-21045-0000
I0211 21:14:48.166836 21059 slave.cpp:1765] Handling status update TASK_FINISHED (UUID: 9af4cdff-74ae-40ab-9788-0d5f8b7435ec) for task 0 of framework 2014-02-11-21:14:47-160088899-53443-21045-0000 from executor(1)@67.195.138.9:43514
I0211 21:14:48.166858 21059 slave.cpp:3214] Terminating task 0
I0211 21:14:48.166960 21054 status_update_manager.cpp:314] Received status update TASK_FINISHED (UUID: 9af4cdff-74ae-40ab-9788-0d5f8b7435ec) for task 0 of framework 2014-02-11-21:14:47-160088899-53443-21045-0000
I0211 21:14:48.166988 21054 status_update_manager.cpp:367] Forwarding status update TASK_FINISHED (UUID: 9af4cdff-74ae-40ab-9788-0d5f8b7435ec) for task 0 of framework 2014-02-11-21:14:47-160088899-53443-21045-0000 to master@67.195.138.9:53443
I0211 21:14:48.167079 21055 slave.cpp:1884] Status update manager successfully handled status update TASK_FINISHED (UUID: 9af4cdff-74ae-40ab-9788-0d5f8b7435ec) for task 0 of framework 2014-02-11-21:14:47-160088899-53443-21045-0000
I0211 21:14:48.167095 21055 slave.cpp:1890] Sending acknowledgement for status update TASK_FINISHED (UUID: 9af4cdff-74ae-40ab-9788-0d5f8b7435ec) for task 0 of framework 2014-02-11-21:14:47-160088899-53443-21045-0000 to executor(1)@67.195.138.9:43514
I0211 21:14:48.167100 21059 master.cpp:2026] Status update TASK_FINISHED (UUID: 9af4cdff-74ae-40ab-9788-0d5f8b7435ec) for task 0 of framework 2014-02-11-21:14:47-160088899-53443-21045-0000 from slave(3)@67.195.138.9:53443
I0211 21:14:48.167147 21059 master.hpp:429] Removing task 0 with resources cpus(*):1; mem(*):32 on slave 2014-02-11-21:14:47-160088899-53443-21045-2 (vesta.apache.org)
I0211 21:14:48.167260 21055 hierarchical_allocator_process.hpp:637] Recovered cpus(*):1; mem(*):32 (total allocatable: cpus(*):8; mem(*):6961; disk(*):1.38501e+06; ports(*):[31000-32000]) on slave 2014-02-11-21:14:47-160088899-53443-21045-2 from framework 2014-02-11-21:14:47-160088899-53443-21045-0000
I0211 21:14:48.167284 21144 process.cpp:1010] Socket closed while receiving
I0211 21:14:48.167284 21140 exec.cpp:338] Executor received status update acknowledgement 9af4cdff-74ae-40ab-9788-0d5f8b7435ec for task 0 of framework 2014-02-11-21:14:47-160088899-53443-21045-0000
I0211 21:14:48.167326 21054 sched.cpp:616] Scheduler::statusUpdate took 160237ns
I0211 21:14:48.167469 21057 status_update_manager.cpp:392] Received status update acknowledgement (UUID: 9af4cdff-74ae-40ab-9788-0d5f8b7435ec) for task 0 of framework 2014-02-11-21:14:47-160088899-53443-21045-0000
I0211 21:14:48.167495 21057 status_update_manager.cpp:524] Cleaning up status update stream for task 0 of framework 2014-02-11-21:14:47-160088899-53443-21045-0000
I0211 21:14:48.167501 21144 process.cpp:1010] Socket closed while receiving
I0211 21:14:48.167520 21141 exec.cpp:359] Executor received framework message
I0211 21:14:48.167543 21054 slave.cpp:1371] Status update manager successfully handled status update acknowledgement (UUID: 9af4cdff-74ae-40ab-9788-0d5f8b7435ec) for task 0 of framework 2014-02-11-21:14:47-160088899-53443-21045-0000
I0211 21:14:48.167563 21054 slave.cpp:3237] Completing task 0
I0211 21:14:48.167563 21141 exec.cpp:368] Executor::frameworkMessage took 29844ns
I0211 21:14:48.167691 21057 slave.cpp:1943] Sending message for framework 2014-02-11-21:14:47-160088899-53443-21045-0000 to scheduler(1)@67.195.138.9:53443
I0211 21:14:48.167773 21052 sched.cpp:701] Scheduler::frameworkMessage took 46462ns
I0211 21:14:48.866621 21057 hierarchical_allocator_process.hpp:752] Offering cpus(*):8; mem(*):6961; disk(*):1.38501e+06; ports(*):[31000-32000] on slave 2014-02-11-21:14:47-160088899-53443-21045-0 to framework 2014-02-11-21:14:47-160088899-53443-21045-0000
I0211 21:14:48.866730 21057 hierarchical_allocator_process.hpp:752] Offering cpus(*):8; mem(*):6961; disk(*):1.38501e+06; ports(*):[31000-32000] on slave 2014-02-11-21:14:47-160088899-53443-21045-1 to framework 2014-02-11-21:14:47-160088899-53443-21045-0000
I0211 21:14:48.866799 21057 hierarchical_allocator_process.hpp:752] Offering cpus(*):8; mem(*):6961; disk(*):1.38501e+06; ports(*):[31000-32000] on slave 2014-02-11-21:14:47-160088899-53443-21045-2 to framework 2014-02-11-21:14:47-160088899-53443-21045-0000
I0211 21:14:48.866981 21057 hierarchical_allocator_process.hpp:688] Performed allocation for 3 slaves in 433438ns
I0211 21:14:48.867055 21059 master.hpp:439] Adding offer 2014-02-11-21:14:47-160088899-53443-21045-3 with resources cpus(*):8; mem(*):6961; disk(*):1.38501e+06; ports(*):[31000-32000] on slave 2014-02-11-21:14:47-160088899-53443-21045-2 (vesta.apache.org)
I0211 21:14:48.867164 21059 master.hpp:439] Adding offer 2014-02-11-21:14:47-160088899-53443-21045-4 with resources cpus(*):8; mem(*):6961; disk(*):1.38501e+06; ports(*):[31000-32000] on slave 2014-02-11-21:14:47-160088899-53443-21045-1 (vesta.apache.org)
I0211 21:14:48.867241 21059 master.hpp:439] Adding offer 2014-02-11-21:14:47-160088899-53443-21045-5 with resources cpus(*):8; mem(*):6961; disk(*):1.38501e+06; ports(*):[31000-32000] on slave 2014-02-11-21:14:47-160088899-53443-21045-0 (vesta.apache.org)
I0211 21:14:48.867285 21059 master.cpp:2278] Sending 3 offers to framework 2014-02-11-21:14:47-160088899-53443-21045-0000
I0211 21:14:48.869622 21053 sched.cpp:525] Scheduler::resourceOffers took 2.155683ms
I0211 21:14:48.869803 21059 master.hpp:449] Removing offer 2014-02-11-21:14:47-160088899-53443-21045-3 with resources cpus(*):8; mem(*):6961; disk(*):1.38501e+06; ports(*):[31000-32000] on slave 2014-02-11-21:14:47-160088899-53443-21045-2 (vesta.apache.org)
I0211 21:14:48.869858 21059 master.cpp:1574] Processing reply for offers: [ 2014-02-11-21:14:47-160088899-53443-21045-3 ] on slave 2014-02-11-21:14:47-160088899-53443-21045-2 (vesta.apache.org) for framework 2014-02-11-21:14:47-160088899-53443-21045-0000
I0211 21:14:48.869946 21059 master.hpp:411] Adding task 3 with resources cpus(*):1; mem(*):32 on slave 2014-02-11-21:14:47-160088899-53443-21045-2 (vesta.apache.org)
I0211 21:14:48.869969 21059 master.cpp:2447] Launching task 3 of framework 2014-02-11-21:14:47-160088899-53443-21045-0000 with resources cpus(*):1; mem(*):32 on slave 2014-02-11-21:14:47-160088899-53443-21045-2 (vesta.apache.org)
I0211 21:14:48.870033 21053 slave.cpp:736] Got assigned task 3 for framework 2014-02-11-21:14:47-160088899-53443-21045-0000
I0211 21:14:48.870142 21059 master.hpp:449] Removing offer 2014-02-11-21:14:47-160088899-53443-21045-4 with resources cpus(*):8; mem(*):6961; disk(*):1.38501e+06; ports(*):[31000-32000] on slave 2014-02-11-21:14:47-160088899-53443-21045-1 (vesta.apache.org)
I0211 21:14:48.870169 21058 hierarchical_allocator_process.hpp:547] Framework 2014-02-11-21:14:47-160088899-53443-21045-0000 left cpus(*):7; mem(*):6929; disk(*):1.38501e+06; ports(*):[31000-32000] unused on slave 2014-02-11-21:14:47-160088899-53443-21045-2
I0211 21:14:48.870193 21059 master.cpp:1574] Processing reply for offers: [ 2014-02-11-21:14:47-160088899-53443-21045-4 ] on slave 2014-02-11-21:14:47-160088899-53443-21045-1 (vesta.apache.org) for framework 2014-02-11-21:14:47-160088899-53443-21045-0000
I0211 21:14:48.870250 21059 master.hpp:411] Adding task 4 with resources cpus(*):1; mem(*):32 on slave 2014-02-11-21:14:47-160088899-53443-21045-1 (vesta.apache.org)
I0211 21:14:48.870275 21059 master.cpp:2447] Launching task 4 of framework 2014-02-11-21:14:47-160088899-53443-21045-0000 with resources cpus(*):1; mem(*):32 on slave 2014-02-11-21:14:47-160088899-53443-21045-1 (vesta.apache.org)
I0211 21:14:48.870281 21058 hierarchical_allocator_process.hpp:590] Framework 2014-02-11-21:14:47-160088899-53443-21045-0000 filtered slave 2014-02-11-21:14:47-160088899-53443-21045-2 for 5secs
I0211 21:14:48.870331 21058 slave.cpp:736] Got assigned task 4 for framework 2014-02-11-21:14:47-160088899-53443-21045-0000
I0211 21:14:48.870414 21058 slave.cpp:845] Launching task 4 for framework 2014-02-11-21:14:47-160088899-53443-21045-0000
I0211 21:14:48.870406 21059 master.hpp:449] Removing offer 2014-02-11-21:14:47-160088899-53443-21045-5 with resources cpus(*):8; mem(*):6961; disk(*):1.38501e+06; ports(*):[31000-32000] on slave 2014-02-11-21:14:47-160088899-53443-21045-0 (vesta.apache.org)
I0211 21:14:48.870468 21058 slave.cpp:980] Sending task '4' to executor 'default' of framework 2014-02-11-21:14:47-160088899-53443-21045-0000
I0211 21:14:48.870475 21059 master.cpp:1574] Processing reply for offers: [ 2014-02-11-21:14:47-160088899-53443-21045-5 ] on slave 2014-02-11-21:14:47-160088899-53443-21045-0 (vesta.apache.org) for framework 2014-02-11-21:14:47-160088899-53443-21045-0000
I0211 21:14:48.870528 21059 hierarchical_allocator_process.hpp:547] Framework 2014-02-11-21:14:47-160088899-53443-21045-0000 left cpus(*):7; mem(*):6929; disk(*):1.38501e+06; ports(*):[31000-32000] unused on slave 2014-02-11-21:14:47-160088899-53443-21045-1
I0211 21:14:48.870601 21059 hierarchical_allocator_process.hpp:590] Framework 2014-02-11-21:14:47-160088899-53443-21045-0000 filtered slave 2014-02-11-21:14:47-160088899-53443-21045-1 for 5secs
I0211 21:14:48.870632 21053 slave.cpp:845] Launching task 3 for framework 2014-02-11-21:14:47-160088899-53443-21045-0000
I0211 21:14:48.870656 21059 hierarchical_allocator_process.hpp:547] Framework 2014-02-11-21:14:47-160088899-53443-21045-0000 left cpus(*):8; mem(*):6961; disk(*):1.38501e+06; ports(*):[31000-32000] unused on slave 2014-02-11-21:14:47-160088899-53443-21045-0
I0211 21:14:48.870666 21053 slave.cpp:980] Sending task '3' to executor 'default' of framework 2014-02-11-21:14:47-160088899-53443-21045-0000
I0211 21:14:48.870735 21059 hierarchical_allocator_process.hpp:590] Framework 2014-02-11-21:14:47-160088899-53443-21045-0000 filtered slave 2014-02-11-21:14:47-160088899-53443-21045-0 for 5secs
I0211 21:14:48.870842 21135 process.cpp:1010] Socket closed while receiving
I0211 21:14:48.870910 21133 exec.cpp:292] Executor asked to run task '4'
I0211 21:14:48.870980 21144 process.cpp:1010] Socket closed while receiving
I0211 21:14:48.871078 21136 exec.cpp:292] Executor asked to run task '3'
Running task 4
I0211 21:14:48.871618 21133 exec.cpp:301] Executor::launchTask took 669868ns
Sending status update...
Running task 3
Sent status update
Sending status update...
Sent status update
I0211 21:14:48.872700 21134 exec.cpp:524] Executor sending status update TASK_RUNNING (UUID: 1402086c-13c6-4892-a87a-603864039b45) for task 4 of framework 2014-02-11-21:14:47-160088899-53443-21045-0000
I0211 21:14:48.872844 21136 exec.cpp:301] Executor::launchTask took 1.735607ms
I0211 21:14:48.872951 21057 slave.cpp:1765] Handling status update TASK_RUNNING (UUID: 1402086c-13c6-4892-a87a-603864039b45) for task 4 of framework 2014-02-11-21:14:47-160088899-53443-21045-0000 from executor(1)@67.195.138.9:55901
I0211 21:14:48.873046 21055 status_update_manager.cpp:314] Received status update TASK_RUNNING (UUID: 1402086c-13c6-4892-a87a-603864039b45) for task 4 of framework 2014-02-11-21:14:47-160088899-53443-21045-0000
I0211 21:14:48.873066 21055 status_update_manager.cpp:493] Creating StatusUpdate stream for task 4 of framework 2014-02-11-21:14:47-160088899-53443-21045-0000
I0211 21:14:48.873123 21055 status_update_manager.cpp:367] Forwarding status update TASK_RUNNING (UUID: 1402086c-13c6-4892-a87a-603864039b45) for task 4 of framework 2014-02-11-21:14:47-160088899-53443-21045-0000 to master@67.195.138.9:53443
I0211 21:14:48.873214 21057 master.cpp:2026] Status update TASK_RUNNING (UUID: 1402086c-13c6-4892-a87a-603864039b45) for task 4 of framework 2014-02-11-21:14:47-160088899-53443-21045-0000 from slave(1)@67.195.138.9:53443
I0211 21:14:48.873245 21058 slave.cpp:1884] Status update manager successfully handled status update TASK_RUNNING (UUID: 1402086c-13c6-4892-a87a-603864039b45) for task 4 of framework 2014-02-11-21:14:47-160088899-53443-21045-0000
I0211 21:14:48.873268 21058 slave.cpp:1890] Sending acknowledgement for status update TASK_RUNNING (UUID: 1402086c-13c6-4892-a87a-603864039b45) for task 4 of framework 2014-02-11-21:14:47-160088899-53443-21045-0000 to executor(1)@67.195.138.9:55901
I0211 21:14:48.873344 21055 sched.cpp:616] Scheduler::statusUpdate took 109430ns
I0211 21:14:48.873440 21055 status_update_manager.cpp:392] Received status update acknowledgement (UUID: 1402086c-13c6-4892-a87a-603864039b45) for task 4 of framework 2014-02-11-21:14:47-160088899-53443-21045-0000
I0211 21:14:48.873472 21135 process.cpp:1010] Socket closed while receiving
I0211 21:14:48.873497 21057 slave.cpp:1371] Status update manager successfully handled status update acknowledgement (UUID: 1402086c-13c6-4892-a87a-603864039b45) for task 4 of framework 2014-02-11-21:14:47-160088899-53443-21045-0000
I0211 21:14:48.874042 21134 exec.cpp:524] Executor sending status update TASK_FINISHED (UUID: 9ef8e17e-8569-41fc-93f2-df09a42bf876) for task 4 of framework 2014-02-11-21:14:47-160088899-53443-21045-0000
I0211 21:14:48.874084 21134 exec.cpp:338] Executor received status update acknowledgement 1402086c-13c6-4892-a87a-603864039b45 for task 4 of framework 2014-02-11-21:14:47-160088899-53443-21045-0000
I0211 21:14:48.874217 21055 slave.cpp:1765] Handling status update TASK_FINISHED (UUID: 9ef8e17e-8569-41fc-93f2-df09a42bf876) for task 4 of framework 2014-02-11-21:14:47-160088899-53443-21045-0000 from executor(1)@67.195.138.9:55901
I0211 21:14:48.874234 21055 slave.cpp:3214] Terminating task 4
I0211 21:14:48.874250 21136 exec.cpp:524] Executor sending status update TASK_RUNNING (UUID: 14454a53-c4e5-49bd-be22-cc119dbf206e) for task 3 of framework 2014-02-11-21:14:47-160088899-53443-21045-0000
I0211 21:14:48.874305 21055 status_update_manager.cpp:314] Received status update TASK_FINISHED (UUID: 9ef8e17e-8569-41fc-93f2-df09a42bf876) for task 4 of framework 2014-02-11-21:14:47-160088899-53443-21045-0000
I0211 21:14:48.874326 21055 status_update_manager.cpp:367] Forwarding status update TASK_FINISHED (UUID: 9ef8e17e-8569-41fc-93f2-df09a42bf876) for task 4 of framework 2014-02-11-21:14:47-160088899-53443-21045-0000 to master@67.195.138.9:53443
I0211 21:14:48.874400 21055 slave.cpp:1765] Handling status update TASK_RUNNING (UUID: 14454a53-c4e5-49bd-be22-cc119dbf206e) for task 3 of framework 2014-02-11-21:14:47-160088899-53443-21045-0000 from executor(1)@67.195.138.9:43514
I0211 21:14:48.874440 21052 master.cpp:2026] Status update TASK_FINISHED (UUID: 9ef8e17e-8569-41fc-93f2-df09a42bf876) for task 4 of framework 2014-02-11-21:14:47-160088899-53443-21045-0000 from slave(1)@67.195.138.9:53443
I0211 21:14:48.874461 21055 slave.cpp:1884] Status update manager successfully handled status update TASK_FINISHED (UUID: 9ef8e17e-8569-41fc-93f2-df09a42bf876) for task 4 of framework 2014-02-11-21:14:47-160088899-53443-21045-0000
I0211 21:14:48.874471 21055 slave.cpp:1890] Sending acknowledgement for status update TASK_FINISHED (UUID: 9ef8e17e-8569-41fc-93f2-df09a42bf876) for task 4 of framework 2014-02-11-21:14:47-160088899-53443-21045-0000 to executor(1)@67.195.138.9:55901
I0211 21:14:48.874487 21052 master.hpp:429] Removing task 4 with resources cpus(*):1; mem(*):32 on slave 2014-02-11-21:14:47-160088899-53443-21045-1 (vesta.apache.org)
I0211 21:14:48.874555 21052 status_update_manager.cpp:314] Received status update TASK_RUNNING (UUID: 14454a53-c4e5-49bd-be22-cc119dbf206e) for task 3 of framework 2014-02-11-21:14:47-160088899-53443-21045-0000
I0211 21:14:48.874575 21052 status_update_manager.cpp:493] Creating StatusUpdate stream for task 3 of framework 2014-02-11-21:14:47-160088899-53443-21045-0000
I0211 21:14:48.874647 21052 status_update_manager.cpp:367] Forwarding status update TASK_RUNNING (UUID: 14454a53-c4e5-49bd-be22-cc119dbf206e) for task 3 of framework 2014-02-11-21:14:47-160088899-53443-21045-0000 to master@67.195.138.9:53443
I0211 21:14:48.874707 21135 process.cpp:1010] Socket closed while receiving
I0211 21:14:48.874711 21055 sched.cpp:616] Scheduler::statusUpdate took 141827ns
I0211 21:14:48.874744 21132 exec.cpp:338] Executor received status update acknowledgement 9ef8e17e-8569-41fc-93f2-df09a42bf876 for task 4 of framework 2014-02-11-21:14:47-160088899-53443-21045-0000
I0211 21:14:48.874634 21054 hierarchical_allocator_process.hpp:637] Recovered cpus(*):1; mem(*):32 (total allocatable: cpus(*):8; mem(*):6961; disk(*):1.38501e+06; ports(*):[31000-32000]) on slave 2014-02-11-21:14:47-160088899-53443-21045-1 from framework 2014-02-11-21:14:47-160088899-53443-21045-0000
I0211 21:14:48.874775 21054 slave.cpp:1884] Status update manager successfully handled status update TASK_RUNNING (UUID: 14454a53-c4e5-49bd-be22-cc119dbf206e) for task 3 of framework 2014-02-11-21:14:47-160088899-53443-21045-0000
I0211 21:14:48.874785 21054 slave.cpp:1890] Sending acknowledgement for status update TASK_RUNNING (UUID: 14454a53-c4e5-49bd-be22-cc119dbf206e) for task 3 of framework 2014-02-11-21:14:47-160088899-53443-21045-0000 to executor(1)@67.195.138.9:43514
I0211 21:14:48.874802 21052 master.cpp:2026] Status update TASK_RUNNING (UUID: 14454a53-c4e5-49bd-be22-cc119dbf206e) for task 3 of framework 2014-02-11-21:14:47-160088899-53443-21045-0000 from slave(3)@67.195.138.9:53443
I0211 21:14:48.874904 21055 sched.cpp:616] Scheduler::statusUpdate took 76541ns
I0211 21:14:48.874948 21144 process.cpp:1010] Socket closed while receiving
I0211 21:14:48.874986 21055 status_update_manager.cpp:392] Received status update acknowledgement (UUID: 14454a53-c4e5-49bd-be22-cc119dbf206e) for task 3 of framework 2014-02-11-21:14:47-160088899-53443-21045-0000
I0211 21:14:48.875021 21058 status_update_manager.cpp:392] Received status update acknowledgement (UUID: 9ef8e17e-8569-41fc-93f2-df09a42bf876) for task 4 of framework 2014-02-11-21:14:47-160088899-53443-21045-0000
I0211 21:14:48.875051 21055 slave.cpp:1371] Status update manager successfully handled status update acknowledgement (UUID: 14454a53-c4e5-49bd-be22-cc119dbf206e) for task 3 of framework 2014-02-11-21:14:47-160088899-53443-21045-0000
I0211 21:14:48.875064 21058 status_update_manager.cpp:524] Cleaning up status update stream for task 4 of framework 2014-02-11-21:14:47-160088899-53443-21045-0000
I0211 21:14:48.875071 21135 process.cpp:1010] Socket closed while receiving
I0211 21:14:48.875093 21131 exec.cpp:359] Executor received framework message
II0211 21:14:48.875123 21131 exec.cpp:368] Executor::frameworkMessage took 17599ns
0211 21:14:48.875120 21136 exec.cpp:524] Executor sending status update TASK_FINISHED (UUID: 292fb4b8-d187-497c-8d7f-b8e6f3bba219) for task 3 of framework 2014-02-11-21:14:47-160088899-53443-21045-0000
I0211 21:14:48.875174 21136 exec.cpp:338] Executor received status update acknowledgement 14454a53-c4e5-49bd-be22-cc119dbf206e for task 3 of framework 2014-02-11-21:14:47-160088899-53443-21045-0000
I0211 21:14:48.875203 21059 slave.cpp:1371] Status update manager successfully handled status update acknowledgement (UUID: 9ef8e17e-8569-41fc-93f2-df09a42bf876) for task 4 of framework 2014-02-11-21:14:47-160088899-53443-21045-0000
I0211 21:14:48.875219 21059 slave.cpp:3237] Completing task 4
I0211 21:14:48.875272 21059 slave.cpp:1943] Sending message for framework 2014-02-11-21:14:47-160088899-53443-21045-0000 to scheduler(1)@67.195.138.9:53443
I0211 21:14:48.875335 21058 slave.cpp:1765] Handling status update TASK_FINISHED (UUID: 292fb4b8-d187-497c-8d7f-b8e6f3bba219) for task 3 of framework 2014-02-11-21:14:47-160088899-53443-21045-0000 from executor(1)@67.195.138.9:43514
I0211 21:14:48.875358 21058 slave.cpp:3214] Terminating task 3
I0211 21:14:48.875360 21059 sched.cpp:701] Scheduler::frameworkMessage took 53004ns
I0211 21:14:48.875466 21059 status_update_manager.cpp:314] Received status update TASK_FINISHED (UUID: 292fb4b8-d187-497c-8d7f-b8e6f3bba219) for task 3 of framework 2014-02-11-21:14:47-160088899-53443-21045-0000
I0211 21:14:48.875488 21059 status_update_manager.cpp:367] Forwarding status update TASK_FINISHED (UUID: 292fb4b8-d187-497c-8d7f-b8e6f3bba219) for task 3 of framework 2014-02-11-21:14:47-160088899-53443-21045-0000 to master@67.195.138.9:53443
I0211 21:14:48.875579 21057 slave.cpp:1884] Status update manager successfully handled status update TASK_FINISHED (UUID: 292fb4b8-d187-497c-8d7f-b8e6f3bba219) for task 3 of framework 2014-02-11-21:14:47-160088899-53443-21045-0000
I0211 21:14:48.875582 21058 master.cpp:2026] Status update TASK_FINISHED (UUID: 292fb4b8-d187-497c-8d7f-b8e6f3bba219) for task 3 of framework 2014-02-11-21:14:47-160088899-53443-21045-0000 from slave(3)@67.195.138.9:53443
I0211 21:14:48.875604 21057 slave.cpp:1890] Sending acknowledgement for status update TASK_FINISHED (UUID: 292fb4b8-d187-497c-8d7f-b8e6f3bba219) for task 3 of framework 2014-02-11-21:14:47-160088899-53443-21045-0000 to executor(1)@67.195.138.9:43514
I0211 21:14:48.875639 21058 master.hpp:429] Removing task 3 with resources cpus(*):1; mem(*):32 on slave 2014-02-11-21:14:47-160088899-53443-21045-2 (vesta.apache.org)
I0211 21:14:48.875778 21055 sched.cpp:616] Scheduler::statusUpdate took 143427ns
I0211 21:14:48.875794 21144 process.cpp:1010] Socket closed while receiving
I0211 21:14:48.875833 21058 hierarchical_allocator_process.hpp:637] Recovered cpus(*):1; mem(*):32 (total allocatable: cpus(*):8; mem(*):6961; disk(*):1.38501e+06; ports(*):[31000-32000]) on slave 2014-02-11-21:14:47-160088899-53443-21045-2 from framework 2014-02-11-21:14:47-160088899-53443-21045-0000
I0211 21:14:48.875932 21138 exec.cpp:338] Executor received status update acknowledgement 292fb4b8-d187-497c-8d7f-b8e6f3bba219 for task 3 of framework 2014-02-11-21:14:47-160088899-53443-21045-0000
I0211 21:14:48.875988 21055 status_update_manager.cpp:392] Received status update acknowledgement (UUID: 292fb4b8-d187-497c-8d7f-b8e6f3bba219) for task 3 of framework 2014-02-11-21:14:47-160088899-53443-21045-0000
I0211 21:14:48.876013 21055 status_update_manager.cpp:524] Cleaning up status update stream for task 3 of framework 2014-02-11-21:14:47-160088899-53443-21045-0000
I0211 21:14:48.876006 21144 process.cpp:1010] Socket closed while receiving
I0211 21:14:48.876032 21137 exec.cpp:359] Executor received framework message
I0211 21:14:48.876083 21137 exec.cpp:368] Executor::frameworkMessage took 36509ns
I0211 21:14:48.876106 21058 slave.cpp:1371] Status update manager successfully handled status update acknowledgement (UUID: 292fb4b8-d187-497c-8d7f-b8e6f3bba219) for task 3 of framework 2014-02-11-21:14:47-160088899-53443-21045-0000
I0211 21:14:48.876121 21058 slave.cpp:3237] Completing task 3
I0211 21:14:48.876209 21055 slave.cpp:1943] Sending message for framework 2014-02-11-21:14:47-160088899-53443-21045-0000 to scheduler(1)@67.195.138.9:53443
I0211 21:14:48.876293 21055 sched.cpp:701] Scheduler::frameworkMessage took 59391ns
I0211 21:14:48.876307 21055 sched.cpp:727] Stopping framework '2014-02-11-21:14:47-160088899-53443-21045-0000'
I0211 21:14:48.876369 21052 master.cpp:1024] Asked to unregister framework 2014-02-11-21:14:47-160088899-53443-21045-0000
I0211 21:14:48.876387 21052 master.cpp:2682] Removing framework 2014-02-11-21:14:47-160088899-53443-21045-0000
I0211 21:14:48.876422 21055 hierarchical_allocator_process.hpp:408] Deactivated framework 2014-02-11-21:14:47-160088899-53443-21045-0000
I0211 21:14:48.876449 21055 slave.cpp:1142] Asked to shut down framework 2014-02-11-21:14:47-160088899-53443-21045-0000 by master@67.195.138.9:53443
I0211 21:14:48.876461 21055 slave.cpp:1167] Shutting down framework 2014-02-11-21:14:47-160088899-53443-21045-0000
Enabling authentication for the framework
Registered with framework ID 2014-02-11-21:14:47-160088899-53443-21045-0000
Got 3 resource offers
Got resource offer 2014-02-11-21:14:47-160088899-53443-21045-0
Accepting offer on vesta.apache.org to start task 0
Got resource offer 2014-02-11-21:14:47-160088899-53443-21045-1
Accepting offer on vesta.apache.org to start task 1
Got resource offer 2014-02-11-21:14:47-160088899-53443-21045-2
Accepting offer on vesta.apache.org to start task 2
Task 2 is in state 1
Task 2 is in state 2
Task 1 is in state 1
Task 1 is in state 2
Received message: 'data with a \x00 byte'
Received message: 'data with a \x00 byte'
Task 0 is in state 1
Task 0 is in state 2
Received message: 'data with a \x00 byte'
Got 3 resource offers
Got resource offer 2014-02-11-21:14:47-160088899-53443-21045-3
Accepting offer on vesta.apache.org to start task 3
Got resource offer 2014-02-11-21:14:47-160088899-53443-21045-4
Accepting offer on vesta.apache.org to start task 4
Got resource offer 2014-02-11-21:14:47-160088899-53443-21045-5
Task 4 is in state 1
Task 4 is in state 2
Task 3 is in state 1
Received message: 'data with a \x00 byte'
Task 3 is in state 2
All tasks done, waiting for final framework message
Received message: 'data with a \x00 byte'
All tasks done, and all messages received, exiting
I0211 21:14:48.876477 21055 slave.cpp:2431] Shutting down executor 'default' of framework 2014-02-11-21:14:47-160088899-53443-21045-0000
I0211 21:14:48.876525 21052 slave.cpp:1142] Asked to shut down framework 2014-02-11-21:14:47-160088899-53443-21045-0000 by master@67.195.138.9:53443
I0211 21:14:48.876545 21052 slave.cpp:1167] Shutting down framework 2014-02-11-21:14:47-160088899-53443-21045-0000
I0211 21:14:48.876555 21052 slave.cpp:2431] Shutting down executor 'default' of framework 2014-02-11-21:14:47-160088899-53443-21045-0000
I0211 21:14:48.876582 21055 slave.cpp:1142] Asked to shut down framework 2014-02-11-21:14:47-160088899-53443-21045-0000 by master@67.195.138.9:53443
I0211 21:14:48.876597 21055 slave.cpp:1167] Shutting down framework 2014-02-11-21:14:47-160088899-53443-21045-0000
I0211 21:14:48.876606 21055 slave.cpp:2431] Shutting down executor 'default' of framework 2014-02-11-21:14:47-160088899-53443-21045-0000
I0211 21:14:48.876662 21053 hierarchical_allocator_process.hpp:363] Removed framework 2014-02-11-21:14:47-160088899-53443-21045-0000
I0211 21:14:48.876766 21133 exec.cpp:378] Executor asked to shutdown
I0211 21:14:48.876785 21135 process.cpp:1010] Socket closed while receiving
I0211 21:14:48.876814 21133 exec.cpp:393] Executor::shutdown took 7179ns
I0211 21:14:48.876843 21133 exec.cpp:77] Scheduling shutdown of the executor
I0211 21:14:48.876899 21144 process.cpp:1010] Socket closed while receiving
I0211 21:14:48.876956 21138 exec.cpp:378] Executor asked to shutdown
I0211 21:14:48.876988 21126 process.cpp:1010] Socket closed while receiving
I0211 21:14:48.877019 21138 exec.cpp:393] Executor::shutdown took 22029ns
I0211 21:14:48.877027 21136 exec.cpp:77] Scheduling shutdown of the executor
I0211 21:14:48.877110 21122 exec.cpp:378] Executor asked to shutdown
I0211 21:14:48.877168 21122 exec.cpp:393] Executor::shutdown took 15665ns
I0211 21:14:48.877185 21123 exec.cpp:77] Scheduling shutdown of the executor
I0211 21:14:48.881350 21045 master.cpp:587] Master terminating
I0211 21:14:48.881434 21045 master.cpp:247] Shutting down master
I0211 21:14:48.881440 21058 slave.cpp:1965] master@67.195.138.9:53443 exited
W0211 21:14:48.881453 21058 slave.cpp:1968] Master disconnected! Waiting for a new master to be elected
I0211 21:14:48.881456 21045 master.cpp:290] Removing slave 2014-02-11-21:14:47-160088899-53443-21045-2 (vesta.apache.org)
I0211 21:14:48.881464 21052 slave.cpp:1965] master@67.195.138.9:53443 exited
W0211 21:14:48.881475 21052 slave.cpp:1968] Master disconnected! Waiting for a new master to be elected
I0211 21:14:48.881438 21053 slave.cpp:1965] master@67.195.138.9:53443 exited
W0211 21:14:48.881515 21053 slave.cpp:1968] Master disconnected! Waiting for a new master to be elected
I0211 21:14:48.881549 21045 master.cpp:290] Removing slave 2014-02-11-21:14:47-160088899-53443-21045-1 (vesta.apache.org)
I0211 21:14:48.881618 21045 master.cpp:290] Removing slave 2014-02-11-21:14:47-160088899-53443-21045-0 (vesta.apache.org)
I0211 21:14:48.882072 21045 slave.cpp:394] Slave terminating
I0211 21:14:48.882113 21045 slave.cpp:1142] Asked to shut down framework 2014-02-11-21:14:47-160088899-53443-21045-0000 by @0.0.0.0:0
W0211 21:14:48.882135 21045 slave.cpp:1163] Ignoring shutdown framework 2014-02-11-21:14:47-160088899-53443-21045-0000 because it is terminating
I0211 21:14:49.300734 21126 process.cpp:1010] Socket closed while receiving
I0211 21:14:49.300804 21121 exec.cpp:439] Ignoring exited event because the driver is aborted!
II0211 21:14:49.300813 21135 process.cpp:1010] Socket closed while receiving
0211 21:14:49.300820 21144 process.cpp:1010] Socket closed while receiving
II0211 21:14:49.300904 21138 exec.cpp:439] Ignoring exited event because the driver is aborted!
0211 21:14:49.300907 21133 exec.cpp:439] Ignoring exited event because the driver is aborted!
tests/script.cpp:81: Failure
Failed
python_framework_test.sh terminated with signal 'Segmentation fault'
[  FAILED  ] ExamplesTest.PythonFramework (2484 ms)
{noformat}",Bug,Major,vinodkone,2016-01-14T21:44:25.000+0000,5,Resolved,Complete,ExamplesTest.PythonFramework is flaky,2016-01-15T00:37:22.000+0000,MESOS-988,3.0,mesos,
,2014-02-06T20:25:17.000+0000,vinodkone,"[==========] Running 1 test from 1 test case.
[----------] Global test environment set-up.
[----------] 1 test from SlaveRecoveryTest/1, where TypeParam = mesos::internal::slave::CgroupsIsolator
[ RUN      ] SlaveRecoveryTest/1.SchedulerFailover
I0206 20:18:31.525116 56447 master.cpp:239] Master ID: 2014-02-06-20:18:31-1740121354-55566-56447 Hostname: smfd-bkq-03-sr4.devel.twitter.com
I0206 20:18:31.525295 56481 master.cpp:321] Master started on 10.37.184.103:55566
I0206 20:18:31.525315 56481 master.cpp:324] Master only allowing authenticated frameworks to register!
I0206 20:18:31.527093 56481 master.cpp:756] The newly elected leader is master@10.37.184.103:55566
I0206 20:18:31.527122 56481 master.cpp:764] Elected as the leading master!
I0206 20:18:31.530642 56473 slave.cpp:112] Slave started on 9)@10.37.184.103:55566
I0206 20:18:31.530802 56473 slave.cpp:212] Slave resources: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I0206 20:18:31.531203 56473 slave.cpp:240] Slave hostname: smfd-bkq-03-sr4.devel.twitter.com
I0206 20:18:31.531221 56473 slave.cpp:241] Slave checkpoint: true
I0206 20:18:31.531991 56482 cgroups_isolator.cpp:225] Using /tmp/mesos_test_cgroup as cgroups hierarchy root
I0206 20:18:31.532470 56478 state.cpp:33] Recovering state from '/tmp/SlaveRecoveryTest_1_SchedulerFailover_7dC2N1/meta'
I0206 20:18:31.532698 56469 status_update_manager.cpp:188] Recovering status update manager
I0206 20:18:31.533962 56472 sched.cpp:265] Authenticating with master master@10.37.184.103:55566
I0206 20:18:31.534102 56472 sched.cpp:234] Detecting new master
I0206 20:18:31.534124 56484 authenticatee.hpp:124] Creating new client SASL connection
I0206 20:18:31.534299 56473 master.cpp:2317] Authenticating framework at scheduler(9)@10.37.184.103:55566
I0206 20:18:31.534459 56461 authenticator.hpp:140] Creating new server SASL connection
I0206 20:18:31.534572 56466 authenticatee.hpp:212] Received SASL authentication mechanisms: CRAM-MD5
I0206 20:18:31.534595 56466 authenticatee.hpp:238] Attempting to authenticate with mechanism 'CRAM-MD5'
I0206 20:18:31.534667 56474 authenticator.hpp:243] Received SASL authentication start
I0206 20:18:31.534732 56474 authenticator.hpp:325] Authentication requires more steps
I0206 20:18:31.534814 56468 authenticatee.hpp:258] Received SASL authentication step
I0206 20:18:31.534946 56466 authenticator.hpp:271] Received SASL authentication step
I0206 20:18:31.535007 56466 authenticator.hpp:317] Authentication success
I0206 20:18:31.535084 56471 authenticatee.hpp:298] Authentication success
I0206 20:18:31.535107 56461 master.cpp:2357] Successfully authenticated framework at scheduler(9)@10.37.184.103:55566
I0206 20:18:31.535392 56476 sched.cpp:339] Successfully authenticated with master master@10.37.184.103:55566
I0206 20:18:31.535512 56465 master.cpp:812] Received registration request from scheduler(9)@10.37.184.103:55566
I0206 20:18:31.535570 56465 master.cpp:830] Registering framework 2014-02-06-20:18:31-1740121354-55566-56447-0000 at scheduler(9)@10.37.184.103:55566
I0206 20:18:31.535856 56465 hierarchical_allocator_process.hpp:332] Added framework 2014-02-06-20:18:31-1740121354-55566-56447-0000
I0206 20:18:31.537802 56482 cgroups_isolator.cpp:840] Recovering isolator
I0206 20:18:31.538462 56472 slave.cpp:2760] Finished recovery
I0206 20:18:31.538910 56472 slave.cpp:508] New master detected at master@10.37.184.103:55566
I0206 20:18:31.539036 56478 status_update_manager.cpp:162] New master detected at master@10.37.184.103:55566
I0206 20:18:31.539223 56464 master.cpp:1834] Attempting to register slave on smfd-bkq-03-sr4.devel.twitter.com at slave(9)@10.37.184.103:55566
I0206 20:18:31.539271 56472 slave.cpp:533] Detecting new master
I0206 20:18:31.539330 56464 master.cpp:2804] Adding slave 2014-02-06-20:18:31-1740121354-55566-56447-0 at smfd-bkq-03-sr4.devel.twitter.com with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I0206 20:18:31.539454 56472 slave.cpp:551] Registered with master master@10.37.184.103:55566; given slave ID 2014-02-06-20:18:31-1740121354-55566-56447-0
I0206 20:18:31.539620 56472 slave.cpp:564] Checkpointing SlaveInfo to '/tmp/SlaveRecoveryTest_1_SchedulerFailover_7dC2N1/meta/slaves/2014-02-06-20:18:31-1740121354-55566-56447-0/slave.info'
I0206 20:18:31.539834 56475 hierarchical_allocator_process.hpp:445] Added slave 2014-02-06-20:18:31-1740121354-55566-56447-0 (smfd-bkq-03-sr4.devel.twitter.com) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (and cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] available)
I0206 20:18:31.540341 56472 master.cpp:2272] Sending 1 offers to framework 2014-02-06-20:18:31-1740121354-55566-56447-0000
I0206 20:18:31.543433 56472 master.cpp:1568] Processing reply for offers: [ 2014-02-06-20:18:31-1740121354-55566-56447-0 ] on slave 2014-02-06-20:18:31-1740121354-55566-56447-0 (smfd-bkq-03-sr4.devel.twitter.com) for framework 2014-02-06-20:18:31-1740121354-55566-56447-0000
I0206 20:18:31.543642 56472 master.hpp:411] Adding task d045a0bd-2ed2-410a-bd1f-5bd9219896e3 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave 2014-02-06-20:18:31-1740121354-55566-56447-0 (smfd-bkq-03-sr4.devel.twitter.com)
I0206 20:18:31.543781 56472 master.cpp:2441] Launching task d045a0bd-2ed2-410a-bd1f-5bd9219896e3 of framework 2014-02-06-20:18:31-1740121354-55566-56447-0000 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave 2014-02-06-20:18:31-1740121354-55566-56447-0 (smfd-bkq-03-sr4.devel.twitter.com)
I0206 20:18:31.544002 56484 slave.cpp:736] Got assigned task d045a0bd-2ed2-410a-bd1f-5bd9219896e3 for framework 2014-02-06-20:18:31-1740121354-55566-56447-0000
I0206 20:18:31.544097 56484 slave.cpp:2899] Checkpointing FrameworkInfo to '/tmp/SlaveRecoveryTest_1_SchedulerFailover_7dC2N1/meta/slaves/2014-02-06-20:18:31-1740121354-55566-56447-0/frameworks/2014-02-06-20:18:31-1740121354-55566-56447-0000/framework.info'
I0206 20:18:31.544272 56484 slave.cpp:2906] Checkpointing framework pid 'scheduler(9)@10.37.184.103:55566' to '/tmp/SlaveRecoveryTest_1_SchedulerFailover_7dC2N1/meta/slaves/2014-02-06-20:18:31-1740121354-55566-56447-0/frameworks/2014-02-06-20:18:31-1740121354-55566-56447-0000/framework.pid'
I0206 20:18:31.544617 56484 slave.cpp:845] Launching task d045a0bd-2ed2-410a-bd1f-5bd9219896e3 for framework 2014-02-06-20:18:31-1740121354-55566-56447-0000
I0206 20:18:31.546721 56484 slave.cpp:3169] Checkpointing ExecutorInfo to '/tmp/SlaveRecoveryTest_1_SchedulerFailover_7dC2N1/meta/slaves/2014-02-06-20:18:31-1740121354-55566-56447-0/frameworks/2014-02-06-20:18:31-1740121354-55566-56447-0000/executors/d045a0bd-2ed2-410a-bd1f-5bd9219896e3/executor.info'
I0206 20:18:31.547317 56484 slave.cpp:3257] Checkpointing TaskInfo to '/tmp/SlaveRecoveryTest_1_SchedulerFailover_7dC2N1/meta/slaves/2014-02-06-20:18:31-1740121354-55566-56447-0/frameworks/2014-02-06-20:18:31-1740121354-55566-56447-0000/executors/d045a0bd-2ed2-410a-bd1f-5bd9219896e3/runs/9adabe16-5d84-45c9-bc83-1a72a6d1c986/tasks/d045a0bd-2ed2-410a-bd1f-5bd9219896e3/task.info'
I0206 20:18:31.547514 56484 slave.cpp:955] Queuing task 'd045a0bd-2ed2-410a-bd1f-5bd9219896e3' for executor d045a0bd-2ed2-410a-bd1f-5bd9219896e3 of framework '2014-02-06-20:18:31-1740121354-55566-56447-0000
I0206 20:18:31.547590 56481 cgroups_isolator.cpp:517] Launching d045a0bd-2ed2-410a-bd1f-5bd9219896e3 (/home/vinod/mesos/build/src/mesos-executor) in /tmp/SlaveRecoveryTest_1_SchedulerFailover_7dC2N1/slaves/2014-02-06-20:18:31-1740121354-55566-56447-0/frameworks/2014-02-06-20:18:31-1740121354-55566-56447-0000/executors/d045a0bd-2ed2-410a-bd1f-5bd9219896e3/runs/9adabe16-5d84-45c9-bc83-1a72a6d1c986 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] for framework 2014-02-06-20:18:31-1740121354-55566-56447-0000 in cgroup mesos_test/framework_2014-02-06-20:18:31-1740121354-55566-56447-0000_executor_d045a0bd-2ed2-410a-bd1f-5bd9219896e3_tag_9adabe16-5d84-45c9-bc83-1a72a6d1c986
I0206 20:18:31.548408 56481 cgroups_isolator.cpp:717] Changing cgroup controls for executor d045a0bd-2ed2-410a-bd1f-5bd9219896e3 of framework 2014-02-06-20:18:31-1740121354-55566-56447-0000 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I0206 20:18:31.548833 56481 cgroups_isolator.cpp:1007] Updated 'cpu.shares' to 2048 for executor d045a0bd-2ed2-410a-bd1f-5bd9219896e3 of framework 2014-02-06-20:18:31-1740121354-55566-56447-0000
I0206 20:18:31.549294 56481 cgroups_isolator.cpp:1117] Updated 'memory.soft_limit_in_bytes' to 1GB for executor d045a0bd-2ed2-410a-bd1f-5bd9219896e3 of framework 2014-02-06-20:18:31-1740121354-55566-56447-0000
I0206 20:18:31.550107 56481 cgroups_isolator.cpp:1147] Updated 'memory.limit_in_bytes' to 1GB for executor d045a0bd-2ed2-410a-bd1f-5bd9219896e3 of framework 2014-02-06-20:18:31-1740121354-55566-56447-0000
I0206 20:18:31.550571 56481 cgroups_isolator.cpp:1174] Started listening for OOM events for executor d045a0bd-2ed2-410a-bd1f-5bd9219896e3 of framework 2014-02-06-20:18:31-1740121354-55566-56447-0000
I0206 20:18:31.551553 56481 cgroups_isolator.cpp:569] Forked executor at = 56671
Checkpointing executor's forked pid 56671 to '/tmp/SlaveRecoveryTest_1_SchedulerFailover_7dC2N1/meta/slaves/2014-02-06-20:18:31-1740121354-55566-56447-0/frameworks/2014-02-06-20:18:31-1740121354-55566-56447-0000/executors/d045a0bd-2ed2-410a-bd1f-5bd9219896e3/runs/9adabe16-5d84-45c9-bc83-1a72a6d1c986/pids/forked.pid'
I0206 20:18:31.552222 56472 slave.cpp:2098] Monitoring executor d045a0bd-2ed2-410a-bd1f-5bd9219896e3 of framework 2014-02-06-20:18:31-1740121354-55566-56447-0000 forked at pid 56671
Fetching resources into '/tmp/SlaveRecoveryTest_1_SchedulerFailover_7dC2N1/slaves/2014-02-06-20:18:31-1740121354-55566-56447-0/frameworks/2014-02-06-20:18:31-1740121354-55566-56447-0000/executors/d045a0bd-2ed2-410a-bd1f-5bd9219896e3/runs/9adabe16-5d84-45c9-bc83-1a72a6d1c986'
I0206 20:18:31.604012 56472 slave.cpp:1431] Got registration for executor 'd045a0bd-2ed2-410a-bd1f-5bd9219896e3' of framework 2014-02-06-20:18:31-1740121354-55566-56447-0000
I0206 20:18:31.604167 56472 slave.cpp:1516] Checkpointing executor pid 'executor(1)@10.37.184.103:46181' to '/tmp/SlaveRecoveryTest_1_SchedulerFailover_7dC2N1/meta/slaves/2014-02-06-20:18:31-1740121354-55566-56447-0/frameworks/2014-02-06-20:18:31-1740121354-55566-56447-0000/executors/d045a0bd-2ed2-410a-bd1f-5bd9219896e3/runs/9adabe16-5d84-45c9-bc83-1a72a6d1c986/pids/libprocess.pid'
I0206 20:18:31.605183 56472 slave.cpp:1552] Flushing queued task d045a0bd-2ed2-410a-bd1f-5bd9219896e3 for executor 'd045a0bd-2ed2-410a-bd1f-5bd9219896e3' of framework 2014-02-06-20:18:31-1740121354-55566-56447-0000
Registered executor on smfd-bkq-03-sr4.devel.twitter.com
Starting task d045a0bd-2ed2-410a-bd1f-5bd9219896e3
sh -c 'sleep 1000'
Forked command at 56712
I0206 20:18:31.613098 56481 slave.cpp:1765] Handling status update TASK_RUNNING (UUID: fc151a46-751b-4c4b-b048-1727752f34e3) for task d045a0bd-2ed2-410a-bd1f-5bd9219896e3 of framework 2014-02-06-20:18:31-1740121354-55566-56447-0000 from executor(1)@10.37.184.103:46181
I0206 20:18:31.613628 56469 status_update_manager.cpp:314] Received status update TASK_RUNNING (UUID: fc151a46-751b-4c4b-b048-1727752f34e3) for task d045a0bd-2ed2-410a-bd1f-5bd9219896e3 of framework 2014-02-06-20:18:31-1740121354-55566-56447-0000
I0206 20:18:31.614006 56469 status_update_manager.hpp:342] Checkpointing UPDATE for status update TASK_RUNNING (UUID: fc151a46-751b-4c4b-b048-1727752f34e3) for task d045a0bd-2ed2-410a-bd1f-5bd9219896e3 of framework 2014-02-06-20:18:31-1740121354-55566-56447-0000
I0206 20:18:31.795529 56469 status_update_manager.cpp:367] Forwarding status update TASK_RUNNING (UUID: fc151a46-751b-4c4b-b048-1727752f34e3) for task d045a0bd-2ed2-410a-bd1f-5bd9219896e3 of framework 2014-02-06-20:18:31-1740121354-55566-56447-0000 to master@10.37.184.103:55566
I0206 20:18:31.795992 56480 slave.cpp:1890] Sending acknowledgement for status update TASK_RUNNING (UUID: fc151a46-751b-4c4b-b048-1727752f34e3) for task d045a0bd-2ed2-410a-bd1f-5bd9219896e3 of framework 2014-02-06-20:18:31-1740121354-55566-56447-0000 to executor(1)@10.37.184.103:46181
I0206 20:18:31.796131 56471 master.cpp:2020] Status update TASK_RUNNING (UUID: fc151a46-751b-4c4b-b048-1727752f34e3) for task d045a0bd-2ed2-410a-bd1f-5bd9219896e3 of framework 2014-02-06-20:18:31-1740121354-55566-56447-0000 from slave(9)@10.37.184.103:55566
I0206 20:18:31.797099 56483 status_update_manager.cpp:392] Received status update acknowledgement (UUID: fc151a46-751b-4c4b-b048-1727752f34e3) for task d045a0bd-2ed2-410a-bd1f-5bd9219896e3 of framework 2014-02-06-20:18:31-1740121354-55566-56447-0000
I0206 20:18:31.797165 56483 status_update_manager.hpp:342] Checkpointing ACK for status update TASK_RUNNING (UUID: fc151a46-751b-4c4b-b048-1727752f34e3) for task d045a0bd-2ed2-410a-bd1f-5bd9219896e3 of framework 2014-02-06-20:18:31-1740121354-55566-56447-0000
I0206 20:18:31.882767 56481 slave.cpp:394] Slave terminating
I0206 20:18:31.883112 56481 master.cpp:641] Slave 2014-02-06-20:18:31-1740121354-55566-56447-0 (smfd-bkq-03-sr4.devel.twitter.com) disconnected
I0206 20:18:31.883200 56476 hierarchical_allocator_process.hpp:484] Slave 2014-02-06-20:18:31-1740121354-55566-56447-0 disconnected
I0206 20:18:31.888206 56473 sched.cpp:265] Authenticating with master master@10.37.184.103:55566
I0206 20:18:31.888473 56473 sched.cpp:234] Detecting new master
I0206 20:18:31.888556 56469 authenticatee.hpp:124] Creating new client SASL connection
I0206 20:18:31.888978 56484 master.cpp:2317] Authenticating framework at scheduler(10)@10.37.184.103:55566
I0206 20:18:31.889348 56469 authenticator.hpp:140] Creating new server SASL connection
I0206 20:18:31.889925 56469 authenticatee.hpp:212] Received SASL authentication mechanisms: CRAM-MD5
I0206 20:18:31.889989 56469 authenticatee.hpp:238] Attempting to authenticate with mechanism 'CRAM-MD5'
I0206 20:18:31.890059 56469 authenticator.hpp:243] Received SASL authentication start
I0206 20:18:31.890233 56469 authenticator.hpp:325] Authentication requires more steps
I0206 20:18:31.890399 56468 authenticatee.hpp:258] Received SASL authentication step
I0206 20:18:31.890554 56484 authenticator.hpp:271] Received SASL authentication step
I0206 20:18:31.890630 56484 authenticator.hpp:317] Authentication success
I0206 20:18:31.890728 56470 authenticatee.hpp:298] Authentication success
I0206 20:18:31.890748 56484 master.cpp:2357] Successfully authenticated framework at scheduler(10)@10.37.184.103:55566
I0206 20:18:31.892210 56469 sched.cpp:339] Successfully authenticated with master master@10.37.184.103:55566
I0206 20:18:31.892410 56473 master.cpp:900] Re-registering framework 2014-02-06-20:18:31-1740121354-55566-56447-0000 at scheduler(10)@10.37.184.103:55566
I0206 20:18:31.892460 56473 master.cpp:926] Framework 2014-02-06-20:18:31-1740121354-55566-56447-0000 failed over
W0206 20:18:31.892691 56465 master.cpp:1048] Ignoring deactivate framework message for framework 2014-02-06-20:18:31-1740121354-55566-56447-0000 from 'scheduler(9)@10.37.184.103:55566' because it is not from the registered framework 'scheduler(10)@10.37.184.103:55566'
I0206 20:18:31.897049 56466 slave.cpp:112] Slave started on 10)@10.37.184.103:55566
I0206 20:18:31.897207 56466 slave.cpp:212] Slave resources: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I0206 20:18:31.897536 56466 slave.cpp:240] Slave hostname: smfd-bkq-03-sr4.devel.twitter.com
I0206 20:18:31.897554 56466 slave.cpp:241] Slave checkpoint: true
I0206 20:18:31.898388 56463 cgroups_isolator.cpp:225] Using /tmp/mesos_test_cgroup as cgroups hierarchy root
I0206 20:18:31.898936 56472 state.cpp:33] Recovering state from '/tmp/SlaveRecoveryTest_1_SchedulerFailover_7dC2N1/meta'
I0206 20:18:31.901702 56465 slave.cpp:2828] Recovering framework 2014-02-06-20:18:31-1740121354-55566-56447-0000
I0206 20:18:31.901759 56465 slave.cpp:3020] Recovering executor 'd045a0bd-2ed2-410a-bd1f-5bd9219896e3' of framework 2014-02-06-20:18:31-1740121354-55566-56447-0000
I0206 20:18:31.902716 56464 status_update_manager.cpp:188] Recovering status update manager
I0206 20:18:31.902884 56464 status_update_manager.cpp:196] Recovering executor 'd045a0bd-2ed2-410a-bd1f-5bd9219896e3' of framework 2014-02-06-20:18:31-1740121354-55566-56447-0000
I0206 20:18:34.475915 56463 cgroups_isolator.cpp:840] Recovering isolator
I0206 20:18:34.476066 56463 cgroups_isolator.cpp:847] Recovering executor 'd045a0bd-2ed2-410a-bd1f-5bd9219896e3' of framework 2014-02-06-20:18:31-1740121354-55566-56447-0000
I0206 20:18:34.477478 56463 cgroups_isolator.cpp:1174] Started listening for OOM events for executor d045a0bd-2ed2-410a-bd1f-5bd9219896e3 of framework 2014-02-06-20:18:31-1740121354-55566-56447-0000
I0206 20:18:34.478728 56463 slave.cpp:2700] Sending reconnect request to executor d045a0bd-2ed2-410a-bd1f-5bd9219896e3 of framework 2014-02-06-20:18:31-1740121354-55566-56447-0000 at executor(1)@10.37.184.103:46181
I0206 20:18:34.480114 56476 slave.cpp:1597] Re-registering executor d045a0bd-2ed2-410a-bd1f-5bd9219896e3 of framework 2014-02-06-20:18:31-1740121354-55566-56447-0000
I0206 20:18:34.480566 56476 cgroups_isolator.cpp:717] Changing cgroup controls for executor d045a0bd-2ed2-410a-bd1f-5bd9219896e3 of framework 2014-02-06-20:18:31-1740121354-55566-56447-0000 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I0206 20:18:34.481370 56476 cgroups_isolator.cpp:1007] Updated 'cpu.shares' to 2048 for executor d045a0bd-2ed2-410a-bd1f-5bd9219896e3 of framework 2014-02-06-20:18:31-1740121354-55566-56447-0000
I0206 20:18:34.481827 56476 cgroups_isolator.cpp:1117] Updated 'memory.soft_limit_in_bytes' to 1GB for executor d045a0bd-2ed2-410a-bd1f-5bd9219896e3 of framework 2014-02-06-20:18:31-1740121354-55566-56447-0000
Re-registered executor on smfd-bkq-03-sr4.devel.twitter.com
I0206 20:18:34.489497 56471 slave.cpp:1713] Cleaning up un-reregistered executors
I0206 20:18:34.489588 56471 slave.cpp:2760] Finished recovery
I0206 20:18:34.490048 56463 slave.cpp:508] New master detected at master@10.37.184.103:55566
I0206 20:18:34.490257 56475 status_update_manager.cpp:162] New master detected at master@10.37.184.103:55566
I0206 20:18:34.490357 56463 slave.cpp:533] Detecting new master
W0206 20:18:34.490603 56480 master.cpp:1878] Slave at slave(10)@10.37.184.103:55566 (smfd-bkq-03-sr4.devel.twitter.com) is being allowed to re-register with an already in use id (2014-02-06-20:18:31-1740121354-55566-56447-0)
I0206 20:18:34.490927 56479 slave.cpp:601] Re-registered with master master@10.37.184.103:55566
I0206 20:18:34.491322 56461 hierarchical_allocator_process.hpp:498] Slave 2014-02-06-20:18:31-1740121354-55566-56447-0 reconnected
I0206 20:18:34.491421 56468 slave.cpp:1312] Updating framework 2014-02-06-20:18:31-1740121354-55566-56447-0000 pid to scheduler(10)@10.37.184.103:55566
I0206 20:18:34.491444 56480 master.cpp:1673] Asked to kill task d045a0bd-2ed2-410a-bd1f-5bd9219896e3 of framework 2014-02-06-20:18:31-1740121354-55566-56447-0000
I0206 20:18:34.491488 56468 slave.cpp:1320] Checkpointing framework pid 'scheduler(10)@10.37.184.103:55566' to '/tmp/SlaveRecoveryTest_1_SchedulerFailover_7dC2N1/meta/slaves/2014-02-06-20:18:31-1740121354-55566-56447-0/frameworks/2014-02-06-20:18:31-1740121354-55566-56447-0000/framework.pid'
I0206 20:18:34.491497 56480 master.cpp:1707] Telling slave 2014-02-06-20:18:31-1740121354-55566-56447-0 (smfd-bkq-03-sr4.devel.twitter.com) to kill task d045a0bd-2ed2-410a-bd1f-5bd9219896e3 of framework 2014-02-06-20:18:31-1740121354-55566-56447-0000
I0206 20:18:34.491657 56468 slave.cpp:1013] Asked to kill task d045a0bd-2ed2-410a-bd1f-5bd9219896e3 of framework 2014-02-06-20:18:31-1740121354-55566-56447-0000
Shutting down
Killing process tree at pid 56712
Killed the following process trees:
[ 
--- 56712 sleep 1000 
]
Command terminated with signal Killed (pid: 56712)
I0206 20:18:34.615216 56463 slave.cpp:1765] Handling status update TASK_KILLED (UUID: d9d37827-3002-4a67-8659-fa36f1986fc7) for task d045a0bd-2ed2-410a-bd1f-5bd9219896e3 of framework 2014-02-06-20:18:31-1740121354-55566-56447-0000 from executor(1)@10.37.184.103:46181
I0206 20:18:34.615556 56483 cgroups_isolator.cpp:717] Changing cgroup controls for executor d045a0bd-2ed2-410a-bd1f-5bd9219896e3 of framework 2014-02-06-20:18:31-1740121354-55566-56447-0000 with resources 
I0206 20:18:34.615624 56476 status_update_manager.cpp:314] Received status update TASK_KILLED (UUID: d9d37827-3002-4a67-8659-fa36f1986fc7) for task d045a0bd-2ed2-410a-bd1f-5bd9219896e3 of framework 2014-02-06-20:18:31-1740121354-55566-56447-0000
I0206 20:18:34.615701 56476 status_update_manager.hpp:342] Checkpointing UPDATE for status update TASK_KILLED (UUID: d9d37827-3002-4a67-8659-fa36f1986fc7) for task d045a0bd-2ed2-410a-bd1f-5bd9219896e3 of framework 2014-02-06-20:18:31-1740121354-55566-56447-0000
I0206 20:18:34.706945 56476 status_update_manager.cpp:367] Forwarding status update TASK_KILLED (UUID: d9d37827-3002-4a67-8659-fa36f1986fc7) for task d045a0bd-2ed2-410a-bd1f-5bd9219896e3 of framework 2014-02-06-20:18:31-1740121354-55566-56447-0000 to master@10.37.184.103:55566
I0206 20:18:34.707263 56476 slave.cpp:1890] Sending acknowledgement for status update TASK_KILLED (UUID: d9d37827-3002-4a67-8659-fa36f1986fc7) for task d045a0bd-2ed2-410a-bd1f-5bd9219896e3 of framework 2014-02-06-20:18:31-1740121354-55566-56447-0000 to executor(1)@10.37.184.103:46181
I0206 20:18:34.707352 56469 master.cpp:2020] Status update TASK_KILLED (UUID: d9d37827-3002-4a67-8659-fa36f1986fc7) for task d045a0bd-2ed2-410a-bd1f-5bd9219896e3 of framework 2014-02-06-20:18:31-1740121354-55566-56447-0000 from slave(10)@10.37.184.103:55566
I0206 20:18:34.707620 56469 master.hpp:429] Removing task d045a0bd-2ed2-410a-bd1f-5bd9219896e3 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave 2014-02-06-20:18:31-1740121354-55566-56447-0 (smfd-bkq-03-sr4.devel.twitter.com)
I0206 20:18:34.708348 56466 hierarchical_allocator_process.hpp:637] Recovered cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (total allocatable: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]) on slave 2014-02-06-20:18:31-1740121354-55566-56447-0 from framework 2014-02-06-20:18:31-1740121354-55566-56447-0000
I0206 20:18:34.708673 56469 status_update_manager.cpp:392] Received status update acknowledgement (UUID: d9d37827-3002-4a67-8659-fa36f1986fc7) for task d045a0bd-2ed2-410a-bd1f-5bd9219896e3 of framework 2014-02-06-20:18:31-1740121354-55566-56447-0000
I0206 20:18:34.708749 56469 status_update_manager.hpp:342] Checkpointing ACK for status update TASK_KILLED (UUID: d9d37827-3002-4a67-8659-fa36f1986fc7) for task d045a0bd-2ed2-410a-bd1f-5bd9219896e3 of framework 2014-02-06-20:18:31-1740121354-55566-56447-0000
I0206 20:18:34.709411 56470 master.cpp:2272] Sending 1 offers to framework 2014-02-06-20:18:31-1740121354-55566-56447-0000
I0206 20:18:34.809782 56447 master.cpp:583] Master terminating
I0206 20:18:34.810066 56447 master.cpp:246] Shutting down master
I0206 20:18:34.810134 56482 slave.cpp:1965] master@10.37.184.103:55566 exited
W0206 20:18:34.810184 56482 slave.cpp:1968] Master disconnected! Waiting for a new master to be elected
I0206 20:18:34.810652 56447 master.cpp:289] Removing slave 2014-02-06-20:18:31-1740121354-55566-56447-0 (smfd-bkq-03-sr4.devel.twitter.com)
I0206 20:18:34.813144 56447 slave.cpp:394] Slave terminating
I0206 20:18:34.821583 56467 cgroups.cpp:1209] Trying to freeze cgroup /tmp/mesos_test_cgroup/mesos_test
I0206 20:18:34.821652 56467 cgroups.cpp:1248] Successfully froze cgroup /tmp/mesos_test_cgroup/mesos_test after 1 attempts
I0206 20:18:34.823129 56471 cgroups.cpp:1224] Trying to thaw cgroup /tmp/mesos_test_cgroup/mesos_test
I0206 20:18:34.823247 56471 cgroups.cpp:1334] Successfully thawed /tmp/mesos_test_cgroup/mesos_test
I0206 20:18:34.923945 56470 cgroups.cpp:1209] Trying to freeze cgroup /tmp/mesos_test_cgroup/mesos_test/framework_2014-02-06-20:18:31-1740121354-55566-56447-0000_executor_d045a0bd-2ed2-410a-bd1f-5bd9219896e3_tag_9adabe16-5d84-45c9-bc83-1a72a6d1c986
I0206 20:18:34.924018 56470 cgroups.cpp:1248] Successfully froze cgroup /tmp/mesos_test_cgroup/mesos_test/framework_2014-02-06-20:18:31-1740121354-55566-56447-0000_executor_d045a0bd-2ed2-410a-bd1f-5bd9219896e3_tag_9adabe16-5d84-45c9-bc83-1a72a6d1c986 after 1 attempts
I0206 20:18:34.925506 56461 cgroups.cpp:1224] Trying to thaw cgroup /tmp/mesos_test_cgroup/mesos_test/framework_2014-02-06-20:18:31-1740121354-55566-56447-0000_executor_d045a0bd-2ed2-410a-bd1f-5bd9219896e3_tag_9adabe16-5d84-45c9-bc83-1a72a6d1c986
I0206 20:18:34.925580 56461 cgroups.cpp:1334] Successfully thawed /tmp/mesos_test_cgroup/mesos_test/framework_2014-02-06-20:18:31-1740121354-55566-56447-0000_executor_d045a0bd-2ed2-410a-bd1f-5bd9219896e3_tag_9adabe16-5d84-45c9-bc83-1a72a6d1c986
[       OK ] SlaveRecoveryTest/1.SchedulerFailover (3408 ms)
[----------] 1 test from SlaveRecoveryTest/1 (3409 ms total)

[----------] Global test environment tear-down
../../src/tests/environment.cpp:247: Failure
Failed
Tests completed with child processes remaining:
-+- 56447 /home/vinod/mesos/build/src/.libs/lt-mesos-tests --verbose --gtest_filter=*SlaveRecoveryTest/1.SchedulerFailover* --gtest_repeat=10 
 \--- 56671 ()
",Bug,Major,vinodkone,,1,Open,New,SlaveRecoveryTest/1.SchedulerFailover is flaky,2014-12-04T21:26:01.000+0000,MESOS-976,1.0,mesos,Q3 Sprint 1
neilc,2014-01-21T18:43:58.000+0000,bmahler,"The following is no longer correct:
http://mesos.apache.org/documentation/latest/logging-and-debugging/

We should either delete this document or re-write it entirely.",Bug,Major,bmahler,2015-12-22T21:04:38.000+0000,5,Resolved,Complete,'Logging and Debugging' document is out-of-date.,2015-12-22T21:04:38.000+0000,MESOS-934,1.0,mesos,Mesosphere Sprint 25
karya,2014-01-17T19:09:50.000+0000,bmahler,"We've observed issues where the masters are slow to respond. Two perf traces collected while the masters were slow to respond:

{noformat}
 25.84%  [kernel]                [k] default_send_IPI_mask_sequence_phys
 20.44%  [kernel]                [k] native_write_msr_safe
  4.54%  [kernel]                [k] _raw_spin_lock
  2.95%  libc-2.5.so             [.] _int_malloc
  1.82%  libc-2.5.so             [.] malloc
  1.55%  [kernel]                [k] apic_timer_interrupt
  1.36%  libc-2.5.so             [.] _int_free
{noformat}

{noformat}
 29.03%  [kernel]                [k] default_send_IPI_mask_sequence_phys
  9.64%  [kernel]                [k] _raw_spin_lock
  7.38%  [kernel]                [k] native_write_msr_safe
  2.43%  libc-2.5.so             [.] _int_malloc
  2.05%  libc-2.5.so             [.] _int_free
  1.67%  [kernel]                [k] apic_timer_interrupt
  1.58%  libc-2.5.so             [.] malloc
{noformat}

These have been found to be attributed to the posix_fadvise calls made by glog. We can disable these via the environment:

{noformat}
GLOG_DEFINE_bool(drop_log_memory, true, ""Drop in-memory buffers of log contents. ""
                 ""Logs can grow very quickly and they are rarely read before they ""
                 ""need to be evicted from memory. Instead, drop them from memory ""
                 ""as soon as they are flushed to disk."");

{noformat}

{code}
    if (FLAGS_drop_log_memory) {
      if (file_length_ >= logging::kPageSize) {
        // don't evict the most recent page
        uint32 len = file_length_ & ~(logging::kPageSize - 1);
        posix_fadvise(fileno(file_), 0, len, POSIX_FADV_DONTNEED);
      }
    }
{code}

We should set GLOG_drop_log_memory=false prior to making our call to google::InitGoogleLogging, to avoid others running into this issue.",Improvement,Blocker,bmahler,2016-01-25T06:05:34.000+0000,5,Resolved,Complete,Set GLOG_drop_log_memory=false in environment prior to logging initialization.,2016-02-27T00:14:32.000+0000,MESOS-920,2.0,mesos,Mesosphere Sprint 27
greggomann,2013-11-20T23:14:32.000+0000,vinodkone,"Identify the cause of the following test failure:

[ RUN      ] ExamplesTest.JavaFramework
Using temporary directory '/tmp/ExamplesTest_JavaFramework_wSc7u8'
Enabling authentication for the framework
I1120 15:13:39.820032 1681264640 master.cpp:285] Master started on 172.25.133.171:52576
I1120 15:13:39.820180 1681264640 master.cpp:299] Master ID: 201311201513-2877626796-52576-3234
I1120 15:13:39.820194 1681264640 master.cpp:302] Master only allowing authenticated frameworks to register!
I1120 15:13:39.821197 1679654912 slave.cpp:112] Slave started on 1)@172.25.133.171:52576
I1120 15:13:39.821795 1679654912 slave.cpp:212] Slave resources: cpus(*):4; mem(*):7168; disk(*):481998; ports(*):[31000-32000]
I1120 15:13:39.822855 1682337792 slave.cpp:112] Slave started on 2)@172.25.133.171:52576
I1120 15:13:39.823652 1682337792 slave.cpp:212] Slave resources: cpus(*):4; mem(*):7168; disk(*):481998; ports(*):[31000-32000]
I1120 15:13:39.825330 1679118336 master.cpp:744] The newly elected leader is master@172.25.133.171:52576
I1120 15:13:39.825445 1679118336 master.cpp:748] Elected as the leading master!
I1120 15:13:39.825907 1681264640 state.cpp:33] Recovering state from '/tmp/ExamplesTest_JavaFramework_wSc7u8/0/meta'
I1120 15:13:39.826127 1681264640 status_update_manager.cpp:180] Recovering status update manager
I1120 15:13:39.826331 1681801216 process_isolator.cpp:317] Recovering isolator
I1120 15:13:39.826738 1682874368 slave.cpp:2743] Finished recovery
I1120 15:13:39.827747 1682337792 state.cpp:33] Recovering state from '/tmp/ExamplesTest_JavaFramework_wSc7u8/1/meta'
I1120 15:13:39.827945 1680191488 slave.cpp:112] Slave started on 3)@172.25.133.171:52576
I1120 15:13:39.828415 1682337792 status_update_manager.cpp:180] Recovering status update manager
I1120 15:13:39.828608 1680728064 sched.cpp:260] Authenticating with master master@172.25.133.171:52576
I1120 15:13:39.828606 1680191488 slave.cpp:212] Slave resources: cpus(*):4; mem(*):7168; disk(*):481998; ports(*):[31000-32000]
I1120 15:13:39.828680 1682874368 slave.cpp:497] New master detected at master@172.25.133.171:52576
I1120 15:13:39.828765 1682337792 process_isolator.cpp:317] Recovering isolator
I1120 15:13:39.829828 1680728064 sched.cpp:229] Detecting new master
I1120 15:13:39.830288 1679654912 authenticatee.hpp:100] Initializing client SASL
I1120 15:13:39.831635 1680191488 state.cpp:33] Recovering state from '/tmp/ExamplesTest_JavaFramework_wSc7u8/2/meta'
I1120 15:13:39.831991 1679118336 status_update_manager.cpp:158] New master detected at master@172.25.133.171:52576
I1120 15:13:39.832042 1682874368 slave.cpp:524] Detecting new master
I1120 15:13:39.832314 1682337792 slave.cpp:2743] Finished recovery
I1120 15:13:39.832309 1681264640 master.cpp:1266] Attempting to register slave on vkone.local at slave(1)@172.25.133.171:52576
I1120 15:13:39.832929 1680728064 status_update_manager.cpp:180] Recovering status update manager
I1120 15:13:39.833371 1681801216 slave.cpp:497] New master detected at master@172.25.133.171:52576
I1120 15:13:39.833273 1681264640 master.cpp:2513] Adding slave 201311201513-2877626796-52576-3234-0 at vkone.local with cpus(*):4; mem(*):7168; disk(*):481998; ports(*):[31000-32000]
I1120 15:13:39.833595 1680728064 process_isolator.cpp:317] Recovering isolator
I1120 15:13:39.833859 1681801216 slave.cpp:524] Detecting new master
I1120 15:13:39.833861 1682874368 status_update_manager.cpp:158] New master detected at master@172.25.133.171:52576
I1120 15:13:39.834092 1680191488 slave.cpp:542] Registered with master master@172.25.133.171:52576; given slave ID 201311201513-2877626796-52576-3234-0
I1120 15:13:39.834486 1681264640 master.cpp:1266] Attempting to register slave on vkone.local at slave(2)@172.25.133.171:52576
I1120 15:13:39.834549 1681264640 master.cpp:2513] Adding slave 201311201513-2877626796-52576-3234-1 at vkone.local with cpus(*):4; mem(*):7168; disk(*):481998; ports(*):[31000-32000]
I1120 15:13:39.834750 1680191488 slave.cpp:555] Checkpointing SlaveInfo to '/tmp/ExamplesTest_JavaFramework_wSc7u8/0/meta/slaves/201311201513-2877626796-52576-3234-0/slave.info'
I1120 15:13:39.834875 1682874368 hierarchical_allocator_process.hpp:445] Added slave 201311201513-2877626796-52576-3234-0 (vkone.local) with cpus(*):4; mem(*):7168; disk(*):481998; ports(*):[31000-32000] (and cpus(*):4; mem(*):7168; disk(*):481998; ports(*):[31000-32000] available)
I1120 15:13:39.835155 1680728064 slave.cpp:542] Registered with master master@172.25.133.171:52576; given slave ID 201311201513-2877626796-52576-3234-1
I1120 15:13:39.835458 1679118336 slave.cpp:2743] Finished recovery
I1120 15:13:39.835739 1680728064 slave.cpp:555] Checkpointing SlaveInfo to '/tmp/ExamplesTest_JavaFramework_wSc7u8/1/meta/slaves/201311201513-2877626796-52576-3234-1/slave.info'
I1120 15:13:39.835922 1682874368 hierarchical_allocator_process.hpp:445] Added slave 201311201513-2877626796-52576-3234-1 (vkone.local) with cpus(*):4; mem(*):7168; disk(*):481998; ports(*):[31000-32000] (and cpus(*):4; mem(*):7168; disk(*):481998; ports(*):[31000-32000] available)
I1120 15:13:39.836120 1681264640 slave.cpp:497] New master detected at master@172.25.133.171:52576
I1120 15:13:39.836340 1679118336 status_update_manager.cpp:158] New master detected at master@172.25.133.171:52576
I1120 15:13:39.836436 1681264640 slave.cpp:524] Detecting new master
I1120 15:13:39.836629 1682874368 master.cpp:1266] Attempting to register slave on vkone.local at slave(3)@172.25.133.171:52576
I1120 15:13:39.836653 1682874368 master.cpp:2513] Adding slave 201311201513-2877626796-52576-3234-2 at vkone.local with cpus(*):4; mem(*):7168; disk(*):481998; ports(*):[31000-32000]
I1120 15:13:39.836804 1680728064 slave.cpp:542] Registered with master master@172.25.133.171:52576; given slave ID 201311201513-2877626796-52576-3234-2
I1120 15:13:39.837190 1680728064 slave.cpp:555] Checkpointing SlaveInfo to '/tmp/ExamplesTest_JavaFramework_wSc7u8/2/meta/slaves/201311201513-2877626796-52576-3234-2/slave.info'
I1120 15:13:39.837569 1682874368 hierarchical_allocator_process.hpp:445] Added slave 201311201513-2877626796-52576-3234-2 (vkone.local) with cpus(*):4; mem(*):7168; disk(*):481998; ports(*):[31000-32000] (and cpus(*):4; mem(*):7168; disk(*):481998; ports(*):[31000-32000] available)
I1120 15:13:39.852011 1679654912 authenticatee.hpp:124] Creating new client SASL connection
I1120 15:13:39.852219 1680191488 master.cpp:1734] Authenticating framework at scheduler(1)@172.25.133.171:52576
I1120 15:13:39.852577 1682337792 authenticator.hpp:83] Initializing server SASL
I1120 15:13:39.856160 1682337792 authenticator.hpp:140] Creating new server SASL connection
I1120 15:13:39.856334 1681264640 authenticatee.hpp:212] Received SASL authentication mechanisms: CRAM-MD5
I1120 15:13:39.856360 1681264640 authenticatee.hpp:238] Attempting to authenticate with mechanism 'CRAM-MD5'
I1120 15:13:39.856421 1681264640 authenticator.hpp:243] Received SASL authentication start
I1120 15:13:39.856487 1681264640 authenticator.hpp:325] Authentication requires more steps
I1120 15:13:39.856531 1681264640 authenticatee.hpp:258] Received SASL authentication step
I1120 15:13:39.856576 1681264640 authenticator.hpp:271] Received SASL authentication step
I1120 15:13:39.856643 1681264640 authenticator.hpp:317] Authentication success
I1120 15:13:39.856724 1681264640 authenticatee.hpp:298] Authentication success
I1120 15:13:39.856768 1681264640 master.cpp:1774] Successfully authenticated framework at scheduler(1)@172.25.133.171:52576
I1120 15:13:39.857028 1681264640 sched.cpp:334] Successfully authenticated with master master@172.25.133.171:52576
I1120 15:13:39.857139 1681264640 master.cpp:798] Received registration request from scheduler(1)@172.25.133.171:52576
I1120 15:13:39.857306 1681264640 master.cpp:816] Registering framework 201311201513-2877626796-52576-3234-0000 at scheduler(1)@172.25.133.171:52576
I1120 15:13:39.862296 1680191488 hierarchical_allocator_process.hpp:332] Added framework 201311201513-2877626796-52576-3234-0000
I1120 15:13:39.863867 1680191488 master.cpp:1700] Sending 3 offers to framework 201311201513-2877626796-52576-3234-0000
Registered! ID = 201311201513-2877626796-52576-3234-0000
Launching task 0
Launching task 1
Launching task 2
I1120 15:13:39.905390 1680191488 master.cpp:2026] Processing reply for offer 201311201513-2877626796-52576-3234-0 on slave 201311201513-2877626796-52576-3234-1 (vkone.local) for framework 201311201513-2877626796-52576-3234-0000
I1120 15:13:39.905825 1680191488 master.hpp:400] Adding task 0 with resources cpus(*):1; mem(*):128 on slave 201311201513-2877626796-52576-3234-1 (vkone.local)
I1120 15:13:39.905886 1680191488 master.cpp:2150] Launching task 0 of framework 201311201513-2877626796-52576-3234-0000 with resources cpus(*):1; mem(*):128 on slave 201311201513-2877626796-52576-3234-1 (vkone.local)
I1120 15:13:39.906422 1680191488 master.cpp:2026] Processing reply for offer 201311201513-2877626796-52576-3234-1 on slave 201311201513-2877626796-52576-3234-2 (vkone.local) for framework 201311201513-2877626796-52576-3234-0000
I1120 15:13:39.906664 1680191488 master.hpp:400] Adding task 1 with resources cpus(*):1; mem(*):128 on slave 201311201513-2877626796-52576-3234-2 (vkone.local)
I1120 15:13:39.906721 1680191488 master.cpp:2150] Launching task 1 of framework 201311201513-2877626796-52576-3234-0000 with resources cpus(*):1; mem(*):128 on slave 201311201513-2877626796-52576-3234-2 (vkone.local)
I1120 15:13:39.907171 1680191488 master.cpp:2026] Processing reply for offer 201311201513-2877626796-52576-3234-2 on slave 201311201513-2877626796-52576-3234-0 (vkone.local) for framework 201311201513-2877626796-52576-3234-0000
I1120 15:13:39.907419 1680191488 master.hpp:400] Adding task 2 with resources cpus(*):1; mem(*):128 on slave 201311201513-2877626796-52576-3234-0 (vkone.local)
I1120 15:13:39.907480 1680191488 master.cpp:2150] Launching task 2 of framework 201311201513-2877626796-52576-3234-0000 with resources cpus(*):1; mem(*):128 on slave 201311201513-2877626796-52576-3234-0 (vkone.local)
I1120 15:13:39.907938 1680191488 slave.cpp:722] Got assigned task 0 for framework 201311201513-2877626796-52576-3234-0000
I1120 15:13:39.908473 1680191488 slave.cpp:833] Launching task 0 for framework 201311201513-2877626796-52576-3234-0000
I1120 15:13:39.914427 1682874368 slave.cpp:722] Got assigned task 1 for framework 201311201513-2877626796-52576-3234-0000
I1120 15:13:39.914594 1680728064 slave.cpp:722] Got assigned task 2 for framework 201311201513-2877626796-52576-3234-0000
I1120 15:13:39.914844 1681801216 hierarchical_allocator_process.hpp:590] Framework 201311201513-2877626796-52576-3234-0000 filtered slave 201311201513-2877626796-52576-3234-1 for 1secs
I1120 15:13:39.915292 1682874368 slave.cpp:833] Launching task 1 for framework 201311201513-2877626796-52576-3234-0000
I1120 15:13:39.915424 1681801216 hierarchical_allocator_process.hpp:590] Framework 201311201513-2877626796-52576-3234-0000 filtered slave 201311201513-2877626796-52576-3234-2 for 1secs
I1120 15:13:39.915685 1681801216 hierarchical_allocator_process.hpp:590] Framework 201311201513-2877626796-52576-3234-0000 filtered slave 201311201513-2877626796-52576-3234-0 for 1secs
I1120 15:13:39.915828 1680728064 slave.cpp:833] Launching task 2 for framework 201311201513-2877626796-52576-3234-0000
I1120 15:13:39.917840 1680191488 slave.cpp:943] Queuing task '0' for executor default of framework '201311201513-2877626796-52576-3234-0000
I1120 15:13:39.917935 1679118336 process_isolator.cpp:100] Launching default (/Users/vinod/workspace/apache/mesos/build/src/examples/java/test-executor) in /tmp/ExamplesTest_JavaFramework_wSc7u8/1/slaves/201311201513-2877626796-52576-3234-1/frameworks/201311201513-2877626796-52576-3234-0000/executors/default/runs/375b31a9-7093-4db1-964d-e6b425b1e4b4 with resources ' for framework 201311201513-2877626796-52576-3234-0000
I1120 15:13:39.922019 1679118336 process_isolator.cpp:163] Forked executor at 3268
I1120 15:13:39.922703 1679118336 slave.cpp:2073] Monitoring executor default of framework 201311201513-2877626796-52576-3234-0000 forked at pid 3268
I1120 15:13:39.929134 1682874368 slave.cpp:943] Queuing task '1' for executor default of framework '201311201513-2877626796-52576-3234-0000
I1120 15:13:39.929323 1682874368 process_isolator.cpp:100] Launching default (/Users/vinod/workspace/apache/mesos/build/src/examples/java/test-executor) in /tmp/ExamplesTest_JavaFramework_wSc7u8/2/slaves/201311201513-2877626796-52576-3234-2/frameworks/201311201513-2877626796-52576-3234-0000/executors/default/runs/2bd0e75d-a2b9-4ae6-be08-9782612309a5 with resources ' for framework 201311201513-2877626796-52576-3234-0000
I1120 15:13:39.931243 1682874368 process_isolator.cpp:163] Forked executor at 3269
I1120 15:13:39.931612 1681801216 slave.cpp:2073] Monitoring executor default of framework 201311201513-2877626796-52576-3234-0000 forked at pid 3269
E1120 15:13:39.931836 1681801216 slave.cpp:2099] Failed to watch executor default of framework 201311201513-2877626796-52576-3234-0000: Already watched
I1120 15:13:39.936460 1680728064 slave.cpp:943] Queuing task '2' for executor default of framework '201311201513-2877626796-52576-3234-0000
I1120 15:13:39.936619 1681801216 process_isolator.cpp:100] Launching default (/Users/vinod/workspace/apache/mesos/build/src/examples/java/test-executor) in /tmp/ExamplesTest_JavaFramework_wSc7u8/0/slaves/201311201513-2877626796-52576-3234-0/frameworks/201311201513-2877626796-52576-3234-0000/executors/default/runs/16d600da-da86-4614-91cb-58a7b27ab534 with resources ' for framework 201311201513-2877626796-52576-3234-0000
I1120 15:13:39.941299 1681801216 process_isolator.cpp:163] Forked executor at 3270
I1120 15:13:39.942179 1681801216 slave.cpp:2073] Monitoring executor default of framework 201311201513-2877626796-52576-3234-0000 forked at pid 3270
E1120 15:13:39.942395 1681801216 slave.cpp:2099] Failed to watch executor default of framework 201311201513-2877626796-52576-3234-0000: Already watched
Fetching resources into '/tmp/ExamplesTest_JavaFramework_wSc7u8/2/slaves/201311201513-2877626796-52576-3234-2/frameworks/201311201513-2877626796-52576-3234-0000/executors/default/runs/2bd0e75d-a2b9-4ae6-be08-9782612309a5'
Fetching resources into '/tmp/ExamplesTest_JavaFramework_wSc7u8/1/slaves/201311201513-2877626796-52576-3234-1/frameworks/201311201513-2877626796-52576-3234-0000/executors/default/runs/375b31a9-7093-4db1-964d-e6b425b1e4b4'
Fetching resources into '/tmp/ExamplesTest_JavaFramework_wSc7u8/0/slaves/201311201513-2877626796-52576-3234-0/frameworks/201311201513-2877626796-52576-3234-0000/executors/default/runs/16d600da-da86-4614-91cb-58a7b27ab534'
I1120 15:13:40.372573 1681801216 slave.cpp:1406] Got registration for executor 'default' of framework 201311201513-2877626796-52576-3234-0000
I1120 15:13:40.373258 1681801216 slave.cpp:1527] Flushing queued task 1 for executor 'default' of framework 201311201513-2877626796-52576-3234-0000
I1120 15:13:40.388317 1681801216 slave.cpp:1406] Got registration for executor 'default' of framework 201311201513-2877626796-52576-3234-0000
I1120 15:13:40.388983 1681801216 slave.cpp:1527] Flushing queued task 0 for executor 'default' of framework 201311201513-2877626796-52576-3234-0000
I1120 15:13:40.398084 1679654912 slave.cpp:1406] Got registration for executor 'default' of framework 201311201513-2877626796-52576-3234-0000
I1120 15:13:40.399344 1679654912 slave.cpp:1527] Flushing queued task 2 for executor 'default' of framework 201311201513-2877626796-52576-3234-0000
Registered executor on vkone.local
I1120 15:13:40.491843 1679654912 slave.cpp:1740] Handling status update TASK_RUNNING (UUID: f04b1852-3669-444a-906f-3675f784c14f) for task 1 of framework 201311201513-2877626796-52576-3234-0000 from executor(1)@172.25.133.171:52577
I1120 15:13:40.492202 1679654912 status_update_manager.cpp:305] Received status update TASK_RUNNING (UUID: f04b1852-3669-444a-906f-3675f784c14f) for task 1 of framework 201311201513-2877626796-52576-3234-0000
I1120 15:13:40.492424 1679654912 status_update_manager.cpp:356] Forwarding status update TASK_RUNNING (UUID: f04b1852-3669-444a-906f-3675f784c14f) for task 1 of framework 201311201513-2877626796-52576-3234-0000 to master@172.25.133.171:52576
Registered executor on vkone.local
I1120 15:13:40.492671 1682337792 master.cpp:1452] Status update TASK_RUNNING (UUID: f04b1852-3669-444a-906f-3675f784c14f) for task 1 of framework 201311201513-2877626796-52576-3234-0000 from slave(3)@172.25.133.171:52576
I1120 15:13:40.492735 1682337792 slave.cpp:1865] Sending acknowledgement for status update TASK_RUNNING (UUID: f04b1852-3669-444a-906f-3675f784c14f) for task 1 of framework 201311201513-2877626796-52576-3234-0000 to executor(1)@172.25.133.171:52577
Status update: task 1 is in state TASK_RUNNING
I1120 15:13:40.502235 1679654912 status_update_manager.cpp:380] Received status update acknowledgement (UUID: f04b1852-3669-444a-906f-3675f784c14f) for task 1 of framework 201311201513-2877626796-52576-3234-0000
Registered executor on vkone.local
I1120 15:13:40.531292 1679654912 slave.cpp:1740] Handling status update TASK_RUNNING (UUID: c19b6a5a-19ce-4613-8a5a-08fe807ff27c) for task 2 of framework 201311201513-2877626796-52576-3234-0000 from executor(1)@172.25.133.171:52579
I1120 15:13:40.532091 1680728064 status_update_manager.cpp:305] Received status update TASK_RUNNING (UUID: c19b6a5a-19ce-4613-8a5a-08fe807ff27c) for task 2 of framework 201311201513-2877626796-52576-3234-0000
I1120 15:13:40.532305 1680728064 status_update_manager.cpp:356] Forwarding status update TASK_RUNNING (UUID: c19b6a5a-19ce-4613-8a5a-08fe807ff27c) for task 2 of framework 201311201513-2877626796-52576-3234-0000 to master@172.25.133.171:52576
I1120 15:13:40.532776 1682874368 slave.cpp:1865] Sending acknowledgement for status update TASK_RUNNING (UUID: c19b6a5a-19ce-4613-8a5a-08fe807ff27c) for task 2 of framework 201311201513-2877626796-52576-3234-0000 to executor(1)@172.25.133.171:52579
I1120 15:13:40.532951 1681801216 master.cpp:1452] Status update TASK_RUNNING (UUID: c19b6a5a-19ce-4613-8a5a-08fe807ff27c) for task 2 of framework 201311201513-2877626796-52576-3234-0000 from slave(1)@172.25.133.171:52576
Status update: task 2 is in state TASK_RUNNING
I1120 15:13:40.538895 1682874368 status_update_manager.cpp:380] Received status update acknowledgement (UUID: c19b6a5a-19ce-4613-8a5a-08fe807ff27c) for task 2 of framework 201311201513-2877626796-52576-3234-0000
I1120 15:13:40.541267 1682874368 slave.cpp:1740] Handling status update TASK_RUNNING (UUID: c218b0c3-d77c-4901-8570-391c330ba117) for task 0 of framework 201311201513-2877626796-52576-3234-0000 from executor(1)@172.25.133.171:52578
I1120 15:13:40.541555 1682874368 status_update_manager.cpp:305] Received status update TASK_RUNNING (UUID: c218b0c3-d77c-4901-8570-391c330ba117) for task 0 of framework 201311201513-2877626796-52576-3234-0000
I1120 15:13:40.541725 1682874368 status_update_manager.cpp:356] Forwarding status update TASK_RUNNING (UUID: c218b0c3-d77c-4901-8570-391c330ba117) for task 0 of framework 201311201513-2877626796-52576-3234-0000 to master@172.25.133.171:52576
I1120 15:13:40.542196 1682874368 master.cpp:1452] Status update TASK_RUNNING (UUID: c218b0c3-d77c-4901-8570-391c330ba117) for task 0 of framework 201311201513-2877626796-52576-3234-0000 from slave(2)@172.25.133.171:52576
I1120 15:13:40.542251 1682874368 slave.cpp:1865] Sending acknowledgement for status update TASK_RUNNING (UUID: c218b0c3-d77c-4901-8570-391c330ba117) for task 0 of framework 201311201513-2877626796-52576-3234-0000 to executor(1)@172.25.133.171:52578
Status update: task 0 is in state TASK_RUNNING
I1120 15:13:40.545537 1682874368 status_update_manager.cpp:380] Received status update acknowledgement (UUID: c218b0c3-d77c-4901-8570-391c330ba117) for task 0 of framework 201311201513-2877626796-52576-3234-0000
Running task value: ""1""

I1120 15:13:40.764219 1682337792 slave.cpp:1740] Handling status update TASK_FINISHED (UUID: 4a163594-146a-46f7-bd43-f906e76ad84c) for task 1 of framework 201311201513-2877626796-52576-3234-0000 from executor(1)@172.25.133.171:52577
I1120 15:13:40.764629 1682337792 status_update_manager.cpp:305] Received status update TASK_FINISHED (UUID: 4a163594-146a-46f7-bd43-f906e76ad84c) for task 1 of framework 201311201513-2877626796-52576-3234-0000
I1120 15:13:40.764698 1682337792 status_update_manager.cpp:356] Forwarding status update TASK_FINISHED (UUID: 4a163594-146a-46f7-bd43-f906e76ad84c) for task 1 of framework 201311201513-2877626796-52576-3234-0000 to master@172.25.133.171:52576
I1120 15:13:40.765043 1682337792 master.cpp:1452] Status update TASK_FINISHED (UUID: 4a163594-146a-46f7-bd43-f906e76ad84c) for task 1 of framework 201311201513-2877626796-52576-3234-0000 from slave(3)@172.25.133.171:52576
I1120 15:13:40.765192 1682337792 master.hpp:418] Removing task 1 with resources cpus(*):1; mem(*):128 on slave 2Status update: task 1 is in state TASK_FINISHED
Finished tasks: 1
01311201513-2877626796-52576-3234-2 (vkone.local)
I1120 15:13:40.765363 1679118336 slave.cpp:1865] Sending acknowledgement for status update TASK_FINISHED (UUID: 4a163594-146a-46f7-bd43-f906e76ad84c) for task 1 of framework 201311201513-2877626796-52576-3234-0000 to executor(1)@172.25.133.171:52577
I1120 15:13:40.772738 1682337792 hierarchical_allocator_process.hpp:637] Recovered cpus(*):1; mem(*):128 (total allocatable: cpus(*):4; mem(*):7168; disk(*):481998; ports(*):[31000-32000]) on slave 201311201513-2877626796-52576-3234-2 from framework 201311201513-2877626796-52576-3234-0000
I1120 15:13:40.773190 1679118336 status_update_manager.cpp:380] Received status update acknowledgement (UUID: 4a163594-146a-46f7-bd43-f906e76ad84c) for task 1 of framework 201311201513-2877626796-52576-3234-0000
Running task value: ""0""

Running task value: ""2""

I1120 15:13:40.790068 1679118336 slave.cpp:1740] Handling status update TASK_FINISHED (UUID: 13265a94-50f1-4bc4-b2e2-9f60a1bb4086) for task 0 of framework 201311201513-2877626796-52576-3234-0000 from executor(1)@172.25.133.171:52578
I1120 15:13:40.790411 1680728064 status_update_manager.cpp:305] Received status update TASK_FINISHED (UUID: 13265a94-50f1-4bc4-b2e2-9f60a1bb4086) for task 0 of framework 201311201513-2877626796-52576-3234-0000
I1120 15:13:40.790493 1680728064 status_update_manager.cpp:356] Forwarding status update TASK_FINISHED (UUID: 13265a94-50f1-4bc4-b2e2-9f60a1bb4086) for task 0 of framework 201311201513-2877626796-52576-3234-0000 to master@172.25.133.171:52576
I1120 15:13:40.790674 1679118336 master.cpp:1452] Status update TASK_FINISHED (UUID: 13265a94-50f1-4bc4-b2e2-9f60a1bb4086) for task 0 of framework 201311201513-2877626796-52576-3234-0000 from slave(2)@172.25.133.171:52576
I1120 15:13:40.790798 1679118336 master.hpp:418] Removing task 0 with resources cpus(*):1; mem(*):128 on slave 201311201513-2877626796-52576-3234-1 (vkone.local)
I1120 15:13:40.790928 1679118336 slave.cpp:1865] Sending acknowledgement for status update TASK_FINISHED (UUID: 13265a94-50f1-4bc4-b2e2-9f60a1bb4086) for task 0 of framework 201311201513-2877626796-52576-3234-0000 to executor(1)@172.25.133.171:52578
Status update: task 0 is in state TASK_FINISHED
Finished tasks: 2
I1120 15:13:40.791225 1680191488 hierarchical_allocator_process.hpp:637] Recovered cpus(*):1; mem(*):128 (total allocatable: cpus(*):4; mem(*):7168; disk(*):481998; ports(*):[31000-32000]) on slave 201311201513-2877626796-52576-3234-1 from framework 201311201513-2877626796-52576-3234-0000
I1120 15:13:40.794234 1679118336 status_update_manager.cpp:380] Received status update acknowledgement (UUID: 13265a94-50f1-4bc4-b2e2-9f60a1bb4086) for task 0 of framework 201311201513-2877626796-52576-3234-0000
I1120 15:13:40.795830 1681801216 slave.cpp:1740] Handling status update TASK_FINISHED (UUID: f6f28e88-5ea6-4519-ba92-65ead6236fff) for task 2 of framework 201311201513-2877626796-52576-3234-0000 from executor(1)@172.25.133.171:52579
I1120 15:13:40.796111 1679118336 status_update_manager.cpp:305] Received status update TASK_FINISHED (UUID: f6f28e88-5ea6-4519-ba92-65ead6236fff) for task 2 of framework 201311201513-2877626796-52576-3234-0000
I1120 15:13:40.796182 1679118336 status_update_manager.cpp:356] Forwarding status update TASK_FINISHED (UUID: f6f28e88-5ea6-4519-ba92-65ead6236fff) for task 2 of framework 201311201513-2877626796-52576-3234-0000 to master@172.25.133.171:52576
I1120 15:13:40.796352 1680728064 master.cpp:1452] Status update TASK_FINISHED (UUID: f6f28e88-5ea6-4519-ba92-65ead6236fff) for task 2 of framework 201311201513-2877626796-52576-3234-0000 from slave(1)@172.25.133.171:52576
I1120 15:13:40.796398 1679118336 slave.cpp:1865] Sending acknowledgement for status update TASK_FINISHED (UUID: f6f28e88-5ea6-4519-ba92-65ead6236fff) for task 2 of framework 201311201513-2877626796-52576-3234-0000 to executor(1)@172.25.133.171:52579
I1120 15:13:40.796466 1680728064 master.hpp:418] Removing task 2 with resources cpus(*):1; mem(*):128 on slave 201311201513-2877626796-52576-3234-0 (vkone.local)
I1120 15:13:40.796707 1679118336 hierarchical_allocator_process.hpp:637] Recovered cpus(*):1; mem(*):128 (total allocatable: cpus(*):4; mem(*):7168; disk(*):481998; ports(*):[31000-32000]) on slave 201311201513-2877626796-52576-3234-0 from framework 201311201513-2877626796-52576-3234-0000
Status update: task 2 is in state TASK_FINISHED
Finished tasks: 3
I1120 15:13:40.797384 1680728064 status_update_manager.cpp:380] Received status update acknowledgement (UUID: f6f28e88-5ea6-4519-ba92-65ead6236fff) for task 2 of framework 201311201513-2877626796-52576-3234-0000
I1120 15:13:40.824383 1681801216 master.cpp:1700] Sending 3 offers to framework 201311201513-2877626796-52576-3234-0000
Launching task 3
Launching task 4
I1120 15:13:40.826971 1679118336 master.cpp:2026] Processing reply for offer 201311201513-2877626796-52576-3234-3 on slave 201311201513-2877626796-52576-3234-1 (vkone.local) for framework 201311201513-2877626796-52576-3234-0000
I1120 15:13:40.827268 1679118336 master.hpp:400] Adding task 3 with resources cpus(*):1; mem(*):128 on slave 201311201513-2877626796-52576-3234-1 (vkone.local)
I1120 15:13:40.827348 1679118336 master.cpp:2150] Launching task 3 of framework 201311201513-2877626796-52576-3234-0000 with resources cpus(*):1; mem(*):128 on slave 201311201513-2877626796-52576-3234-1 (vkone.local)
I1120 15:13:40.827487 1680728064 slave.cpp:722] Got assigned task 3 for framework 201311201513-2877626796-52576-3234-0000
I1120 15:13:40.827857 1680728064 slave.cpp:833] Launching task 3 for framework 201311201513-2877626796-52576-3234-0000
I1120 15:13:40.827913 1679118336 master.cpp:2026] Processing reply for offer 201311201513-2877626796-52576-3234-4 on slave 201311201513-2877626796-52576-3234-2 (vkone.local) for framework 201311201513-2877626796-52576-3234-0000
I1120 15:13:40.827986 1680728064 slave.cpp:968] Sending task '3' to executor 'default' of framework 201311201513-2877626796-52576-3234-0000
I1120 15:13:40.828126 1679118336 master.hpp:400] Adding task 4 with resources cpus(*):1; mem(*):128 on slave 201311201513-2877626796-52576-3234-2 (vkone.local)
I1120 15:13:40.828187 1679118336 master.cpp:2150] Launching task 4 of framework 201311201513-2877626796-52576-3234-0000 with resources cpus(*):1; mem(*):128 on slave 201311201513-2877626796-52576-3234-2 (vkone.local)
I1120 15:13:40.828632 1679118336 master.cpp:2026] Processing reply for offer 201311201513-2877626796-52576-3234-5 on slave 201311201513-2877626796-52576-3234-0 (vkone.local) for framework 201311201513-2877626796-52576-3234-0000
I1120 15:13:40.828655 1680728064 hierarchical_allocator_process.hpp:590] Framework 201311201513-2877626796-52576-3234-0000 filtered slave 201311201513-2877626796-52576-3234-1 for 1secs
I1120 15:13:40.829005 1679118336 slave.cpp:722] Got assigned task 4 for framework 201311201513-2877626796-52576-3234-0000
I1120 15:13:40.829027 1680728064 hierarchical_allocator_process.hpp:590] Framework 201311201513-2877626796-52576-3234-0000 filtered slave 201311201513-2877626796-52576-3234-2 for 1secs
I1120 15:13:40.829260 1679118336 slave.cpp:833] Launching task 4 for framework 201311201513-2877626796-52576-3234-0000
I1120 15:13:40.829273 1680728064 hierarchical_allocator_process.hpp:590] Framework 201311201513-2877626796-52576-3234-0000 filtered slave 201311201513-2877626796-52576-3234-0 for 1secs
I1120 15:13:40.829390 1679118336 slave.cpp:968] Sending task '4' to executor 'default' of framework 201311201513-2877626796-52576-3234-0000
Running task value: ""3""

Running task value: ""4""

I1120 15:13:40.839279 1682337792 slave.cpp:1740] Handling status update TASK_RUNNING (UUID: a8d02ae6-3138-441c-a004-465d879b1277) for task 3 of framework 201311201513-2877626796-52576-3234-0000 from executor(1)@172.25.133.171:52578
I1120 15:13:40.839534 1679118336 status_update_manager.cpp:305] Received status update TASK_RUNNING (UUID: a8d02ae6-3138-441c-a004-465d879b1277) for task 3 of framework 201311201513-2877626796-52576-3234-0000
I1120 15:13:40.839705 1679118336 status_update_manager.cpp:356] Forwarding status update TASK_RUNNING (UUID: a8d02ae6-3138-441c-a004-465d879b1277) for task 3 of framework 201311201513-2877626796-52576-3234-0000 to master@172.25.133.171:52576
I1120 15:13:40.839944 1682337792 master.cpp:1452] Status update TASK_RUNNING (UUID: a8d02ae6-3138-441c-a004-465d879b1277) for task 3 of framework 201311201513-2877626796-52576-3234-0000 from slave(2)@172.25.133.171:52576
Status update: task 3 is in state TASK_RUNNING
I1120 15:13:40.839947 1679118336 slave.cpp:1865] Sending acknowledgement for status update TASK_RUNNING (UUID: a8d02ae6-3138-441c-a004-465d879b1277) for task 3 of framework 201311201513-2877626796-52576-3234-0000 to executor(1)@172.25.133.171:52578
I1120 15:13:40.856334 1679118336 slave.cpp:1740] Handling status update TASK_FINISHED (UUID: 3fc45cb8-fd7f-4bed-a21d-76f234af6b36) for task 3 of framework 201311201513-2877626796-52576-3234-0000 from executor(1)@172.25.133.171:52578
I1120 15:13:40.856650 1679118336 status_update_manager.cpp:380] Received status update acknowledgement (UUID: a8d02ae6-3138-441c-a004-465d879b1277) for task 3 of framework 201311201513-2877626796-52576-3234-0000
I1120 15:13:40.856818 1679118336 status_update_manager.cpp:305] Received status update TASK_FINISHED (UUID: 3fc45cb8-fd7f-4bed-a21d-76f234af6b36) for task 3 of framework 201311201513-2877626796-52576-3234-0000
I1120 15:13:40.856875 1679118336 status_update_manager.cpp:356] Forwarding status update TASK_FINISHED (UUID: 3fc45cb8-fd7f-4bed-a21d-76f234af6b36) for task 3 of framework 201311201513-2877626796-52576-3234-0000 to master@172.25.133.171:52576
I1120 15:13:40.857105 1679118336 slave.cpp:1740] Handling status update TASK_RUNNING (UUID: 4f97c8df-1cc0-4eeb-8469-9d72df09ec73) for task 4 of framework 201311201513-2877626796-52576-3234-0000 from executor(1)@172.25.133.171:52577
I1120 15:13:40.857369 1679118336 slave.cpp:1865] Sending acknowledgement for status update TASK_FINISHED (UUID: 3fc45cb8-fd7f-4bed-a21d-76f234af6b36) for task 3 of framework 201311201513-2877626796-52576-3234-0000 to executor(1)@172.25.133.171:52578
I1120 15:13:40.857498 1680728064 status_update_manager.cpp:305] Received status update TASK_RUNNING (UUID: 4f97c8df-1cc0-4eeb-8469-9d72df09ec73) for task 4 of framework 201311201513-2877626796-52576-3234-0000
I1120 15:13:40.857518 1682337792 master.cpp:1452] Status update TASK_FINISHED (UUID: 3fc45cb8-fd7f-4bed-a21d-76f234af6b36) for task 3 of framework 201311201513-2877626796-52576-3234-0000 from slave(2)@172.25.133.171:52576
I1120 15:13:40.857635 1680728064 status_update_manager.cpp:356] Forwarding status update TASK_RUNNING (UUID: 4f97c8df-1cc0-4eeb-8469-9d72df09ec73) for task 4 of framework 201311201513-2877626796-52576-3234-0000 to master@172.25.133.171:52576
I1120 15:13:40.857630 1682337792 master.hpp:418] Removing task 3 with resources cpus(*):1; mem(*):128 on slave 201311201513-2877626796-52576-3234-1 (vkone.local)
I1120 15:13:40.857843 1682337792 master.cpp:1452] Status update TASK_RUNNING (UUID: 4f97c8df-1cc0-4eeb-8469-9d72df09ec73) for task 4 of framework 201311201513-2877626796-52576-3234-0000 from slave(3)@172.25.133.171:52576
I1120 15:13:40.858043 1680728064 hierarchical_allocator_process.hpp:637] Recovered cpus(*):1; mem(*):128 (total allocatable: cpus(*):4; mem(*):7168; disk(*):481998; ports(*):[31000-32000]) on slave 201311201513-2877626796-52576-3234-1 from framework 201311201513-2877626796-52576-3234-0000
I1120 15:13:40.858098 1680728064 slave.cpp:1865] Sending acknowledgement for status update TASK_RUNNING (UUID: 4f97c8df-1cc0-4eeb-8469-9d72df09ec73) for task 4 of framework 201311201513-2877626796-52576-3234-0000 to executor(1)@172.25.133.171:52577
Status update: task 3 is in state TASK_FINISHED
Finished tasks: 4
Status update: task 4 is in state TASK_RUNNING
I1120 15:13:40.858896 1682337792 status_update_manager.cpp:380] Received status update acknowledgement (UUID: 3fc45cb8-fd7f-4bed-a21d-76f234af6b36) for task 3 of framework 201311201513-2877626796-52576-3234-0000
I1120 15:13:40.858957 1680728064 status_update_manager.cpp:380] Received status update acknowledgement (UUID: 4f97c8df-1cc0-4eeb-8469-9d72df09ec73) for task 4 of framework 201311201513-2877626796-52576-3234-0000
I1120 15:13:40.859905 1679654912 slave.cpp:1740] Handling status update TASK_FINISHED (UUID: 876d7ddc-5d58-48df-a590-d82cf39d4978) for task 4 of framework 201311201513-2877626796-52576-3234-0000 from executor(1)@172.25.133.171:52577
I1120 15:13:40.860174 1680728064 status_update_manager.cpp:305] Received status update TASK_FINISHED (UUID: 876d7ddc-5d58-48df-a590-d82cf39d4978) for task 4 of framework 201311201513-2877626796-52576-3234-0000
I1120 15:13:40.860245 1680728064 status_update_manager.cpp:356] Forwarding status update TASK_FINISHED (UUID: 876d7ddc-5d58-48df-a590-d82cf39d4978) for task 4 of framework 201311201513-2877626796-52576-3234-0000 to master@172.25.133.171:52576
I1120 15:13:40.860437 1679654912 master.cpp:1452] Status update TASK_FINISHED (UUID: 876d7ddc-5d58-48df-a590-d82cf39d4978) for task 4 of framework 201311201513-2877626796-52576-3234-0000 from slave(3)@172.25.133.171:52576
I1120 15:13:40.860486 1680728064 slave.cpp:1865] Sending acknowledgement for status update TASK_FINISHED (UUID: 876d7ddc-5d58-48df-a590-d82cf39d4978) for task 4 of framework 201311201513-2877626796-52576-3234-0000 to executor(1)@172.25.133.171:52577
I1120 15:13:40.860550 1679654912 master.hpp:418] Removing task 4 with resources cpus(*):1; mem(Status update: task 4 is in state TASK_FINISHED
Finished tasks: 5
*):128 on slave 201311201513-2877626796-52576-3234-2 (vkone.local)
I1120 15:13:40.863689 1679654912 master.cpp:996] Asked to unregister framework 201311201513-2877626796-52576-3234-0000
I1120 15:13:40.863750 1679654912 master.cpp:2385] Removing framework 201311201513-2877626796-52576-3234-0000
../../src/tests/script.cpp:81: Failure
Failed
java_framework_test.sh terminated with signal 'Abort trap: 6'
[  FAILED  ] ExamplesTest.JavaFramework (2688 ms)
[----------] 1 test from ExamplesTest (2688 ms total)

[----------] Global test environment tear-down
[==========] 1 test from 1 test case ran. (2692 ms total)
[  PASSED  ] 0 tests.
[  FAILED  ] 1 test, listed below:
[  FAILED  ] ExamplesTest.JavaFramework
",Bug,Major,vinodkone,2015-08-16T17:55:52.000+0000,5,Resolved,Complete,ExamplesTest.JavaFramework is flaky,2015-08-16T17:55:52.000+0000,MESOS-830,8.0,mesos,Mesosphere Sprint 16
vinodkone,2013-10-30T19:39:15.000+0000,vinodkone,"Current semantics:

1) Framework connects w/ master very first time --> registered()
2) Framework reconnects w/ same master after a zk blip --> reregistered()
3) Framework reconnects w/ failed over master --> registered()
4) Failed over framework connects w/ same master --> registered()
5) Failed over framework connects w/ failed over master --> registered() 

Updated semantics:

Everything same except 
3) Framework reconnects w/ failed over master --> reregistered()",Bug,Major,vinodkone,2015-07-07T19:35:39.000+0000,5,Resolved,Complete,Update semantics of when framework registered()/reregistered() get called,2015-07-27T21:41:56.000+0000,MESOS-786,3.0,mesos,Twitter Mesos Q2 Sprint 6
,2013-10-18T22:09:37.000+0000,vinodkone,"[ RUN      ] SlaveRecoveryTest/0.ReconcileTasksMissingFromSlave
Checkpointing executor's forked pid 32281 to '/tmp/SlaveRecoveryTest_0_ReconcileTasksMissingFromSlave_NT1btb/meta/slaves/201310151913-16777343-35153-31491-0/frameworks/201310151913-16777343-35153-31491-0000/executors/0514b52f-3c17-4ee5-ba16-635198701ca2/runs/97c9e2cc-ceea-40a8-a915-aed5fed1dcb3/pids/forked.pid'
Fetching resources into '/tmp/SlaveRecoveryTest_0_ReconcileTasksMissingFromSlave_NT1btb/slaves/201310151913-16777343-35153-31491-0/frameworks/201310151913-16777343-35153-31491-0000/executors/0514b52f-3c17-4ee5-ba16-635198701ca2/runs/97c9e2cc-ceea-40a8-a915-aed5fed1dcb3'
Registered executor on localhost.localdomain
Starting task 0514b52f-3c17-4ee5-ba16-635198701ca2
Forked command at 32317
sh -c 'sleep 10'
tests/slave_recovery_tests.cpp:1927: Failure
Mock function called more times than expected - returning directly.
    Function call: statusUpdate(0x7fffae636eb0, @0x7f1590027a00 64-byte object <F0-2F D0-A1 15-7F 00-00 00-00 00-00 00-00 00-00 40-E9 01-90 15-7F 00-00 20-6B 03-90 15-7F 00-00 48-91 C3-00 00-00 00-00 B0-3B 01-90 15-7F 00-00 05-00 00-00 00-00 00-00 17-00 00-00 00-00 00-00>)
         Expected: to be called once
           Actual: called twice - over-saturated and active
Command exited with status 0 (pid: 32317)
",Bug,Major,vinodkone,,1,Open,New,SlaveRecoveryTest/0.ReconcileTasksMissingFromSlave test is flaky,2014-12-04T21:24:52.000+0000,MESOS-752,1.0,mesos,Q3 Sprint 1
vinodkone,2013-10-06T01:35:55.000+0000,vinodkone,This could be useful information if there are bugs in master/slave that causes slaves to overcommit its resources.,Improvement,Major,vinodkone,2014-11-08T00:46:37.000+0000,5,Resolved,Complete,Expose total number of resources allocated to the slave in its endpoint,2014-11-10T20:04:11.000+0000,MESOS-723,2.0,mesos,Twitter Mesos Q4 Sprint 3
,2013-09-26T17:27:00.000+0000,ssorallen,"Static assets served by the Mesos master don't return ""Last-Modified"" HTTP headers. That means clients receive a 200 status code and re-download assets on every page request even if the assets haven't changed. Because Angular JS does most of the work, the downloading happens only when you navigate to Mesos master in your browser or use the browser's refresh.

Example header for ""mesos.css"":

    HTTP/1.1 200 OK
    Date: Thu, 26 Sep 2013 17:18:52 GMT
    Content-Length: 1670
    Content-Type: text/css

Clients sometimes use the ""Date"" header for the same effect as ""Last-Modified"", but the date is always the time of the response from the server, i.e. it changes on every request and makes the assets look new every time.

The ""Last-Modified"" header should be added and should be the last modified time of the file. On subsequent requests for the same files, the master should return 304 responses with no content rather than 200 with the full files. It could save clients a lot of download time since Mesos assets are rather heavyweight.",Improvement,Major,ssorallen,2015-07-09T15:42:44.000+0000,10020,Accepted,In Progress,"Static files missing ""Last-Modified"" HTTP headers",2015-11-23T10:04:20.000+0000,MESOS-708,2.0,mesos,Mesosphere Sprint 10
pbrett,2013-07-30T01:33:22.000+0000,benjaminhindman,We current check if you have any changes before we run post-reviews.py but we don't check for staged changes which IIUC could get lost.,Bug,Major,benjaminhindman,2015-04-07T17:50:44.000+0000,5,Resolved,Complete,Also check 'git diff --shortstat --staged' in post-reviews.py.,2015-07-02T18:43:12.000+0000,MESOS-598,1.0,mesos,Twitter Mesos Q1 Sprint 6
vinodkone,2013-05-29T22:29:32.000+0000,vinodkone,"I suspect this has to do with the latest flags refactor.

[vinod@smfd-bkq-03-sr4 build]$  sudo GLOG_v=1 ./bin/mesos-tests.sh --gtest_filter=""*Balloon*"" --verbose
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0529 22:28:13.094351 31506 process.cpp:1426] libprocess is initialized on 10.37.184.103:53425 for 24 cpus
I0529 22:28:13.095010 31506 logging.cpp:91] Logging to STDERR
Source directory: /home/vinod/mesos
Build directory: /home/vinod/mesos/build
-------------------------------------------------------------
We cannot run any cgroups tests that require mounting
hierarchies because you have the following hierarchies mounted:
/cgroup
We'll disable the CgroupsNoHierarchyTest test fixture for now.
-------------------------------------------------------------
Note: Google Test filter = *Balloon*-CgroupsNoHierarchyTest.ROOT_CGROUPS_NOHIERARCHY_MountUnmountHierarchy:
[==========] Running 1 test from 1 test case.
[----------] Global test environment set-up.
[----------] 1 test from CgroupsIsolatorTest
[ RUN      ] CgroupsIsolatorTest.ROOT_CGROUPS_BalloonFramework
Using temporary directory '/tmp/CgroupsIsolatorTest_ROOT_CGROUPS_BalloonFramework_pWWdE1'
Launched master at 31574
Failed to load unknown flag 'build_dir'
Usage: lt-mesos-master [...]

Supported options:
  --allocation_interval=VALUE     Amount of time to wait between performing
                                   (batch) allocations (e.g., 500ms, 1sec, etc) (default: 1secs)
  --cluster=VALUE                 Human readable name for the cluster,
                                  displayed in the webui
  --framework_sorter=VALUE        Policy to use for allocating resources
                                  between a given user's frameworks. Options
                                  are the same as for user_allocator (default: drf)
  --[no-]help                     Prints this help message (default: false)
  --ip=VALUE                      IP address to listen on
  --log_dir=VALUE                 Location to put log files (no default, nothing
                                  is written to disk unless specified;
                                  does not affect logging to stderr)
  --logbufsecs=VALUE              How many seconds to buffer log messages for (default: 0)
  --port=VALUE                    Port to listen on (default: 5050)
  --[no-]quiet                    Disable logging to stderr (default: false)
  --[no-]root_submissions         Can root submit frameworks? (default: true)
  --slaves=VALUE                  Initial slaves that should be
                                  considered part of this cluster
                                  (or if using ZooKeeper a URL) (default: *)
  --user_sorter=VALUE             Policy to use for allocating resources
                                  between users. May be one of:
                                    dominant_resource_fairness (drf) (default: drf)
  --webui_dir=VALUE               Location of the webui files/assets (default: /usr/local/share/mesos/webui)
  --whitelist=VALUE               Path to a file with a list of slaves
                                  (one per line) to advertise offers for;
                                  should be of the form: file://path/to/file (default: *)
  --zk=VALUE                      ZooKeeper URL (used for leader election amongst masters)
                                  May be one of:
                                    zk://host1:port1,host2:port2,.../path
                                    zk://username:password@host1:port1,host2:port2,.../path
                                    file://path/to/file (where file contains one of the above) (default: )
{RED}Master crashed; failing test
/home/vinod/mesos/src/tests/balloon_framework_test.sh: line 31: kill: (31574) - No such process
../../src/tests/script.cpp:76: Failure
Failed
balloon_framework_test.sh exited with status 2
[  FAILED  ] CgroupsIsolatorTest.ROOT_CGROUPS_BalloonFramework (2031 ms)
[----------] 1 test from CgroupsIsolatorTest (2031 ms total)

[----------] Global test environment tear-down
[==========] 1 test from 1 test case ran. (2031 ms total)
[  PASSED  ] 0 tests.
[  FAILED  ] 1 test, listed below:
[  FAILED  ] CgroupsIsolatorTest.ROOT_CGROUPS_BalloonFramework

 1 FAILED TEST
",Bug,Major,vinodkone,2014-11-03T23:46:04.000+0000,5,Resolved,Complete,Balloon framework fails to run due to bad flags,2014-11-03T23:46:04.000+0000,MESOS-487,1.0,mesos,Twitter Mesos Q4 Sprint 3
dhamon,2013-02-04T18:27:52.000+0000,bmahler,"We now have a message string inside TaskStatus that provides human readable information about TASK_FAILED.

It would be good to add some structure to the failure reasons, for framework schedulers to act on programmatically.

E.g.

enum TaskFailure {
  EXECUTOR_OOM;
  EXECUTOR_OUT_OF_DISK;
  EXECUTOR_TERMINATED;
  SLAVE_LOST;
  etc..
}",Story,Minor,bmahler,2014-11-04T22:20:07.000+0000,5,Resolved,Complete,Expose TASK_FAILED reason to Frameworks.,2015-03-20T07:24:19.000+0000,MESOS-343,8.0,mesos,Twitter Mesos Q4 Sprint 3
bernd-mesos,2013-01-18T19:19:36.000+0000,wickman,"The slave should be smarter about how it handles pulling down executors.  In our environment, executors rarely change but the slave will always pull it down from regardless HDFS.  This puts undue stress on our HDFS clusters, and is not resilient to reduced HDFS availability.",Epic,Major,wickman,2015-08-05T11:23:00.000+0000,5,Resolved,Complete,Mesos slave should cache executors,2015-08-15T00:26:03.000+0000,MESOS-336,5.0,mesos,
zhitao,2012-11-19T21:12:22.000+0000,woggle,"The Scheduler interface has a callback for executorLost, but currently it is never called.",Improvement,Major,woggle,2016-01-07T18:47:24.000+0000,5,Resolved,Complete,Report executor terminations to framework schedulers.,2016-02-27T00:05:06.000+0000,MESOS-313,2.0,mesos,Mesosphere Sprint 24
