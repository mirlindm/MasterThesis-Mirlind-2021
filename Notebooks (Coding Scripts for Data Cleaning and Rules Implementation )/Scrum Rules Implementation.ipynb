{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Scrum Rules Implementation.ipynb","provenance":[],"collapsed_sections":["8VJicEfpLB9c","oG6K9BLWqViv","0zCQdq1fqige","4HXzXYrrD5Oa","bZc7d6MwdCFx","a6sLFUmiLfVM","bN0PEwEKiF7M","5R-13bVN6dwS","hLSFSvI0FF1F","CXSRk5dlFaoF","RoXfwlD5FnYV","ietzInY0am_d","kTcjzxaFOu7N","0QJ5P82tB4SB","Tt7w1m2HxCni","-XH7cMyNB6X9","9WxFVFNFCDFi","4WBw5SbmEbGe","HTYQRb0Uovyg","CA13T3jfHdoK","T3zA8-5vjgME","zI25XfAtfS_x","gz_DghfbPs1A","RCOmTuFIKbqA","4WZbO8oULT8V"]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"8VJicEfpLB9c"},"source":["## **Uploading and reading the dataset**"]},{"cell_type":"code","metadata":{"id":"2fKGUgJHKetd"},"source":["from google.colab import files \n","uploaded = files.upload()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oG6K9BLWqViv"},"source":["### **Run this cell of code only for the first time!**"]},{"cell_type":"code","metadata":{"id":"-5A6S3Zezzec"},"source":["!pip install colored"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0zCQdq1fqige"},"source":["### **Read the Dataset and provide initial information about it**"]},{"cell_type":"code","metadata":{"id":"Gw85soiLLE6F"},"source":["import numpy as np\n","import pandas as pd\n","import colored\n","\n","\n","# COMMENT OUT AND IN THE DATASET OF THE PROJECT TO BE ANALYZED \n","\n","df = pd.read_csv(\"xd_cleaned.csv\")\n","#df = pd.read_csv(\"apstud_cleaned.csv\")\n","#df = pd.read_csv(\"tistud_cleaned.csv\")\n","#df = pd.read_csv(\"mobile_cleaned.csv\")\n","#df = pd.read_csv(\"mdl_cleaned.csv\")\n","#df = pd.read_csv(\"dnn_cleaned.csv\")\n","#df = pd.read_csv(\"mesos_cleaned.csv\")\n","#df = pd.read_csv(\"mule_cleaned.csv\")\n","#df = pd.read_csv(\"nexus_cleaned.csv\")\n","#df = pd.read_csv(\"timob_cleaned.csv\")\n","\n","pd.set_option('display.max_rows', df.shape[0]+1)\n","pd.set_option('display.max_colwidth', None)\n","pd.set_option('display.width', 150)\n","\n","print(\"\\n******************************\\n\")\n","print(\"Printing Dataset Initial Shape: \\n\")\n","print(df.shape)\n","print(\"\\n******************************\\n\")\n","print(\"Printing Dataset Initial Descriptives: \\n\")\n","print(df.describe())\n","print(\"\\n******************************\\n\")\n","print(\"Printing Dataset Information: \\n\")\n","print(df.info())\n","print(\"\\nCheck the data fields that contain missing values in the Open-Source Project \\n\")\n","print(df.isnull().sum())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4HXzXYrrD5Oa"},"source":["### **Read the Changeset Dataset and provide initial information about it**"]},{"cell_type":"code","metadata":{"id":"9s4fj-3RxU_z"},"source":["import numpy as np\n","import pandas as pd\n","import colored\n","\n","\n","# COMMENT OUT AND IN THE CHANGELOG DATASET OF THE PROJECT TO BE ANALYZED \n","\n","df_changelog = pd.read_csv(\"xd_dataframe.csv\")\n","#df_changelog = pd.read_csv(\"apstud_cleaned.csv\")\n","#df_changelog = pd.read_csv(\"tistud_cleaned.csv\")\n","#df_changelog = pd.read_csv(\"mobile_cleaned.csv\")\n","#df_changelog = pd.read_csv(\"mdl_cleaned.csv\")\n","#df_changelog = pd.read_csv(\"dnn_cleaned.csv\")\n","#df_changelog = pd.read_csv(\"mesos_cleaned.csv\")\n","#df_changelog = pd.read_csv(\"mule_cleaned.csv\")\n","#df_changelog = pd.read_csv(\"nexus_cleaned.csv\")\n","#df_changelog = pd.read_csv(\"timob_cleaned.csv\")\n","\n","pd.set_option('display.max_rows', df_changelog.shape[0]+1)\n","pd.set_option('display.max_colwidth', None)\n","pd.set_option('display.width', 150)\n","\n","print(\"\\n******************************\\n\")\n","print(\"Printing Dataset Initial Shape: \\n\")\n","print(df_changelog.shape)\n","print(\"\\n******************************\\n\")\n","print(\"Printing Dataset Initial Descriptives: \\n\")\n","print(df_changelog.describe())\n","print(\"\\n******************************\\n\")\n","print(\"Printing Dataset Information: \\n\")\n","print(df_changelog.info())\n","print(\"\\nCheck the data fields that contain missing values in the Open-Source Project \\n\")\n","print(df_changelog.isnull().sum())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dAmAlbQxfGkZ"},"source":["### **Insights on total Number of Issues, Sprints and Developers before running the rules:**"]},{"cell_type":"code","metadata":{"id":"Jq7AhJ4WUbMF"},"source":["print(\"Number of issues: \\n\")\n","print(df.key.nunique())\n","print(df.key.unique())\n","print('\\n\\n')\n","print(len(df.key.unique()))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rGKAVpEMbElH"},"source":["print(\"Number of sprints: \\n\")\n","print(df.sprint.nunique())\n","print(df.sprint.unique())\n","print('\\n\\n')\n","print(len(df.sprint.unique()))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6jeirKnObE6z"},"source":["print(\"Number of developers: \\n\")\n","print(df['assignee.name'].nunique())\n","print(df['assignee.name'].unique())\n","print('\\n\\n')\n","print(len(df['assignee.name'].unique()))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bZc7d6MwdCFx"},"source":["## **#Rules for tracking issue statuses!**"]},{"cell_type":"code","metadata":{"id":"suQhCJOZR4mV"},"source":["df_changelog.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oqL8haJpRtIe"},"source":["df_changelog.dropna(subset = [\"from\"], inplace=True)\n","df_changelog.dropna(subset = [\"to\"], inplace=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Q1uBSScwUqgs"},"source":["df_changelog = df_changelog[~df_changelog['from'].str.contains(\"[a-zA-Z]\").fillna(False)]\n","df_changelog = df_changelog[~df_changelog['to'].str.contains(\"[a-zA-Z]\").fillna(False)]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ovrTfdyST4iM"},"source":["df_changelog['from'] = pd.to_numeric(df_changelog['from'], errors='coerce')\n","df_changelog['to'] = pd.to_numeric(df_changelog['to'], errors='coerce')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6QCDnx7gWR6N"},"source":["df_changelog.dtypes"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KG3Vmj4nWtMo"},"source":["df_changelog.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vF_0v3aDVi-c"},"source":["df_changelog['from'].value_counts()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QMfE0MKlSEAy"},"source":["statuses_to_keep = [3, 10000, 10001, 10006]\n","\n","df_changelog = df_changelog[df_changelog['to'].isin(statuses_to_keep)]\n","df_changelog = df_changelog[df_changelog['from'].isin(statuses_to_keep)]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gNYlF5BH4Cxv"},"source":["df_changelog = df_changelog.sort_values(['key', 'created'], ascending=[True, True])\n","\n","grouped_df = df_changelog.groupby(\"key\")\n","\n","for key, item in grouped_df:\n","    print(grouped_df[['key', 'from', 'to']].get_group(key).to_markdown())\n","   \n","   \n","   \n","   \n","   \n","    # if (pd.isnull(grouped_df['key'])):\n","    #   print('\\n\\n*        FALSEEE1        *\\n\\n')\n","    #   #exit()\n","    # else:\n","    #   print('\\n\\n*        TRUE        *\\n\\n')\n","    #   print('\\n\\n*        NEXT SPRINT        *\\n\\n')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7VFwqL3KOWeE"},"source":["df_changelog = df_changelog.sort_values(['key', 'created'], ascending=[True, True])\n","\n","df_changelog = df_changelog.set_index('key')\n","\n","print (df_changelog.index[df_changelog['from']].tolist())\n","print (df_changelog.index[df_changelog['to']].tolist())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HiBiWHcqIEjn"},"source":["df['status.name'].unique()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DcZVxEALIOpm"},"source":["df['status.id'].unique()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SnZwIM8HIRv2"},"source":["df[['key','status.name', 'status.id']].head(50)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"a6sLFUmiLfVM"},"source":["# **Scrum Rules Implementation**"]},{"cell_type":"markdown","metadata":{"id":"HOW1R10DuVSH"},"source":["## **Data Pre-processing!**"]},{"cell_type":"code","metadata":{"id":"IM6_hwE0uc-P"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bN0PEwEKiF7M"},"source":["## **#R1 - No more than five weeks should elapse for a single sprint.**"]},{"cell_type":"markdown","metadata":{"id":"irmMq4JnrgRa"},"source":["### **Code for Mobile and MDL Projects!**"]},{"cell_type":"code","metadata":{"id":"jbIIFLfllaTd"},"source":["# REMOVE ISSUES WITH EMPTY SPRINT VALUE\n","df = df[df['sprint'].notna()]\n","\n","df['sprint_start_date'] = df['sprint'].astype(str).str.extract('startDate=(.{,24})')\n","df['sprint_end_date'] = df['sprint'].astype(str).str.extract('endDate=(.{,24})')\n","df[\"sprint_id\"] = df[\"sprint\"].str.findall(r\"id\\=(\\d+),\")\n","df['sprint_id'] = df['sprint_id'].str.join(',')\n","\n","# THE LINE OF CODE BELOW BELONGS ONLY TO MDL PROJECT \n","#df = df.drop(df[df['sprint_start_date']== '<null>,endDate=<null>,co'].index)\n","\n","df['sprint_start_date'] = pd.to_datetime(df['sprint_start_date'])\n","df['sprint_end_date'] = pd.to_datetime(df['sprint_end_date'])\n","\n","df.sprint_id.value_counts()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hmdGxvf8yhiF"},"source":["len(df.sprint.value_counts())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wUL5NhaUmTiL"},"source":["df[\"sprint_id\"] = df[\"sprint_id\"].str.replace(\",\",\"invalid\")\n","df = df[~df.sprint_id.str.contains('invalid')]\n","\n","df.sprint_id = pd.to_numeric(df.sprint_id, errors='coerce')\n","df = df.sort_values(by=['sprint'])\n","df['difference_between_sprints_in_days'] = df['sprint_end_date'] - df['sprint_start_date']\n","\n","grouped_df = df.sort_values('sprint_start_date').groupby([\"sprint_id\"])\n","#display(df[['sprint_id', 'sprint_start_date', 'sprint_end_date', 'difference_between_sprints_in_days']])\n","first_values = grouped_df.first()\n","display(first_values[['sprint_start_date', 'sprint_end_date', 'difference_between_sprints_in_days']])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Vcz4s-bd1vjz"},"source":["len(df.sprint_id.unique())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xmFuBFh4zspV"},"source":["df.sprint_id.unique()\n","\n","sprints = [125, 106, 167, 289, 264, 195, 114, 136,  85, 170, 165,  79,  92, 160, 143, 202, 176,  75, 229,  95,  74, 152]\n","sprints.sort()\n","df = df[df['sprint_id'].isin(sprints)]\n","\n","df[['sprint_id', 'difference_between_sprints_in_days']]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JuX4oVCUrPyg"},"source":["df['difference_between_sprints_in_days'].value_counts()\n","\n","#df.sprint.nunique()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BH_iBg9XZGmM"},"source":["df['difference_between_sprints_in_days'].describe()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FWuOgbyHt0Nt"},"source":["df['sprint_id'].value_counts()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-litVgJcu9xU"},"source":["df['difference_between_sprints_in_days'].describe()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5R-13bVN6dwS"},"source":["### **Code for Mule Project!**"]},{"cell_type":"code","metadata":{"id":"pGSG6wNqBKGX"},"source":["df['sprint'].value_counts()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"h-PbsE__7mnR"},"source":["# remove empty sprints\n","df = df[df['sprint'].notna()]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"re3AHDmT6hvj"},"source":["df['sprint'] = df['sprint'].str.replace('startDate=', '')\n","df['sprint'] = df['sprint'].astype(object).replace('<null>', np.nan)\n","\n","df['sprint']= pd.to_datetime(df['sprint'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UDGFJSdt6mad"},"source":["len(df['sprint'].unique())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"F015Am1uBNy0"},"source":["df['sprint'].value_counts()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BU0Ri2FK6rMe"},"source":["df_sorted = df.sort_values(by=['sprint'])\n","df_sorted['difference'] = (df_sorted['sprint'] - df_sorted['sprint'].shift(1)).dt.days\n","print(df_sorted.difference.describe())\n","df_sorted = df_sorted.groupby('sprint')\n","first_values = df_sorted.first()\n","display(first_values[['difference']])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"k8LFIMfWDYSU"},"source":["df_sorted['difference'].value_counts()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2dhCexvRCDZa"},"source":["# df['count_difference'] = (df['sprint'] - df['sprint'].shift(1)).dt.days \n","\n","# df[df['count_difference'] > 28.0 ].count() "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8QltSVc17YfQ"},"source":["df_sorted.difference.describe()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"S6tOsuemFAgH"},"source":["## **#R4 - The duration of all sprints should follow similar pace.**\n","\n","## **#This Rule is checked and reported through R1!**"]},{"cell_type":"markdown","metadata":{"id":"hLSFSvI0FF1F"},"source":["## **#R5 - The next Sprint execution should begin only after the previous Sprint's resolution.**"]},{"cell_type":"code","metadata":{"id":"x7gnKIqc0EJH"},"source":["df = df[df['sprint'].notna()]\n","\n","df['sprint_start_date'] = df['sprint'].astype(str).str.extract('startDate=(.{,24})')\n","df['sprint_end_date'] = df['sprint'].astype(str).str.extract('endDate=(.{,24})')\n","\n","df[\"sprint_id\"] = df[\"sprint\"].str.findall(r\"id\\=(\\d+),\")\n","df['sprint_id'] = df['sprint_id'].str.join(',')\n","\n","df = df.drop(df[df['sprint_start_date']== '<null>,endDate=<null>,co'].index)\n","\n","df['sprint_start_date'] = pd.to_datetime(df['sprint_start_date'])\n","df['sprint_end_date'] = pd.to_datetime(df['sprint_end_date'])\n","\n","df['day_of_creation'] = df.sprint_start_date.dt.dayofyear\n","df['day_of_completion'] = df.sprint_end_date.dt.dayofyear \n","\n","#df"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WneVixPYeTZf"},"source":["final_df = df.sort_values(by=['sprint_start_date'], ascending=True)\n","\n","final_df.head(20)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EUhWOMAO0_kB"},"source":["# Run only for MDL\n","\n","df[\"sprint_id\"] = df[\"sprint\"].str.findall(r\"id\\=(\\d+),\")\n","df['sprint_id'] = df['sprint_id'].str.join(',')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1YXlRl91fvgJ"},"source":["df[\"sprint_id\"] = df[\"sprint_id\"].str.replace(\",\",\"invalid\")\n","df = df[~df.sprint_id.str.contains('invalid')]\n","\n","df.sprint_id = pd.to_numeric(df.sprint_id, errors='coerce')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tKs4CFl30Ge1"},"source":["final_df = df.sort_values(by=['sprint_start_date'], ascending=True)\n","\n","final_df['difference'] = (final_df['sprint_start_date'] - final_df['sprint_end_date'].shift(1)).dt.days\n","final_df.head(20)\n","\n","#df = df.sort_values(by=['sprint_id'])\n","\n","grouped_df = final_df.groupby(\"sprint_id\")\n","first_values = grouped_df.first()\n","display(first_values[[ 'sprint_start_date', 'sprint_end_date', 'day_of_creation', 'day_of_completion',  'difference']])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"37uePcR34mnw"},"source":["first_values.difference.value_counts()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"V8qBvcw5451N"},"source":["first_values.difference.describe()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CXSRk5dlFaoF"},"source":["## **R6 - There should be a project clarity identifier attached to each issue.**"]},{"cell_type":"code","metadata":{"id":"j8NsDuqXGRWq"},"source":["# THIS RULE CHECKS IF ALL ISSUES IN THE PROJECT, WITHIN THEIR CORRESPONDING SPRINTS\n","# HAVE a PROJECT Clarity Identifier assosciated with them, which indicates that the issue belongs to a valid project.\n","\n","print(f'Total number of issues present in this Open-Source Project is: {df.key.nunique()}.')\n","print(f'Total number of missing issues present in this Open-Source Project is: {df.key.isnull().sum()}.\\n')\n","print(f'Total number of sprints present in this Open-Source Project is: {df.sprint.nunique()}.')\n","print(f'Total number of missing sprint IDs present in this Open-Source Project is: {df.sprint.isnull().sum()}.\\n')\n","\n","grouped_df = df.groupby(\"sprint\")\n","\n","print('\\n\\n*      FIRST SPRINT       *\\n\\n')\n","for key, item in grouped_df:\n","  if item.iloc[0]['project'] != item['project'].iloc[0]:\n","    print(\"False\")\n","  else: \n","    print(\"True\")\n","    # print(grouped_df[['key', 'project']].get_group(key).to_markdown())\n","    #print('\\n\\n*      NEXT SPRINT       *\\n\\n')\n","    \n","r7_df = df[(df['project'].isnull()) & pd.notnull(df[\"key\"])]\n","\n","if (len(r7_df[['key', 'sprint']]) == 0) :\n","  print(colored.fg(\"green\") + \"\\nRule 7 passed for this Open-Source Project!\\n\")\n","  print(f'Total number of missing project clarity IDs present in this Open-Source Project is: {df.project.isnull().sum()}.\\n')\n","else :\n","  print(colored.fg(\"red\") + \"\\nRule 7 failed for this Open-Source Project!\\n\")\n","\n","r7_df[['sprint', 'key']]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kX3z3ypXFfiV"},"source":["## **#R7 - No considerable amount of time should elapse between the finish of a sprint and the beginning of the new sprint.**\n","\n","## **This rule is checked through R1!**"]},{"cell_type":"markdown","metadata":{"id":"RoXfwlD5FnYV"},"source":["## **#R8 - There should not be a considerable amount of time for a developer to volunteer and start a new issue after she/he has completed the previous one.**\n","\n","## **This rule should be checked AFTER #R28 has been checked!**"]},{"cell_type":"code","metadata":{"id":"GXnpnY-eGSfo"},"source":["# rename the column: assignee.name, to have the new name: developers\n","#df.rename(columns={'assignee.name': 'developers'}, inplace=True)\n","\n","#df.head()\n","\n","MAX_DAY = 2.0\n","counter = 0\n","\n","# For each project, take the list of active developers, and then remove all other developers that are not active part of the team!\n","\n","# print(\"for XD\")\n","#ACTIVE_DEVS = ['grussell', 'dturanski', 'iperumal', 'hillert', 'eric.bottard', 'thomas.risberg', 'grenfro']\n","\n","# print(\"for APSTUD\")\n","#ACTIVE_DEVS = ['cwilliams', 'pinnamuri', 'mxia', 'sgibly']\n","\n","# print(\"for TISTUD\")\n","#ACTIVE_DEVS = ['pinnamuri', 'sgibly', 'mxia', 'cwilliams', 'kkolipaka']\n","\n","# print(\"for MOBILE\")\n","#ACTIVE_DEVS = ['dpalou', 'jleyva', 'pferre22']\n","\n","# print(\"for MDL\")\n","#ACTIVE_DEVS = ['timhunt', 'mudrd8mz', 'dobedobedoh', 'danmarsden', 'jleyva', 'stronk7', 'marina', 'dmonllao', 'fred', 'quen', 'markn', 'damyon', 'dougiamas', 'moodle.com', 'ankit_frenz', 'poltawski', 'rajeshtaneja', 'andyjdavis', 'skodak', 'samhemelryk', 'jerome', 'dongsheng', 'lazyfish']\n","\n","# print(\"for DNN\")\n","#ACTIVE_DEVS = ['bing.wu', 'robert.cui', 'KenGrierson', 'Amritpal.Manak', 'mohit', 'behzad.basir']\n","\n","# print(\"for MESOS\")\n","#ACTIVE_DEVS = ['js84', 'jieyu', 'kaysoky', 'jojy', 'anandmazumdar', 'jvanremoortere', 'greggomann', 'gilbert', 'vinodkone', 'alexr', 'mcypark', 'karya', 'bmahler']\n","\n","# print(\"for MULE\")\n","#ACTIVE_DEVS = ['mariano.gonzalez', 'rodrigo.merino', 'afelisatti', 'pablo.lagreca.ce', 'pablo.kraan', 'andres.gregoire', 'marcosnc']\n","\n","# print(\"for NEXUS\")\n","#ACTIVE_DEVS = ['jtom', 'cstamas', 'alin', 'plynch']\n","\n","# print(\"for TIMOB\")\n","#ACTIVE_DEVS = ['kota', 'cwilliams', 'msamah', 'gmathews', 'hansknoechel', 'cng', 'penrique', 'hpham', 'cbarber', 'vduggal']\n","\n","df = df[df['developers'].isin(ACTIVE_DEVS)]\n","print(df.shape)\n","\n","\n","# Remove issues with no end date, and issues belonging to no sprints!\n","df = df[df['resolutiondate'].notna()]\n","df = df[df['sprint'].notna()]\n","df = df[df['developers'].notna()]\n","#df.shape\n","\n","# Extract days from created and resolutiondate\n","df['created'] = pd.to_datetime(df.created, utc=True)\n","df['resolutiondate'] = pd.to_datetime(df.resolutiondate, utc=True)\n","df['day_of_creation'] = df.created.dt.dayofyear\n","df['day_of_completion'] = df.resolutiondate.dt.dayofyear \n","df['issue_completion_time'] =  df.resolutiondate - df.created\n","\n","df.head()\n","\n","grouped_df = df.sort_values(['sprint', 'created', 'key']).groupby([\"sprint\", \"developers\"])\n","\n","# grouped_df.sort_values(by=['sprint', 'created', 'priority.name'])\n","print('\\n\\n*      FIRST SPRINT       *\\n\\n')\n","for key, item in grouped_df:\n","    print(f\"Key is: {key}\")\n","    #item.sort_index()\n","    item['diff'] = (item['created'] - item['resolutiondate'].shift(1)).dt.days \n","    print(f\"{item[['key', 'developers', 'issue_completion_time', 'diff']].to_markdown()}\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ietzInY0am_d"},"source":["## **#R24 - The backlog does not contain meaningless or empty issues. I consider meaningless issues the issues that belong to no project and have no issue body, description, start and end time and other details that are relevant to developers and other roles.**"]},{"cell_type":"code","metadata":{"id":"hVjpgnc8ayM5"},"source":["# A meaningless issue corresponds to an issue which does not have any description and/or summary. \n","# Description field is more important in this rule.  \n","# Moreover, we can check in this rule whether all issues have any missing status names, priority labels and story points."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Uo2leqJbc22l"},"source":["# rename the column: status.name, to have the new name: status\n","df.rename(columns={'status.name': 'status'}, inplace=True)\n","#df.head()\n","\n","for column in df:\n","    if df[column].isnull().any():\n","       print('{0} has {1} null values'.format(column, df[column].isnull().sum()))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OdFfU35jVsEV"},"source":["cols = ['description', 'summary', 'status', 'project', 'storypoints', 'priority.name']\n","#df.sprint = pd.to_numeric(df.sprint, errors='coerce')\n","df = df.sort_values(by=\"sprint\", ascending=False)\n","print(\"\\n      CHECKING IF THERE ARE MISSING INFORMATION IN THE DESCRITPIVE FIELDS OF THE ISSUES \\n\\n\")\n","df.set_index('sprint')[cols].isna().sum(level=0)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TOxMC0RUcaNp"},"source":["meaningless_issues_per_sprint = df.groupby('sprint').description.nunique()\n","pd.set_option('display.max_rows', df.shape[0]+1)\n","meaningless_issues_per_sprint"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lueIGMy74UIt"},"source":["df.isnull().sum()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"muI0IUyAkHoe"},"source":["df.sprint.value_counts()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hoZ6-h5845Aw"},"source":["no_end_date_df = df[['key', 'sprint', 'priority.name']][df['priority.name'].isna()]\n","no_end_date_df.sort_values(by=\"sprint\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"S_R5mwUwkwCa"},"source":["no_end_date_df = df[['key', 'sprint', 'priority.name', 'project', 'description']][df['storypoints'].isna()]\n","no_end_date_df.sort_values(by=\"sprint\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"p81biEVm9kLV"},"source":["no_end_date_df = df[['key', 'sprint']][df['resolutiondate'].isna()]\n","no_end_date_df.sort_values(by=\"sprint\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kTcjzxaFOu7N"},"source":["## **#R27 - \"No more than 8 active developers should be involved in development tasks\"**"]},{"cell_type":"code","metadata":{"id":"wTMZwI-YRFFd"},"source":["# rename the column: assignee.name, to have the new name: developers\n","df.rename(columns={'assignee.name': 'developers'}, inplace=True)\n","\n","df.head(2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Oakw6ZEkXfpk"},"source":["print(df.key.count())\n","print('\\n\\n')\n","print(df.isnull().sum())\n","\n","print(f\"\\n\\nOut of {df.key.count()} issues, {df.key.count()-df.developers.isnull().sum()} of them have developers assigned to them.\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zvQNGjm8PxA0"},"source":["# Checking how many unique sprints are there (excluding if there is nan values):\n","\n","print(\"\\nCheck the total number of sprints in the Open-Source Project \\n\")\n","\n","print(df.sprint.nunique())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"atEd7cwbXG73"},"source":["# Checking the most active developers\n","print(df.developers.describe())\n","print('\\n\\n')\n","print(df.developers.value_counts())\n","print(df.developers.value_counts().mean())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PjCaviGkQptH"},"source":["value_counts = df['developers'].value_counts()\n","\n","#to_remove = value_counts[value_counts <= df.developers.value_counts().mean()].index\n","\n","#print('for xd')  \n","#to_remove = value_counts[value_counts <= 98].index\n","\n","#print('for apstud')\n","#to_remove = value_counts[value_counts <= 59].index\n","\n","#print('tistud')\n","#to_remove = value_counts[value_counts <= 100].index\n","\n","#print('mobile')\n","#to_remove = value_counts[value_counts <= 100].index\n","\n","#mdl\n","#to_remove = value_counts[value_counts <= 386].index\n","\n","#dnn\n","#to_remove = value_counts[value_counts <= 62].index\n","\n","#mesos\n","#to_remove = value_counts[value_counts <= 31].index\n","\n","#mule\n","#to_remove = value_counts[value_counts <= 73].index\n","\n","#nexus\n","#to_remove = value_counts[value_counts <= 56].index\n","\n","#timob  \n","#to_remove = value_counts[value_counts <= 73].index\n","\n","# Keep rows where the developers column is not in to_remove\n","df = df[~df.developers.isin(to_remove)]\n","\n","devs_per_sprint = df.groupby('sprint').developers.nunique()\n","\n","#pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)\n","pd.set_option('display.max_rows', df.shape[0]+1)\n","\n","devs_per_sprint.sort_values(ascending=False)\n","#print(devs_per_sprint.count())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iQFR52hkwT2U"},"source":["# Checking how many unique issues are there after removing non-active developers:\n","\n","print(\"\\nCheck the total number of sprints in the Open-Source Project \\n\")\n","\n","print(df.key.nunique())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"buEywpM0v_pk"},"source":["# Checking how many unique sprints are there after removing non-active developers:\n","\n","print(\"\\nCheck the total number of sprints in the Open-Source Project \\n\")\n","\n","print(df.sprint.nunique())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DzE0q_R1uijV"},"source":["print(\"Number of active developers: \\n\")\n","print(df['developers'].nunique())\n","print(df['developers'].unique())\n","print('\\n\\n')\n","print(len(df['developers'].unique()))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0QJ5P82tB4SB"},"source":["## **#R28 - The status of issues should always follow the agreed workflow, depending on the project and development team. (#TODO)**"]},{"cell_type":"code","metadata":{"id":"PVjmuBmRCLea"},"source":["print(df['status.name'].value_counts())\n","#df.groupby('key')\n","#df[['sprint', 'key', 'status.id',\t'status.name']].head(20)\n","print(df.project[0])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Tt7w1m2HxCni"},"source":["## **R30: No previously resolved issue should reappear in a future sprint. (#TODO)**"]},{"cell_type":"code","metadata":{"id":"efPvKilzxFad"},"source":["#TODO"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-XH7cMyNB6X9"},"source":["## **#R30 - Higher priority issues should have an earlier resolution date than low priority issues, if created at the same time. (not feasible - delted)**"]},{"cell_type":"markdown","metadata":{"id":"9WxFVFNFCDFi"},"source":["### **ALGORITHM:**\n","\n","group issues by sprint\n","\n","order issues by end_date\n","\n","if(issue['priority'] == null):\n","\n","  discard issue\n","\n","else:\n","\n","  print(issue['start_date', 'end_date', 'priority'])\n","\n","for each pair of consecutive issue (issue_1, issue_2):\n","\n","  if(issue1.priority == 'major' AND issue_2.priority !== 'major' ):\n","\n","  if(issue1.end_date < issue2.end_date):\n","\n","  FALSE\n","\n","  else\n","\n","  TRUE"]},{"cell_type":"code","metadata":{"id":"gAsRP43kuRuU"},"source":["df.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4hnZyLgmh-ZW"},"source":["#Converting the dates to pandas datetime format, which will be easier to process.\n","\n","df['created'] = pd.to_datetime(df.created)\n","df['resolutiondate'] = pd.to_datetime(df.resolutiondate)\n","df.dtypes\n","df['day_of_creation'] = df.created.dt.dayofyear\n","df['day_of_completion'] = df.resolutiondate.dt.dayofyear \n","\n","df['issue_completion_time'] =  df.resolutiondate - df.created\n","df['issue_completion_time'].value_counts()\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xDCt9v0LCOKL"},"source":["#https://stackoverflow.com/questions/54244171/how-do-i-loop-over-each-row-in-a-pandas-groupby\n","\n","# drop issues with no end date\n","df = df[df['resolutiondate'].notna()]\n","#df.shape\n","\n","\n","# extract days from created and resolutiondate\n","\n","df['created'] = pd.to_datetime(df.created)\n","df['resolutiondate'] = pd.to_datetime(df.resolutiondate)\n","df['day_of_creation'] = df.created.dt.dayofyear\n","df['day_of_completion'] = df.resolutiondate.dt.dayofyear \n","df['issue_completion_time'] =  df.resolutiondate - df.created\n","\n","df.head()\n","\n","\n","grouped_issues = df.groupby('sprint').nunique()\n","pd.set_option('display.max_rows', df.shape[0]+1)\n","#display(grouped_issues.key)\n","grouped_issues\n","#print(df.columns)\n","issues_df = df[['sprint', 'key', 'priority.name', 'created', 'resolutiondate', 'issue_completion_time', 'day_of_creation', 'day_of_completion']]\n","# priorities_df = issues_df['priority.name'].unique()\n","# print(priorities_df)\n","print('\\n\\n')\n","issues_df.sort_values(by=['sprint', 'day_of_creation', 'priority.name'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"w3RIumpFNcRt"},"source":["df['created'] = pd.to_datetime(df.created)\n","df['day_of_creation'] = df.created.dt.dayofyear \n","df.created = df.created.dt.strftime('%Y-%m-%d')\n","#grouped_issues = df.groupby(['sprint', 'day_of_creation'])\n","\n","issues_df = df[['key', 'priority.name', 'created', 'resolutiondate', 'sprint', 'day_of_creation']]\n","#issues_df.groupby(['sprint'])\n","issues_df.sort_values(by=['sprint', 'created', 'priority.name', 'resolutiondate'])\n","issues_df"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3Ax_bmQ-9Uk6"},"source":["# grouped_df = issues_df.groupby(\"sprint\")\n","\n","# for key, item in grouped_df:\n","#     print(grouped_df[['sprint', 'key', 'priority.name', 'created', 'resolutiondate', 'issue_completion_time', 'day_of_creation', 'day_of_completion']].get_group(key).to_markdown())\n","#     #if (grouped_df['day_of_creation'].iloc[key] == grouped_df['day_of_creation'].iloc[key+1]):\n","#         if (grouped_df['priority.name'].iloc[key] != grouped_df['priority.name'].iloc[key+1]):\n","#           print('\\n\\n*        FALSEEE1        *\\n\\n')\n","#       #exit()\n","#     else:\n","#       print('\\n\\n*        TRUE        *\\n\\n')\n","#       print('\\n\\n*        NEXT SPRINT        *\\n\\n')\n","\n","\n","      "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IY80woMLMIfa"},"source":["df.groupby('priority.name').filter(lambda x: x.created.nunique()>1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7L0Wyn0u2XXL"},"source":["for (columnName, columnData) in df.iteritems():\n","  if(columnName )\n","   print('Colunm Name : ', columnName)\n","   print('Column Contents : ', columnData.values)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2A2cc9d71XVE"},"source":["#column = 'sprint'\n","for column in df:\n","    if df[column].isnull().any():\n","       print('{0} has {1} null values'.format(column, df[column].isnull().sum()))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tBL0-zHTuLsi"},"source":["# drop issues with no end date\n","df = df[df['resolutiondate'].notna()]\n","df = df[df['sprint'].notna()]\n","#df.shape\n","\n","# extract days from created and resolutiondate\n","\n","df['created'] = pd.to_datetime(df.created)\n","df['resolutiondate'] = pd.to_datetime(df.resolutiondate)\n","df['day_of_creation'] = df.created.dt.dayofyear\n","df['day_of_completion'] = df.resolutiondate.dt.dayofyear \n","df['issue_completion_time'] =  df.resolutiondate - df.created\n","\n","df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EGhYzk2QAQe6"},"source":["duplicateRowsDF = df[df.duplicated(['day_of_creation'])]\n","\n","\n","#duplicateRowsDF[['sprint', 'key', 'day_of_creation']].head(20)\n","\n","duplicateRowsDF = duplicateRowsDF[['sprint', 'key', 'priority.name', 'issue_completion_time', 'day_of_creation', 'day_of_completion']]\n","# priorities_df = issues_df['priority.name'].unique()\n","# print(priorities_df)\n","print('\\n\\n')\n","duplicateRowsDF.sort_values(by=['sprint', 'day_of_creation', 'priority.name'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kpfMP5BuvXPh"},"source":["df.sort_values(by=[ 'day_of_creation', 'priority.name'])\n","grouped_df = df.groupby([\"sprint\"])\n","\n","#grouped_df.sort_values(by=['sprint', 'created', 'priority.name'])\n","\n","print('\\n\\n*      FIRST SPRINT       *\\n\\n')\n","for key, item in grouped_df:\n","    print(f\"Key is: {key}\")\n","    item.sort_values('created')\n","    print(f\"{item[['sprint', 'key','priority.name', 'day_of_creation', 'day_of_completion', 'issue_completion_time']].to_markdown()}\")\n","    \n","    #print(grouped_df[['sprint', 'key', 'priority.name', 'day_of_creation', 'day_of_completion', 'issue_completion_time']].get_group(key).to_markdown())\n","    "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"h4P_4u5hucIF"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4WBw5SbmEbGe"},"source":["## **#R31 - Each Scrum Sprints can be considered a short project, therefore there should be a unique identifier/name associated with each Sprint**"]},{"cell_type":"code","metadata":{"id":"zmBO2qjWFQBc"},"source":["# THIS RULE CHECKS IF ALL EXISTING SPRINTS ARE UNIQUELY IDENTIFIED, NOT IF THERE ARE ISSUES NOT BELONGING TO A SPRINT!! \n","\n","#print(f\"\\nTotal records of Sprints in the Open-Source Project: {df['sprint'].count()}\\n\")\n","\n","print(f\"Number of unique sprints (excluding the nan) in the Open-Source Project: {df.sprint.nunique()}\\n\")\n","#print(df.sprint.unique())\n","\n","print(f\"The number of missing values in the Sprint field in the Open-Source Project is: {df.sprint.isnull().sum()}\\n\")\n","print(f\"The number of non-missing values in the Sprint field in the Open-Source Project is: {df.sprint.notnull().sum()}\\n\")\n","list_of_sprints = df['sprint'].unique()\n","print(f'There are {len(list_of_sprints)} unique sprints in this project!\\n')\n","\n","if (df.sprint.nunique() == len(list_of_sprints)-1):\n","  print(colored.fg(\"green\") + f\"Rule 33 Passes for project: {df.project[0]}!\")\n","else:\n","  print(colored.fg(\"red\") + f\"Rule 33 Fails for project: {df.project[0]} !\")\n","\n","#sprint_df = df[df[\"sprint\"].notna()]\n","#sprint_df.sprint.drop_duplicates().sort_values(ascending=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HAnxSmK5FKaU"},"source":["display(df.sprint.value_counts())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Cj6xfAFpFZbR"},"source":["# column = ['sprint']\n","\n","# for (columnName, columnData) in df.iteritems():\n","#   if columnName:\n","#     print('Column Name : ', columnName)\n","#     print('Column Contents : ', columnData.values)\n","\n","# for name in df.columns:\n","#   for row in df.index:\n","#       if df.loc[name] == 'sprint':\n","#           print(\"sprint\")\n","            #df.loc[row,name] = name"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zDkPDJYUBkms"},"source":["spike_cols = [col for col in df.columns if 'sprint' in col]\n","print(list(df.columns))\n","print(spike_cols)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Pjv5nwT_CB4u"},"source":["df['empty_sprint_IDs'] = df['sprint'].apply(lambda x: 'True' if pd.isnull(x) else 'False')\n","\n","df['empty_sprint_IDs'].value_counts()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HTYQRb0Uovyg"},"source":["## **#R32 - There should be a type, such as bug, improvement or task, associated to each issue**"]},{"cell_type":"code","metadata":{"id":"H6HhBTOfo55e"},"source":["# THIS RULE CHECKS IF ALL ISSUES IN THE PROJECT, ARE ASSIGNED AN ISSUE STATUS (IN PROGRESS, IN TEST, ETC.)\n","# NOTE: Each project has its own types, this rule only checks if issues have types assigned to them throughout their lifecycle. \n","\n","\n","df.rename(columns={'issuetype.name': 'types'}, inplace=True)\n","\n","print(f'Total number of issue types present in this Open-Source Project is: {df.types.nunique()}.')\n","print('\\nThese different statuses and their number of occurence for this Open-Source Project are:')\n","print(df['types'].value_counts())\n","\n","print('\\n\\nFinally, we are checking if there are issues having no issue types in the dataset: \\n')\n","types_df = df[(df['types'].isnull()) & pd.notnull(df[\"key\"])]\n","\n","print(f\"There are {len(df['types'])} values present in the issuetype.name data field.\")\n","print(f\"Total number of issues having no issue status is: {len(types_df[['key', 'types']])}.\\n\")\n","if (len(types_df[['key', 'types']]) == 0) :\n","    print(colored.fg(\"green\") + \"\\nRule 34 passed for this Open-Source Project!\\n\")\n","else :\n","  print(colored.fg(\"red\") + \"\\nRule 34 failed for this Open-Source Project!\\n\")\n","\n","types_df[['key', 'types']]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CA13T3jfHdoK"},"source":["## **#R33 - Each issue belongs to a specific Sprint, therefore there should be a sprint identifier attached to each issue.**"]},{"cell_type":"code","metadata":{"id":"RH97zfBpH2Pr"},"source":["# THIS RULE CHECKS IF ALL ISSUES IN THE PROJECT, ARE CREATED AS PART OF A RUNNING SPRINT,\n","# IN THE CASE THE TEAMS WERE ORGANIZING THE WORK IN SPRINTS.\n","\n","print(f'Total number of unique sprints present in this Open-Source Project is: {df.sprint.nunique()}.')\n","print(f'Total number of missing sprints present in this Open-Source Project is: {df.sprint.isnull().sum()}.')\n","print(f'Total number of unique issues present in this Open-Source Project is: {df.key.nunique()}.')\n","print(f'Total number of missing issues present in this Open-Source Project is: {df.key.isnull().sum()}.')\n","\n","print('\\nFinally, we are checking if there are issues having no issue statuses in the dataset: \\n')\n","issue_sprint_df = df[(df['sprint'].isnull()) & pd.notnull(df[\"key\"])]\n","\n","print(len(issue_sprint_df[['key', 'sprint']]))\n","print(f\"Total number of issues belonging to no sprints is: {len(issue_sprint_df[['key', 'sprint']])}.\\n\")\n","if (len(issue_sprint_df[['key', 'sprint']]) == 0) :\n","    print(colored.fg(\"green\") + \"\\nRule 38 passed for this Open-Source Project!\\n\")\n","else :\n","  print(colored.fg(\"red\") + \"\\nRule 38 failed for this Open-Source Project!\\n\")\n","\n","#issue_sprint_df[['key', 'sprint', 'resolutiondate']]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YtdjKqAoOq-z"},"source":["## **#R34 - All issues must be uniquely identifiable, therefore there should be a unique identifier associated with each issue.**"]},{"cell_type":"code","metadata":{"id":"hDKRhfuOOwJK"},"source":["# THIS RULE CHECKS IF ALL ISSUES IN THE PROJECT, WITHIN THEIR CORRESPONDING SPRINTS\n","# HAVE THEIR UNIQUE ID-s, MAKING THEM IDENTIFYIABLE FROM OTHER ISSUES\n","\n","print(f'Total number of issues present in this Open-Source Project is: {df.key.nunique()}.')\n","print(f'Total number of missing issues present in this Open-Source Project is: {df.key.isnull().sum()}.\\n')\n","print(f'Total number of sprints present in this Open-Source Project is: {df.sprint.nunique()}.')\n","print(f'Total number of missing sprint IDs present in this Open-Source Project is: {df.sprint.isnull().sum()}.\\n')\n","\n","grouped_df = df.groupby(\"sprint\")\n","\n","print('\\n\\n*      FIRST SPRINT       *\\n\\n')\n","for key, item in grouped_df:\n","    print(grouped_df[['sprint', 'key']].get_group(key).to_markdown())\n","    if (pd.isnull(grouped_df['key'])):\n","      print('\\n\\n*        FALSEEE1        *\\n\\n')\n","      #exit()\n","    else:\n","      print('\\n\\n*        TRUE        *\\n\\n')\n","      print('\\n\\n*        NEXT SPRINT        *\\n\\n')\n","    \n","\n","r39_df = df[(df['key'].isnull()) & pd.notnull(df[\"sprint\"])]\n","\n","if (len(r39_df[['key', 'sprint']]) == 0) :\n","  print(colored.fg(\"green\") + \"\\nRule 36 passed for this Open-Source Project!\\n\")\n","else :\n","  print(colored.fg(\"red\") + \"\\nRule 36 failed for this Open-Source Project!\\n\")\n","\n","r39_df[['sprint', 'key']]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"f1rjXOrUrujp"},"source":["# if pd.isnull(df['key']):\n","#       print('\\n\\n*        FALSEEE1        *\\n\\n')\n","# else:\n","#   print(\"True\")\n","\n","def condition(df):\n","  if df['key'].notna: return \"text a\"\n","  if pd.isna(df['key']): return df['key']\n","\n","condition(df)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"h4Te_XlJtzGT"},"source":["new_df = df.loc[df['description'].str.startswith('The column titles of')].copy()\n","\n","new_df"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"T3zA8-5vjgME"},"source":["## **#R35 - Scrum Sprints have a starting time. There should be timestamp indicating the sprints kick-off.**"]},{"cell_type":"code","metadata":{"id":"SbQM6-BpjhrM"},"source":["# THIS RULE CHECKS IF SPRINTS WITHIN THE OPEN-SOURCE PROJECTS ARE DEFINITE IN TIME, \n","# MEANING THAT EACH SPRINT MUST HAVE A STARTING TIME AS WELL AS AN ENDING TIME.\n","\n","# EXPLANATION: Since not all projects have defined sprints start and end time,\n","# for other projects I am grouping the issues as per the sprint they were implemented in.\n","# After that, I am only displaying the first issue and last issue of the specific sprint.\n","# This way, we can see the start time of the first issue, i.e. the start time of the sprint,\n","# as well as the end time of the last issue from that sprint, i.e. the end time of the sprint itself.\n","\n","print(f\"Number of unique sprints (excluding the nan) in the Open-Source Project: {df.sprint.nunique()}\\n\")\n","print(df.sprint.unique())\n","print('\\n\\n')\n","#df[['sprint', 'key', 'created', 'resolutiondate']].groupby(['sprint'])\n","sprint_start_time_df = df[['sprint', 'key', 'created', 'resolutiondate' ]].groupby(['sprint'])\n","display(sprint_start_time_df.tail(1).sort_values(['sprint', 'created'], ascending=[True, True]))\n","# (pd.concat([g.tail(1), g.head(1)])\n","#    .drop_duplicates()\n","#    .sort_values(['sprint', 'created'], ascending=[True, True])\n","#    .reset_index(drop=True))\n","\n","if pd.isnull(sprint_start_time_df.resolutiondate) == True:\n","    print(colored.fg(\"red\") + \"\\nRule 40 failed for this Open-Source Project!\\n\")\n","else:\n","    print(colored.fg(\"green\") + \"\\nRule 40 passed for this Open-Source Project!\\n\")\n","\n","\n","if df['sprint'].str.contains('startDate=null').any():\n","  print(\"Yep\")\n","else:\n","  print(\"No\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EiqXd5lKvfXL"},"source":["start_dates_count = df['sprint'].str.contains('startDate=').sum()\n","if start_dates_count > 0:\n","    print (\"There are {m} sprints\".format(m=start_dates_count))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JBZkb8Q_voqs"},"source":["#df['sprint'].str.contains('startDate=')\n","\n","if df['key'].isnull().any():\n","    print(\"Yep\")\n","else:\n","  print(\"No\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"i7L14Sd7vG-T"},"source":["if df['sprint'].str.contains('startDate=').any():\n","  print(\"True\")\n","else :\n","  print(\"False\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4ELzFznopAS2"},"source":["df[['sprint', 'key', 'created', 'resolutiondate']].groupby(['sprint']).apply(display)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zI25XfAtfS_x"},"source":["## **#R36 - Scrum Sprints have a completion time. There should be timestamp indicating the sprints completion/resolution.**"]},{"cell_type":"code","metadata":{"id":"zb2Ym2HAfmAQ"},"source":["# THIS RULE CHECKS IF SPRINTS WITHIN THE OPEN-SOURCE PROJECTS ARE DEFINITE IN TIME, \n","# MEANING THAT EACH SPRINT MUST HAVE A STARTING TIME AS WELL AS AN ENDING TIME.\n","\n","# EXPLANATION: Since not all projects have defined sprints start and end time,\n","# for other projects I am grouping the issues as per the sprint they were implemented in.\n","# After that, I am only displaying the first issue and last issue of the specific sprint.\n","# This way, we can see the start time of the first issue, i.e. the start time of the sprint,\n","# as well as the end time of the last issue from that sprint, i.e. the end time of the sprint itself.\n","\n","print(f\"Number of unique sprints (excluding the nan) in the Open-Source Project: {df.sprint.nunique()}\\n\")\n","print(df.sprint.unique())\n","print('\\n\\n')\n","#df[['sprint', 'key', 'created', 'resolutiondate']].groupby(['sprint'])\n","sprint_end_time_df = df[['sprint', 'key', 'resolutiondate']].groupby(['sprint'])\n","display(sprint_end_time_df.head(1).sort_values(['sprint', 'resolutiondate'], ascending=[True, True]))\n","# (pd.concat([g.tail(1), g.head(1)])\n","#    .drop_duplicates()\n","#    .sort_values(['sprint', 'created'], ascending=[True, True])\n","#    .reset_index(drop=True))\n","if pd.isnull(sprint_end_time_df.resolutiondate):\n","  print(colored.fg(\"red\") + \"\\nRule 38 failed for this Open-Source Project!\\n\")\n","else:\n","  print(colored.fg(\"green\") + \"\\nRule 38 passed for this Open-Source Project!\\n\")\n","\n","\n","if df['sprint'].str.contains('endDate=null').any():\n","  print(\"Yep\")\n","else:\n","  print(\"No\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gz_DghfbPs1A"},"source":["## **#R37 - There should be a minimum of one issue, representing a Sprint Backlog Item, per each Sprint.**"]},{"cell_type":"code","metadata":{"id":"58PMhxvsPxDt"},"source":["# THIS RULE CHECKS IF ANY OF THE SPRINTS WITHIN THE OPEN-SOURCE PROJECTS\n","# CONTAINS NO ISSUES, i.e. NO PRODUCT BACKLOG ITEMS/SPRINT BACKLOG ITEMS \n","\n","\n","# def get_max_items_per_sprint(g):\n","#     return g['key'].value_counts().idxmax() \n","\n","print(f\"\\n The sprint with most completed issues is: {df.groupby('sprint').count().key.idxmax()}\\n\")\n","\n","\n","counts = df[['key', 'sprint']].groupby(['sprint']).describe()\n","display(counts)\n","print('\\n\\n')\n","\n","pbis_per_sprint = df.groupby(['sprint'])['key'].apply(lambda grp: list(grp.value_counts().index)).to_dict()\n","print(pbis_per_sprint)\n","\n","\n","\n","# the logic of this sprint is that the sprints' length is not 0, meaning that each sprint has some issues or PBIs within. \n","value = input(\"Check a number of PBIs that sprints might have: \")\n","\n","total_pbis_per_sprint = [len(v) for v in pbis_per_sprint.values()]\n","\n","\n","# check if value of 0 exist in dict using \"in\" & values()\n","if pd.to_numeric(value) == 0:\n","  if pd.to_numeric(value) in total_pbis_per_sprint :\n","    print(f\"\\nYes, this project contains sprints which have {value} PBIs!\")\n","    print(colored.fg(\"red\") + \"\\nRule 42 failed for this Open-Source Project!\\n\")\n","  else: \n","    print(f\"\\nNo, this project does not contain sprints with {value} PBIs!\")\n","    print(colored.fg(\"green\") + \"\\nRule 42 passed for this Open-Source Project!\\n\")\n","elif pd.to_numeric(value) != 0 and pd.to_numeric(value) in total_pbis_per_sprint:\n","  print(f\"\\nYes, this project contains sprints which have {value} PBIs!\")  \n","  #print(colored.fg(\"green\") + \"\\nRule 42 passed for this Open-Source Project!\\n\")\n","else:\n","  print(f\"\\nNo, this project does not contain sprints with '{value}' PBIs!\")  "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RCOmTuFIKbqA"},"source":["## **#R38 - There should be timestamp indicating the issue development kick-off.**"]},{"cell_type":"code","metadata":{"id":"nP-CFi-RLSPk"},"source":["import colored\n","\n","print(f\"\\nTotal number of issues in the Open-Source Project are: {df['key'].count()}.\")\n","\n","print(f\"Total number of unique issues (excluding nan) in the Open-Source Project are: {df.key.nunique()}. \\n\")\n","print('\\nThese issues are as printed below:')\n","print(df.key.unique())\n","\n","\n","# Selecting all duplicate rows based on column: key\n","duplicateRowsDF = df[df.duplicated(['key'])]\n","print(f\"\\nThere are: {len(duplicateRowsDF)} issue duplicates!\")\n","\n","# Here I am removing all issue duplicates, while only keeping the first instance!\n","print('In case of duplicate issues, the duplicates will be removed! However, this step was already performed in the data cleaning part.')\n","df = df.drop_duplicates(subset='key', keep='first')\n","\n","print(f\"\\nTotal number of issues (key column), after cleaning is: {df['key'].count()}\\n\")\n","print(f\"{df['key'].value_counts().head(10)}\\n\\n\")\n","print(f\"Total number of missing values in the *key* column in the Open-Source Project: {df.key.isnull().sum()}.\\n\")\n","print(df.isnull().sum())\n","\n","\n","print('\\n\\nFinally, we are checking if there are issues having no start date left in the dataset: \\n')\n","startdate_df = df[(df['created'].isnull()) & pd.notnull(df[\"key\"])]\n","\n","print(f\"Total number of issues having no created date is: {len(startdate_df[['key', 'created']])}.\\n\")\n","if (len(startdate_df[['key', 'created']]) == 0) :\n","  print(colored.fg(\"green\") + \"\\nRule 40 passed for this Open-Source Project!\\n\")\n","else :\n","  print(colored.fg(\"red\") + \"\\nRule 40 failed for this Open-Source Project!\\n\")\n","\n","startdate_df[['key', 'created']]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4WZbO8oULT8V"},"source":["## **#R39 - There should be timestamp indicating the issue development completion.**"]},{"cell_type":"code","metadata":{"id":"NW1aecPwLXUS"},"source":["print(f\"\\nTotal number of issues in the Open-Source Project are: {df['key'].count()}.\")\n","\n","print(f\"Total number of unique issues (excluding nan) in the Open-Source Project are: {df.key.nunique()}. \\n\")\n","print('\\nThese issues are as printed below:')\n","print(df.key.unique())\n","\n","\n","# Selecting all duplicate rows based on column: key\n","duplicateRowsDF = df[df.duplicated(['key'])]\n","print(f\"\\nThere are: {len(duplicateRowsDF)} issue duplicates!\")\n","\n","\n","# Here I am removing all issue duplicates, while only keeping the first instance!\n","print('In case of duplicate issues, the duplicates will be removed! However, this step was already performed in the data cleaning part.')\n","df = df.drop_duplicates(subset='key', keep='first')\n","\n","print(f\"\\nTotal number of issues (key column), after cleaning is: {df['key'].count()}\\n\")\n","print(f\"{df['key'].value_counts().head(10)}\\n\\n\")\n","print(f\"Total number of missing values in the *key* column in the Open-Source Project: {df.key.isnull().sum()}.\\n\")\n","print(df.isnull().sum())\n","\n","\n","print('\\n\\nFinally, we are checking if there are issues having no end date left in the dataset: \\n')\n","enddate_df = df[(df['resolutiondate'].isnull()) & pd.notnull(df[\"key\"])]\n","\n","print(f\"Total number of issues having no end date is: {len(enddate_df[['key', 'resolutiondate']])}.\\n\")\n","if (len(enddate_df[['key', 'created']]) == 0) :\n","    print(colored.fg(\"green\") + \"\\nRule 41 passed for this Open-Source Project!\\n\")\n","else :\n","  print(colored.fg(\"red\") + \"\\nRule 41 failed for this Open-Source Project!\\n\")\n","enddate_df[['key', 'resolutiondate', 'sprint']]"],"execution_count":null,"outputs":[]}]}